Title: Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey

URL Source: http://arxiv.org/pdf/2506.11102v1

Published Time: Mon, 16 Jun 2025 00:05:55 GMT

Markdown Content:
> arXiv:2506.11102v1 [cs.CL] 6 Jun 2025

JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1

# Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey 

Jiachen Zhu*, Menghui Zhu*, Renting Rui, Rong Shan, Congmin Zheng, Bo Chen, Yunjia Xi, Jianghao Lin, Weiwen Liu, Ruiming Tang, Yong Yu, Weinan Zhang 

Abstract —The advent of large language models (LLMs), such as GPT, Gemini, and DeepSeek, has significantly advanced natural language processing, giving rise to sophisticated chatbots capable of diverse language-related tasks. The transition from these traditional LLM chatbots to more advanced AI agents represents a pivotal evolutionary step. However, existing evaluation frameworks often blur the distinctions between LLM chatbots and AI agents, leading to confusion among researchers selecting appropriate benchmarks. To bridge this gap, this paper introduces a systematic analysis of current evaluation approaches, grounded in an evolutionary perspective. We provide a detailed analytical framework that clearly differentiates AI agents from LLM chatbots along five key aspects: complex environment, multi-source instructor, dynamic feedback, multi-modal perception, and advanced capability. Further, we categorize existing evaluation benchmarks based on external environments driving forces, and resulting advanced internal capabilities. For each category, we delineate relevant evaluation attributes, presented comprehensively in practical reference tables. Finally, we synthesize current trends and outline future evaluation methodologies through four critical lenses: environment, agent, evaluator, and metrics. Our findings offer actionable guidance for researchers, facilitating the informed selection and application of benchmarks in AI agent evaluation, thus fostering continued advancement in this rapidly evolving research domain. 

Index Terms —AI agent evaluation, Large Language Mod-els(LLMs), Evolutionary perspective, Evaluation taxonomy, Bench-mark Selection. 

I. INTRODUCTION 

# THe advent of Transformer [ 1] has revolutionized natural language processing (NLP) and enabled large language models (LLMs) for chatbots, such as GPT [ 2], LLaMA [ 3], Gemini [ 4], Qwen [ 5], and DeepSeek [ 6], achieving unprece-dented performance across diverse text-based tasks. These mod-els, trained on massive corpora, exhibit emergent capabilities in text generation, comprehension, and reasoning. Their ability to generalize across domains has positioned LLM chatbots as the foundation for modern AI systems, ranging from conversational interfaces to knowledge-intensive problem-solving. The emerging AI agents mark a further significant evolution beyond traditional LLM chatbots by enabling rich environmen-tal interaction and broader functionality. Unlike chatbots, which 

* Equal Contribution. J. Zhu, R. Rui, R. Shan, C. Zheng, Y. Xi, J. Lin, W. Liu, Y. Yu, and W. Zhang are with the Shanghai Jiao Tong University. E-mail:{gebro13, ruirent-ing, shanrong, despzcm, xiyunjia, chiangel, wwliu, wnzhang}@sjtu.edu.cn, yyu@apex.sjtu.edu.cn. M. Zhu, B. Chen and R. Tang are with Huawei Noah’s Ark Lab. E-mail:{zhumenghui1, chenbo116, tangruiming}@huawei.com. Weinan Zhang is the corresponding author. LLM          

> Perception
> Instructor
> Capability Environment
> Feedback
> Image Text Video Audio
> Risk Error
> Analysis Metric
> Code
> Web Scientific Planning Interactions
> Memory
> LLM Chat bo t s
> AI AGENTs
> Text
> Reply
> Text
> Human
> No Env Chatting
> Human
> Self Agents
> Fi v e As pe c t s
> Dynamic
> Feedback
> Complex
> Environment
> Multi-source
> Instructor
> Advanced
> Capability
> Multimodal
> Perception

Figure 1: Evolution between AI agents and LLM chatbots in five key aspects. only respond to human prompts in isolation, AI agents can interact with the web [ 7], invoke APIs [ 8 ], and adapt based on real-world feedback, allowing them to handle more complex tasks [ 9 ]. Essentially, the transition from LLM chatbots to fully functional AI agents is an evolutionary process. Therefore, it is imperative to first clearly delineate the advancements that have occurred during this evolution. As shown in Figure 1, LLM chatbots operate as reactive conversational engines, isolated from their surroundings and dependent solely on human input. In contrast, AI agents can be systematically delineated across five primary dimensions: complex environments, multi-source instructors, dynamic feedback, multi-modal perception and advanced capabilities. Details for the evolution from LLM chatbots to AI agents are illustrated in Section II. Such a huge evolution of AI agents necessitates new philosophical and methodological approaches to AI agent evaluation, which raises our core question in this paper: 

Given the rapid advancement of AI agents, how can we systematically evaluate LLM-based AI agents from an evolutionary perspective? 

In response to this question, numerous evaluation bench-marks for AI agents have emerged, alongside a growing number of surveys dedicated to LLM evaluation [ 10 , 11 , 12 ]. However, these works tend to focus either solely on LLM chatbots or provide only broad overviews of AI agent benchmarks without clearly defining the key distinctions between LLM chatbots and AI agents. Furthermore, they often overlook the JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2

evolutionary process from chatbot to agent, as well as the nuanced differences among evaluation benchmarks. As a result, researchers still face substantial ambiguity when selecting appropriate benchmarks to evaluate specific types of agents amidst a rapidly expanding landscape of evaluation tools. To address these gaps, we adopt an evolutionary perspective, illustrating that the development of AI agents is fundamentally driven by the interplay between external environmental driving forces and the resulting advanced internal capabilities .Accordingly, as shown in Figure 2, we systematically categorize and synthesize the landscape of existing AI agent evaluation work using the dual lenses of Environment and Capability—the two most critical dimensions in the agent evolution process. For the external environments where agents operate, we iden-tify several key categories in Section III: coding environments, web environments, operating system (OS) environments, mobile environments, scientific environments, and game environments. In terms of internal capabilities in Section IV, we identify planning, memory, self-reflection, and interaction as core competencies, and further discuss the current landscape of general capability evaluation. Moreover, for each environment and capability, we summarize a set of valuable attributes for evaluation. These attributes form the basis for detailed tables provided in Appendix A, which serve as practical references for future researchers selecting benchmarks for agent evaluation. Furthermore, Section V offers a synthesis and future outlook on the evolving trends in evaluation methodology from four distinct perspectives to address a central question facing the research community: When a new agent is developed, how should the appropriate evaluation benchmark be chosen? 

We explore this question from both a present-oriented and a forward-looking perspective, aiming to provide actionable guidance for the continued advancement of agent evaluation research. The main contributions are summarized as follows:  

> •

We propose an analytical framework to distinguish AI agents from LLM chatbots from five key aspects ( i.e. , envi-ronment, instructor, feedback, perception, and capability), systematically characterizing the evolutionary progression from simple chatbots to advanced AI agent systems.  

> •

We categorize existing evaluation benchmarks for AI agents along two critical axes: external environments and internal capabilities. For each category, detailed attribute tables and disucssions are provided to furhter benefit the research community  

> •

We analyze the evolutionary trends in evaluation bench-marks across four aspects: environment, agent, evaluator, and metric. Based on these trends, we discuss future directions and offer practical guidelines on selecting appropriate benchmarks, supporting continued progress in agent evaluation research. II. FROM LLM C HATBOTS TO AI A GENTS :PRELIMINARY 

In this section, we discuss the evolutionary progression from LLM chatbots to AI agents from five key aspects, which leads to the taxonomy on agent evaluation benchmarks in later sections. 

A. Background: Large Language Model (LLM) Chatbots 

The advent of Transformer architectures has laid the foun-dation for the current era of large language models (LLMs). These models, trained on vast and diverse textual corpora, have demonstrated remarkable performance in language un-derstanding [ 2 ], generation [ 170 ], and reasoning [ 6]. Within this landscape, the LLM chatbot paradigm has emerged as a dominant application. LLM chatbots are designed as reactive conversational systems that receive textual prompts from users, process these inputs using pretrained knowledge, and generate coherent textual responses. Typical use cases include open-domain dialogue, customer support, question answering [ 6]. Despite their versatility, traditional LLM chatbots are limited in that they operate in closed environments, devoid of active perception or real-world context. Their interactions are funda-mentally constrained to static, turn-based exchanges, lacking the means to sense or affect their operational environment. 

B. The Emergence and Definition of AI Agents 

Recent advances have shifted the focus from reactive LLM chatbots to proactive AI agents. AI agents can actively interact with the environment and make decisions by processing language, perceiving, reasoning about multi-modal inputs, and acting within dynamic contexts. Representative examples include web search agents [ 7], API-calling agents for online tasks [ 8 ], and code agents that iteratively debug based on execution feedback [ 9]. Figure 1 summarizes five key dis-tinctions—perception, instructor, capability, environment, and feedback—showcasing how AI agents advance far beyond traditional chatbots in scope and intelligence. 

Complex Environment . The most fundamental distinction lies in the environment dimension, which serves as the primary external driving force behind agent evolution. Traditional LLM chatbots are confined to closed environments, interacting solely with humans through static dialogue and lacking awareness or control over their surroundings. In contrast, AI agents operate within diverse and complex environments, e.g. , software plat-forms, scientific computing, Internet ecosystems and operating systems. This enables them to interpret dynamic contexts and take actions that affect the external world, transforming them from passive responders into proactive collaborators and task executors. 

Multi-source Instructor . AI agents also advance the in-structor dimension. Unlike LLM chatbots that depend heavily on human prompts, agents integrate instructions from multiple sources, including self-reflection, collaboration with other agents, and hierarchical commands in multi-agent systems. This multi-source guidance empowers agents to make complex decisions, self-correct, resulting in higher autonomy and robustness. 

Dynamic Feedback . While LLM chatbots primarily re-ceive feedback through conversation or user correction, AI agents operate in environments with continuous, multifaceted feedback—including metric-based analyses, risk assessments, explicit error signals, and environment-derived rewards or penalties. This rich feedback ecosystem enables ongoing adaptation, self-improvement, and long-term optimization. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3

Agent Evaluation 

Environment Taxonomy (§III) 

Code-related Environments (§ III-A )

HumanEval [ 13 ]; Classeval [ 14 ]; CoderEval [ 15 ]; Yuan et al. [ 16 ]; CodeCriticBench [ 17 ]; CRUXEval [ 18 ]; BigCodeBench [ 19 ]; ARCADE [ 20 ]; SWE-bench [ 21 ]; SWE-Agent [ 22 ]; SWE-Search [ 23 ]; SWEbench-java [ 24 ]; SWE-Arena [ 25 ]; SWEbench-Lite [ 26 ]; SWEbench+ [ 27 ]; RepoBench [ 28 ]; SWT-Bench [ 29 ]; ChatDev [ 30 ]; DevEval [ 31 ]; ML-Bench [ 32 ]; PyBench [ 33 ]

Web Environments (§ III-B )

MiniWob [ 34 ]; FormWob [ 34 ]; QAWob [ 34 ]; MiniWob++ [ 35 ]; WebShop [ 36 ]; Mind2Web [ 37 ]; Webvoyage [ 38 ]; WebLinx [ 39 ]; WebArena [ 40 ]; VisualWebArena [ 41 ]; WorkArena [ 42 ]; WorkArena++ [ 43 ]; MMInA [ 44 ]; AssistantBench [ 45 ]; WebCanvas [ 46 ]; ST-WebAgentBench [ 47 ]; VideoWebArena [ 48 ]; TUR[K]INGBENCH [ 49 ]; BEARCUBS [ 50 ]; TheAgentCompany [ 51 ]; WABER [ 52 ]; VisualAgentBench [ 53 ]; Online-Mind2Web [ 54 ]; REAL [ 55 ]; OmniAct [ 56 ]; Chatshop [ 57 ]

OS Environments (§ III-C ) OSWorld [ 58 ]; WindowsAgentArena [ 59 ]; AgentStu-dio [ 60 ]; OmniACT [ 61 ]; OfficeBench [ 62 ]; PC-Eval [ 63 ]

Mobile Enviorn-ments (§ III-D )

PixelHelp [ 64 ]; UGIF [ 65 ]; MoTIF [ 66 ]; AITW [ 67 ]; ANDROIDCONTROL [ 68 ]; AMEX [ 69 ]; B-MOCA [ 70 ]; Androidworld [ 71 ]; Mobile-bench [ 72 ]; A3 [ 73 ]; Mobile-Env [ 74 ]; AndroidArena [ 75 ]; MobileSafetyBench [ 76 ]; MobileAgent-Bench [ 77 ]; SPA-BENCH [ 78 ]; Llamatouch [ 79 ]; Androidlab [ 80 ]; AutoEval [ 81 ]

Scientific Enviornments (§ III-E )

ARC [ 82 ]; SCIENCEQA [ 83 ]; QASPER [ 84 ]; QASA [ 85 ]; SWIFT [ 86 ]; AAAR-1.0 [ 87 ]; M S 2 [88 ]; LAB-Bench [ 89 ]; ResearchArena [ 90 ]; Si et al. [91] ; Pa-perQA [ 92 ]; SCIENCEWORLD [ 93 ]; DISCOVERYWORLD [ 94 ]; ScienceAgent-Bench [ 95 ]; MLGym [ 96 ]; MLAgentBench [ 97 ]; DSBench [ 98 ]; DSEval [ 99 ]; DA-Code [ 100 ]; ML-Bench [ 32 ]; Spider2-V [ 101 ]; LTMbenchmark [ 102 ]

Game Enviorn-ments (§ III-F )

BALROG [ 103 ]; SmartPlay [ 104 ]; LVLM-Playground [ 105 ]; VGRP-Bench [ 106 ]; ING-VP [ 107 ]; DSGBench [ 108 ]; Gamebench [ 109 ]; γ-Bench [ 110 ]; Gtbench [ 111 ]; GameArena [ 112 ]

Agent Capabilities Evaluation (§IV) 

Planning (§ IV-A )

PlanBench [ 113 ]; On the Planning Abilities of LLMs [ 114 ]; AutoPlanBench [ 115 ] ; ACPBench [ 116 ]; WorFBench [ 117 ] ; FlowBench [ 118 ] ;REALM-Bench [ 119 ] ; ET-Plan-Bench [ 120 ]

Self-reflection (§ IV-B )

LLF-Bench [ 121 ]; LLM-Evolve [ 122 ]; Reflection-Bench [ 123 ]; "When Benchmarks Talk" [ 124 ]

Interaction (§ IV-C )

ToolAlpaca [ 125 ]; APIBench [ 126 ]; ToolBench [ 127 ]; BFCL [ 128 ]; ToolSandBox [ 129 ]; Seal-Tools [ 130 ]; API-Bank [ 131 ]; NexusRaven [ 132 ]; API-Blend [ 133 ]; RestBench [ 134 ]; APIGen [ 135 ]; StableToolBench [ 136 ]; ComplexFuncBench [ 137 ]; NESTFUL [ 138 ]; ToolEyes [ 139 ]; PEToolBench [ 140 ]

τ -bench [ 141 ]; ABCD [ 142 ]; MultiWoZ [ 143 ]; ALMITA [ 144 ]; IntellAgent [ 145 ]

BattleAgentBench [ 146 ]; MultiAgentBench [ 147 ]; Collab-Overcooked [ 148 ]; So-topia [ 149 ]; AutoArena [ 150 ]; SmartEval [ 151 ]; MindAgent [ 152 ]; GOVSIM [ 153 ]

Memory (§ IV-D )

NarrativeQA [ 154 ]; QMSum [ 155 ]; QuALITY [ 156 ]; DialSim [ 157 ]; LoCoMo [ 158 ]; LTM-Benchmark [ 102 ]; LongMemEval [ 159 ]; Episodic Memory[ 160 ]; PerLTQA [ 161 ]; StreamBench [ 162 ]; 

General (§ IV-E ) AgentBench [ 163 ]; GAIA [ 164 ]; MMAU [ 165 ]; Galileo-Agent-LeaderBoard [ 166 ]; HAL [ 167 ]; Agent-Arena [ 168 ]; CapaBench [ 169 ]

Discussion (§V) 

Env Perspective (§ V-A ) From Single Modality to Multi Modality; From Static to Evovable; From Stateless to Stateful 

Agent Perspective (§ V-B ) From Single-Agent to Multi-Agent; From Single-Turn to Multi-Turn 

Evaluator Perspective (§ V-C ) From Human-Judge to Agent-Judge; From General to Personalized 

Metric Perspective (§ V-D ) From Coarse-Grained to Fine-Grained Label; From Effective-ness to Efficiency; From Accuracy-Oriented to Social-Good 

Benchmark Selection Methodology (§ V-E ) Present-focused Selection; Future-oriented Selection 

Figure 2: The overall structure of this paper. 

Multimodal Perception . To function in real-world settings and respond to complex instructions, AI agents are equipped with multimodal sensing—processing not just text, but also visual, auditory, and even tactile or environmental sensor data. The development of Multimodal Large Language Models (MLLMs) exemplifies this leap, allowing agents to understand and reason across diverse modalities and vastly expanding their intelligence and applicability. 

Advanced Capability . Advancements across environment, instructor, feedback, and perception dimensions collectively drive the evolution of agents’ internal capabilities. Dynamic environments, richer instructions and feedback, and enhanced perception propel AI agents far beyond basic conversations. Agents now exhibit complex planning, persistent memory, JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4

adaptive reasoning, and autonomous task execution. This marks a transformative step in intelligent systems, demonstrating how external demands and internal advances together drive the shift from reactive LLM chatbots to autonomous AI agents. The progression from LLM chatbot to AI agent is primarily catalyzed by two forces: external environment complexity and the corresponding development of internal capabilities .External driving forces stimulate internal growth. As agents are deployed in richer, more demanding environments, they must evolve to incorporate sophisticated perception, planning, and adaptation mechanisms. This external-internal interplay underlies the rapid advancement of modern AI agents. In the next sections, as shown in Figure 2, we categorize and discuss agent evaluation from both external environment and internal capability perspectives. III. EVALUATION FOR DIFFERENT ENVIRONMENTS 

The environment, as an abstraction and simplification of the world, plays a pivotal role in the evolution from LLM chatbots to AI agents. It is an external system that the agent interacts directly with. Agents are equipped with different types of environments tailored to task categories and complexity levels, enabling capabilities such as self-reflection and multi-step planning. In this section, a systematic categorization and analysis of the environments employed in various agent evaluations is presented. The objective is to delineate their developmental trajectory and discern the similarities and distinctions among different environments. 

A. Coding Environments 

In the context of general-purpose agents, codes represent a pivotal means of interactive engagement with the external environment and exerting influence on it. Consequently, the coding abilities are of paramount importance for agents. The common environments for code agents include referenced code files, code repositories, code executors and so on, which are determined by specific tasks. It is worthy of mentioning that a number of the earlier works are not related to code environments, in which case the benchmarks are only evaluated on simple tasks and the agents involved are more akin to code LLMs. We will only briefly list these benchmarks and describe the simple tasks on which they were evaluated [ 13 , 14 , 15 , 16 , 17 , 18 , 19 , 13 ]. As a pioneer of this area, HumanEval [ 13 ] tests the LLMs by asking them to synthesise programs from docstrings. Likewise, CoderEval [ 15 ]enhances HumanEval mainly on the complexity of the tasks, especially on the cyclomatic complexity and lines of codes. Similarly, CodeCriticBench [ 17 ] assesses LLMs’ aptitude for code critique, with a particular emphasis on code generation and quality assurance (QA) tasks. BigCodeBench [ 19 ] enhances HumanEval [ 13 ] and offers more challenging and practical tasks, which has higher requirements for LLMs like utilizing diverse function calls as tools. ARCADE [ 20 ] is also an evaluation on code generation, but it pays attention to interactive notebooks in data science. As code-related tasks become more complex and compre-hensive, it is no longer enough for agents to take in only input from the instructors. They need to operate on more objects and performs skills such as self-reflection, where the environment provides the basis for the agents. To better understand the environments in code agents, here we take the representative benchmark on code agents SWE-bench [ 21 ] as the example. In SWE-bench, agents are prompted to understand the real issues of GitHub and generate the code patches to resolve the issues. During inference, related codes and demonstrations are inserted into the prompts beforehand and LLMs directly generate the patches in text, where the models behave more like semi-agents. The environment now only includes the codebases and the automated unit tests. In original SWE-bench paper, the best-performing model, Claude 2, though equipped with a simple retrieval mechanism, could only solve 1.96% of the issues. After that, in order to archieve better performances on these tasks, a series of agent frameworks are proposed to challenge the tasks on SWE-bench such as SWE-Agent [ 22 ] and SWE-Search [ 23 ]. To support the agents’ abilities such as memory and self-reflection, an external termi-nal is added into the environment, through which the agents could perform some higher-level operations like file editing and searching. Except SWE-bench, a series of similar benchmarks are proposed and their environments differs from that in SWE-bench mainly on the modal [ 22 ], programming language [ 24 ], agents’ comparison [ 25 ], execution-free evaluation [ 26 ] and data quality [27]. In addition, in the field of code agents, there are various benchmarks with different environments towards different specific code-related tasks. RepoBench [ 28 ] evaluates on the tasks of code retrieval and code completion and provides the agents with the code repositories as the environment. Similarly, for the task of code completion at the repository level, the environment in RepoEval [ 171 ] provides the capability of retrieval. This is the basis of the iterative retrieval-generation mechanism in the paper. SWT-Bench [ 29 ] is based on SWE-bench and adopts similar environments to benchmark agents on the generation of test cases according to the issues. Based on ChatDev [ 30 ], DevEval’s [ 31 ] environments allow to evaluate agent on the full stages of software development, including software design, environment setup, implementation and testing. ML-Bench [ 32 ] offers a ML-related repository and a Linux sandbox and ask the agents to perform machine learning tasks. Pybench [ 33 ] introduces a Python code interpreter to the environment. The agents in the benchmark operate on the different type of files in the environment to complete some real-world tasks like chart analysis, text analysis, image & audio editing and so on. 

B. Web Environments 

Web agents, autonomous systems that navigate, interact with, and extract information from web pages, have emerged as a powerful paradigm for tasks ranging from form filling and information retrieval to complex multi -step workflows on enterprise platforms. Evaluating their performance is crucial for monitoring progress and identifying remaining challenges. Over the past few years numerous benchmarks have been proposed; we summarize them in Table II and organize our discussion by the realism of the environment. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5

Early benchmarks use fully synthetic web pages, designed to stress reinforcement -learning agents in a controlled setting. Two pioneering works are MiniWoB [ 34 ] and MiniWoB++ [ 35 ]. MiniWoB provides a suite of simple tasks (e.g., clicking buttons, filling out toy mail and calendar forms) rendered in canvas elements. Its synthetic nature allows reproducible offline RL training and evaluation. MiniWoB++ extends MiniWoB with longer sequences, random layouts, and soft -text reasoning tasks (e.g., checkbox grids, multi -layout navigation, simple social - media interactions), further challenging sequence mod-eling and exploration. These early synthetic suites established the importance of standardized evaluation for web agents and paved the way for more realistic benchmarks. As language - model -based agents matured, purely synthetic data proved insufficient. Semi -real benchmarks load snapshots of real websites and host simplified or anonymous modi-fications on data replicas to afford reproducible evaluation on genuine layouts and data. FormWoB [ 34 ] offers form-filling tasks to assess agents’ abilities in handling structured inputs. QAWoB [ 34 ] designs question-answering tasks to test agents’ information retrieval and comprehension skills. WebShop [ 36 ] simulates online shopping scenarios, challeng-ing agents in multi-step decision-making and understanding user preferences. ChatShop [ 57 ] combines conversational systems with shopping tasks, evaluating agents’ consistency and task completion in multi-turn dialogues. WebArena [ 40 ]constructs simulated environments based on real websites, encompassing various web interaction tasks to test agents’ generalization capabilities. Its variants VisualWebArena [ 41 ]and VideoWebArena [ 48 ] incorporates visual information and video tutorials, respectively, challenging agents’ performance in processing mixed text, image content and long-context videos. WABER [ 52 ] provides a multilingual, multi-task evaluation environment to test agents’ cross-lingual capabilities and task adaptability. REAL [ 55 ] recreates eleven high - fidelity website simulations with deterministic snapshots and combines script - based state validation with an LLM -based rubric for open - ended retrieval tasks. ST -WebAgentBench [ 47 ] offers a framework for evaluating agents in multi-turn dialogues and complex tasks, testing long-term planning and context under-standing. TUR[K]INGBENCH [ 49 ] uses original Mechanical Turk HTML pages, evaluating agents on naturally crowdsourced web tasks with metrics like ROUGE - L and IoU, underscoring diversity of real -world page designs. TheAgentCompany [ 51 ]simulates a small software company environment, focusing on agents’ performance in real-world professional scenarios. In contrast to the aforementioned semi-realistic benchmarks, a growing number of fully dynamic and real-world benchmarks have recently been proposed. These benchmarks either encap-sulate real environments into snapshot-based simulations or directly connect to the live Internet for task execution. Such evaluation paradigms better reflect real-world complexities and are more effective at assessing agents’ actual performance and robustness in dynamic conditions. Mind2Web [ 37 ] requires agents to understand rich page structures and plan multi-step goals. Online-Mind2Web [ 54 ] further extends this by introducing challenges related to dynamic content changes and real-time DOM variations. WebVoyager [ 38 ] emphasizes effective intermediate state modeling and reasoning over long-term dependencies between distant pages. WebLINX [ 39 ]focuses on reasoning through numerous branching paths and integrating historical click sequences to reach target information. WorkArena [ 42 ] and WorkArena++ [ 43 ] assess agents’ long-context understanding, reflecting more realistic workflows en-countered by office or workers in real enterprises. MMInA [ 44 ]introduces challenges in multimodal and multi-hop web tasks. AssistantBench [ 45 ] mainly tests agents’ ability to follow time-consuming and multi-turn workflows. WebCanvas [ 46 ]poses challenges in visual parsing and UI element grounding under dynamic layouts by examining intermediate nodes. BEARCUBS [ 50 ] centers on knowledge-intensive tasks that require agents retrieve and utilize webpage content and external knowledge sources. 

C. OS Environments 

Despite the web environments, there are some benchmarks that are based on the desktop, which is more complex and challenging. OSWorld [ 58 ] serves as a representative desktop benchmark, demonstrating the optimal characteristics of a desktop environment. It employs virtual machines (VMs) to facilitate a desktop environment that is executable and controllable. The agents could interact with the environment just like operating the computer in reality. The environments are initialized by config files, which describes some operations like open a file after starting the virtual machine. During the interaction process with the environment, the observation of agents encompasses the capture of screenshots of the desktop and the accessibility (a11y) tree. The action space defined in OSWorld is diverse and it supports all mouse and keyboard actions, including movement, clicks and so on. There are totally 369 evaluated tasks in OSWorld, including Office tasks, OS tasks, Daily tasks, Workflow tasks and Professional tasks. Furthermore, there are several distinct benchmarks that emphasize disparate fine-grained scenarios. Different from OSWorld, whose tasks are mainly on Ubuntu, Win-dowsAgentArena [ 59 ] offers a general environment focusing on the Windows operating system. AgentStudio [ 60 ] constitutes a triad of virtual agent research environments, tools, and benchmarks. AgentStudio’s environment is conducive to the observation of text, image, and video formats. Its action space supports both GUI operations and API calls. OmniACT [ 61 ]provides a static dataset for the evaluation of generalist autonomous agents. This dataset contains tasks in natural lan-guage, screenshots, and PyAutoGUI scripts that serve as ground truth. The environment at OfficeBench [ 62 ] is characterized by its concentration on office tasks, which distinguishes it from the general desktop agent evaluation environments, such as OSWorld. The action space in OfficeBench is restricted and depends on the applications that agents are currently utilizing. For instance, the action of run_command could be operated in Shell but invalid in other applications like Word. In OfficeBench, agents could observe both the current and the previous states and actions, which is a departure from the settings in previous benchmarks. Although analogous to OSWorld, PC-Eval [ 63 ] augments it with more practical and challenging tasks, which are checked by human annotators. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6

D. Mobile Environments 

The advent of mobile Internet has led to the gradual integration of mobile devices, such as smartphones and tablet computers, into our daily life. These devices have become increasingly indispensable, playing a crucial role in various aspects of our lives. However, the increasing functionality of mobile devices poses challenges to their usability. Many mobile agents such as Siri, Bixby and XiaoAi have been proposed to relieve the operation issues. Nevertheless, in general, there is still significant room for improvement in task completion capabilities, personalization, and other aspects for these agents. Therefore, many benchmarks are proposed to provide the testbeds and promote the development of mobile agents. The environments of earlier benchmarks are usually static, or in other way to say, are just fixed datasets to evaluate LLM chatbots. Early works such as PixelHelp [ 64 ], UGIF [ 65 ], Mo-TIF [ 66 ] and AITW [ 67 ] offer the LLMs with the screenshots and tree-based representations such as View Hierarchy (VH) and regard the demonstrations from the humans as ground truths to evaluate the agents. Among these benchmarks, the process of evaluation is static and does not need the involvement of executable environments, where the LLMs could not interact with the environments in multiple rounds. Besides the ones mentioned above, ANDROIDCONTROL [ 68 ] mainly improves on the granularity of the instructions and the freshness of the APPs. AMEX [ 69 ] further offers the screen descriptions and screen elements for the agents. However, when the environments are asked to evaluate AI agents rather than LLM chatbots, things get a little different. The complexity of the tasks inherently determines the requirements of multi-step execution. In addition, the agents usually need multi-round interactions to handle their inner mechanisms such as planning and self-reflection. Also, there are usually multiple trajectories to complete the tasks, which makes static datasets even less suitable for evaluating mobile agents. In the era of agents, the environments typically include an emulator and an adapter. The emulator, which is the core of the environment, behaves like a real mobile device and responds to the actions of the agents. For example, if the agents give the action of tapping and dragging up, the emulator will scroll down the screen. The adapter, on the other hand, acts as a bridge between the emulator and the agents. When the agents perform actions, the adapter translates the agents’ instructions into shells and passes them to the emulator through tools such as ADB. Similarly, it gets the states of the environment such as the screenshot and XML from the emulator and converts them into the format that the agents can understand. With the above background knowledge, we now proceed to a detailed characterization of existing evaluations. B-MOCA [ 70 ]is an earlier benchmark with dynamic environments and includes a simple range of daily tasks like web tasks, shopping tasks, system tasks and so on. Androidworld [ 71 ] expands the scope of APPs. It’s equipped with the ability to dynamically construct the initial states of tasks and vary the task parameters in unlimited ways. Mobile-bench [ 72 ] extends the agent’s action space with API operations, allowing the agent to perform tasks such as opening a specific APP without screen operations. Additionally, to evaluate the agents more accurately, it proposes checkpoints to evaluate the agents on whether reach essential points during the planning and reasoning steps. A3 [ 73 ] focuses on the daily used APPs and offers a more flexible action space for the agents, which supports additional actions like Long Press. The environments in Mobile-Env [ 74 ] supports intermediate rewards and intermediate instructions for the agents. AndroidArena [ 75 ] pays more attention to the cross-APP tasks and constrained tasks. The environment of cross-APP tasks involves multiple APPs and the agents require cooperation between multiple APPs. Besides for the regular instructions, the environment should also offer constraints such as "do not click the payment button" . MobileSafetyBench [ 76 ] develops a set of safety-related tasks to evaluate the mobile agents, such as messaging and banking applications. In addition to the aforementioned benchmarks, there are works that mainly improve the evaluation methods. This is also a key point of the environments. MobileAgentBench [ 77 ]utilizes Android Accessibility Service to capture app events and uses them to check whether the task is completed. SPA-BENCH [ 78 ] proposes a coarse-to-fine success detection pipeline to judge if an agent succeeds. It first applies coarse de-tection with Optical Character Recognition (OCR) and matches the trajectories with the key components. After that, it deploys a trained multimodal LLM for final success determination. Llamatouch [ 79 ] automatically evaluates agents by whether the agent traverses all manually annotated application / system states. Similarly, in Androidlab [ 80 ], each task is divided into multiple page states as sub-goals. A task is considered complete only if all sub-goals are correctly addressed. AutoEval [ 81 ]adopts an automatic judge system to evalute the agents, which composed of three components: Capturer, Reasoner and Checker. The Capturer generates descriptions about the screenshots and the Reasoner judge the agent’s performance according to the screen and task descriptions. Finally, the Checker is added to guarantee the Reasoner’s output is acceptable and consistent with the reasoning process. 

E. Scientific Environments 

The advancements of AI agents have sparked interest in building the entire automated process of scientific discovery or assist researchers in different researching stages. Early benchmarks mainly assess the scientific reasoning and knowl-edge retrieval capabilities of LLM chatbots. For example, ARC [ 82 ] and SCIENCEQA [ 83 ] evaluate LLM chatbots on scientific multi-choice question answering. QASPER [ 84 ] and QASA [ 85 ] focus on information seeking and answer anchoring in research papers. SWIFT [ 86 ] measures the research paper comment generation and weakness identification abilities. AAAR-1.0 [ 87 ] evaluates LLM chatbots on research tasks of equation inference, experiment design, paper weakness, and review critique. M S 2 [88 ] emphasizes multi-document summarization of medical studies, while LAB-Bench [ 89 ]measures a broad range of tasks for scientific research, e.g. 

analysis of tables and figures. These benchmarks are typically conducted in an offline static manner, and there is no clear involvement of an external environment. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7

Instead of a straightforward evaluation, there are also benchmarks involving a scientific article pool as an external environment, which is utilized for retrieval and assists in different research stages, including academic surveying, idea generation, experiment design, writing assistance and so on. For example, ResearchArena [ 90 ] benchmarks agents’ ability to collect surveys and organize information. Si et al. [91] compares the ideas generated by research agents with expert NLP researchers, which are based on paper retrieval for RAG. PaperQA [ 92 ] performs information retrieval across full-text scientific articles for question answering. These agents harness extracted knowledge from the environment, but lack explicit interaction. Since developing and evaluating an agent’s capacity for real-world scientific discovery is often expensive and challeng-ing, some benchmarks construct a simulated, interactive and text-based environment to evaluate the agents’ capabilities. Representatives of them are SCIENCEWORLD [ 93 ] and DISCOVERYWORLD [ 94 ]. SCIENCEWORLD tests agents’ scientific reasoning capabilities in a text environment with a simulation for thermodynamics, electrical circuits, chemical reactions, and biological processes. DISCOVERYWORLD benchmarks agents’ ability to perform complete cycles of scientific discovery. As the applications of scientific agents become more complex, the demands on their capabilities continue to grow, and the environments in which they interact are also becoming more complex and realistic. Specifically, in order to automate the complete cycle of scientific discovery, agents may need to interact with the environment by reading in-memory data, handling file permission, editing files, executing scripts, and collecting results. In this case, the environment is complex, which can be composed of file systems, code interpreter, shell, database and so on. We refer to this compound environment as 

workspace , which is typically a jupyter notebook or a virtual machine in many cases. ScienceAgentBench [ 95 ] evaluates agents in an environment equipped with bash shell, web browser, code interpreter on coding for workflow of scientific discovery. MLGym [ 96 ] designs a gym-like environment where agents can manipulate with shell, tools, python dependencies and permission for various files. MLAgentBench [ 97 ], DS-Bench [ 98 ] and MLE-bench [ 172 ] mainly focus on Kaggle challenges and completions, assessing agents’ end-to-end machine learning engineering capabilities. The workspace allows file editing and code running by these research agents. DSEval [ 99 ] also assesses the performance of agents throughout the entire data science lifecycle, with a runtime session ( i.e. 

jupyter notebook) composed of in-memory data, external files, execution history and code executor. DA-Code [ 100 ] and ML-Bench [32] evaluate agents in a Docker on data science code generation and repository-level code manipulation respectively. Spider2-V [ 101 ] focuses on multimodal agents for data science automation, which also relies on a Docker environment for file transfer, application launch, remote API calls, script execution and playright automation. 

F. Game Environments 

Prior to the advent of large language models, game envi-ronments were extensively utilized as testbeds for AI agents. In contrast to agents such as code agents and web agents, the value in application of agents in game environments is relatively low. In general, benchmarks with game environments do not evaluate agents on specific tasks; rather, they aim to evaluate general capabilities such as planning and reasoning. In this section, we primarily introduce several game environments that have been utilized in existed benchmarks for agents. One branch of game environments for agent evaluations involve only one agent and focus on the absolute capabilities of the agents. BALROG [ 103 ] evaluates agents on common games like BabyAI [ 173 ], Crafter [ 174 ], TextWorld [ 175 ] and so on. For VLMs (Vision Language Model), the environments provide directly the screenshots of the games. While for LLMs, all of the observations in games are transformed into texts for the agents to understand. SmartPlay [ 104 ] provides a compre-hensive evaluation of the agent’s capabilities in various domains, including long text understanding, reasoning, instruction/rule following, and so on. The evaluation process encompasses a diverse array of both visual and text-based games, including Bandits, Rock-Paper-Scissors, Tower of Hanoi, Minecraft, Messenger, and Crafter. In certain environments, internal rewards are utilized as the evaluation metric, while in others, completion rates are employed to assess the performance of the agents. In a similar manner, LVLM-Playground [ 105 ] adopts the games including TicTacToe, Reversi, Sudoku to evaluate the agents’ capabilities on perception, reasoning, decision and adversary. The evaluation metrics are multifaceted and differ depending on the specific task. For instance, for perceiving task, which asks the agents to convert a visual game state into a structured matrix representation, and the accuracy is utilized as the metric. In contrast to the aforementioned work, VGRP-Bench [ 106 ] focuses on puzzle games and employs them as a means to assess the agents. ING-VP [ 107 ] focuses evaluation of multi-step planning based on spatial relationships in images. The environments in ING-VP include six representative games: Sokoban, Maze, Sudoku, 8-queens, Tower of Hanoi, and 15-puzzle. In the evaluation of agents, three metrics are considered: accuracy, completion degree, and action efficiency. DSGBench [ 108 ] provides a rigorous evaluation platform and includes six complex strategic games like StarCraft II, Civilization and so on to evaluate the agents on decision making. In all of the environments, the original observations—such as screenshots—are transformed into text prompts for the agents. For each game, there exist numerous human-designed metrics that are employed to assess the performance of agents. For instance, in StarCraft II, the following metrics are adopted: RPM (Resource Per Minute), EER (Efficiency of Resource Utilization), SUR (Supply Usage Rate), TCR (Technology Completion Rate), APM (Actions Per Minute), EPM (Ecffective Actions Per Minute), WR (Win Rate) and GA (Grounding Accuracy). Another branches of benchmarks try to compare the agents and their environments usually support multiple agents. Gamebench [ 109 ] evaluates strategic reasoning abilities of AI JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8

agents, where nine multi-player board games are served as the testbeds, including Air, Land, Sea (ALS), Arctic Scavengers (ARC) and so on. The Bradley-Terry system [ 176 ] is employed to calculate the scores of the agents in order to make a comparison of their relative abilities. In a similar vein, the 

γ-Bench [ 110 ] and Gtbench [ 111 ] assess the performance of agents in a manner analogous to that of competing models, albeit within the context of game-theoretic environments. GameArena [ 112 ] is distinguished from prior multi-agent game environments in that it incorporates human participation and provides human feedback during the evaluation process. For instance, in the AI Akinator game of GameArena, the agents determine what object the user is thinking of. During the game, the user responds to the agents’ questions with a "yes" or "no" response. IV. EVALUATION ON AGENT CAPABILITIES 

Evaluating AI agents necessitates a nuanced understanding of their advanced internal capabilities, which are fundamental to their performance across various tasks. This section categorizes existing evaluation methods based on these intrinsic abilities. 

A. Planning 

As the complexity and diversity of environments confronted by AI agents increase, single-step agents designed for isolated question answering fail to tackle tasks requiring multi-step reasoning, such as solving advanced mathematical problems or navigating across multiple documents. To address these limitations, researchers have introduced methods to stimulate multi-step reasoning in AI agents, including chain-of-thought prompting (CoT) [ 177 ] and tree-of-thoughts (ToT) [ 178 ]strategies. The simplest approach to assess multi-step reasoning is through static datasets spanning diverse domains. In mathemat-ical reasoning, AQUA-RAT [ 179 ] evaluates algebraic planning with generated rationales, GSM8K [ 180 ] and MATH [ 181 ]cover elementary and secondary school problem solving, Game of 24 [ 182 ] measures arithmetic CoT planning, and SVAMP [ 183 ] introduces perturbations such as operand swaps and paraphrases to increase difficulty on Math Word Problems. Document-navigation benchmarks like HotpotQA [ 184 ] and StrategyQA [ 185 ] require multi-hop navigations across multiple passages. ScienceQA [83 ] extends the standard multiple-choice science questions with multimodal information(e.g., charts and images), whereas ARC [ 82 ] consists of text-based multiple-choice science questions. Logic-inference workflows are evaluated by FOLIO [ 186 ] and P-FOLIO [ 187 ], which treat logical inference chains as planning workflows. Beyond implicit reasoning, explicit planning evaluation em-ploys the Planning Domain Definition Language (PDDL) from classical AI planning. PlanBench leverages canonical domains such as Blocksworld and Logistics to test logical, causal, and spatial planning across eight subtasks [ 113 ]. Valmeekam et al. [114] provide a critical investigation showing agent planning success is limited but improves when used as heuristic guidance for external planners. AutoPlanBench [ 115 ] automates the conversion of PDDL domains into natural language prompts, enabling large-scale evaluation of agent planning methods and demonstrating that automatically generated prompts match or exceed manually crafted ones. Bohnet et al. [188] introduces a benchmark suite covering both classical and natural-language planning tasks, investigating many-shot in-context learning and fine-tuning strategies to enhance planning performance. ACPBench [ 116 ] further decomposes planning into seven atomic reasoning tasks, such as action applicability, progression, reachability, and landmark detection, across thirteen domains to probe constraint-based planning abilities. While traditional benchmarks focus on abstract or virtual tasks, workflow-based benchmarks evaluate an agent’s abil-ity to decompose real-world tasks into executable subtasks. NATURALPLAN [ 189 ] introduces realistic planning tasks like trip planning, meeting scheduling, and calendar management by providing tool outputs as context, eliminating the need for external execution environments. FlowBench [ 118 ] revisits workflow-guided planning by formalizing multiple formats of workflow knowledge and covering 51 scenarios across six domains, revealing that current agents struggle with workflow hallucinations and require significant improvement. WorFBench [ 117 ] offers a unified benchmark for workflow generation with complex graph structures and a systematic evaluation protocol utilizing subsequence and subgraph match-ing, highlighting distinct gaps between sequence and graph planning capabilities. Finally, to incorporate environment feedback and iterative instruction, benchmarks such as MINT [ 190 ] and REALM-Bench [ 119 ] assess agents in interactive, online settings. MINT evaluates multi-turn interaction capabilities by combining tool use and natural language feedback, showing that feedback can substantially improve performance but also revealing that instruction fine-tuning may hurt multi-turn reasoning. REALM-Bench provides an online simulator with eleven real-world planning scenarios, such as supply chain and disaster response, that can be scaled in parallelism, dependency complexity, and disturbance frequency, enabling rigorous testing of both single-agent and multi-agent planning under dynamic conditions. 

B. Self-reflection 

As LLM chatbots evolve into more autonomous AI agents, the ability to self-reflect and improve through interac-tion—rather than relying solely on human supervision—has become increasingly critical. A growing body of research explores whether agents can engage in self-reflection through interactive feedback and subsequently refine their reasoning to reduce errors in multi-step tasks. This process demands not only an understanding of feedback but also the dynamic updating of internal beliefs, allowing agents to adaptively revise actions or reasoning paths over extended interaction trajectories. Early evaluations of agent self-reflection are often simplistic and limited. These studies typically employ static environments with repetitive access and measure improvement only by checking whether the final answer is corrected after multiple attempts [ 191 , 192 ]. However, such evaluations tend to be coarse and lack the capacity to generalize to new data or real-world feedback scenarios. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9

Furthermore, nascent benchmarks primarily focused on inter-nal self-reflection, providing minimal environmental feedback and relying heavily on prompts such as ’Review your previous answer and find problems with your answer’ [ 193 , 191 ]. This internal-only mode of reflection limits the performance ceiling of agents, as relying solely on intrinsic reasoning cannot fully replace the adaptive learning signal provided by interactive rewards from the environment. Therefore, there is a pressing need for evaluation protocols that assess an agent’s ability to perform self-reflection grounded in external feedback signals. To address this gap, LLF-Bench [ 121 ] was proposed as a standardized benchmark for interactive self-reflection. LLF-Bench constructs domain-specific language feedback across multiple tasks, enabling a systematic evaluation of AI agents’ ability to learn from external linguistic corrections. The benchmark simulates diverse feedback scenarios and tests whether AI agents can adapt and improve based on natural language guidance. Similarly, LLM-Evolve [ 122 ] targets the self-reflection ability of AI agents on standard benchmarks such as MMLU [ 194 ]. In this framework, agents are exposed to dynamic feedback and allowed to retrieve few-shot examples for reflection and correction. The benchmark measures whether the integration of feedback leads to meaningful performance improvements across tasks. Reflection-Bench [ 123 ] introduces novel evaluation metrics from a cognitive perspective to assess the quality, coherence, and logical consistency of reflective outputs. It explicitly examines whether a model can articulate the cause of its errors, propose plausible fixes, and ultimately achieve better decision-making as a result of reflection. Finally, Pan et al. [124] focuses on the domain of code generation, where self-reflection is evaluated in code environments based on human-provided feedback. Instead of only assessing whether a model generates correct code on the first try, the benchmark emphasizes the ability to revise and improve through iterative reflection, simulating real-world debugging and correction scenarios on three existing benchmarks APPS [ 195 ], LiveCodeBench [ 196 ], and ClassEval [14]. 

C. Interaction 

To systematically evaluate the interaction capabilities of AI agents, we propose a three-level taxonomy of agent-environment interaction, each representing a progressively more complex and autonomous behavior. 

1) Interaction with Static Systems : First, we consider interaction with static systems, where the agent interacts with predefined tools or user interfaces (e.g., GUI APIs) via tool use or function calling . These interactions typically rely on a fixed set of functionalities. The focus of evaluation here is whether the agent can correctly interpret user intent, identify the appropriate tool, and execute the call correctly. This ability constitutes the foundation of agent competence. Numerous benchmarks have been developed to assess the performance of tool agents across various tasks. These benchmarks evaluate aspects such as tool selection, parameter filling, output formatting, and the ability to handle complex, multi-step processes. A comprehensive overview of these benchmarks is presented in Table VIII and Table IX. 

General-Purpose Tool Agent Benchmarks Several bench-marks have emerged to evaluate agents’ capabilities in handling a wide range of tool interactions. These tools involves a wide domain as weather reports, news retrieval and trans-lation. Benchmarks like ToolAlpaca [ 125 ], ToolBench [ 127 ], BFCL [ 128 ], Seal-Tools [ 130 ] exemplify this phase. To be specific, early ones ToolAlpaca [ 125 ] focuses on generalized tool learning by providing a diverse set of API interactions, and ToolBench [ 127 ] is designed to evaluate agents’ ability to manipulate real-world tools. BFCL [ 128 ] maintains the quality and consistency of live documentation by updating BFCLv2 and BFCLv3 to assess function calling capabilities. Seal-Tools [ 130 ]employs self-instructed learning to generate tool usage scenar-ios. API-Bank [ 131 ] focuses on evaluating agents’ ability to search and retrieve specific APIs. NexusRaven [ 132 ] assesses agents’ performance in nested and parallel function calling scenarios. API-Blend [ 133 ] emphasizes accurate slot filling and sequencing in complex scenarios. APIGen [ 135 ] focuses on multi-turn function calling scenarios. StableToolBench [ 136 ]aims to provide stable and realistic tool evaluation by simulating real-world API interactions. 

Specialized Tool Agent Benchmarks In addition to general-purpose benchmarks, several specialized benchmarks have been developed to assess agents’ capabilities of tool calling in specific domains. APIBench [ 126 ] challenges agents on evalu-ating the performance of agents in utilizing Python APIs for machine learning tasks. ToolSandBox [ 129 ] evaluates agents’ ability to interact with mobile app tools. Its primary difficulty is managing stateful interactions and implicit dependencies between tools. RestBench [ 134 ] assesses agents’ performance in interacting with movie and music databases. Complex-FuncBench [ 137 ] focuses on handling complex function calls, such as booking and reservation APIs while managing user constraints and preferences. NESTFUL [ 138 ] evaluates agents’ ability to handle nested sequences of API calls, where outputs from one call serve as inputs to another. A core challenge is managing dependencies and ensuring correct sequencing. 

2) Interaction with Human : Second, we extend to inter-action with humans, where the agent must engage in multi-turn interactions to accomplish a single goal, and humans exhibit changing states or behaviors. Agents have necessity to ask clarifying questions or request further instructions from a human user, due to the inherently dynamic and underspecified nature of human intent. Early benchmarks focused on constructing multi-turn human-agent dialogues manually to evaluate human-agent interactions like ABCD [ 142 ] and MultiWoZ [ 143 ]. The Action-Based Conversations Dataset (ABCD) [ 142 ] constructs dialogues requiring unique sequences of actions, constrained by company policies, to achieve task success. This dataset emphasizes the importance of aligning agent actions with organizational guidelines in realistic customer service scenarios. Similarly, the MultiWOZ dataset [ 143 ] offers a large-scale collection of human-human written conversations spanning multiple domains. It serves as a valuable resource for developing and evaluating task-oriented dialogue systems across diverse contexts. With the advent of AI agents, researchers began exploring the simulation of human users by agents instead of manual JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10 

construction to enhance the realism of evaluation scenarios. The τ -bench [ 141 ] benchmark emulates dynamic conversations between a user agent and a language agent equipped with domain-specific API tools and policy guidelines. This setup allows for the assessment of an agent’s ability to interact with simulated users. In recent years, benchmarks have begun to combine au-tomated and manual processes to generate diver see and realistic dialogues. Specifically, ALMITA [ 144 ] proposed a new dialogue dataset and framework specifically aimed at evaluating tool-augmented dialogue AI agents in customer support scenarios. IntellAgent [ 145 ] employs a graph-based representation to model the relationships, likelihoods, and complexities of various policies. This approach enables the simulation of multi-policy scenarios, capturing the nuanced interplay between agent capabilities and policy constraints. We have systematically compiled these works into a com-parative Table X. From this table, it becomes evident that the primary task in human-agent interaction benchmarks is often tool calling. Consequently, the ability to effectively utilize tools serves as the cornerstone of agent interaction capabilities. Subsequent interactions with humans are typically aimed at obtaining feedback or further instructions to enhance the completion of tool-based tasks. 

3) Interactions with Other Agents : Once agents acquire the capability to independently solve problems, a third mode of interaction emerges, which is agent-agent interaction. The purpose of this interaction is twofold: to collaboratively accomplish tasks that a single agent cannot achieve alone, or to engage in competitive scenarios to determine a winner. To systematically evaluate these interactions, we categorize existing benchmarks into two paradigms: cooperative and 

competitive . These paradigms serve as the foundation for assessing agent-agent interactions, as shown in Table XI. According to cooperative agent evaluations, Collab-Overcooked [ 148 ] extends the Overcooked-AI game to evaluate AI agents’ collaborative capabilities by incorporating process-oriented evaluation metrics such as Trajectory Efficiency Score (TES) and Incremental Trajectory Efficiency Score (ITES). SmartEvals [ 151 ] encompasses six distinct games, including Rock-Paper-Scissors, Tower of Hanoi, and Minecraft, each augmented with language descriptors for visual observation. The benchmark uniquely challenges nine critical capabilities of intelligent AI agents, such as reasoning with object dependen-cies, planning ahead, spatial reasoning, learning from history, and understanding randomness. MindAgent [ 152 ] leverages existing gaming frameworks to require not only understanding of a multi-agent system but also collaboration with human players via un-finetuned instructions. GOVSIM [ 153 ] presents scenarios such as fishery, pasture, and pollution, focusing on agents engaged in cooperative behavior and effective governance to achieve sustainable outcomes For competitive agents evaluation, AutoArena [ 150 ] is an automated evaluation framework that assesses AI agents through peer battles and committee discussions. It utilizes an agent examiner to generate questions, followed by multi-round peer battles between agent candidates, and concludes with a committee of agent judges collaboratively deciding the winner. This approach reduces bias and enhances evaluation fairness Recently, researchers have established multi-agent bench-marks that simultaneously consider cooperation and compe-tition. BattleAgentBench [ 146 ] evaluates AI agents across seven sub-stages with three varying difficulty levels. It as-sesses single-agent navigation, paired-agent task execution, and multi-agent collaboration and competition capabilities. MultiAgentBench [ 147 ] captures coordination dynamics and competitive interactions, providing tailored metrics such as Key Performance Indicators (KPIs), structured planning and communication scores, and competition scores to systematically assess agent performance. Sotopia [ 149 ] simulates complex social interactions between artificial agents and humans. It eval-uates agents’ social intelligence through role-play interactions under various scenarios, assessing dimensions like believability, relationship, knowledge, secrecy, social rules, financial/material benefits, and goal completion. 

D. Memory 

Memory encompasses an agent’s ability to store, retrieve, and apply information over time. A robust memory system allows agents to maintain coherence in long interactions, retrieves relevant knowledge when needed, and adapts behavior based on past experiences. As memory becomes an increasingly vital component for complex reasoning and decision-making tasks, several benchmarks have been developed to systematically assess different aspects of agent memory. We summarize these benchmarks in Table XII. Some benchmarks focus on evaluating specific memory capabilities, such as question answering (QA) or summarization. NarrativeQA [ 154 ] tests an agent’s ability to answer questions based on long books or screenplays, emphasizing deep narrative understanding. QMSum [ 155 ] evaluates memory through the task of generating multi-domain meeting summaries from lengthy transcripts. QuALITY [ 156 ] targets reading comprehen-sion, challenging agents to answer multiple-choice questions that require understanding and recalling extended documents. DialSim [ 157 ] focuses on dialogue-based QA for TV series, requiring agents to maintain and reason over dialogue history and spatiotemporal memory. Other benchmarks adopt a more holistic approach, ex-amining multiple memory skills or diverse content types. LoCoMo [ 158 ] assesses agents through question answering, event summarization, and multimodal dialogue generation tasks. LTM-Benchmark [ 102 ] evaluates conversational agents’ long-term memory and information integration within a single dynamic multitask dialogue. LongMemEval [ 159 ] tests agents across information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. Episodic Mem-ory Benchmark [ 160 ] assesses episodic memory capabilities, focusing on spatiotemporal-contextual event recall, cross-event relational reasoning, and entity state tracking. PerLTQA [ 161 ]evaluates agents’ ability to integrate and apply semantic and episodic memories in personalized long-term memory question answering. Finally, StreamBench [ 162 ] focuses on online, streaming learning, assessing an agent’s capacity to incrementally update its memory and refine performance based on feedback streams. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11 

E. General 

Beyond the aforementioned agent benchmarks that assess per-formance in specific capabilities, there exist several benchmarks that explicitly evaluate multiple abilities, including planning and memory. These benchmarks enable a holistic assessment of an agent’s generalization capabilities, transcending the limitations of particular environments or individual skills. AgentBench [ 163 ] pioneered the realm of general capabilities benchmarks. It is the first multi-dimensional evaluation frame-work for AI agents, encompassing eight distinct environments, including operating systems, databases, and games. The bench-mark evaluates 27 commercial LLMs, revealing performance variations across complex scenarios. Similarly, GAIA [ 164 ]and MMAU [ 165 ] also offer offline evaluation benchmarks. GAIA focuses on real-world assistant tasks. It consists of 466 human-designed and annotated questions that are text-based and may include files like images or spreadsheets. These questions are intended to reflect real-world challenges. GAIA evaluates AI systems against real-world tasks through these questions, testing fundamental abilities like reasoning, multi-modality handling, web browsing, and tool-use proficiency. MMAU constructs five major domains encompassing tool usage, question answering, mathematics, and machine learning coding. It assesses AI agents from multiple perspectives, including reasoning, planning, problem-solving, and self-correction. As research progresses, the limitations of offline evaluations, such as timeliness and lack of realism, have prompted the de-velopment of online evaluation benchmarks. Notably, Galileo’s Agent Leaderboard [ 166 ] introduces a novel evaluation frame-work that assesses language models’ proficiency in real-world agentic tasks by measuring Tool Selection Quality (TSQ), offering a comprehensive ranking across multiple domains such as mathematics, entertainment, education, and retail. HAL (Holistic Agent Leaderboard) [ 167 ] provides a standardized, cost-aware platform for evaluating AI agents, integrating a unified evaluation harness that supports various benchmarks and facilitates reproducible assessments, thereby enabling transparent comparisons across different agent implementations. Additionally, AgentArena [ 168 ], akin to ChatbotArena [ 197 ], allows human evaluators to anonymously score two agents, facilitating authentic preference assessments. This approach not only provides evaluations but also generates human preference data, fostering a positive feedback loop for further agent training. Furthermore, several innovative general agent benchmarks have emerged. Agent-as-a-Judge [ 198 ] departs from traditional human evaluations by employing agents to assess other agents, thereby streamlining the evaluation process and mitigating the cognitive limitations inherent in human assessments. We consider this a promising trend, enhancing scalability in agent evaluations and addressing the challenges associated with human evaluators, which will be discussed in Section V. Lastly, Capabench [ 169 ] utilizes Shapley values to analyze the contributions of different modules within an agent across various tasks and environments, offering insights into the modular performance and optimization of AI agents. V. DISCUSSION : T HE EVOLUTION OF AGENT 

EVALUATION 

After presenting a variety of agent evaluation methods and benchmarks, a natural question arises: Given the rapid advancement of AI agents, how can we systematically evaluate LLM-based AI agents from an evolutionary perspective? This question will be answered in the final section of this discussion. In this section, we will discuss the evolution of agent evaluation from four perspectives, providing both a summary of past work and guidance for future research. The evolutionary framework is shown in Figure 3, where we can observe the evaluation process and its four key modules. First, a task is extracted from the environment for the agent to solve. Then, the agent produces a response, which is evaluated by an evaluator through processes such as calculation and judgment. Finally, various evaluation metrics are generated. We will explore each of these four modules in detail to discuss the future directions of Agent Evaluation. 

A. Environment Perspective 

From Single Modality to Multi Modality . Early AI agents were primarily text-based, and consequently, most early benchmarks focused on evaluating agents using text-based inputs. This was reasonable at the time. As shown in Tables VIII and X, most benchmarks involving tools, APIs, and human-agent interactions relied on text. However, as the complexity of environments has increased, we are now witnessing the introduction of additional modalities, such as images in web environments or chat audio in mobile environments. Studies have demonstrated that agents perform better when they have access to visual information [ 41 ]. Thus, incorporating multimodal evaluation is crucial and beneficial. We are already seeing multimodal benchmarks related to images and videos, such as VisualAgentBench [ 53 ] and VideoWebArena [ 41 ], and we believe this will be the future trend. Interestingly, in gaming environment benchmarks (e.g., SmartPlay [ 104 ], 

τ -bench [ 141 ]), while game images are present, benchmarks have chosen to represent the positions of all characters using coordinates, converting images into text-based inputs for agents. We believe that future evaluations will involve more direct image input rather than converting it into text. 

From Static to Evolving. Earlier benchmarks are relatively simple, where researchers collect data and publish it in offline systems for evaluation. Such benchmarks are static. However, in the real world, the environment evolves rapidly, particularly on the Internet, where new information emerges daily. If bench-marks are not regularly updated, they risk becoming obsolete. Under this philosophy, creating evolving, scalable benchmarks is essential. One approach is to manually update benchmarks, as seen with BFCL [ 128 ], BFCLv2, BFCLv3, and the swe-bench [ 24 ] series. By continually injecting new knowledge into benchmarks, they can stay relevant. Another approach involves connecting to real-time online evaluations, such as webvoyage [ 38 ] and WorkArena [ 42 ]. These benchmarks are highly scalable because they are directly connected to the ever-evolving Internet, maintaining their relevance and expansion capabilities. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12 Response 

> Eval
> 4 Pe r s pe c t i v e s
> o f Ev o l u t i o n
> Task
> From Single Modality to Multi Modality
> From Static to Evovable
> From Stateless to Stateful
> From Single-Agent to Multi-Agent
> From Single-Turn to Multi-Turn
> From Coarse-Grained to Fine-Grained Label
> From Effectiveness to Efficiency
> From Accuracy-Oriented to Social-Good
> From Human-Judge to Agent-Judge
> From General to Personalized
> Env
> Agent
> Metric
> Evaluator

Figure 3: Four perspectives of evolution prospect about AI agent evaluation. 

From Stateless to Stateful. As agent evaluation evolves, there is a shift from stateless evaluations to stateful ones. Early benchmarks typically provide isolated, single-instance evaluations. However, agents in the real world interact with complex, evolving states. Thus, incorporating stateful evalua-tion, where the agent’s past interactions and context influence its performance, is increasingly important. This approach allows for more accurate evaluations of an agent’s ability to handle dynamic and persistent tasks over time, rather than just isolated scenarios. Earlier benchmarks, especially in web or mobile environ-ments, are generally stateless, focusing only on the agent’s final state. Most benchmarks initialize the agent to a starting state, required the agent to complete a task, and then assessed whether it reached the final state (e.g., a booking confirmation page or a retrieved piece of information). However, the environments in these benchmarks were limited in their ability to evaluate the intermediate states of the agent’s process. As the field progresses, there is a shift toward considering the entire sequence of an agent’s actions, not just the final outcome. Some benchmarks, such as ToolSandBox [ 129 ] and OmniAct [ 56 ], now assess intermediate states by introducing milestones or calculating a sequence score instead of just the final result. This evolution towards stateful evaluation is increasingly relevant in real-world tasks where agents must consider multiple intermediate states, such as handling interruptions in complex workflows. For example, in a multi-task scenario like "book a flight in 20 minutes, organize a document, and search for information simultaneously," agents need to manage both task-specific states and more complex intermediate states. Recent benchmarks, like Worfbench [ 117 ], have already begun addressing such multi-task inputs, signaling a broader trend from stateless to stateful evaluation. 

B. Agent Perspective 

From Single-Agent to Multi-Agent. Most current bench-marks focus on single-agent tasks, where the agent operates independently to solve a problem. However, as the field matures, the focus is shifting toward multi-agent systems, where agents collaborate or compete to achieve their respective goals. Multi-agent environments introduce new challenges in coordination, communication, and negotiation, all of which require novel benchmarking approaches. We believe multi-agent benchmarks hold significant potential. Currently, they face two key challenges. Primarily, current benchmarks predominantly feature game-based or virtual tasks, lacking scenarios reflective of real-world collaborative activities, such as internet-based teamwork. Benchmarks like MultiAgentBench [ 147 ] and BattleAgent-Bench [ 146 ], while valuable for evaluating coordination and competition, do not adequately represent practical collaborative contexts. Furthermore, most existing benchmarks focus on homo-geneous multi-agent systems, neglecting the essential role of heterogeneous agents with specialized skills—such as reasoning (Deepseek [ 6 ]), retrieval (GPT [ 2 ]), and coding (Claude [ 170 ])—and hierarchical structures commonly seen in real-world settings (e.g., manager-executer dynamics). Although frameworks like DeepClaude [ 199 ] and Agent-Verse [ 200 ] have begun exploring these specialized and hierar-chical collaborations, standardized benchmarks for evaluating heterogeneous agent teams remain underdeveloped. 

From Single-Turn to Multi-Turn. In the early stages of AI agent evaluation, benchmarks are often designed for single-turn interactions, where the agent receives a prompt and responds immediately. Tasks like single-turn retrieval or end-to-end task execution require minimal interaction. Benchmarks like WebArena [ 40 ] are designed for such scenarios. As agent capabilities improve, agents now engage in multi-turn dialogues to refine user needs or receive further instructions. Benchmarks like τ -Bench [ 141 ] or Weblinx [ 39 ] now accommodate these extended interactions, and we believe the future of agent evaluation will increasingly involve multi-turn conversations. This trend will naturally lead to the emergence of benchmarks that evaluate the ability to manage long contextual dialogue sequences. 

C. Evaluator Perspective 

From Human-Judge to Agent-Judge. Initially, humans are seen as the best evaluators for agents, given that agents were designed to assist humans with tasks. Early evaluations, such as WebArena [ 40 ] or AgentArena [ 168 ], rely on human judges to assess agent performance. However, over time, it becomes apparent that using agent-based evaluators (agent-judges) JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13 

could offer significant advantages. First, agent-judges can scale effectively without requiring additional human resources. Second, as AI agents’ capabilities evolve and surpass human performance in some domains, human evaluation may no longer align with the agents’ full potential. As such, using agents to evaluate other agents is increasingly seen as a necessary evolution in the field. 

From General to Personalized. Currently, most agents are designed to be general-purpose, aimed at completing tasks based on fixed instructions. Benchmarks reflect this approach, with tasks such as retrieving information or booking tickets. However, once agents are deployed in real-world commercial applications, they must consider factors like user preferences, profiles, and personal history. Personalized agent benchmarks, such as PetoolBench [ 140 ], have emerged to evaluate how well agents adapt to individual users. These benchmarks will play a crucial role as personalized agents become more common in commercial settings. 

D. Metric Perspective 

From Coarse-Grained to Fine-Grained Label. Most benchmarks provide coarse-grained evaluations, offering broad feedback based on the final task outcome. However, a more granular evaluation system is necessary to capture the nuances of an agent’s performance. Fine-grained labels, such as textual comments on an agent’s decision-making process (e.g., “this agent’s planning incurs high time costs” or “this agent’s decisions are fast but risky”), could provide more valuable insights to developers. Although such fine-grained feedback may not be suitable for directly comparing agents by score or ranking which one is better, it can provide developers with more valuable insights, enabling the creation of improved agents. 

From Effectiveness to Efficiency. While effectiveness met-rics—such as task completion or accuracy—have traditionally been the focus of agent evaluation, efficiency is becoming an increasingly important consideration. How quickly and resource-efficiently an agent performs a task is essential. Benchmarks like Worfbench [ 117 ] and GOVSIM [ 153 ] already assess efficiency alongside effectiveness, and future benchmarks will need to balance both aspects to ensure that agents complete tasks efficiently, without overconsuming resources or time. 

From Accuracy-Oriented to Social-Good. As AI agents are integrated into society, evaluating their alignment with societal values such as fairness, safety, and ethical behavior is becoming a critical focus. Current benchmarks have begun to incorporate safety metrics, such as those found in ST-WebAgentBench [ 47 ]. However, additional metrics, such as trustworthiness, robustness, and the ethical implications of agents’ actions (e.g., making erroneous payments or generating harmful content), are still lacking and should be developed further in future benchmarks. We hope that agents will contribute to society in a socially beneficial way, rather than causing any harmful issues. 

E. Benchmark Selection Methodology 

Having detailed the evolution of agent evaluation from four distinct perspectives, we now return to address the core question posed at the beginning of our discussion section: Given the rapid advancement of AI agents, how can we systematically evaluate LLM-based AI agents from an evolutionary perspec-tive? To tackle this question, we propose a two-stage benchmark selection methodology containing present-focused Selection and future-oriented selection, and illustrate it vividly through a practical use case. 

Step 1: Present-focused Selection. This step addresses the immediate evaluation needs that arise when developers have newly created an agent requiring immediate assessment. For example, consider the developer Z, who has developed an initial agent capable of booking flights and hotels. In this scenario, the developer can first consult Figure 2 to identify relevant benchmark categories by considering the agent’s external environment and internal capabilities. For this particular use case, the environment is web-based, and the primary capability involves interaction, specifically tool calling. Subsequently, the developer Z may proceed to the corresponding category sections to obtain an overview of each benchmark. Crucially, the developer could examine the associated category table provided in the Appendix A, which enumerates attributes we deem essential for informed benchmark selection such as modality, task domain, data source, and evaluation metrics. Based on these attributes, the developer Z can identify suitable benchmarks. In this use case scenario, benchmarks such as WebVoyager [ 38 ] (for web environments) and ComplexFuncBench [ 137 ] (for interaction capabilities) would be appropriate choices. 

Step 2: Future-oriented Selection. This second step considers the prospective evolution and ongoing improve-ment of agents, addressing the potential shifts in evaluation methodologies required as the agent matures. Developers can leverage Figure 3, which provides a high-level prospective view on the evolution of agent evaluation. This figure assists developers in two critical areas: (1) identifying directions for further optimization of their agents, and (2) anticipating the future dimensions along which their agents might be assessed. Continuing our previous use case, the external environment might evolve dynamically, necessitating continuous attention to benchmarks adaptable to environmental changes exemplified by BFCL. Similarly, evaluation metrics may shift from purely task-oriented accuracy towards socially beneficial considerations like safety and robustness, reminding the developer Z of monitoring benchmarks like ST-WebAgentBench [ 47 ]. Additionally, as the developer Z’s product moves towards commercialization, incorporating user history, greater attention to personalized benchmarks such as PeToolBench [ 140 ] will become increas-ingly important. Through these two methodological steps, coupled with the illustrative use case provided, developers can systematically select suitable benchmarks for their agents. Furthermore, this approach equips developers with a high-level perspective on future evaluation trends, aiding in the continuous enhancement and comprehensive assessment of agent performance. VI. CONCLUSION 

In this paper, we have systematically explored the evo-lutionary transition from traditional LLM chatbots to ad-JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14 

vanced AI agents, highlighting critical evolution across five aspects: complex environment, multi-source instructor, dynamic feedback, multi-modal perception, and advanced capability. By adopting an evolutionary perspective, we have addressed existing ambiguities in evaluation practices and offered a structured categorization of benchmarks aligned with external environments and internal capabilities. Our comprehensive attribute-based tables serve as practical resources for researchers aiming to select suitable evaluation frameworks. Furthermore, we have discussed evolving trends and provided forward-looking insights on evaluation methodologies from four per-spectives of environment, agent, evaluator, and metric. This work not only clarifies the evaluation landscape for AI agents but also sets the stage for future research directions, ultimately contributing to the ongoing advancement and refinement of AI agent systems. REFERENCES 

[1] A. Vaswani, N. Shazeer et al. , “Attention is all you need,” in Advances in Neural Information Processing Systems , 2017, pp. 5998–6008. [Online]. Available: http://arxiv.org/abs/1706.03762 [2] OpenAI, “Gpt-4 technical report,” arXiv preprint arXiv:2303.08774 , 2023. [Online]. Available: https: //doi.org/10.48550/arXiv.2303.08774 [3] H. Touvron, T. Lavril et al. , “Llama: Open and efficient foundation language models,” CoRR , vol. abs/2302.13971, 2023. [Online]. Available: https: //doi.org/10.48550/arXiv.2302.13971 [4] R. Anil, S. Borgeaud et al. , “Gemini: A family of highly capable multimodal models,” arXiv preprint arXiv:2312.11805 , 2023. [Online]. Available: https: //arxiv.org/abs/2312.11805 [5] Y. Bai, Y. Lin et al. , “Qwen technical report,” arXiv preprint arXiv:2309.16609 , 2023. [Online]. Available: https://arxiv.org/abs/2309.16609 [6] D. Guo, D. Yang et al. , “Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,” 

arXiv preprint arXiv:2501.12948 , 2025. [Online]. Available: https://arxiv.org/abs/2501.12948 [7] T. Abuelsaad, D. Akkil et al. , “Agent-e: From autonomous web navigation to foundational design principles in agentic systems,” arXiv preprint arXiv:2407.13032 , 2024. [Online]. Available: https://arxiv.org/abs/2407.13032 [8] Y. Song, F. Xu et al. , “Beyond browsing: Api-based web agents,” arXiv preprint arXiv:2410.16464 , 2024. [Online]. Available: https://arxiv.org/abs/2410.16464 [9] N. Shinn, F. Cassano et al. , “Reflexion: Language agents with verbal reinforcement learning,” arXiv preprint arXiv:2303.11366 , 2023. [Online]. Available: https://arxiv.org/abs/2303.11366 [10] Y. Chang, X. Wang et al. , “A survey on evaluation of large language models,” 2023. [Online]. Available: https://arxiv.org/abs/2307.03109 [11] M. T. R. Laskar, S. Alqahtani et al. , “A systematic survey and critical review on evaluating large language models: Challenges, limitations, and recommendations,” 2024. [Online]. Available: https://arxiv.org/abs/2407.04069 [12] Z. Guo, R. Jin et al. , “Evaluating large language models: A comprehensive survey,” 2023. [Online]. Available: https://arxiv.org/abs/2310.19736 [13] M. Chen, J. Tworek et al. , “Evaluating large lan-guage models trained on code,” arXiv preprint arXiv:2107.03374 , 2021. [14] X. Du, M. Liu et al. , “Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation,” arXiv preprint arXiv:2308.01861 , 2023. [15] H. Yu, B. Shen et al. , “Codereval: A benchmark of pragmatic code generation with generative pre-trained models,” in Proceedings of the 46th IEEE/ACM Inter-national Conference on Software Engineering , 2024, pp. 1–12. [16] Z. Yuan, J. Liu et al. , “Evaluating instruction-tuned large language models on code comprehension and generation,” 

arXiv preprint arXiv:2308.01240 , 2023. [17] A. Zhang, M. Dong et al. , “Codecriticbench: A holistic code critique benchmark for large language models,” 

arXiv preprint arXiv:2502.16614 , 2025. [18] A. Gu, B. Rozière et al. , “Cruxeval: A benchmark for code reasoning, understanding and execution,” arXiv preprint arXiv:2401.03065 , 2024. [19] T. Y. Zhuo, M. C. Vu et al. , “Bigcodebench: Bench-marking code generation with diverse function calls and complex instructions,” arXiv preprint arXiv:2406.15877 ,2024. [20] P. Yin, W.-D. Li et al. , “Natural language to code generation in interactive data science notebooks,” arXiv preprint arXiv:2212.09248 , 2022. [21] C. E. Jimenez, J. Yang et al. , “Swe-bench: Can language models resolve real-world github issues?” arXiv preprint arXiv:2310.06770 , 2023. [22] J. Yang, C. E. Jimenez et al. , “Swe-bench multimodal: Do ai systems generalize to visual software domains?” 

arXiv preprint arXiv:2410.03859 , 2024. [23] A. Antoniades, A. Örwall, K. Zhang et al. , “Swe-search: Enhancing software agents with monte carlo tree search and iterative refinement,” arXiv preprint arXiv:2410.20285 , 2024. [24] D. Zan, Z. Huang et al. , “Swe-bench-java: A github issue resolving benchmark for java, 2024,” URL https://arxiv. org/abs/2408.14354 .[25] “Swe arena: An open evaluation platform for automated software engineering,” 2024. [26] A. Yadavally, H. Nguyen et al. , “Large language model critics for execution-free evaluation of code changes,” 

arXiv preprint arXiv:2501.16655 , 2025. [27] R. Aleithan, H. Xue, and et al., “Swe-bench+: En-hanced coding benchmark for llms,” arXiv preprint arXiv:2410.06992 , 2024. [28] T. Liu, C. Xu, and J. McAuley, “Repobench: Bench-marking repository-level code auto-completion systems,” 

arXiv preprint arXiv:2306.03091 , 2023. [29] N. Mündler, M. Müller, J. He, and M. Vechev, “Swt-bench: Testing and validating real-world bug-fixes with JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 15 

code agents,” Advances in Neural Information Process-ing Systems , vol. 37, pp. 81 857–81 887, 2024. [30] C. Qian, W. Liu et al. , “Chatdev: Communicative agents for software development,” arXiv preprint arXiv:2307.07924 , 2023. [31] B. Li, W. Wu et al. , “Prompting large language models to tackle the full software development lifecycle: A case study,” arXiv preprint arXiv:2403.08604 , 2024. [32] X. Tang, Y. Liu et al. , “Ml-bench: Evaluating large lan-guage models and agents for machine learning tasks on repository-level code,” arXiv preprint arXiv:2311.09835 ,2023. [33] Y. Zhang, Y. Pan, Y. Wang, and J. Cai, “Pybench: Evaluating llm agent on various real-world coding tasks,” 2024. [Online]. Available: https://arxiv.org/abs/ 2407.16732 [34] T. Shi, A. Karpathy et al. , “World of bits: An open-domain platform for web-based agents,” in Proceedings of the 34th International Conference on Machine Learning , ser. Proceedings of Machine Learning Research, D. Precup and Y. W. Teh, Eds., vol. 70. PMLR, 06–11 Aug 2017, pp. 3135–3144. [Online]. Available: https://proceedings.mlr.press/v70/shi17a.html [35] E. Z. Liu, K. Guu et al. , “Reinforcement learning on web interfaces using workflow-guided exploration,” 2018. [Online]. Available: https://arxiv.org/abs/1802.08802 [36] S. Yao, H. Chen et al. , “Webshop: Towards scalable real-world web interaction with grounded language agents,” 2023. [Online]. Available: https://arxiv.org/abs/ 2207.01206 [37] X. Deng, Y. Gu et al. , “Mind2web: Towards ageneralist agent for the web,” 2023. [Online]. Available: https://arxiv.org/abs/2306.06070 [38] H. He, W. Yao et al. , “Webvoyager: Building an end-to-end web agent with large multimodal models,” arXiv preprint arXiv:2401.13919 , 2024. [39] X. H. Lù, Z. Kasner, and S. Reddy, “Weblinx: Real-world website navigation with multi-turn dialogue,” arXiv preprint arXiv:2402.05930 , 2024. [40] S. Zhou, F. F. Xu et al. , “Webarena: A realistic web environment for building autonomous agents,” 2024. [Online]. Available: https://arxiv.org/abs/2307.13854 [41] J. Y. Koh, R. Lo et al. , “Visualwebarena: Evaluating multimodal agents on realistic visual web tasks,” 2024. [Online]. Available: https://arxiv.org/abs/2401.13649 [42] A. Drouin, M. Gasse et al. , “Workarena: How capable are web agents at solving common knowledge work tasks?” 2024. [Online]. Available: https://arxiv.org/abs/ 2403.07718 [43] L. Boisvert, M. Thakkar et al. , “Workarena++: Towards compositional planning and reasoning-based common knowledge work tasks,” 2025. [Online]. Available: https://arxiv.org/abs/2407.05291 [44] Z. Zhang, S. Tian, L. Chen, and Z. Liu, “Mmina: Benchmarking multihop multimodal internet agents,” 2024. [Online]. Available: https://arxiv.org/abs/2404. 09992 [45] O. Yoran, S. J. Amouyal et al. , “Assistantbench: Can web agents solve realistic and time-consuming tasks?” 2024. [Online]. Available: https://arxiv.org/abs/2407.15711 [46] Y. Pan, D. Kong et al. , “Webcanvas: Benchmarking web agents in online environments,” 2024. [Online]. Available: https://arxiv.org/abs/2406.12373 [47] I. Levy, B. Wiesel et al. , “St-webagentbench: Abenchmark for evaluating safety and trustworthiness in web agents,” 2024. [Online]. Available: https: //arxiv.org/abs/2410.06703 [48] L. Jang, Y. Li et al. , “Videowebarena: Evaluating long context multimodal agents with video understanding web tasks,” 2025. [Online]. Available: https://arxiv.org/ abs/2410.19100 [49] K. Xu, Y. Kordi et al. , “Tur[k]ingbench: A challenge benchmark for web agents,” 2025. [Online]. Available: https://arxiv.org/abs/2403.11905 [50] Y. Song, K. Thai et al. , “Bearcubs: A benchmark for computer-using web agents,” 2025. [Online]. Available: https://arxiv.org/abs/2503.07919 [51] F. F. Xu, Y. Song et al. , “Theagentcompany: Benchmarking llm agents on consequential real world tasks,” 2024. [Online]. Available: https://arxiv.org/abs/ 2412.14161 [52] S. Kara, F. Faisal, and S. Nath, “Waber: Evaluating reliability and efficiency of web agents with existing benchmarks,” in ICLR 2025 Workshop on Foundation Models in the Wild .[53] X. Liu, T. Zhang et al. , “Visualagentbench: Towards large multimodal models as visual foundation agents,” 2024. [Online]. Available: https://arxiv.org/abs/2408. 06327 [54] T. Xue, W. Qi et al. , “An illusion of progress? assessing the current state of web agents,” 2025. [Online]. Available: https://arxiv.org/abs/2504.01382 [55] D. Garg, S. VanWeelden et al. , “Real: Benchmarking autonomous agents on deterministic simulations of real websites,” 2025. [Online]. Available: https: //arxiv.org/abs/2504.11543 [56] R. Kapoor, Y. P. Butala et al. , “Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web,” 2024. [Online]. Available: https://arxiv.org/abs/2402.17553 [57] S. Chen, S. Wiseman, and B. Dhingra, “Chatshop: Interactive information seeking with language agents,” 2024. [Online]. Available: https://arxiv.org/abs/2404. 09911 [58] T. Xie, D. Zhang et al. , “Osworld: Benchmarking multimodal agents for open-ended tasks in real com-puter environments,” Advances in Neural Information Processing Systems , vol. 37, pp. 52 040–52 094, 2024. [59] R. Bonatti, D. Zhao et al. , “Windows agent arena: Eval-uating multi-modal os agents at scale,” arXiv preprint arXiv:2409.08264 , 2024. [60] L. Zheng, Z. Huang et al. , “Agentstudio: A toolkit for building general virtual agents,” arXiv preprint arXiv:2403.17918 , 2024. [61] R. Kapoor, Y. P. Butala et al. , “Omniact: A dataset and benchmark for enabling multimodal generalist JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 16 

autonomous agents for desktop and web,” in European Conference on Computer Vision . Springer, 2024, pp. 161–178. [62] Z. Wang, Y. Cui et al. , “Officebench: Benchmarking language agents across multiple applications for office automation,” arXiv preprint arXiv:2407.19056 , 2024. [63] H. Liu, X. Zhang et al. , “Pc-agent: A hierarchical multi-agent collaboration framework for complex task automation on pc,” arXiv preprint arXiv:2502.14282 ,2025. [64] Y. Li, J. He et al. , “Mapping natural language instruc-tions to mobile ui action sequences,” arXiv preprint arXiv:2005.03776 , 2020. [65] S. G. Venkatesh, P. Talukdar, and S. Narayanan, “Ugif: Ui grounded instruction following,” arXiv preprint arXiv:2211.07615 , 2022. [66] A. Burns, D. Arsan et al. , “Mobile app tasks with iterative feedback (motif): Addressing task feasibil-ity in interactive visual environments,” arXiv preprint arXiv:2104.08560 , 2021. [67] C. Rawles, A. Li et al. , “Androidinthewild: A large-scale dataset for android device control,” Advances in Neural Information Processing Systems , vol. 36, pp. 59 708– 59 728, 2023. [68] W. Li, W. E. Bishop et al. , “On the effects of data scale on ui control agents,” Advances in Neural Information Processing Systems , vol. 37, pp. 92 130–92 154, 2024. [69] Y. Chai, S. Huang et al. , “Amex: Android multi-annotation expo dataset for mobile gui agents,” arXiv preprint arXiv:2407.17490 , 2024. [70] J. Lee, T. Min et al. , “Benchmarking mobile device control agents across diverse configurations,” arXiv preprint arXiv:2404.16660 , 2024. [71] C. Rawles, S. Clinckemaillie et al. , “Androidworld: A dynamic benchmarking environment for autonomous agents,” arXiv preprint arXiv:2405.14573 , 2024. [72] S. Deng, W. Xu et al. , “Mobile-bench: An evaluation benchmark for llm-based mobile agents,” arXiv preprint arXiv:2407.00993 , 2024. [73] Y. Chai, H. Li et al. , “A3: Android agent arena for mobile gui agents,” arXiv preprint arXiv:2501.01149 ,2025. [74] D. Zhang, Z. Shen et al. , “Mobile-env: Building qualified evaluation benchmarks for llm-gui interaction,” arXiv preprint arXiv:2305.08144 , 2023. [75] M. Xing, R. Zhang et al. , “Understanding the weakness of large language model agents within a complex android environment,” in Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining ,2024, pp. 6061–6072. [76] J. Lee, D. Hahm et al. , “Mobilesafetybench: Evaluating safety of autonomous agents in mobile device control,” 

arXiv preprint arXiv:2410.17520 , 2024. [77] L. Wang, Y. Deng et al. , “Mobileagentbench: An efficient and user-friendly benchmark for mobile llm agents,” 

arXiv preprint arXiv:2406.08184 , 2024. [78] J. Chen, D. Yuen et al. , “Spa-bench: A comprehensive benchmark for smartphone agent evaluation,” in NeurIPS 2024 Workshop on Open-World Agents , 2024. [79] L. Zhang, S. Wang et al. , “Llamatouch: A faithful and scalable testbed for mobile ui task automation,” in 

Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology , 2024, pp. 1–13. [80] Y. Xu, X. Liu et al. , “Androidlab: Training and sys-tematic benchmarking of android autonomous agents,” 

arXiv preprint arXiv:2410.24024 , 2024. [81] J. Sun, Z. Hua, and Y. Xia, “Autoeval: A practical framework for autonomous evaluation of mobile agents,” 

arXiv preprint arXiv:2503.02403 , 2025. [82] P. Clark, I. Cowhey et al. , “Think you have solved question answering? try arc, the ai2 reasoning challenge,” 

arXiv preprint arXiv:1803.05457 , 2018. [83] P. Lu, S. Mishra et al. , “Learn to explain: Multimodal reasoning via thought chains for science question answering,” 2022. [Online]. Available: https://arxiv.org/ abs/2209.09513 [84] P. Dasigi, K. Lo et al. , “A dataset of information-seeking questions and answers anchored in research papers,” 

arXiv preprint arXiv:2105.03011 , 2021. [85] Y. Lee, K. Lee et al. , “Qasa: advanced question answer-ing on scientific articles,” in International Conference on Machine Learning . PMLR, 2023, pp. 19 036–19 052. [86] E. Chamoun, M. Schlichktrull, and A. Vlachos, “Auto-mated focused feedback generation for scientific writing assistance,” arXiv preprint arXiv:2405.20477 , 2024. [87] R. Lou, H. Xu et al. , “Aaar-1.0: Assessing ai’s potential to assist research,” arXiv preprint arXiv:2410.22394 ,2024. [88] J. DeYoung, I. Beltagy et al. , “Ms2: Multi-document summarization of medical studies,” arXiv preprint arXiv:2104.06486 , 2021. [89] J. M. Laurent, J. D. Janizek et al. , “Lab-bench: Measur-ing capabilities of language models for biology research,” 

arXiv preprint arXiv:2407.10362 , 2024. [90] H. Kang and C. Xiong, “Researcharena: Benchmarking llms’ ability to collect and organize information as research agents,” arXiv preprint arXiv:2406.10291 , 2024. [91] C. Si, D. Yang, and T. Hashimoto, “Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers,” arXiv preprint arXiv:2409.04109 ,2024. [92] J. Lála, O. O’Donoghue et al. , “Paperqa: Retrieval-augmented generative agent for scientific research,” 

arXiv preprint arXiv:2312.07559 , 2023. [93] R. Wang, P. Jansen et al. , “Scienceworld: Is your agent smarter than a 5th grader?, 2022,” URL https://arxiv. org/abs/2203.07540 , 2022. [94] P. Jansen, M.-A. Côté et al. , “Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents, 2024,” URL https://arxiv. org/abs/2406.06769 .[95] Z. Chen, S. Chen et al. , “Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery,” arXiv preprint arXiv:2410.05080 ,2024. [96] D. Nathani, L. Madaan et al. , “Mlgym: A new framework JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 17 

and benchmark for advancing ai research agents,” arXiv preprint arXiv:2502.14499 , 2025. [97] Q. Huang, J. Vora, P. Liang, and J. Leskovec, “Mlagent-bench: Evaluating language agents on machine learning experimentation, 2024,” URL https://arxiv. org/abs/2310 ,vol. 3302. [98] L. Jing, Z. Huang et al. , “Dsbench: How far are data science agents to becoming data science experts?” arXiv preprint arXiv:2409.07703 , 2024. [99] Y. Zhang, Q. Jiang et al. , “Benchmarking data science agents,” arXiv preprint arXiv:2402.17168 , 2024. [100] Y. Huang, J. Luo et al. , “Da-code: Agent data science code generation benchmark for large language models,” 

arXiv preprint arXiv:2410.07331 , 2024. [101] R. Cao, F. Lei et al. , “Spider2-v: How far are multimodal agents from automating data science and engineering workflows?” Advances in Neural Information Processing Systems , vol. 37, pp. 107 703–107 744, 2024. [102] D. Castillo-Bolado, J. Davidson, F. Gray, and M. Rosa, “Beyond prompts: Dynamic conversational bench-marking of large language models,” arXiv preprint arXiv:2409.20222 , 2024. [103] D. Paglieri, B. Cupiał et al. , “Balrog: Benchmarking agentic llm and vlm reasoning on games,” arXiv preprint arXiv:2411.13543 , 2024. [104] Y. Wu, X. Tang, T. M. Mitchell, and Y. Li, “Smartplay: A benchmark for llms as intelligent agents,” arXiv preprint arXiv:2310.01557 , 2023. [105] X. Wang, B. Zhuang, and Q. Wu, “Are large vision language models good game players?” arXiv preprint arXiv:2503.02358 , 2025. [106] Y. Ren, K. Tertikas et al. , “Vgrp-bench: Visual grid reasoning puzzle benchmark for large vision-language models,” arXiv preprint arXiv:2503.23064 , 2025. [107] H. Zhang, H. Guo et al. , “Ing-vp: Mllms cannot play easy vision-based games yet,” arXiv preprint arXiv:2410.06555 , 2024. [108] W. Tang, Y. Zhou et al. , “Dsgbench: A diverse strategic game benchmark for evaluating llm-based agents in complex decision-making environments,” arXiv preprint arXiv:2503.06047 , 2025. [109] A. Costarelli, M. Allen et al. , “Gamebench: Evaluating strategic reasoning abilities of llm agents,” arXiv preprint arXiv:2406.06613 , 2024. [110] J.-t. Huang, E. J. Li et al. , “Competing large language models in multi-agent gaming environments,” in The Thirteenth International Conference on Learning Repre-sentations , 2025. [111] J. Duan, R. Zhang et al. , “Gtbench: Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations,” arXiv preprint arXiv:2402.12348 , 2024. [112] L. Hu, Q. Li et al. , “Gamearena: Evaluating llm reasoning through live computer games,” arXiv preprint arXiv:2412.06394 , 2024. [113] K. Valmeekam, M. Marquez et al. , “Planbench: An exten-sible benchmark for evaluating large language models on planning and reasoning about change,” Advances in Neural Information Processing Systems , vol. 36, pp. 38 975–38 987, 2023. [114] K. Valmeekam, S. Sreedharan et al. , “On the planning abilities of large language models (a critical investi-gation with a proposed benchmark),” arXiv preprint arXiv:2302.06706 , 2023. [115] K. Stein, D. Fišer et al. , “Autoplanbench: Automatically generating benchmarks for llm planners from pddl,” 

arXiv preprint arXiv:2311.09830 , 2023. [116] H. Kokel, M. Katz et al. , “Acpbench: Reasoning about action, change, and planning,” in Proceedings of the AAAI Conference on Artificial Intelligence , vol. 39, no. 25, 2025, pp. 26 559–26 568. [117] S. Qiao, R. Fang et al. , “Benchmarking agentic workflow generation,” arXiv preprint arXiv:2410.07869 , 2024. [118] R. Xiao, W. Ma et al. , “Flowbench: Revisiting and benchmarking workflow-guided planning for llm-based agents,” arXiv preprint arXiv:2406.14884 , 2024. [119] L. Geng and E. Y. Chang, “Realm-bench: A real-world planning benchmark for llms and multi-agent systems,” 

arXiv preprint arXiv:2502.18836 , 2025. [120] L. Zhang, Y. Wang, H. Gu et al. , “Et-plan-bench: Embodied task-level planning benchmark towards spatial-temporal cognition with foundation models,” arXiv preprint arXiv:2410.14682 , 2024. [121] C.-A. Cheng, A. Kolobov et al. , “Llf-bench: Benchmark for interactive learning from language feedback,” 2023. [Online]. Available: https://arxiv.org/abs/2312.06853 [122] J. You, M. Liu et al. , “LLM-evolve: Evaluation for LLM‘s evolving capability on benchmarks,” in 

Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, Eds. Miami, Florida, USA: Association for Computational Linguistics, Nov. 2024, pp. 16 937–16 942. [Online]. Available: https://aclanthology.org/2024.emnlp-main.940/ [123] L. Li, Y. Wang et al. , “Reflection-bench: probing ai intelligence with reflection,” 2024. [Online]. Available: https://arxiv.org/abs/2410.16270 [124] J. Pan, R. Shar et al. , “When benchmarks talk: Re-evaluating code llms with interactive feedback,” 2025. [Online]. Available: https://arxiv.org/abs/2502.18413 [125] Q. Tang et al. , “Toolalpaca: Generalized tool learning for language models with 3000 simulated cases,” arXiv ,2023. [Online]. Available: https://arxiv.org/abs/2306. 05301 [126] Y. Li et al. , “Revisiting, benchmarking and exploring api recommendation: How far are we?” arXiv , 2021. [Online]. Available: https://arxiv.org/abs/2112.12653 [127] S. Patil et al. , “Toolllm: Facilitating large language models to master 16000+ real-world apis,” arXiv , 2023. [Online]. Available: https://arxiv.org/abs/2305.16504 [128] G. Team, “Berkeley function calling leaderboard v3 (aka berkeley tool calling leaderboard v3),” Gorilla ,2024. [Online]. Available: https://gorilla.cs.berkeley.edu/ leaderboard.html [129] J. Lu et al. , “Toolsandbox: A stateful, conversational, interactive evaluation benchmark for llm tool use capabilities,” arXiv , 2024. [Online]. Available: https: JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 18 

//arxiv.org/abs/2408.04682 [130] F. Team, “Seal-tools: Self-instruct tool learning dataset for agent tuning and evaluation,” arXiv , 2024. [Online]. Available: https://arxiv.org/abs/2405.08355 [131] Y. Li et al. , “Api-bank: A comprehensive benchmark for tool-augmented llms,” EMNLP , 2023. [Online]. Avail-able: https://aclanthology.org/2023.emnlp-main.187/ [132] N. Team, “Nexusraven-13b, a new sota open-source llm for function calling,” Nexusflow , 2024. [Online]. Available: https://github.com/nexusflowai/NexusRaven [133] K. Basu et al. , “A comprehensive corpora for training and benchmarking api llms,” ACL , 2024. [Online]. Available: https://aclanthology.org/2024.acl-long.694/ [134] R. Team, “Restgpt: Connecting llms with real-world restful apis,” RestGPT , 2023. [Online]. Available: https://restgpt.github.io/ [135] N. Team, “Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets,” NeurIPS ,2024. [Online]. Available: https://arxiv.org/abs/2406. 18518 [136] Z. Guo et al. , “Towards stable large-scale benchmarking on tool learning of large language models,” ACL Findings , 2024. [Online]. Available: https://aclanthology. org/2024.findings-acl.664/ [137] T. Team, “Exploring multi-step and constrained function calling under long-context scenario,” arXiv , 2025. [Online]. Available: https://arxiv.org/abs/2501.10132 [138] K. Basu et al. , “A benchmark for evaluating llms on nested sequences of api calls,” OpenReview , 2024. [Online]. Available: https://openreview.net/forum?id= r7staQknbI [139] T. Team, “Tooleyes: Fine-grained evaluation for tool learning capabilities of large language models,” 

COLING , 2025. [Online]. Available: https://aclanthology. org/2025.coling-main.12/ [140] P. Team, “Petoolbench: Personalized evaluation benchmark for tool-augmented llms,” arXiv , 2024. [Online]. Available: https://arxiv.org/abs/2407.02912 [141] S. Yao, N. Shinn, P. Razavi, and K. Narasimhan, “τ -bench: A benchmark for tool-agent-user interaction in real-world domains,” 2024. [Online]. Available: https://arxiv.org/abs/2406.12045 [142] D. Chen, H. Chen, Y. Yang, A. Lin, and Z. Yu, “Action-based conversations dataset: A corpus for building more in-depth task-oriented dialogue systems,” 2021. [Online]. Available: https://arxiv.org/abs/2104.00783 [143] P. Budzianowski, T.-H. Wen et al. , “Multiwoz – a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling,” 2020. [Online]. Available: https://arxiv.org/abs/1810.00278 [144] S. Arcadinho, D. Aparicio, and M. Almeida, “Automated test generation to evaluate tool-augmented llms as conversational ai agents,” 2024. [Online]. Available: https://arxiv.org/abs/2409.15934 [145] S. Yao, N. Shinn, P. Razavi, and K. Narasimhan, “Intellagent: A benchmark for evaluating tool-augmented agents in real-world domains,” 2024. [Online]. Available: https://arxiv.org/abs/2406.12045 [146] W. Wang, D. Zhang et al. , “Battleagentbench: Abenchmark for evaluating cooperation and competition capabilities of language models in multi-agent systems,” 

arXiv preprint arXiv:2408.15971 , 2024. [Online]. Available: https://arxiv.org/abs/2408.15971 [147] A. Gangrade et al. , “Evaluating the collaboration and competition of llm agents,” arXiv preprint arXiv:2503.01935 , 2025. [Online]. Available: https: //arxiv.org/abs/2503.01935 [148] R. Wang et al. , “Collab-overcooked: Benchmarking and evaluating large language models as collaborative agents,” 

arXiv preprint arXiv:2502.20073 , 2025. [Online]. Available: https://arxiv.org/abs/2502.20073 [149] ——, “Sotopia: Interactive evaluation for social intelligence in language agents,” arXiv preprint arXiv:2310.11667 , 2023. [Online]. Available: https: //arxiv.org/abs/2310.11667 [150] R. Zhao et al. , “Auto-arena: Automating llm evaluations with agent peer battles and committee discussions,” 

arXiv preprint arXiv:2405.20267 , 2024. [Online]. Available: https://arxiv.org/abs/2405.20267 [151] M. Alahi et al. , “Smarteval: Evaluation system for descriptive answers in examinations using iot-enabled technologies and artificial intelligence,” Sensors ,vol. 23, no. 11, p. 5206, 2023. [Online]. Available: https://doi.org/10.3390/s23115206 [152] H. Yu et al. , “Mindagent: Emergent gaming interaction,” in Findings of the Association for Computational Linguistics: NAACL 2024 , 2024, pp. 200–210. [Online]. Available: https://aclanthology.org/2024.findings-naacl. 200/ [153] G. Piatti et al. , “Cooperate or collapse: Emergence of sustainable cooperation in a society of llm agents,” arXiv preprint arXiv:2404.16698 , 2024. [Online]. Available: https://arxiv.org/abs/2404.16698 [154] T. Koˇ cisk `y, J. Schwarz et al. , “The narrativeqa reading comprehension challenge,” Transactions of the Associa-tion for Computational Linguistics , vol. 6, pp. 317–328, 2018. [155] M. Zhong, D. Yin et al. , “Qmsum: A new benchmark for query-based multi-domain meeting summarization,” 

arXiv preprint arXiv:2104.05938 , 2021. [156] R. Y. Pang, A. Parrish et al. , “Quality: Question answering with long input texts, yes!” arXiv preprint arXiv:2112.08608 , 2021. [157] J. Kim, W. Chay et al. , “Dialsim: A real-time sim-ulator for evaluating long-term multi-party dialogue understanding of conversational agents,” arXiv preprint arXiv:2406.13144 , 2024. [158] A. Maharana, D.-H. Lee et al. , “Evaluating very long-term conversational memory of llm agents,” arXiv preprint arXiv:2402.17753 , 2024. [159] D. Wu, H. Wang et al. , “Longmemeval: Benchmarking chat assistants on long-term interactive memory,” arXiv preprint arXiv:2410.10813 , 2024. [160] A. Huet, Z. B. Houidi, and D. Rossi, “Episodic memories generation and evaluation benchmark for large language models,” arXiv preprint arXiv:2501.13121 , 2025. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 19 

[161] Y. Du, H. Wang et al. , “Perltqa: A personal long-term memory dataset for memory classification, retrieval, and synthesis in question answering,” arXiv preprint arXiv:2402.16288 , 2024. [162] C.-K. Wu, Z. R. Tam et al. , “Streambench: Towards benchmarking continuous improvement of language agents,” Advances in Neural Information Processing Systems , vol. 37, pp. 107 039–107 063, 2024. [163] X. Liu, H. Yu et al. , “Agentbench: Evaluating llms as agents,” 2023. [Online]. Available: https: //arxiv.org/abs/2308.03688 [164] G. Mialon, C. Fourrier et al. , “Gaia: a benchmark for general ai assistants,” 2023. [Online]. Available: https://arxiv.org/abs/2311.12983 [165] G. Yin, H. Bai, S. Ma et al. , “Mmau: A holistic benchmark of agent capabilities across diverse domains,” 

arXiv preprint arXiv:2407.18961 , 2024. [Online]. Available: https://arxiv.org/pdf/2407.18961 [166] G. A. Team, “Galileo ai agent leaderboard,” 2023. [Online]. Available: https://huggingface.co/spaces/ galileo-ai/agent-leaderboard [167] B. Stroebl, S. Kapoor, and A. Narayanan, “Hal: A holistic agent leaderboard for centralized and reproducible agent evaluation,” https://github.com/ princeton-pli/hal-harness, 2025. [168] N. Yekollu, A. Bohra, A. Chirumamilla et al. , “Agent arena: A platform for evaluating and comparing llm agents,” 2025. [Online]. Available: https://www. agent-arena.com/ [169] Y. Yang, B. Huang, S. Qi et al. , “Who’s the mvp? a game-theoretic evaluation benchmark for modular attribution in llm agents,” arXiv preprint arXiv:2502.00510 , 2025. [Online]. Available: https://arxiv.org/abs/2502.00510 [170] Anthropic, “Claude ai,” 2025, accessed: 2025-04-27. [Online]. Available: https://claude.ai/ [171] F. Zhang, B. Chen et al. , “Repocoder: Repository-level code completion through iterative retrieval and generation,” arXiv preprint arXiv:2303.12570 , 2023. [172] J. S. Chan, N. Chowdhury et al. , “Mle-bench: Eval-uating machine learning agents on machine learning engineering,” arXiv preprint arXiv:2410.07095 , 2024. [173] T. Carta, C. Romac et al. , “Grounding large language models in interactive environments with online reinforce-ment learning,” in International Conference on Machine Learning . PMLR, 2023, pp. 3676–3713. [174] D. Hafner, “Benchmarking the spectrum of agent capa-bilities,” arXiv preprint arXiv:2109.06780 , 2021. [175] M.-A. Côté, A. Kádár et al. , “Textworld: A learning environment for text-based games,” in Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers 7 . Springer, 2019, pp. 41–75. [176] R. A. Bradley and M. E. Terry, “Rank analysis of incomplete block designs: I. the method of paired comparisons,” Biometrika , vol. 39, no. 3-4, pp. 324– 345, 1952. [177] J. Wei, X. Wang et al. , “Chain of thought prompting elicits reasoning in large language models,” in 

Advances in Neural Information Processing Systems ,vol. 35, 2022, pp. 24 824–24 837. [Online]. Available: https://arxiv.org/abs/2201.11903 [178] X. Yao, Z. Ye et al. , “Tree of thoughts: Deliberate problem solving with large language models,” in 

Advances in Neural Information Processing Systems ,vol. 36, 2023. [Online]. Available: https://arxiv.org/abs/ 2305.10601 [179] W. Ling, D. Yogatama et al. , “Program induction by rationale generation: Learning to solve and explain alge-braic word problems,” arXiv preprint arXiv:1705.04146 ,2017. [180] K. Cobbe, V. Kosaraju et al. , “Training verifiers to solve math word problems,” arXiv preprint arXiv:2110.14168 ,2021. [181] D. Hendrycks, C. Burns et al. , “Measuring mathematical problem solving with the math dataset,” arXiv preprint arXiv:2103.03874 , 2021. [182] Y. Zhang, J. Yang et al. , “Cumulative reason-ing with large language models,” arXiv preprint arXiv:2308.04371 , 2023. [183] A. Patel, S. Bhattamishra, and N. Goyal, “Are nlp models really able to solve simple math word problems?” arXiv preprint arXiv:2103.07191 , 2021. [184] Z. Yang, P. Qi et al. , “Hotpotqa: A dataset for diverse, ex-plainable multi-hop question answering,” arXiv preprint arXiv:1809.09600 , 2018. [185] M. Geva, D. Khashabi et al. , “Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies,” Transactions of the Association for Computational Linguistics , vol. 9, pp. 346–361, 2021. [186] S. Han, H. Schoelkopf et al. , “Folio: Natural language reasoning with first-order logic,” 2024. [Online]. Available: https://arxiv.org/abs/2209.00840 [187] S. Han, A. Yu, R. Shen et al. , “P-folio: Evalu-ating and improving logical reasoning with abun-dant human-written reasoning chains,” arXiv preprint arXiv:2410.09207 , 2024. [188] B. Bohnet, A. Nova et al. , “Exploring and benchmarking the planning capabilities of large language models,” 

arXiv preprint arXiv:2406.13094 , 2024. [189] H. S. Zheng, S. Mishra et al. , “Natural plan: Benchmark-ing llms on natural language planning,” arXiv preprint arXiv:2406.04520 , 2024. [190] X. Wang, Z. Wang et al. , “Mint: Evaluating llms in multi-turn interaction with tools and language feedback,” 2024. [Online]. Available: https://arxiv.org/abs/2309.10691 [191] M. Renze and E. Guven, “The benefits of a concise chain of thought on problem-solving in large language models,” in 2024 2nd International Conference on Foundation and Large Language Models (FLLM) .IEEE, Nov. 2024, p. 476–483. [Online]. Available: http://dx.doi.org/10.1109/FLLM63129.2024.10852493 [192] F. Liu, N. AlDahoul et al. , “Self-reflection makes large language models safer, less biased, and ideologically neutral,” 2025. [Online]. Available: https://arxiv.org/abs/ 2406.10400 JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 20 

[193] J. Huang, X. Chen et al. , “Large language models cannot self-correct reasoning yet,” 2024. [Online]. Available: https://arxiv.org/abs/2310.01798 [194] W. Zhang, Y. Shen et al. , “Self-contrast: Better reflection through inconsistent solving perspectives,” 2024. [Online]. Available: https://arxiv.org/abs/2401.02009 [195] D. Hendrycks, S. Basart et al. , “Measuring coding challenge competence with apps,” 2021. [Online]. Available: https://arxiv.org/abs/2105.09938 [196] N. Jain, K. Han et al. , “Livecodebench: Holistic and contamination free evaluation of large language models for code,” 2024. [Online]. Available: https: //arxiv.org/abs/2403.07974 [197] W.-L. Chiang, L. Zheng et al. , “Chatbot arena: An open platform for evaluating llms by human preference,” 2024. [198] M. Zhuge, C. Zhao, D. Ashley et al. , “Agent-as-a-judge: Evaluate agents with agents,” arXiv preprint arXiv:2410.10934 , 2024. [Online]. Available: https://arxiv.org/abs/2410.10934 [199] Asterisk, “Deepclaude: Combining deepseek r1’s reasoning with claude’s creativity and code generation,” 2025, accessed: 2025-04-27. [Online]. Available: https://deepclaude.com/ [200] W. Chen, Y. Su et al. , “Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors,” 2023. [Online]. Available: https://arxiv.org/abs/2308. 10848 [201] C. Chen, X. Hao et al. , “Acebench: Who wins the match point in tool usage?” 2025. [Online]. Available: https://arxiv.org/abs/2501.12851 APPENDIX ATABLES OF TAXONOMY 

To assist researchers in efficiently and accurately selecting appropriate evaluation benchmarks for various types of AI agents, we have systematically organized the majority of the categories identified in our taxonomy into tables. Specifically, for each benchmark category, we extract and summarize the most valuable attributes that characterize its intrinsic properties and suitability for different evaluation scenarios. These attributes were carefully selected to guide researchers in clearly understanding the strengths and limitations of each benchmark, thus providing structured guidance for evaluating AI agents and benchmark selection. Below, we present the summarized tables for each benchmark category: 

A. Coding Environments 

We categorize benchmarks of coding environments in Table I. The columns in Table I have the following meanings:  

> •

Code Environments/ Datasets : Refers to the specific benchmarks or datasets used to evaluate AI agents or language models on coding-related tasks.  

> •

Static/ Interactive : Specifically classifies each benchmark as either static (isolated tasks without ongoing interaction) or interactive (tasks requiring continuous engagement with an environment or repository).  

> •

Tasks : Describes the primary coding tasks evaluated by each benchmark, such as code generation, defect detection, retrieval, summarization, or software engineering tasks.  

> •

Evaluation Metric : Lists the metrics used to quantify model performance, including common metrics like Passk, Accuracy, Resolved Rate, Edit Similarity, and specialized indicators like Method/Field Dependencies.  

> •

Evaluation Dimensions : Clarifies the specific aspects or dimensions being assessed by each benchmark, for example, correctness of code, independency from context, accuracy in defect detection, or the overall completion quality of tasks. 

B. Web Environments 

We categorize benchmarks of web environments in Table II. The columns in Table II have the following meanings:  

> •

Web Environment : Identifies the specific benchmarks or datasets designed for evaluating AI agents in web-based tasks or scenarios.  

> •

Realism : Describes how closely the evaluation environment replicates real-world conditions, ranging from synthetic (highly controlled or artificial), semi-real (partially realistic), to fully real environments.  

> •

Offline/Online : Specifies whether the benchmark tasks are conducted offline (pre-recorded, simulated data or snapshot replay) or online (live, interactive connections with dynamic web content).  

> •

Metrics : Lists the evaluation metrics used to quantify agent performance, including task success rate, trajectory likelihood, element accuracy, operation success, and other task-specific indicators. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 21 

Table I: Code Benchmarks and Their Taxonomy 

Code Environments/-Datasets 

Static/Interactive Tasks Evaluation Metric Evaluation Dimensions 

HumanEval[13] Static Code generation from docstring Pass@k Code correctness Classeval[14] Static Class-level code generation Pass@k, Method/Field Dependencies Code correctness, Independency to the contexts Yuan et al. [16] Static Defect detection, Clone detection, Assert generation, Code summarization Accuracy, F1 score, EM(Exact Match) Correctness of detection, assert and summarization CodeCriticBench[17] Static Code generation, Code QA Accuracy, MSE, Pass@1 Code correctness, Code evaluation accuracy CRUXEval[18] Static Code input and output prediction Pass@k Code prediction correctness BigCodeBench[19] Static Code generation with tools Pass@k Code correctness ARCADE[20] Static Code generation in interactive data science notebooks Pass@k Code correctness SWE-bench[21] Interactive Code generation for resolving the issues in the repository Resolved Rate Correctness of patch files Swe-bench M[22] Interactive Code generation for resolving the issues in the repository with images Resolved Rate, Aver-age cost Correctness of patch files SWE-bench-java-verified[24] Interactive Code generation for resolving the issues in the repository Resolved Rate Correctness of patch files Swe arena[25] Interactive Code generation for resolving the issues in the repository Resolved Rate Correctness of patch files Swe-bench+[27] Interactive Code generation for resolving the issues in the repository Resolved Rate, Aver-age Cost Correctness of patch files RepoBench[28] Interactive Code retrieval and code completion on code repositories Accuracy, Exact Match, Edit Similarity Accuracy of code retrieval, Accu-racy of code completion SWT-Bench[29] Interactive Code generation for resolving the issues in the repository Success Rate, Line Coverage, Success rate, Change coverage, Patch well-Formedness DevEval[31] Interactive Software design, environment setup, imple-mentation and testing LLM-as-a-judge, Suc-cess/Pass Rate General principles and faithfulness for software design, Correctness for environment setup, Correctness of implmentation, Correctness and code coverage of testing ML-Bench[32] Interactive Machine learning tasks like package in-stalling, model code generation Pass@k Completion of machine learning tasks Pybench[33] Interactive Python code generation for Chart Analysis, Text Analysis, Image & Audio Editing, Complex Math and Software & Website Development Pass Rate, Average Turns, LLM-as-a-judge Completion of python related tasks  

> •

Evaluator : Indicates how evaluation results are obtained, whether through human annotators, automated systems, or combined human and automated evaluation methods, including similarity to LLM-generated judgments.  

> •

Input Modalities : Clarifies the types of input data provided to the agents, typically involving textual descriptions and images, or in some cases video and additional media.  

> •

Main Challenges : Summarizes the key difficulties or unique features presented by each benchmark, providing insight into the specific aspects that make these web-based evaluations challenging and relevant for assessing AI agents. These columns collectively help researchers quickly under-stand the scope, complexity, realism, and evaluation approach of each web benchmark, guiding the selection of appropriate benchmarks for assessing web-based AI agents. 

C. OS Environments 

We categorize benchmarks of OS environments in Table III. The columns in Table III have the following meanings:  

> •

Tasks : Lists the main types of tasks the AI agents are expected to perform within these OS benchmarks, such as office-related activities, system operations, web browsing, coding tasks, and daily workflow tasks.  

> •

Observation Type : Details the format and type of in-formation provided to the agent for decision-making or interactions, such as screenshots, accessibility trees (a11y-tree), screen recordings, clipboard content, metadata, or textual descriptions.  

> •

Action Space : Defines the set of actions or interfaces available to the agent, including input devices like keyboards and mice, APIs, or specialized GUI automation libraries (e.g., PyAutoGUI).  

> •

Platform : Indicates the specific operating systems or plat-forms supported by each benchmark, such as Ubuntu, Win-dows, Linux, macOS, or simulated desktop environments.  

> •

Evaluation Method : Describes the approach used to measure agent performance, including predefined scripts for automatic success rate calculation, action sequence matching, human evaluation, exact or fuzzy output matching, and penalties for inaccurate behaviors. Collectively, these columns clearly outline the characteristics, operational details, and evaluation strategies of OS benchmarks, guiding researchers in selecting suitable platforms for assessing AI agents within realistic operating system contexts. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 22 

Table II: Web Benchmarks and Their Taxonomy 

Web Environment Realism Offline/Online Metrics Evaluator Input Modalities Main Challenges 

MiniWoB [34] Synthetic Offline Task Success Rate Human labels + Auto-mated evaluation Text + Images A reproducible offline RL benchmark for open-domain web tasks. FormWoB [34] Semi-real Offline Average Reward Human labels + Auto-mated evaluation Text + Images Same as above. QAWoB [34] Semi-real Offline Test Trajectory Likelihood Human labels + Auto-mated evaluation Text + Images Same as above. MiniWoB++ [35] Synthetic Offline Task Success Rate Human labels + Auto-mated evaluation Text + Images Added longer sequences, random lay-outs, and soft-text reasoning tasks. WebShop [36] Semi-real Offline Independent scoring on at-tributes/options/price/type Human labels + Auto-mated evaluation Text + Images Requires agents to complete full pur-chase flow mind2web [37] Real (snapshot replay) Offline Element Accuracy, Opera-tion F1 Human labels + Auto-mated evaluation Text + Images Very large number of web pages. webvoyage [38] Real (live con-nection) Online Human vs GPT -4V Agree-ment Human +LLM joint evalu-ation Text + Images Online real environment judged directly by humans. WEBLINX [39] Real (snapshot replay) Offline IoU, URL-F1 Human labels + Auto-mated evaluation Text + Images Introduces “conversational web naviga-tion” webarena [40] Semi-real Offline Intermediate page states Human labels + Auto-mated evaluation Text + Images Self -hosted multi -site environment that closely mirrors real websites. visualwebarena [41] Semi-real Offline Task Success Rate Human labels +LLM-based similarity Text + Images Visual - dependent tasks: “see then do” scenarios. WorkArena [42] Real (live con-nection) Online Task Success Rate Fully automated by real web Text + Images Runs on official ServiceNow developer instance; fully realistic. WorkArena++ [43] Real (live con-nection) Online Task Success Rate Fully automated by real web Text + Images Evaluates “compositional planning and reasoning.” MMInA [44] Real (live con-nection) Online Hop Success Rate Human labels +LLM-based similarity Text + Images Multi -hop (cross -site chain operations). AssistantBench [45] Real (live con-nection) Online Task Success Rate Human labels +LLM-based similarity Text + Images Time -consuming, multi -step workflows. WebCanvas [46] Real Online Task Success Rate Human labels + Auto-mated evaluation Text + Images Continuously updated with robust inter-mediate checks. ST-WebAgentBench [47] Semi-real Online Completion -under -Policies, Risk Ratio Human labels + Auto-mated evaluation Text + Images Policy compliance focus under risk con-straints. VideoWebArena [48] Semi-real Online Video QA Accuracy Human labels + Auto-mated evaluation Video + Text +Images Long -video + web interaction (Skill vs Factual tasks). TUR[K]INGBENCH [49] Semi-real Offline ROUGE-L, IoU Human labels + Auto-mated evaluation Text + Images Natural HTML pages originally de-signed for crowdsourcing. BEARCUBS [50] Real Online error rate Human labels + Auto-mated evaluation Video + Text +Images Fine -grained answer labeling with live multimedia items. THEAGENTCOMPANY [ 51 ] Semi-real Offline Full-Completion Score Human + Automated +LLM similarity Text + Images Performing real company tasks. WABER [52] Semi-real Online Reliability, Efficiency Human labels + Auto-mated evaluation Text + Images Injected failures + efficiency benchmark-ing. VisualAgentBench [53] Semi-real Offline Task Success Rate Human labels + Auto-mated evaluation Text + Images Multi -domain, multi -scenario LMM vi-sual agent benchmark. Online-Mind2Web [54] Real (live con-nection) Online Task Success Rate Human labels +LLM-based similarity Text + Images Online adaptation of Mind2Web. REAL [55] Semi-real Offline Rubric -Guided Retrieval Score Human labels +LLM/Rubric similarity Text + Images Tests both robustness and reliability simultaneously. OmniAct [56] Real Offline Sequence Score, Action Score Human labels + Auto-mated evaluation Text + Images Desktop + web automation with De-tACT module. ChatShop [57] Semi-real Offline RL Learning Curves (Chat-ShopBin only) Human labels + Auto-mated evaluation Text + Images Ambiguous initial info requiring strate-gic questioning. 

D. Mobile Environments 

We categorize benchmarks of mobile environments in Ta-ble IV. The columns in Table IV have the following meanings: 

• Static/Executable : Indicates whether the benchmark consists of static scenarios (tasks based on pre-defined and static interfaces or screens) or executable scenarios (dynamic, real-time interactions with mobile applications). 

• Tasks : Specifies the core task evaluated by each benchmark, predominantly focusing on instruction-following capabilities within mobile interfaces. 

• Observation Type : Details the types of visual or textual information provided to the agent, such as screenshots, text view hierarchies, screen representations, HTML/XML content, and additional marks or metadata. 

• Action Space Type : Defines the range of actions available to the agent, typically involving screen operations (e.g., tapping, swiping) and, in some benchmarks, macro functions or API calls. 

• Platform : Denotes the mobile operating system for which the benchmark is intended, predominantly Android. 

• Evaluation Method : Describes how performance is mea-sured, using methods such as action sequence matching, LLM-assisted automatic judgments, rule-based evaluation, self-developed success detection, human validation, or UI state matching. These columns collectively provide a clear overview of mobile benchmarks, highlighting how tasks are structured, how agent observations are presented, and how agent actions and performance are evaluated, thus guiding researchers in selecting appropriate mobile environment evaluation for AI agents. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 23 

Table III: OS Benchmarks and Their Taxonomy                                       

> Benchmark Tasks Observation Action Space Platform Evaluation Method
> OSWorld[58] Office tasks, OS tasks, Daily tasks, Workflow tasks and Professional tasks Screenshots, a11y-tree Keyboard, Mouse Ubuntu Predefined post-processing scripts to calculate success rate WindowsAgentArena[59] Office, Web browsing, Windows system, Coding, Media & Video, Windows utilities Task instruction, Clipboard content, Metadata of the current session, Represen-tation of the current screen Keyboard, Mouse Windows Predefined post-processing scripts to calculate success rate AgentStudio[60] Office tasks, OS tasks, Daily tasks and so on Screen recording, Screen-shots, Code execution out-puts Keyboard, Mouse, APIs/Tools Linux, (Windows, macOS) User-defined functional auto-evaluators based on reward and language feedback OmniACT[61] Shopping tasks, Entertain-ment tasks, Service tasks, Government tasks, Travel tasks, Health tasks Screenshots Actions in PyAu-toGUI library Linux, Windows, macOS Action sequences match-ing, Inaccurate behavior penalty OfficeBench[62] Office tasks, like Word, Ex-cel, PDF, Email and so on Text description of current state and previous outputs of actions APIs Simulated desk-top Exact matching and fuzzy matching on the final out-puts, Scripts to check the final state PC-Eval[63] Commom tasks on Chrome, Word, Excel and so on Screenshots, Operation his-tories Keyboard, Mouse, APIs Windows Human evaluation on suc-cess rate and subtask suc-cess rate

E. Scientific Environments 

We categorize benchmarks of scientific environments in Table V. The columns in Table V have the following meanings:  

> •

Static/Interactive : Indicates whether the benchmark involves static scenarios (predefined, non-interactive tasks) or in-teractive environments (requiring agents to perform tasks dynamically, often in simulated or real workspaces).  

> •

Environment : Describes the context or type of environment in which the benchmark tasks are set, such as research papers, scientific article pools, survey data, or simulated workspaces.  

> •

Evaluation Metric : Specifies the criteria and metrics used to assess agent performance, such as accuracy, F1 score, precision, recall, coverage, retrieval probability, area under the curve (AUC), success rate, error rate, and other task-specific indicators.  

> •

Tasks : Details the main scientific or research-oriented tasks evaluated by each benchmark, including scientific question answering, research paper summarization, feedback gener-ation, equation inference, data modeling, code generation, data analysis, and full-cycle scientific discovery. Together, these columns provide a structured overview of scientific benchmarks, clarifying their operational format, application context, evaluation methodologies, and task focus, to guide researchers in choosing appropriate tools for assessing AI agents in scientific research scenarios. 

F. Game Environments 

We categorize benchmarks of game environments in Table VI. The columns in Table VI have the following meanings:  

> •

Games : Specifies the types of games or specific game titles included within each benchmark, ranging from simple puzzles and board games to complex, strategic, or multiplayer games.  

> •

Single-agent/Multi-agent : Indicates whether the bench-mark focuses on evaluating single-agent scenarios (where an agent operates independently) or multi-agent scenarios (involving collaboration, competition, or interaction among multiple agents).  

> •

Needed Agent Capabilities : Describes the key cognitive and behavioral abilities required from agents to succeed in the respective benchmark. These include skills such as reasoning, planning, decision-making, exploration, handling hidden information, cooperation, communication, and various forms of complex or multi-hop reasoning. Together, these columns provide a comprehensive overview of game benchmarks, clarifying the context, interaction modal-ity, and agent competencies needed, thereby assisting re-searchers in selecting appropriate gaming environments for evaluating diverse agent capabilities. 

G. Planning Capability 

We categorize benchmarks of planning capabilities in Ta-ble VII. The columns in Table VII have the following meanings:  

> •

Domain Category : Specifies the main subject area or reasoning type for each benchmark, such as mathematical reasoning, document navigation, or scientific knowledge.  

> •

Task Type : Describes the core task or problem-solving activity assessed by the benchmark, for example, algebraic word problem solving, multi-hop question answering, or implicit inference.  

> •

Format : Indicates the format in which the benchmark is presented, most commonly as a static dataset containing predefined problems and questions.  

> •

Key Focus : Summarizes the main evaluation focus of the benchmark, highlighting aspects such as rationale generation, multi-step reasoning, performance on simple or complex variations, and the incorporation of charts or images. 

H. Interaction Capability JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 24 

Table IV: Mobile Benchmarks and Their Taxonomy 

Benchmark Static/Executable Tasks Observation Type Action Space Type Platform Evaluation Method 

PixelHelp[64] Static Instructions following Text view hierarchy Screen operations Android Action sequences match UGIF[65] Static Instructions following Text view hierarchy Screen operations + Macro Func-tions Android Action sequences match MoTIF[66] Static Instruction following and in-struction feasible classification Screenshots + Text view hierarchy Screen operations Android Calculate F1-score on the binary classification problem AITW[67] Static Instructions following Screenshots + Screen representation Screen operations Android Action sequences match + Human validation ANDROIDCONTROL[ 68 ] Static Instructions following Screenshots + Text view hierarchy Screen operations + Macro functions Android Action sequences match AMEX[69] Static Instructions following Screenshots + Screen representation (description, element grounding) Screen operations Android Action sequences match B-MOCA[70] Executable Instructions following Screenshots + Text view hierarchy Screen operations Android Self-developed success detector with ADB and Appium Androidworld[71] Executable Instructions following Screenshots + Text view hierarchy Screen operations + APIs Android Inferring task success with ADB Mobile-bench[72] Executable Instructions following HTML converted from XML Screen operations + APIs Android Checkpoints + GPT4 evaluate task completion status A3[73] Executable Instructions following Screenshots + Text view hierarchy Screen operations Android Task-specific evaluation function (element matching, action match-ing) + LLM Evaluation System Mobile-Env[74] Executable Instructions following Screenshots + Text view hierarchy + Set-of-Marks Screen operations Android Evaluation from screen text, VH, system log and RHU AndroidArena[75] Executable Cross-APP instructions follow-ing Screenshots + Text view hierarchy Screen operations + APP-level ac-tions + System-level actions Android Action sequences match + GPT4 judgement on success MobileSafetyBench[76] Executable Safety-related instructions fol-lowing (e.g. Finance, Device/-Data Management) Screenshots + Text view hierarchy Screen operations Android Rule-based evaluators on goal achievement and harm prevention MobileAgentBench[77] Executable Instructions following Screenshots + Text view hierarchy Screen operations Android Final UI state matching with self-developed app SPA-BENCH[78] Executable Single-APP and Cross-APP in-structions following Screenshots + Text view hierarchy Screen operations Android Key components matching +MLLM evaluation Llamatouch[79] Executable Instructions following Screenshots + Text view hierarchy Screen operations Android Evaluating on whether the agent traverses all manually annotated application/system state Androidlab[80] Executable Instructions following Screenshots + Text view hierarchy Screen operations Android Evaluate on the completion of sub-goals AutoEval[81] Executable Instructions following Screenshots + Text view hierarchy Screen operations Android LLM-based automatic judge system, including Capturer, Reasoner and Checker 

1) Static Interaction : We categorize benchmarks of static interaction capabilities in Table VIII and Table IX. The columns in these tables have the following meanings: 

• Task Domain : Specifies the application areas or domains each benchmark covers, such as daily life tasks, machine learning tasks, or mobile environment tasks. 

• Task Type : Describes the structure and complexity of the evaluated tasks, including single-turn or multi-turn API calls, chained or parallel functions, and multi-step tool use. 

• API Data Source and Authenticity : Details the origins and authenticity of the APIs or tools involved, indicating whether they are real, public, synthetic, handwritten, or from official repositories like RapidAPI or PyTorch Hub. 

• Instruction Data Source and Authenticity : Explains how the instructional or annotation data is generated for the benchmark, including whether it is LLM-generated, self-instructed, human-annotated, or derived from synthetic or real-world projects. 

• Metric : Specifies the quantitative criteria or metrics used to evaluate agent performance, such as procedure accuracy, response accuracy, or hallucination rate. 

• Evaluation Label and Method : Describes the way performance is measured and labeled, for example, by API name matching combined with automated evaluation procedures. 

• Input Modality : Indicates the type of input data provided to the agent, such as text. 

• Main Challenges : Summarizes the key difficulties or unique features faced in each benchmark, such as gener-alizing to unseen APIs or handling large, dynamic, and overlapping API sets. 

2) Interaction with Humans : We categorize benchmarks of interaction with human capabilities in Table X. The columns in the table have the following meanings: 

• Dialogue Task Type : Describes the type or domain of dialogue tasks evaluated, such as specific user intents or multi-domain dialogues. 

• Dialogue Data Source : Indicates how the dialogue data is collected or generated, e.g., via simulated users, crowd-sourcing, or automated processes. 

• Metrics : Specifies the criteria or metrics used for evalua-tion, including task accuracy, action state tracking, belief tracking, API correctness, or dialogue success. 

• Labeling and Similarity Evaluation : Explains the method for labeling responses and assessing similarity, which may involve human annotation, automated methods, or manual assessment. 

• Modality : Indicates the type of input or interaction modality used, such as text. 

• Explicit Interaction Evaluation Beyond Task Accuracy :States whether the benchmark includes explicit evaluation of interaction quality in addition to basic task accuracy (typically yes/no). 

3) Interaction with Other Agents : We categorize bench-marks of capabilities of interaction with other agents in JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 25 

Table V: Scientific Benchmarks and Their Taxonomy 

Scientific Benchmarks Static/ Interactive Environment Evaluation Metric Tasks 

ARC [82] Static None Accuracy Scientific question answering SCIENCEQA [83] Static None Accuracy Multimodal scientific question answering QASPER [84] Static None Span-level F1 score Research paper information seeking and answer anchoring QASA [85] Static None Precision, recall, F1 score, ROUGE scores Question answering on scientific articles SWIFT [86] Static None Specificity, actionability, reading comprehension, overall helpfulness Feedback generation for scientific writing AAAR-1.0 [87] Static None Equation inference: Accuracy; Other tasks: human expert ratings Research assistance: equation inference, paper weakness, experiment design, review critique 

M S 2 [88] Static None ∆EI Multi-document summarization of medical studies LAB-Bench [89] Static None Accuracy, precision, coverage Multi-choice questions on biology research ResearchArena [90] Static Academic and survey papers Information retrieval: recall, precision; Information selection: nDCG, MRR; Information Organization: Heading Entity Recall, Heading Soft Recall Academic surveying [91] Static Research paper pool Novelty, excitement, feasibility, expected effectiveness Research idea generation PaperQA [92] Static Scientific article pool Retrieval AUC, retrieval probability Scientific article question answering SCIENCEWORLD [93] Interactive Simulated text environment Task completion rate, step success rate, average completion steps, error rate, learning curve Elementary-level scientific reasoning DISCOVERYWORLD [94] Interactive Simulated text environment Task completion rate, task-relevant actions taken, discovered explanatory knowledge Perform complete cycles of novel scientific discovery ScienceAgentBench [95] Interactive Workspace Valid execution rate, success rate, codeBERTScore, API Cost Data-driven scientific discovery MLGym [96] Interactive Workspace Area Under the Performance Profile (AUP) score Realistic research workflows MLAgentBench [97] Interactive Workspace Success rate, consumed tokens and time Machine learning experimentation tasks DSBench [98] Interactive Workspace Success rate, relative performance gap Complex data science tasks DSEval [99] Interactive Workspace Accuracy, relative performance gap Data analysis tasks and data modeling tasks throughout the entire data science lifecycle DA-Code [100] Interactive Workspace Accuracy Data science code generation ML-Bench [32] Interactive Workspace Pass@k, success rate Repository-level code understanding and execution tasks Spider2-V [101] Interactive Workspace Success rate Automate data science and engineering workflows 

Table XI. The columns in the table have the following meanings: 

• Interaction Paradigm : Specifies the nature of interactions among agents, such as cooperation, competition, or both. 

• Modality : Indicates the format or channel of agent interactions, typically text-based. 

• Metrics : Describes the performance metrics used for evaluation, such as rule understanding, collaboration quality, task completion, score, or resource efficiency. 

• Application Domain : States the primary domain or scenario for the benchmark, such as games, collaborative programming, social tasks, or resource management. 

• Agent Architecture : Indicates whether the agents in the benchmark are homogeneous (identical) or heterogeneous (diverse in capability or function). 

• Key Contributions : Summarizes the unique features, innovations, or evaluation focuses of each benchmark, such as multi-level difficulty, process-oriented metrics, simulation of social intelligence, or evaluation of planning and collaboration abilities. 

I. Memory Capability 

We categorize benchmarks of the memory capability in Table XII. The columns in the table have the following meanings: 

• Data Source : Specifies the origins of the data used in each benchmark, such as books, scripts, real meeting transcripts, synthetic conversations, or manually created questions. 

• Task Domain : Describes the primary subject area or sce-nario being evaluated, including narrative understanding, product design, academic discussions, social dialogue, or programming. 

• Task Type : Outlines the main task structure, such as question answering (QA), query-based summarization, event summarization, memory synthesis, classification, or online streaming learning. 

• Memory Type : Indicates the type of memory evaluated, such as long-range, causal, temporal-spatial, semantic, episodic, foresight, multimodal, dynamic update, or incre-mental improvement. 

• Metrics : Lists the evaluation metrics used to assess performance, for example, BLEU, ROUGE, accuracy, F1 score, NDCG@k, recall@k, MMRelevance, MAP, and hallucination rate. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 26 

Table VI: Game Benchmarks and Their Taxonomy 

Benchmark Games Single-agent/Multi-agent Needed Agent Capabilities 

BALROG[103] BabyAI, TextWorld, Crafter, Baba Is AI, MiniHack, NLE Single-agent Navigation, Exploration, Resource Management, Complex Credit Assignment, Deducing Env. Dynam-ics, Long-term Planning SmartPlay[104] Bandits, Rock-Paper-Scissors, Hanoi, Messenger, Crafter, Minecraft Single-agent Long Text Understanding, Multi-hop Reasoning, In-struction/Rule Following, Planning, Generalization, Understanding the Odds, Learning from Interactions, Error/Mistake Handling, Spatial Reasoning LVLM-Playground [105] TicTacToe, Reversi, Sudoku, Minesweeper, Gomoku, Chess Single-agent Perception, Reasoning, Decision, Adversary VGRP-Bench[106] Binairo, Star-Battle, Colored-Sudoku, Killer-Sudoku (20 visual reasoning puzzles) Single-agent Reasoning, Rule-following, Overall Perception, Cell-level Perception ING-VP[107] Sokoban, Maze, Sudoku, 8-queens, Tower of Hanoi, 15-puzzle Single-agent Perception, Reasoning, Planning, Text Understanding, Spatial Imagination DSGBench[108] Starcraft II, Civilization, Diplomacy, Stratego, Street Fighter III, Were-wolf Single-agent Strategic Planning, Real-Time Decision Making, Adaptive Learning, Social Reasoning, Team Collabo-ration Gamebench[109] Air, Land, Sea, Arctic Scavenger, Are You the Traitor?, Codenames, Hive, Pit, Santorini, Two Rooms and a Boom, Sea Battle Multi-agent Reasoning capabilities on Abstract Strategy, Non-Deterministic, Hidden Information, Language Com-munication, Social Deduction and Cooperation 

γ-Bench[110] Guess 2/3 of the Average, El Farol Bar, Divide the Dollar, Pub-lic Goods Game, Diner’s Dilemma, Sealed-Bid Auction, Battle Royale, Pirate Game Multi-agent Reasoning capabilities on game theoretic environ-ments Gtbench[111] Tic-Tac-Toe, Connect-4, Kuhn Poker, Breakthrough, Liar’s Dice, Blind Auction, Negotiation, Nim, Pig, Iterated Prisoner’s Dilemma Multi-agent Board Strategy, Bids, Collaboration, Bluff, Math GameArena[112] Akinator, Taboo, Bluffing Multi-agent Deductive reasoning, Abductive reasoning, Inductive reasoning, Multi-hop reasoning JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 27 

Table VII: Planning Benchmarks and Their Taxonomy 

Benchmark Domain Category Task Type Format Key Focus 

AQUA-RAT [179] Mathematical Reasoning Algebraic word problem solving with natural-language rationales Static dataset Evaluating generated rationales in algebra problems GSM8K [180] Mathematical Reasoning Elementary arithmetic word problem solving (CoT) Static dataset Evaluating CoT performance on basic arith-metic problems MATH [181] Mathematical Reasoning Competition-level math contest problem solving Static dataset Evaluating multi-step reasoning on challeng-ing competition problems Game of 24 [182] Mathematical Reasoning Cumulative arithmetic game problem solving Static dataset Evaluating cumulative multi-step arithmetic reasoning to reach a target value SVAMP [183] Mathematical Reasoning Math word problem solv-ing with simple variations Static dataset Evaluating brittleness of SOTA models under simple textual variations HotpotQA [184] Document Navigation Multi-hop question an-swering Static dataset Evaluating cross-document multi-hop reason-ing StrategyQA [185] Document Navigation Implicit multi-hop infer-ence Static dataset Evaluating implicit reasoning for strategy questions ScienceQA [83] Science Knowledge Multimodal multiple-choice Static dataset Incorporating charts and images ARC [82] Science Knowledge Text-based multiple-choice Static dataset Purely textual science questions including both Easy and Challenge subsets FOLIO [186] Logic Inference Natural-language reason-ing with first-order logic annotations Static dataset Evaluating NL reasoning and NL →FOL translation with formal proof verification P-FOLIO [187] Logic Inference Step-by-step annotated natural-language proofs for FOL reasoning Static dataset Assessing and improving multi-step logical reasoning in LLMs through human-written proofs PlanBench [113] Classical Planning Planning in canonical IPC domains (Blocksworld & Logistics) PDDL (Static) Evaluating eight core planning capabilities (generation, optimality, verification, execu-tion reasoning, robustness, reuse, replanning, generalization) On Planning Abilities of LLMs [114] Classical Planning Autonomous plan genera-tion and heuristic-guided planning PDDL + LLM prompts Evaluating LLM’s autonomous planning suc-cess and its use as heuristic guidance for external planners AutoPlanBench [115] Classical Planning Automated NL prompt generation from PDDL do-mains NL prompts(Static) Comparing auto-generated vs. manual and template-based NL prompts; enabling large-scale PDDL planning evaluation Exploring Planning Capabilities [188] Planning Evaluation PDDL planning (BlocksWorld, Logistics, Mini-Grid); NL planning (Trip Planning, Calendar Scheduling) PDDL & NL prompts (Static) Automated instance generation; many-shot in-context learning; chain-of-thought reason-ing; fine-tuning; out-of-distribution general-ization; failure mode analysis ACPBench [116] Planning Evaluation Seven atomic planning rea-soning tasks: Applicability, Progression, Reachability, Validation, Action Reach-ability, Justification, Land-mark Detection PDDL-based NL prompts (Static) Benchmarking core planning skills via au-tomatically synthesized Boolean and MCQ questions across 13 domains NATURALPLAN [189] Natural-Language Planning Real-world NL planning tasks: Trip Planning, Meet-ing Planning, Calendar Scheduling Static dataset with in-context tool outputs Evaluating LLMs on realistic NL planning: complexity variation; self-correction; few-shot generalization; long-context plannin FlowBench [118] Workflow Planning Workflow-guided planning tasks across 51 scenarios in 6 domains Static prompts with work-flow knowledge in text, code, and flowchart for-mats Formalizing diverse workflow knowledge formats; multi-tiered evaluation (static turn-level and dynamic session-level); compar-ative analysis of LLM performance across formats WorFBench [117] Workflow Planning Graph-structured workflow generation across four scenarios: problem-solving, function calling, embodied planning, open-grounded planning Static dataset (task descrip-tions + action lists) Multi-scenario DAG workflow generation; evaluation via WORFEVAL’s subsequence and subgraph matching; analyzing sequence vs graph planning gap; assessing generaliza-tion and downstream task enhancement MINT [190] Interactive Planning Multi-turn interactive tool-augmented task solving with natural language feed-back Simulated online environ-ment (tool execution +LLM-based user feedback) Quantifying per-turn gains from tool use and language feedback; analyzing effects of in-struction fine-tuning on multi-turn reasoning REALM-Bench [119] Interactive Planning Single- and multi-agent real-world scenario plan-ning with dynamic environ-ment feedback Simulator-based interactive environment Evaluating adaptive planning under paral-lelism, complex dependencies, and unex-pected disruptions JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 28 

Table VIII: Tool Benchmark and Their Taxonomy 1 

Benchmark Task Domain Task Type API Data Source and Authen-ticity Instruction Data Source and Authenticity 

ToolAlpaca [ 125 ] Multiple daily life domains Single-turn API call, Multi-turn API call Public APIs repository on GitHub LLMs (e.g., GPT-3.5) auto-generated tool usage dialogues APIBench [126] Broad machine learning tasks Single-turn API call PyTorch Hub, HuggingFace Of-ficial GPT-4 self-instructed data synthesis ToolBench [127] Multiple daily life domains Single-turn API call, Multi-turn chained API calls RapidAPI Hub DFSDT algorithm on ChatGPT for searching and annotating BFCL [128] Multiple daily life domains Simple Function, Multiple Functions, Parallel Functions Real usable network APIs and hand-written functions with ex-ecutable feasibility Automatically generated synthetic data ToolSandbox [ 129 ] Mobile environ-ment tasks Single/Multiple Tool Call, Sin-gle/Multiple User Turn Real usable RapidAPI inter-faces and local functions GPT-4o simulated dialogue and execution paths Seal-Tools [130] Multiple daily life domains Single-tool, multiple-tool, par-allel calls, and nested calls Real usable RapidAPI inter-faces Tool definition and example calls are syn-thetic API-Bank [131] Multiple daily life domains Single call, Retrieval+Call, Plan+Retrieval+Call Hand-implemented and de-ployed by engineering teams in the same framework Annotated by multiple computer science students through a discussion-based method NexusRaven [ 132 ] Multiple daily life domains Single call, parallel call, nest-ed/composite call Handwritten but verified for multi-turn executable feasibil-ity Synthetic generated, but from real open-source projects API-Blend [133] Multiple daily life domains Single API recognition, param-eter extraction, multi-step API calls LLM-assisted generated syn-thetic examples 5 real or high-quality open-source bench-marks RestBench [134] TMDB movie database and Spotify music player Multi-step API calls From TMDB and Spotify of-ficial OpenAPI documentation, real and callable 6 NLP experts designed from bottom-up using real-world usage scenarios APIGen [135] Multiple daily life domains Simple Function, Multiple Functions, Parallel Functions, Parallel Multiple Functions RapidAPI Hub real APIs Self-guided synthesis based on models like DeepSeek-V2-Chat, Mixtral StableTool Bench [136] Multiple daily life domains Single-turn API calls, Multi-turn API chain calls From ToolBench published data, responses will consider GPT-generated API responses Human-designed real-world instruction set ComplexFunc Bench [137] Booking.com-related APIs Constraint-based multi-step calls Synthetic generated and human-annotated through multi-turn validation Human-designed real-world instruction set NESTFUL [138] Mathematical computation tools, general programming functions Single-turn Multi-step, parame-ter filling Real executable production-level functions Manually constructed and filtered from real datasets ToolEyes [139] Seven high-demand real-world scenarios Multi-tool calls and multi-step processes Follows OpenAPI/JSON speci-fication HTTP API LangChain official repository (GitHub), Ser-pAPI platform, OpenAI documentation PEToolBench [ 140 ] Personalized tool calling tasks Instruction calls with user his-torical data Real RESTful APIs from Rap-idAPI LLM self-synthesis and multi-turn filtering, non-real user data ACEBench [201] Multiple daily life domains multi-step call, in-complete in-struction following, multi-agent collaboration Synthetic APIs referenced by Real APIs Synthetic instruction data JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 29 

Table IX: Tool Benchmark and Their Taxonomy 2 

Benchmark Metric Evaluation Label and Method Input Modality Main Challenges 

ToolAlpaca [ 125 ] Procedure accuracy, Re-sponse accuracy Label by API name + au-tomated evaluation Text Generalizing to unseen real APIs using a compact language model trained with only 4K diverse simu-lated examples. APIBench [126] Accuracy, Hallucination Rate Label by API name + au-tomated evaluation Text Generalizing a compact LLM to accurately call un-seen APIs in a large, overlapping, and ever-changing API set. ToolBench [127] Pass Rate, Win Rate Label by ChatGPT-generated + LLM evaluation similarity Text Accurate retrieval of relevant APIs and generalization to unseen APIs and complex instructions through multi-step, depth-first decision tree strategy. BFCL‘[128] AST Evaluation, Executable Function Evaluation Label by experts + auto-mated evaluation similarity Text Generalizing to unseen functions in multi-domain, multi-language, and complex multi-step and parallel call scenarios. ToolSandbox [ 129 ] Milestone Similarity, Turn Count Milestones and Minefields provided by humans, auto-mated evaluation based on DAG similarity Text Generalizing tool call and interaction strategies in complex dialogue scenarios with state dependency and incomplete information. Seal-Tools [130] Format Accuracy, Tool Pre-cision/Recall/F1, Parame-ter Precision/Recall/F1 Label generated through LLM self-instructed multi-turn validation, automated evaluation Text Evaluating multi-layer nested calls and implicit de-pendencies, with evaluation criteria including correct parameter passing. API-Bank [131] ROUGE-L, Accuracy Label by manual annota-tors + fully automated eval-uation Text Generalizing LLMs to retrieve and call unknown tools accurately in multi-domain, multi-API pool, and hybrid information retrieval and planning scenarios, with real execution environment verification. NexusRaven [ 132 ] Nested/Composite Success Rate Label manually written + automated evaluation script Text Handling high complexity in unseen function signa-tures, multi-level nesting, and parallel calls under zero-shot conditions. API-Blend [133] API-F1, Parameter-F1, LCS-F1 Label: Dataset original an-notators or paper authors provide + automated eval-uation scripts Text Cross-domain generalization in heterogeneous data and sparse sequencing requirements. RestBench [134] Correct Path Rate, ∆ So-lution Length Label manually written + automated evaluation script Text Breaking down complex tasks into coarse-to-fine online planning while maintaining high robustness and low redundant calls during execution. APIGen [135] AST Evaluation, Executable Function Evaluation Label by experts + auto-mated evaluation similarity Text Building a pipeline for automatic data generation: “Format verification → Execution verification →

Semantic verification.” StableTool Bench [136] Solvable Pass Rate, Solv-able Win Rate Label by three LLM votes + GPT-4 evaluation Text Reducing fluctuations caused by real API downtime or changes through caching and LLM simulation, and evaluating solvable tasks with GPT-4 assessment. ComplexFunc Bench [137] Rule-Based + Response-Based Precision/Recall/F1 Label manually written + automated evaluation script Text Inferring optimal call sequences in multi-user con-straint and implicit parameter scenarios while han-dling long parameters. NESTFUL [138] F1 Func/Param, Partial Se-quence Accuracy Label manually written + automated evaluation script Text Correctly generating function names, parameters, and managing output variables in multi-layer data dependency and variable reference nested calls. ToolEyes [139] Format alignment, Intent understanding, Behavior planning, Tool selection, Answer organization with average score across five dimensions Label manually written + GPT-4 evaluation score Text Capturing LLM capabilities in format compliance, intent reasoning, behavior planning, tool matching, and answer integration at a fine-grained level. PEToolBench [ 140 ] Preference Alignment Im-provement, Tool Call Ac-curacy Label: LLM synthesized + manual writing, automated evaluation matching Text Identifying and selecting the best tool with user implicit preferences from tools with similar func-tionalities and generating precise parameters. ACEBench [201] End-to-End Accuracy, Pro-cess Accuracy Label: Automated synthe-sized + expert checking, Eval: automated Text Achieving efficient, low-cost, and fine-grained au-tomated evaluation of tool invocation in real-world scenarios involving multi-turn, multi-step interactions with incomplete instructions and multiple agents. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 30 

Table X: Human-Agent Interaction Benchmarks and Their Taxonomy 

Benchmark Dialogue Task Type Dialogue Data Source Metrics Labeling and Similarity Evaluation Modality Explicit Interaction Evaluation Beyond Task Accuracy 

τ -bench [141] Airline and retail Simulated users via agent LLM pass k Labels annotated by humans +manual similarity assessment Text No ABCD [142] 55 distinct user in-tents Agent interacting with simulated users Action State Tracking, Cascading Dialogue Success Human annotation and manual simi-larity assessment Text No MultiWOZ [ 143 ] Dialogues across multiple domains Crowd-sourced Belief tracking, dialogue act classification Labels are human-provided; evalua-tions comparing agent responses to human benchmarks Text No ALMITA [ 144 ] 192 conversations for 14 intents Combination of auto-mated generation and manual curation API correctness Human annotation and automated sim-ilarity assessment Text No IntellAgent [ 145 ]Dialogues across multiple domains Automated process generation of diverse synthetic test cases Task accuracy Human annotation and automated sim-ilarity assessment Text No 

Table XI: Multi-Agent Benchmarks and Their Taxonomy 

Benchmark Interaction Paradigm Modality Metrics Application Do-main Agent Architec-ture Key Contributions 

BattleAgent Bench [146] Cooperation and Competition Text Rule Understanding, Spatial Percep-tion, Cooperation and Competition Score Game Homogeneous Multiple difficulty levels, fine-grained evaluation MultiAgent Bench [147] Cooperation and Competition Text Task Completion, Collaboration Quality, Communication Effective-ness, Cognitive Planning Collaborative Pro-gramming, Game Simulation Homogeneous Diverse communication topologies, KPI-based evaluation of individual agent contributions Collab-Overcooked [ 148 ]Cooperation Text Progress Completeness, Initiating Capability, Responding Capability Game Homogeneous Process-oriented collaboration met-rics, assessing collaborative abilities Sotopia [149] Cooperation and Competition Text Task Completion, Social Common-sense Reasoning, Strategic Commu-nication Social Tasks Homogeneous Simulates complex social interac-tions among AI agents, evaluating social intelligence AutoArena [ 150 ] Competition Text Elo Ranking, Correlation with Hu-man Preferences Logical Reason-ing, Coding Abil-ity Homogeneous Swiss-system tournament and Elo ranking for evaluation SmartEval[151] Cooperation Text Completion Rate, Reward, Score Game Homogeneous Each game challenges different abil-ities, including reasoning, planning, spatial perception, etc. MindAgent [ 152 ] Cooperation Text Task Completion, Coordination Ef-ficiency, Instruction Understanding Game Homogeneous Requires coordination and cooper-ation with human players, demon-strating planning and execution abil-ities GOVSIM [ 153 ] Cooperation Text Resource Duration, Total Yield, Ef-ficiency Fisheries, Pastures, Pollution Management Homogeneous Simulates shared resource environ-ments, requiring agents to cooperate to prevent resource depletion 

Table XII: Memory Benchmarks and Their Taxonomy 

Benchmark Data Source Task Domain Task Type Memory Type Metrics 

NarrativeQA [154] Books and Movie Scripts+ Human-Generated QA Pairs Narrative Understanding of Books and Movies QA Long-range, Causal, Temporal-Spatial BLEU-1/4, METEOR, ROUGE-L, MRR QMSum [155] Real Meeting Transcripts +Human-Annotated Queries and Summaries Product Design, Academic, Committee discussions Query-Based Text Summariza-tion Long-range ROUGE-1/2/L QuALITY [156] English source articles+Human-Generated Question Fiction, Nonfiction, Maga-zine QA Long-range, Causal Accuracy DialSim [157] TV Show Scripts + Fan QA + LLM-Generation QA Everyday Interaction Scenes from TV Shows QA Long-range, Temporal-Spatial Accuracy LoCoMo [158] LLM Generation Dialogue with Human Verification Open-Domain Social Dia-logue QA, Event summarization, Multimodal dialogue generation Long-range, Causal, Multimodal F1 Score, Recall@K, ROGUE -1/2/L, FactScore, MMRelevance LTM-Benchmark [102] Synthetic conversation + Real Data Everyday Conversations, Cognitive and Reasoning Scenarios QA, Information Integration, Conflict Resolution Long-range, Causal, Spatial, Foresight Score LongMemEval [159] Manually created questions Personalized Long-term Dialogue Information Extraction, Multi-session Reasoning, Temporal Reasoning, Knowledge Up-dates, Abstention Long-range, Dynamic Update, Temporal-Spatial Accuracy, Recall@k, NDCG@k Episodic Memory[160] Synthetic episodic memory data Fictional Narrative Scenes QA, Temporal Reasoning, Hal-lucination Detection Temporal-Spatial, Long-term State Tracking F1-Score, Kendall’s τ Coeffi-cient, Hallucination Rate PerLTQA [161] LLM Generation with Human Verification Personal Long-term Mem-ory Simulation QA, Memory Synthesis, Clas-sification, Retrieval Semantic, Episodic F1 Score, Accuracy, Re-call@K, MAP StreamBench [162] Real, Human-Annotated Public Datasets Text-to-SQL, Programming, Tool Use, Medical, Encyclopedic Online Streaming Learning and Continuous Model Optimiza-tion Long-range, Incremen-tal Improvement Accuracy, Pass@1, Exact Match
