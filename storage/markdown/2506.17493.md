Title: PreQRAG -- Classify and Rewrite for Enhanced RAG

URL Source: http://arxiv.org/pdf/2506.17493v1

Published Time: Tue, 24 Jun 2025 00:11:45 GMT

Markdown Content:
> arXiv:2506.17493v1 [cs.IR] 20 Jun 2025

# PreQRAG - Classify and Rewrite for Enhanced RAG 

# Damian Martinez 

University of Delaware Newark, Delaware, USA damianm@udel.edu 

# Catalina Riano 

University of Delaware Newark, Delaware, USA criano@udel.edu 

# Hui Fang 

University of Delaware Newark, Delaware, USA hfang@udel.edu 

## Abstract 

This paper presents the submission of the UDInfo team to the SI-GIR 2025 LiveRAG Challenge. We introduce PreQRAG, a Retrieval Augmented Generation (RAG) architecture designed to improve retrieval and generation quality through targeted question prepro-cessing. PreQRAG incorporates a pipeline that first classifies each input question as either single-document or multi-document type. For single-document questions, we employ question rewriting tech-niques to improve retrieval precision and generation relevance. For multi-document questions, we decompose complex queries into focused sub-questions that can be processed more effectively by downstream components. This classification and rewriting strategy improves the RAG performance. Experimental evaluation of the LiveRAG Challenge dataset demonstrates the effectiveness of our question-type-aware architecture, with PreQRAG achieving the preliminary second place in Session 2 of the LiveRAG challenge. 

ACM Reference Format: 

Damian Martinez, Catalina Riano, and Hui Fang. 2025. PreQRAG - Classify and Rewrite for Enhanced RAG. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (SIGIR ’25). ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn 

## 1 Introduction 

Retrieval-Augmented Generation (RAG) has emerged as a powerful technique to enhance the performance of large language models (LLMs) by incorporating information from external knowledge sources. When grounding responses in retrieved documents, RAG systems can provide more accurate, up-to-date, and contextually relevant answers while significantly reducing hallucinations, that is, factually incorrect outputs produced by LLMs in the absence of sufficient knowledge. The SIGIR 2025 LiveRAG Challenge provides an open platform for academic and industry teams to explore and evaluate innova-tions in RAG-based question answering. The challenge is structured around a fixed corpus Fineweb [ 6], and using the open-source lan-guage model Falcon-3B-10B-Instruct [ 7]. Participants were given a set of 500 questions and a strict two-hour window to generate responses, including the corresponding supporting documents from the corpus.  

> Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
> SIGIR ’25, Padova, Italy
> ©2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM https://doi.org/10.1145/nnnnnnn.nnnnnnn

Figure 1: PreQRAG System Architecture 

Our proposed solution introduces a structured pipeline that be-gins with query preprocessing step, where each question is classi-fied based on whether it requires information from a single doc-ument or multiple documents. Depending on this classification, the question is then rewritten to optimize retrieval effectiveness. This reformulation step is designed to improve performance for both sparse and dense retrieval models, which are run to collect relevant documents. The retrieved results from both methods are then merged and passed through a reranking stage to prioritize the most relevant documents. Finally, a fixed number of top-ranked documents are integrated into a prompt, which is used to LLM in generating a coherent and accurate answer. 

## 2 System Design and Implementation 

PreQRAG is a comprehensive RAG system architecture designed to support LLMs in answering both single-document and multi-document questions [ 4]. Our approach is based on the RAG par-adigm [ 3 ] by improving the quality of the retrieved supporting documents and improving the accuracy of the generated answers. The architecture, illustrated in Figure 1, consists of five main com-ponents: Question Classification, Question Rewriting, Retrieval, Reranking, and Generation. 

## 2.1 Question Classification 

An important distinction in each question’s nature is to address how many and which specific documents are needed to answer the question; this will determine the behavior in all the stages of the system. The question classification is the first component of PreQRAG, which, as a preprocessing step, classifies each input question into one of two categories: 

• Single-document questions: Expected to be answerable using a single source passage. 

• Multi-document questions: Expected to be answerable using more than one source passage. SIGIR ’25, July 13–17, 2025, Padova, Italy Martinez et al. 

This classification informs downstream decisions such as how much context to retrieve, how to do the retrieval, and how to struc-ture the prompts. Initially, we used Falcon3-10B-Instruct for this task, but it was later replaced with a lightweight rule-based classifier that demonstrated superior performance, particularly in identifying multi-document questions. Misclassifying a single-document question as multi-document is less problematic than the reverse. In the first case, providing extra context has little negative impact on the answer. In contrast, misclassifying a multi-document question as a single-document can result in insufficient context being retrieved, which increases the risk of producing incorrect or incomplete answers. A combined method was also implemented, using Falcon and the rule-based approach together, but the rule-based classifier by itself consistently achieved the highest classification accuracy for multi-document questions. Table 1 shows the accuracy for each clas-sification method. The accuracy was calculated as the proportion of correctly classified questions (matching the true single or true multi-document label) over the total number of questions evaluated for each classification method. 

Question Type Rule Based Falcon3-10B-Ins t Combined 

Single-doc 0.731 0.822 0.821 Multi-doc 0.953 0.730 0.711 

Table 1: Accuracy of Question Classification. 

## 2.2 Question Rewriting 

In the question rewriting stage, our goal is to transform the in-put questions to improve retrieval accuracy and recall, particularly when used with dense and sparse retrieval models. This process includes correcting spelling errors and refining the phrasing of the questions to make them more compatible with retrieval mecha-nisms. We have implemented three distinct prompting strategies based on the query classification described in the previous section. 

2.2.1 Single-document questions . For single-document ques-tions, we prompt Falcon-3B-10B-Instruct to generate two rewritten versions of the original query, each tailored to a different retrieval strategy. The first rewriting prompt is designed to produce a version optimized for web-style search, aiming to enhance performance with BM25-based sparse retrieval. The second rewriting prompt focuses on dense retrieval, instructing the LLM to remove unneces-sary premises and incorporate key terms directly into the question. This results in a more concise and focused query, improving the effectiveness of dense index retrieval models. This is an example of the original query and its rewritten versions using our two distinct strategies: 

Original Query: 

“I’m developing new methods for protein analysis and wondering what specific technological improvements are being made to HDX methodology for complex protein systems?” 

Sparse-Optimized Rewrite: 

“Recent technological advancements in HDX methodology for complex protein systems” 

Dense-Optimized Rewrite: 

“What specific technological improvements are being made to HDX methodology for complex protein systems?” 

These rewrites aim to align the query structure with the strengths of each retrieval model, preserving natural language and search engine-friendly phrasing for sparse retrieval and maximizing se-mantic focus and keyword density for dense retrieval. As shown in the evaluation results from the single-document setting (Appendix A.1), query rewriting significantly improved retrieval performance across both retrieval paradigms. Specifically, the sparse-optimized rewrite achieved a 13.34% improvement in average Mean Reciprocal Rank (MRR) compared to the original queries when using the BM25-based sparse index. Similarly, the dense-optimized rewrite resulted in a 14.2% increase in MRR over the original queries when evaluated with embedding-based dense retrieval. 

2.2.2 Multi-document question . When the classification sys-tem identifies a multi-document question, we prompt Falcon3-10B-Instruct to decompose the original query into two distinct sub-questions. This decomposition enables the system to retrieve docu-ments supporting each sub-question independently. The retrieved documents from both sub-questions are then aggregated to provide comprehensive evidence for answering the original complex query. This is an example of an original multi-question query and its decomposed sub-questions: 

Original Query: 

“How does Azure Firewall enable network segmentation, and what are the security compliance aspects of integrating it with Azure Key Vault Managed HSM?” 

Sub-question 1: 

How does Azure Firewall enable network segmentation? 

Sub-question 2: 

What are the security compliance aspects of integrating Azure Key Vault Managed HSM with Azure Firewall? 

The results in Table A.2 shows the effectiveness of query decom-position for multi-document questions. The percentage of ground-truth documents retrieved increased substantially at all Top-K levels. For example, using the BM25-based sparse index, Top-10 retrieval improved from 41% to 55%. Similarly, dense retrieval performance at Top-10 rose from 36% to 56%. 

## 2.3 Retrieval Stage 

The goal of this stage is to retrieve the most relevant documents for a given query. We used pre-built indices provided by the LiveRAG 

organizers: a BM25-based sparse index and a dense index. For the dense index, documents were segmented into sentence-based chunks of up to 512 tokens using the LlamaIndex sentence splitter [ 5]. Each chunk was embedded into a 512-dimensional vec-tor using the E5-base embedder [ 8]. For the sparse index, the same chunks were indexed using a BM25-based method implemented on the OpenSearch platform. To assess retrieval effectiveness, we considered an ideal eval-uation setting: retrieving the most relevant document for single-document questions, and the most relevant set of documents for multi-document questions. Since the test dataset includes the orig-inal document(s) used to generate each question, we treat these PreQRAG - Classify and Rewrite for Enhanced RAG SIGIR ’25, July 13–17, 2025, Padova, Italy 

as the ground truth for relevance. While this assumption holds in most cases, it is important to note that for more general questions, valid answers may exist in additional documents as well. We evaluated performance by checking whether PreQRAG could retrieve the original document(s) within different cutoff ranks (TOP-K). Ideally, the original document should appear at the top of the ranked list in either the dense or sparse retrieval output to provide high-quality input for the generation stage. Table 2 presents the percentage of original documents success-fully retrieved using the preprocessed queries across various TOP-K values (1, 2, 3, 10) for both dense and sparse retrieval models. We limited the evaluation to TOP-10 results, as extending beyond this range may increase recall but also introduces noise, which can degrade answer quality in the generation stage. Single-doc Questions Top1 Top2 Top3 Top 10 Sparse (Rewritten) 37.5% 47.7% 52.1% 67.4% Dense (Rewritten) 31.2% 36.2% 42.0% 52.1% Multi-doc Questions Top1 Top2 Top3 Top 10 Sparse (Rewritten) 32% 36% 40% 55% Dense (Rewritten) 26% 30% 36% 56% 

Table 2: Retrieval Performance Metrics for Single-doc and Multi-doc Questions 

During this analysis, we observed that some questions were better served by either the dense or the sparse retrieval model, de-pending on their structure and content. To leverage the strengths of both approaches, we adopted a hybrid retrieval strategy, combining the results from both dense and sparse models. The merged set of candidate documents was then passed to the reranking stage for reordering and further relevance assessment. 

## 2.4 Reranking Stage 

The reranking stage is responsible for refining the set of candidate documents retrieved during the initial retrieval phase. While the sparse and dense retrievers provide a broad set of potentially rel-evant documents, reranking enables the system to reassess and reorder these candidates using a more precise scoring function. The objective is to improve the overall ranking by leveraging a model trained specifically for fine-grained relevance estimation. For this step, we employed bge-reranker-v2 [1], a cross-encoder model that jointly encodes both the question and each candidate document to produce highly accurate relevance scores. Unlike the initial retrieval models, which operate independently over question and document embeddings, the cross-encoder architecture evalu-ates the semantic match between the full question and document pair, enabling more nuanced relevance judgments. We applied the reranker to the combined candidate sets retrieved from both the dense and sparse indices, as introduced in the previ-ous stage. This hybrid retrieval set was then reranked to produce the final ordered list of documents used in the answer generation stage. To evaluate reranking performance, we used the two datasets described in Appendix D. In the single-document subset, each Single-doc Questions Top1 Top2 Top3 Top 10 Hybrid + Rerank 64.2% 68.7% 74.4% 85.2% Multi-doc Questions Top1 Top2 Top3 Top 10 Hybrid + Rerank 42% 47% 53% 70% 

Table 3: Reranking Performance Metrics for Single-doc and Multi-doc Questions 

query was associated with one target document, while in the multi-document subset, each query had two relevant documents. We measured performance based on the percentage of original ground-truth documents successfully retrieved after reranking. The results are presented in Table 3, and can be directly compared to the initial retrieval results shown in Table 2, highlighting the improvements achieved through reranking. 

## 2.5 Generation Stage 

The final stage of the pipeline involved generating answers using Falcon3-10B-Instruct, an instruction-tuned model known for its open access and strong performance in task-oriented generation tasks [ 7]. Falcon’s training objective emphasizes instruction fol-lowing, making it particularly suitable for applications involving prompt-based QA generation. Moreover, its use was a requirement of the challenge. 

Generation Process: The generation stage in PreQRAG follows the standard RAG method; a prompt containing the input question and its associated context is passed to the model, which returns an answer. This process performs well for single-document questions. However, for multi-document questions, we explored three differ-ent variations in how generation is handled to determine whether alternative strategies could yield better results. The results did not show a significant advantage over the standard method, suggesting that prompt and context design, not the generation process itself, has the greatest impact. Refer to Appendix B.1 for the details and performance metrics of the three approaches explored. 

Model Parameters: To explore the impact of decoding strate-gies on answer quality, we tested different generation parameters. We found that deterministic decoding consistently yielded higher-quality answers when context passages were reliably relevant. As a result, we adopted the default non-sampling configuration of Falcon using greedy decoding. This approach ensures stable, reproducible outputs and minimizes generation variance across different runs, also enhancing the answer being grounded in the context passage given to the model. For further details and results of the different tested parameters, refer to Appendix B.2. 

Context Length : Another important part to consider is the effect of context length on generation outcomes, by context length we mean the number of documents provided as input to the model. While it might seem intuitive that more context leads to better answers, this is not always the case. The results, presented in Table 4, show that adding more docu-ments can sometimes degrade performance by introducing irrel-evant, redundant, or conflicting information, which can reduce answer accuracy or conciseness. SIGIR ’25, July 13–17, 2025, Padova, Italy Martinez et al. 

For single-document questions, using only the TOP-1 retrieved document yielded the best overall performance, which is expected if the retrieved document is correct. However, because the top document is not always the expected one, we opted for a TOP-3 context setting in the PreQRAG final configuration. This provides a better chance of including the relevant document while maintaining similar performance to TOP-1. For multi-document questions, TOP-1 performed the worst across all metrics, while TOP-3, TOP-5, and TOP-10 yielded comparable results. Given the trade-off between performance and computational efficiency, we selected TOP-3 as the optimal context length for multi-document generation as well. Please note that by "best overall performance," we mean that each metric is considered equally important in the evaluation. Question Context Equivalence Relevance Faithfulness Single-doc Top1 0.700 0.927 0.927 Top3 0.731 0.931 0.854 Top5 0.722 0.936 0.863 Top10 0.718 0.936 0.890 Multi-doc Top1 0.523 0.830 0.726 Top3 0.604 0.915 0.820 Top5 0.617 0.901 0.849 Top10 0.613 0.905 0.820 

Table 4: Context Length Performance Metrics. 

## 3 Evaluation 3.1 Metrics 

To assess the effectiveness of PreQRAG and guide design decisions, we conducted a comprehensive evaluation process that included an extra metric beyond the baseline framework proposed by the Liv-eRAG Challenge organizers. While the official evaluation focused primarily on correctness and faithfulness, we divided the correct-ness metric to address two different dimensions: equivalence and relevance. This broader set of evaluation metrics provided a clearer understanding of the model behavior and allowed us to better diag-nose the effects of different configurations and methods. 

Equivalence: Captures semantic similarity to expected answers, by comparing the generated answer to the gold answer. It uses a 3-point evaluation scale (fully, partially, or not equivalent). 

Relevance: Measures if the answer addressed the question asked, independent of correctness or grounding. It uses a 3-point evalua-tion scale (fully, partially, or not relevant). 

Faithfulness: Assess whether the generated answers were strictly grounded in the retrieved context. It uses a 3-point scale (fully, par-tially, or not supported). This remained the same as the metric provided by the organizers. The evaluation was done automatically with model assistance us-ing GPT-4o Mini, prompting it to evaluate each metric per question and give a numeric value. The individual scores were then summed across all questions for each metric and divided by the maximum possible score to obtain the final normalized metric value. GPT-4o Mini model was used for all performance metrics reported in this paper due to its significantly lower cost while maintaining reliable evaluation quality. Table 4 shows the final performance of the sys-tem for different context scenarios, as explained in the previous section. For detailed descriptions of the evaluation prompts, see Appendix C.1, and for comparison between the evaluation done by Claude 3.5 Sonnet and GPT-4o Mini refer to Appendix C.2. 

## 3.2 Datasets 

Multiple synthetic datasets tailored to diverse question-answer scenarios were generated to evaluate the performance and gener-alizability of our system. These datasets were created using Data-Morgana [ 2 ] a powerful tool designed for generating controlled, large-scale, and diverse QA benchmarks. DataMorgana provides an interface to define categorizations of question types, formulate rich distributions over categories within those categorizations, and gen-erate high-quality question-answer pairs grounded in the FineWeb corpus [6]. Following best practices and examples outlined by the authors, we adopted the categorization schema proposed in their work. Each dataset variation differs in the probability distribution applied to these categories, allowing us to control for the complexity, natural-ness, and reasoning type involved. This strategy ensures our system is evaluated against a representative spectrum of question types and user profiles. Refer to Appendix D for details on the datasets generated. 

## 4 Results 

Table 5 shows the official preliminary results on the day of the LiveRAG Challenge. PreQRAG achieved a preliminary second place in Session 2, demonstrating the effectiveness of its question-type-aware architecture. In addition to the challenge ranking, we evalu-ated the system performance using our evaluation framework. The evaluation showed an equivalence score of 0.977 and a relevance score of 0.884. The faithfulness metric could not be computed due to the absence of gold answers. However, the results, both from the official ranking and from the internal evaluation, highlight the po-tential of the PreQRAG question-oriented approach as a promising strategy to improve retrieval-augmented generation systems. Rank Team Name Correctness [-1:2] Faithfulness [-1:1] 1 Magikarp 1.231578 0.656464 2 UDInfo 1.200586 0.623175 3 RAGtifier 1.134454 0.552365 4 HLTCOE 1.070111 0.340711 

Table 5: LiveRAG Challenge Session 2 Results. UDInfo per-formance highligted in grey. 

## 5 Conclusion 

PreQRAG’s question-type awareness demonstrates promising re-sults, showing that classifying and rewriting questions significantly enhances the retrieval component of a RAG system. During the retrieval stage, we observed that certain questions are better rep-resented using either dense or sparse retrieval methods; therefore, a hybrid approach proved to be more effective, yielding higher re-trieval efficiency. In the generation stage, the design of prompts and context arrangement plays a critical role in answer quality. This insight is particularly valuable when using a less capable model in terms of the number of parameters, as thoughtful prompt engi-neering and careful selection of context length can enhance perfor-mance. PreQRAG - Classify and Rewrite for Enhanced RAG SIGIR ’25, July 13–17, 2025, Padova, Italy 

## References              

> [1] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. arXiv:2402.03216 [cs.CL] [2] Simone Filice, Guy Horowitz, David Carmel, Zohar Karnin, Liane Lewin-Eytan, and Yoelle Maarek. 2025. Generating Diverse Q&A Benchmarks for RAG Evaluation with DataMorgana. arXiv:2501.12789 [cs.CL] https://arxiv.org/abs/2501.12789 [3] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented Generation for Large Language Models: A Survey. arXiv:2312.10997 [cs.CL] https://arxiv.org/ abs/2312.10997 [4] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Se-bastian Riedel, and Douwe Kiela. 2021. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. arXiv:2005.11401 [cs.CL] https://arxiv.org/abs/ 2005.11401 [5] Jerry Liu. 2022. LlamaIndex . doi:10.5281/zenodo.1234 [6] Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. 2024. The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track .https://openreview.net/forum?id=n6SCkn2QaG [7] Technology Innovation Institute (TII). 2024. Falcon 3B/7B/40B/180B and Falcon-Instruct Models. https://huggingface.co/tiiuae/Falcon3-10B-Instruct-GGUF. https: //huggingface.co/tiiuae/Falcon3-10B-Instruct-GGUF Accessed: 2025-05-22. [8] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text Embeddings by Weakly-Supervised Contrastive Pre-training. arXiv preprint arXiv:2212.03533 (2022).

## Appendix A Questions Rewriting Results A.1 Single-document questions 

To assess the effectiveness of the query rewriting strategies for single-document questions, we used Mean Reciprocal Rank (MRR) as the primary evaluation metric. MRR quantifies retrieval quality by computing the inverse of the rank at which the first relevant document is retrieved, averaged over all queries. The evaluation was performed using the single-document dataset refer to Appendix D. Results are presented in Table A.1, comparing baseline queries against their rewritten versions under both sparse and dense re-trieval settings. Query Type Top-K Average @1 @2 @5 @10 Sparse (Original) 0.025 0.077 0.171 0.370 0.161 Sparse (Rewritten) 0.033 0.083 0.201 0.412 0.182 Dense (Original) 0.023 0.068 0.141 0.326 0.139 Dense (Rewritten) 0.025 0.073 0.163 0.374 0.159 

Table A.1: Mean Reciprocal Rank (MRR) for original and rewritten queries in the single-document setting 

## A.2 Multi-document questions 

To evaluate the effectiveness of query decomposition for multi-document questions, we measured the percentage of ground-truth documents retrieved at various Top-K thresholds. This evaluation compares the baseline approach treating the multi-document ques-tion as a single-doc question against our decomposition strategy, where each question is split into two sub-questions. The evalua-tion was conducted using the multi-document subset of the dataset, as detailed in Appendix D. Results are shown in Table A.2, high-lighting the impact of decomposition across both sparse and dense retrieval models. 

Retrieval Method Approach Top-k @1 @2 @3 @5 @10 Sparse Single-doc 16% 21% 26% 30% 41% Sparse Multi-doc (2 subques-tions) 32% 36% 40% 48% 55% Dense Single-doc 12% 19% 25% 31% 36% Dense Multi-doc (2 subques-tions) 26% 30% 36% 46% 56% 

Table A.2: Percentage of ground-truth documents retrieved for multi-document questions, comparing single-doc and multi-doc approach. 

## B Generation Stage Results B.1 Generation Process 

Three methods for the multi-document question generation process were tested: 

• Method 1 - Standard approach: The retrieved documents are concatenated and used as context. 

• Method 2 - Two-step Context: The question is split into sub-questions, intermediate answers are generated, then used as context for a final answer. 

• Method 3 - Two-step Answer: The question is split into sub-questions, intermediate answers are generated, and these are simply merged without a final generation step. Table B.1 shows the performance metric of each method under different context sizes, top 1, top 2, and top 3 of the retrieved docu-ments; the standard method demonstrated the best performance in general, taking into account that each metric is given the same importance. 

## B.2 Model Parameters 

To explore the impact of decoding strategies on answer quality, we tested different generation parameters that are the commonly used in generation to balance between creative fluency and factual precision, including: 

• Temperature: Controls randomness by scaling the logits be-fore sampling. Lower values make outputs more determinis-tic, while higher values increase diversity. 

• Top-k sampling: Restricts token selection to the top-k most probable tokens, promoting focused yet flexible output gen-eration. 

• Top-p sampling: Choose tokens from the smallest probability mass p that exceeds the threshold, dynamically adapting the token pool. Table B.2, shows the performance of four different configurations of the decoding parameters, supporting our decision to use the SIGIR ’25, July 13–17, 2025, Padova, Italy Martinez et al. 

Top1 Method Equivalence Relevance Faithfulness Method 1 0.60 0.88 0.14 Method 2 0.58 0.92 0.10 Method 3 0.55 0.92 0.12 Top2 Method Equivalence Relevance Faithfulness Method 1 0.63 0.93 0.32 Method 2 0.58 0.94 0.26 Method 3 0.52 0.94 0.17 Top3 Method Equivalence Relevance Faithfulness Method 1 0.64 0.93 0.52 Method 2 0.55 0.93 0.40 Method 3 0.34 0.57 0.33 

Table B.1: Multi-Doc Generation Process Performance Met-rics on Different Methods and Context Size. 

default non-sampling configuration on Falcon, because it has the better performance overall. These results were obtained under two conditions: an ideal context scenario, where the model was provided with the expected (gold) document, and a TOP-1 retrieval scenario, where the context consisted of the first document returned by the retrieval system. Ideal Context Method Configuration Equivalence Relevance Faithfulness Non-sampling 0.924 0.982 0.988 temp=0.7 top_p=0.85 0.919 0.982 0.994 top_k=20 temp=0.9 top_p=0.9 0.919 0.979 0.988 top_k=40 temp=1.2 top_p=0.95 0.910 0.979 0.971 top_k=60 Top1 Method Configuration Equivalence Relevance Faithfulness Non-sampling 0.739 0.965 0.890 temp=0.7 top_p=0.85 0.731 0.959 0.884 top_k=20 temp=0.9 top_p=0.9 0.731 0.965 0.884 top_k=40 temp=1.2 top_p=0.95 0.728 0.959 0.872 top_k=60 

Table B.2: Model Parameters Performance with Different Configurations. 

## B.3 Prompt Strategy: 

Prompt design plays a central role in guiding the model’s response behavior. For that reason, we evaluated four distinct prompting strategies: 

Prompt A - Direct Prompt: Simply asks the model to perform a task without additional context or framing. 

Prompt B - Role-Based Prompt: Assigns the model a specific persona to influence its response style and reasoning. 

Prompt C - Output-Constrained Prompt: Explicitly specifies the required structure or expected format of the output. 

Prompt D - Chain-of-Thought Prompt: Encourages the model to show the step by step reasoning on the given output. Testing across multiple question types revealed that role-based prompting, that is Prompt B, had the best performance overall compared to the other strategies in terms of relevance, clarity, and alignment with the context, as shown in Table B.3. This is partic-ularly effective with Falcon Instruct, which benefits from clearly defined role conditioning due to its supervised instruction tun-ing. By defining the model’s purpose, role-based prompts reduce ambiguity and help guide inference in a more controlled manner. Prompt Type Equivalence Relevance Faithfulness Prompt A 0.890 0.971 0.994 Prompt B 0.924 0.982 0.998 Prompt C 0.910 0.979 0.994 Prompt D 0.904 0.976 0.988 

Table B.3: Prompts Performance Metrics on an Ideal Context Scenario. 

The following are the exact prompts used on the generation stage to evaluate the performance of each Prompt Type: 

A - Direct Prompt: Answer the question using only the pro-vided context. Do not include any information not found in the context. Be concise. Keep the answer under 200 tokens. 

B - Role-Based Prompt: You are an expert assistant. Use only the retrieved context to answer the question clearly and accurately. Do not speculate. Keep the answer under 200 tokens. 

C - Output-Constrained Prompt: Based only on the provided context, answer the question in 1-3 sentences. Do not include any irrelevant information. Keep the answer under 200 tokens. 

D - Chain-of-Thought Prompt: Use only the context to answer the question. If reasoning is needed, do so briefly before giving a clear final answer. Keep the answer under 200 tokens. Do not use outside knowledge. Table B.4 shows the performance metrics of the prompts evalu-ated under a TOP-1 retrieval scenario, where the context consisted of the first document returned by the retrieval system. These results reaffirm our decision, and that Prompt B has the best performance in general, taking into account that each metric is given the same importance. 

## C Evaluation Results C.1 Evaluation Prompts 

The following are the prompts used to perform the evaluation: 

Equivalence Prompt: PreQRAG - Classify and Rewrite for Enhanced RAG SIGIR ’25, July 13–17, 2025, Padova, Italy 

Prompt Type Equivalence Relevance Faithfulness Prompt A 0.699 0.939 0.901 Prompt B 0.739 0.965 0.895 Prompt C 0.710 0.942 0.919 Prompt D 0.725 0.947 0.890 

Table B.4: Prompts Performance Metrics on a Top 1 Retrieved Document Scenario. 

You are an assistant evaluating the equivalence of a generated an-swer compared to the gold answer. Equivalence measures semantic similarity, not surface similarity. Use this scale: 2: Fully equivalent — expresses the same meaning. 1: Partially equivalent — some shared meaning, but incomplete or imprecise. 0: Not equivalent — meaning differs or is wrong. Question: <question> Gold Answer: < gold answer> Generated Answer:<answer> Return ONLY your evaluation like this: Equivalence: <score> 

Relevance Prompt: 

You are an assistant evaluating the relevance of a generated an-swer based on the question. Relevance is defined as how directly and completely the answer addresses the question, regardless of correctness. Use this scale: 2: Fully relevant — directly answers the question. 1: Partially relevant — somewhat related, vague, or off-topic parts. 0: Not relevant — does not answer or is unrelated. Question: <question> Generated Answer: <answer> Return ONLY your evaluation like this: Relevance: <score> 

Faithfulness Prompt: 

You are an assistant evaluating the faithfulness of a generated an-swer based on the given context and question. Faithfulness assesses whether the response is grounded in the retrieved passages. Use this scale: 1: Full support. All answer parts are grounded. 0: Partial support. Some parts are grounded, others are not. -1: No support. All answer parts are unsupported by the context. Context: <context> Question: <question> Generated Answer: <answer> Return ONLY your evaluation like this: Faithfulness: <score> 

## C.2 Evaluation Model Comparison 

Table C.1 shows the performance metrics done with Claude 3.5 Sonnet. When compared with the results done with GPT-4o Mini, which are reported in the paper, Claude’s results have a similar trend across metrics, indicating consistent relative performance patterns. However, the overall scores were consistently lower when using Claude. 

## D Datasets 

These are the categorizations implemented to generate the datasets used to evaluate the PreQRAG system, they were the same as those proposed by the challenge organizers and the authors [2]. Question Context Equivalence Relevance Faithfulness Single-doc Top1 0.701 0.927 0.781 Top3 0.718 0.927 0.818 Top5 0.732 0.931 0.840 Top10 0.662 0.925 0.825 Multi-doc Top1 0.528 0.836 0.126 Top3 0.558 0.889 0.384 Top5 0.550 0.830 0.337 Top10 0.557 0.835 0.307 

Table C.1: Context Length Performance Metrics using Claude 3.5 Sonnet. 1. Answer-Type Categorization: This schema defines what kind of information the question seeks, and whether answering it requires access to single or multiple documents. It includes the following categories: 

• Multi-aspect: Requires reasoning over two distinct aspects of a topic, sourced from two documents. 

• Comparison: Involves comparing two entities or concepts using information drawn from separate documents. 

• Factoid: Seeks concise factual answers like names or dates. 

• Non-factoid: Encourages elaboration or explanation rather than short facts. 

• Direct: Straightforward, context-free queries. 

• With-premise: Includes a user-relevant or personal premise within the query. 

• Long: With at least seven words. 

• First turn: Opening questions in a conversation, self-contained. 

• Non-first turn: Follow-up questions, assuming prior conver-sational context. 

2. Question Formulation Categorization: This schema classi-fies how the question is phrased: 

• Natural: Written in colloquial, conversational style. 

• Search query: Structured as keyword-based web queries. 

3. User Expertise Categorization This schema captures the assumed knowledge level of the user: 

• Expert: Assumes familiarity with specialized terminology. 

• Novice: Assumes minimal domain knowledge. Table D.1 shows the datasets generated for the evaluation of the PreQRAG system. Dataset Number of QA Parameters Single-doc 110 Equal probabilities for all categories. Multi-document categories were excluded. Multi-doc 106 Only multi-document questions were generated, with equal probability. 

Table D.1: Generated Datasets with DataMorgana.
