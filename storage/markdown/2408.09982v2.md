Title: 

URL Source: http://arxiv.org/pdf/2408.09982v2

Published Time: Wed, 21 Aug 2024 00:20:01 GMT

Markdown Content:
Application of Large Language Models in Automated Question 

# Generation: A Case Study on ChatGLM's Structured Questions for 

# National  Teacher Certification Exams 

# Ling He ab , Yanxin Chen ab , Xiaoqiang Hu  ab *

> a

Faculty  of Education, Jiangxi Science and Technology Normal University, Nanchang, China; bVR 

Perception and Interaction Key Laboratory, Nanchang, China ; *C orresponding  Author 

Abstract 

This study delves into the application potential of the large language model s (LLM s) ChatGLM 

in the automatic generation of structured questions for  National T eacher  Certification  Exams 

(NTCE) . Through meticulously designed prompt engineering, we guided ChatGLM to generate a 

series of simulated questions and conducted a comprehensive comparison with questions recollected 

from past examinees. To ensure the objectivity and professionalism of the  evaluation, we invited 

experts in the field of education to asse ss these questions and their scoring criteria. The research 

results indicate that the questions generated by ChatGLM exhibit a high level of rationality, 

scientificity, and practicality similar to those of the real exam questions across most evaluation 

cri teria, demonstrating the model's accuracy and reliability in question generation. Nevertheless, the 

study also reveals limitations in the model's consideration of various rating criteria when generating 

questions, suggesting the need for further optimizati on and adjustment. This research not only 

validates the application potential of ChatGLM in the field of educational assessment but also 

provides crucial empirical support for the development of more efficient and intelligent educational 

automated generati on systems in the future. 

1 Introduce 

1.1  BackGround 

The  National Teacher  Certification Exams (NTCE)  is pivotal in determining whether a 

candidate is qualified to pursue a career in education, encompassing assessments of teaching 

abilities, educational theories, and practical skills. Among these, structured interview questions are 

particularly significan t, evaluating candidates' understanding of the teaching profession, 

professional ethics, and their ability to respond to teaching scenarios. However, due to the official 

non -disclosure of the question bank, candidates' preparation  materials primarily rely on the 

recollections of past examinees, which makes comprehensive preparation challenging. Facing this 

challenge, the advancement of artificial intelligence technology, particularly the application of large 

language models like Ch atGLM, offers a potential solution. Through precise prompt design, these 

large language models  (LLM s) can generate questions that closely align with the actual examination 

requirements, providing candidates with high -quality practice materials. 

1.2  Problem Statement 

This study aims to explore whether LLMs, particularly ChatGLM, can achieve comparable 

effects to human -generated questions in the automatic generation of structured questions for  NTCE .

Through prompt engineering to guide the model in generating structured questions, and by inviting 

experts to evaluate these questions and their scoring criteria, the study examines the model -

generated questions for their rationality, scientificity, and p racticality, thereby validating the 

application potential of LLMs in the  simulation of question generation within the educational field. 1.3  Research Objectives 

The objective of this study is to guide the ChatGLM  through prompt engineering to generate 

structured questions for  NTCE , and to invite educational experts to review these questions along 

with their scoring criteria. The study analyzes the rationality, scientificity, and practicality of the 

questions generated by the LLM s. By statistically analyzing the feedback from expert reviews, the 

study evaluates the performance of the LLM s in simulating question generation, thereby validating 

its application potential in educational assessment. This research provides  theoretical and empirical 

evidence for the development of future educational automated generation systems. 

2 Literature Review 

2.1  The Evolution of Large Language Models 

The evolution of LLMs can be outlined through several key technological and application 

milestones. Initially, the concept of large language models dates back to 2005, when the focus was 

primarily on employing massive n -gram models for machine translation [8] .While these models 

were advanced at the time, they were gradually supplanted by more sophisticated neural network 

models as technology progressed. In 2017, a research team at Google introduced the Transformer 

model, a novel network architecture entirely ba sed on attention mechanisms, which abandoned the 

previously widely used recurrent and convolutional structures [1] .The introduction of the 

Transformer model marked a significant turning point in the field of deep learning for natural 

language processing, as it demonstrated superior performance across multiple machine translation 

tasks and significantly reduced trainin g times. Following this, in 2018 and 2019, the BERT and GPT 

series models emerged, respectively. BERT (Bidirectional Encoder Representations from 

Transformers) significantly enhanced performance across various NLP tasks by pre -training deep 

bidirectional r epresentations from unlabeled text through bidirectional encoding [3] . On the other 

hand, GPT (Generative Pre -trained  Transformer) advanced the development of pre -trained language 

models by employing an autoregressive approach to language modeling [7] . By 2020, OpenAI 

introduced GPT -3, a language model with 175 billion parameters, which made significant strides in 

few -shot learning and achieved levels competitive with human experts on certain tasks [9] . The 

success of GPT -3 not only demonstrated the formidable capabilities of large -scale models but also 

sparked extensive discussions on issues such as model scale, data quality, and bias. In recent years, 

with the increase in computational resources and opt imization of algorithms, LLMs have been able 

to handle increasingly complex tasks, including cross -modal learning and multilingual 

processing [11] . Simultaneously, the applications of these models are continually expanding into 

various fields, including healthcare, engineering, and social sciences [5] . In summary, the 

development of LLMs is a rapidly evolving process that involves algorithmic innovations, hardware 

advancements, and the expansion of application scenarios. Looking ahead, with further 

technological development and deeper application penetra tion, LLMs are expected to play a 

significant role in a broader range of domains. 

2.2  Recent Advances in Automatic Question Generation Using Large Language Models 

The  application of LLMs in the field of automated question generation can be explored from 

several key aspects. Initially, the utilization of LLMs in the domain of automated planning can be 

extended to the realm of automated question generation, where a series  of questions are formulated 

to assess the knowledge and skills of learners [10] . Secondly, the introduction of chain -of -thought 

prompting has demonstrated an enhancement in the model's performance on complex reasoning 

tasks, which is particularly crucial in automated question generation. For instance, when dealing with mathematical problem -solving, the model can exhibit each step of the solution process, thereby 

aiding students in comprehending intricate concepts [4] . Furthermore, fine -tuning based on human 

feedback allows for the collection of insights from educators or experts to optimize the quality and 

relevance of questions, ensuring an effective assessment of students' learning outcomes [6] . Lastly, 

the mention of efficient foundational language models such as LLaMA enables the training of high -

performance models without reliance on proprietary datasets, making them suitable for automated 

question generation, capable of producing high -quality,  targeted questions [2] . In summary, the 

application of LLMs in automated question generation primarily relies on their robust capabilities 

in planning, reasoning, and learning. 

2.3  Characteristics of Structured Questions 

The challenges faced by candidates in the structured interview for  NTCE  can be primarily 

attributed to five aspects: Firstly, the examination of professional basic literacy and practical 

abilities requires candidates to integrate theory with practice, demonstrating their educational 

teaching operational skills [14] . Secondly, the diversity and complexity of the questions demand that 

candidates possess the ability to handle different question types and respond quickly and 

accurately [13] . Thirdly, the objectivity and consistency of the scoring pose challenges to candidates, 

necessitating efforts to minimize the influence of subjective judgments [17] . Fourthly, candidates 

need to master examination techniques to effectively showcase themselves [16] . Fifthly, the 

difficulty in preparing for the interview is significant, as candidates must prepare the content 

independently and enhance their expression and adaptability skills [12] . Additionally, candidates 

must also adapt to adjustments in the interview format and face a professional panel of 

examiners [16] .

3 Method 

3.1  Collect ions of  Questions 

The task of categorizing questions can be divided into authentic questions and simulated 

questions generated by ChatGLM, with authentic questions sourced from recollections and 

compilations of past test -takers. Additionally, the structured interview questi ons for teacher 

certification interviews are a crucial component for assessing the comprehensive qualities of 

candidates, covering seven major categories. Here are the specifics of these categories: 

Self -Cognition: This category primarily assesses the candidate's level of self -awareness, 

including understanding of personal strengths, weaknesses, and plans for future career development. 

Candidates are required to demonstrate their self -evaluation abili ties and career planning awareness 

in this module. 

Interpersonal Communication: This category focuses on the candidate's interpersonal 

communication skills, including techniques for communicating with colleagues, students, parents, 

and other groups. Candidates need to exhibit skills such as active listenin g, clear expression, and 

strong empathy. 

Organizational Management: This category emphasizes the candidate's organizational and 

coordination abilities and management skills. Candidates should demonstrate their capabilities in 

team collaboration, event planning, and resource allocation when answer ing questions. 

Emergency Response: This category aims to assess the candidate's coping abilities in dealing 

with unexpected events. Candidates are expected to showcase their ability to analyze calmly, react 

swiftly, and handle situations properly. 

Comprehensive Analysis: This category requires candidates to possess strong logical thinking and analytical abilities. When faced with complex issues, candidates should be able to apply their 

knowledge to conduct comprehensive and in -depth analysis and propose reasonable solutions. 

Educational Instruction: This category focuses on educational and instructional philosophies, 

methods, and strategies. Candidates need to demonstrate their understanding of educational work 

and how they apply scientific and effective teaching methods in pr actical teaching situations. 

Current Affairs: This category primarily assesses the candidate's attention to and understanding 

of current political hotspots, analytical abilities, and insights. Candidates are required to display 

their knowledge of national policies, social phenomena, e ducational policies, etc., and be able to 

provide unique perspectives and reflections based on their own educational practices. 

It is important to note that due to the strong timeliness and policy changes associated with the 

Current Affairs category, we do not conduct research on this type of question. Therefore, the above 

six categories together constitute the core content of the  structured interview questions for teacher 

certification interviews, aiming to comprehensively assess the candidates' comprehensive qualities 

and abilities. Candidates should fully understand the key points of each category during their 

preparation and rev iew and enhance their abilities in a targeted manner. In summary, we will collect 

a total of 240 questions (40 questions per category, including 20 authentic questions and 20 

questions generated by ChatGLM), as detailed in Table 1. 

During the dialogue process, to ensure that the format of the output content from ChatGLM 

remains consistent, we adopted a strategy that combines instructions with pseudo -code to achieve 

precise control over the output format. The specific details of the P rompt Engineering are showcased 

in Table 2. 

3.2  Development of a Questionnaire for Item Rating 

To ensure the scientific and rational nature of the scoring criteria, our research team has 

meticulously constructed a detailed voting questionnaire that encompasses all potential scoring 

indicators along with their brief descriptions, facilitating accurat e understanding and evaluation by 

industry experts. This study invited five senior experts in the field to participate in the voting 

process, thoroughly explaining the purpose, importance, and specific procedures of the voting to 

them. After the voting dat a collection was completed, our research team conducted an in -depth 

analysis of the results, calculating the support rate for each scoring criterion. Based on the 

statistical analysis, we adopted the criteria that received support from at least three (i.e. , 60%) of 

the experts. This screening process ensured the consensus and authority of the selected standards. 

Subsequently, we refined and updated the content of the questionnaire to ensure it only included 

scoring criteria endorsed by the majority of exper ts, enhancing the questionnaire's pertinence and 

effectiveness. The following is an overview of the optimized scoring criteria (Table 3). The 

adoption of this methodology aims to validate and optimize our preliminary scoring criteria 

system through expert  consensus. 

3.3  Completion of the Questionnaire 

We have invited 5 seasoned experts in the field to evaluate structured questions using our 

newly developed question scoring questionnaire (Table 3). Subsequently, we conducted statistical 

analysis on the questionnaire results. 

4 Results 

The evaluation data from Table 4, which compares the scores of real exam questions and those 

generated by ChatGLM, reveals the average values and standard deviations under different question 

categories and scoring criteria. Table 4 details the performance  of real exam questions and ChatGLM -generated questions across six question categories under 15 scoring criteria. Overall, the 

average scores of real exam questions and ChatGLM -generated questions are similar, with most 

categories averaging between 3.40 and 3.70, indicating a compara ble quality of questions. However, 

in terms of standard deviation, ChatGLM -generated questions show slightly less consistency in 

certain categories and criteria compared to real exam questions, suggesting that the score 

distribution of real exam questions  is more concentrated, while the score distribution of ChatGLM -

generated questions is relatively dispersed. Specifically, the average scores of real exam questions 

in all categories are consistently between 3.40 and 3.70, demonstrating high consistency in q uestion 

scoring. Although ChatGLM -generated questions have similar average scores to real exam 

questions, they exhibit larger standard deviations in certain categories (such as Category 2 and 

Category 6), indicating greater score volatility in these catego ries. 

This study employed Welch's one -way analysis of variance (ANOVA) to conduct a difference 

test on the scoring criteria based on question source. The study included real exam questions and 

questions generated by ChatGLM, totaling 1 5 scoring criteria. The analysis results show that among 

the 1 5 scoring criteria, only Scoring Criterion 2 (F=5.82557, df1=1, df2=1196, p=0.016) exhibited 

a significant difference (p < 0.05), indicating a statistically significant difference between real exam 

questions and ChatGLM -generated questions under this criterion. The p -values for the remaining 

scoring criteria (1, 3 -16) were all greater than 0.05, indicating no significant difference in scores 

between real exam questions and ChatGLM -generated questions under these criteria.  Specifically, 

the average score for real exam questions under Scoring Criterion 2 was 3.51 ±0.77, while the 

average score for ChatGLM -generated questions was 3.40 ±0.74, showing that real exam questions 

scored slightly higher under this criterion. 

5 Discussion 

The experimental results indicate that the questions generated by ChatGLM are generally 

comparable to those of real exam questions across most evaluation criteria, demonstrating high 

rationality and scientific validity. Specifically, the model -generated qu estions exhibit close average 

scores and similar standard deviations to real exam questions in terms of the rationality of question 

design, balance of difficulty, logical coherence, and coverage of knowledge and abilities. This 

suggests that ChatGLM exhibi ts high accuracy and reliability in simulating structured questions for 

teacher qualification exams. However, a significant difference (p < 0.05, 3.51 > 3.40) was observed 

under Evaluation Criterion 2 between real exam questions and those generated by Chat GLM, which 

may indicate that further optimization is needed in ChatGLM's question generation for certain 

evaluation criteria. For instance, the model may underperform in generating practical questions 

compared to real exam questions, potentially due to ins ufficient samples in the training data or 

limitations in the algorithm when dealing with complex scenarios. 

LLMs like ChatGLM show significant potential in the automated generation of structured 

questions for teacher qualification exams. Through precise prompt engineering, these models can 

generate high -quality questions, providing effective preparation material s for candidates and serving 

as a new automated tool for educational assessment. This method of automated question generation 

not only enhances the efficiency of question formulation but also ensures a certain level of diversity 

in questions. Nevertheless,  the application of ChatGLM has its limitations. Firstly, although the 

model performs well in most cases, improvements are still needed in questions generated under 

certain scoring criteria. Secondly, the performance of ChatGLM is constrained by the qualit y and 

quantity of training data; if certain types of questions or scenarios are lacking in the training data, ChatGLM may struggle to generate high -quality related questions. Additionally, the output of 

ChatGLM can be influenced by the sophistication of prompt design, necessitating further research 

to optimize prompt engineering. 

6 Conclusion 

This study guided ChatGLM in generating structured questions for teacher qualification exams 

through prompt engineering and invited educational experts to evaluate these questions and their 

scoring criteria, analyzing the rationality, scientific validity,  and practicality of questions generated 

by large language models (LLMs). The experimental results demonstrate that ChatGLM -generated 

questions are generally comparable to real exam questions across most evaluation criteria, 

exhibiting high accuracy and rel iability. However, further optimization is needed in ChatGLM's 

consideration of various scoring criteria when generating questions, such as in creating practical 

questions. LLMs like ChatGLM show significant potential in the automated generation of structu red 

questions for teacher qualification exams, providing effective preparation materials for candidates 

and serving as a new automated tool for educational assessment. Despite the limitations in the 

application of the model and its constraints due to the q uality and quantity of training data, these 

challenges are expected to be addressed through further research and optimization. 

In summary, LLMs such as ChatGLM hold broad application prospects in the automated 

generation of structured questions for  NTCE , offering new possibilities for the automation and 

intelligence of educational assessment. Future research should continue to explore optimization 

strategies for the model to enhance its performance in generating complex and specialized questions, 

thereby  better serving the needs of the educational field. 

7 Reference 

[1]  Vaswani, A., Shazeer, N. M., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,  Ł., & 

Polosukhin, I. (2017). Attention is all you need. Neural Information Processing Systems. 

[2]  T Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. -A., Lacroix, T., Rozi ère, B., 

Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., & Lample, G. (2023). 

LLaMA: Open and efficient foundation language models. arXiv.org. 

[3]  Devlin, J., Chang, M. -W., Lee, K., & Toutanova, K. (2019). BERT: Pre -training of deep 

bidirectional transformers for language understanding. North American Chapter of the 

Association for Computational Linguistics. 

[4]  Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Xia, F., Le, Q., & Zhou, D. (2022). 

Chain of thought prompting elicits reasoning in large language models. Neural Information 

Processing Systems .

[5]  Fan, L., Li, L., Ma, Z., Lee, S., Yu, H., & Hemphill, L. (2023). A bibliometric review of large 

language models research from 2017 to 2023. arXiv.org. 

[6]  Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., 

Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L. E., Simens, 

M., Askell, A., Welinder, P., Christiano, P., Leike, J., & Lowe, R. J. (202 2). Training language 

models to follow instructions with human feedback. Neural Information Processing Systems. 

[7]  Douglas, M. R. (2023). Large language models. Communications of the ACM, 7(7), 7 -7. 

[8]  Saphra, N., Fleisig, E., Cho, K., & Lopez, A. (2023). First tragedy, then parse: History repeats 

itself in the new era of large language models. arXiv.org .

[9]  Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., 

Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert -Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., 

Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, 

I., & Amodei, D. (2020). Language models are few -shot learners. Neur al Information 

Processing Systems. 

[10]  Pallagani, V., Muppasani, B., Murugesan, K., Rossi, F., Srivastava, B., Horesh, L., Fabiano, 

F., & Loreggia, A. (2023). Understanding the capabilities of large language models for 

automated planning. arXiv.org .

[11]  Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., 

Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., 

Liu, P., Nie,  J., & Wen, J. -R. (2023). A survey of large language models. arXiv.org. 

[12]  Cao, W. P. (2022). Reflections on the practical work of interviewers for the qualification 

certificate of primary and secondary school art teachers. University Education, 148(10), 272 -

274. 

[13]  Feng, J. G. (2015). A preliminary discussion on the interview issues of the qualification 

examination for primary and secondary school teachers. China Examination, 273(01), 40 -44. 

[14]  Li, S. W. (2015). Reflections on the issues and countermeasures of the national examination 

interview for primary and secondary (kindergarten) teacher qualifications. Teaching and 

Management, 640(27), 57 -60. 

[15]  Ma, S. W., & Ma, Y. R. (2019). Research on the interview of teacher qualification 

examination based on educational teaching ability assessment. Examination Research, 

76(05), 40 -46. 

[16]  Ma, S. W., & Ma, Y. R. (2019). Research on the interview of teacher qualification 

examination based on educational teaching ability assessment. Examination Research, 

76(05), 40 -46. 

[17]  Qian, H. F. (2018). Scientificization of teacher qualification interviews: Issues and 

countermeasures. Teacher Education Forum, 31(03), 47 -50. 

8 Appendix 

Table 1  Number of Questions Corresponding to Each Category 

Category 

Questions  Count 

Actual 

Exam 

ChatGLM 

Generated 

Self -Cognition  20  20 

Interpersonal  Communication  20  20 

Organizational  Management  20  20 

Emergency  Response  20  20 

Comprehensive  Analysis  20  20 

Education and  Teaching  20  20 

Total  240 Table 2 Details of Prompt Engineering 

Role: You are  an examiner for a structured 

interview for teacher certification. You 

possess very professional knowledge of 

pedagogy and can understand Chinese 

accurately and without error. 

Task: Create a mock structured interview 

question for teacher certification. 

Question Category: Interpersonal 

Relationships. 

Quantity: 20. 

Examination Syllabus: {Official _published 

examination _syllabus} 

Past Real Questions: {Real _questions 

recalled _by _students} 

Notes: 

1. The examination syllabus and past real 

questions are for reference only. 

2. Do not include specific textbook 

knowledge points. 

Output content format according to pseudo -

code logic: 

[

{" Seq ":XX,"Question":"XXX"} 

]

Table 3:  Questionnaire for Question Scoring 

Question  Evaluation 

Items  Scoring Criteria 

Rating 

Scale 

(1 -5) 

Rating 

题目  A

Rationality 

of Question 

Design 

1. Relevance: Degree of match between the 

question and exam requirements  1-5

2. Practicality: Whether the question can 

assess actual teaching abilities  1-5

3. Open -endedness: Whether the  question 

allows for open -ended thinking  1-5

Balance of 

Question 

Difficulty 

4. Moderate Difficulty: Appropriateness of 

the question difficulty  1-5

5. Differentiation: Ability to distinguish 

between candidates of different levels  1-5

Logical 

Consistency 

of 

Questions 

6. Clarity of Question: Clarity of question 

description  1-5

7. Logical Structure: Rigor of question logic  1-5Table 3:  Questionnaire for Question Scoring 

Question  Evaluation 

Items  Scoring Criteria 

Rating 

Scale 

(1 -5) 

Rating 

Coverage of 

Knowledge 

and 

Abilities 

8. Multi -dimensional Ability Assessment: 

Whether the question covers  multiple abilities  1-5

9. Breadth of Knowledge: Whether the 

question involves multidisciplinary 

knowledge 

1-5

10. Diversity of Scenarios: Whether the 

scenarios reflect various educational settings  1-5

Innovation 

of 

Questions 

11. Novelty:  Whether the question has an 

innovative angle  1-5

12. Challenge: Whether the question provides 

an appropriate challenge to candidates  1-5

Fairness of 

Questions 

14. Fair Testing: Ability to test skills without 

relying on specific  backgrounds  1-5

Overall 

Rating 

15. Overall Rating: Overall quality of the 

question  1-5

Table 4  Summary of Mean and Standard Deviation for Question Categories 

Questio 

ns 

Source 

Actual Exam (Mean  ± STD)  ChatGLM  Generated (Mean  ± STD) 

Question  Categories 

Scoring 

Criteria  1 2 3 4 5 6 1 2 3 4 5 6

1

3.55 

±

0.81 

3.51 

±

0.78 

3.54 

±

0.80 

3.46 

±

0.80 

3.56 

±

0.89 

3.51 

±

0.77 

3.58 

±

0.75 

3.48 

±

0.81 

3.54 

±

0.73 

3.47 

±

0.76 

3.51 

±

0.80 

3.48 

±

0.76 

2

3.58 

±

0.74 

3.49 

±

0.76 

3.39 

±

0.76 

3.65 

±

0.86 

3.46 

±

0.67 

3.47 

±

0.80 

3.45 

±

0.73 

3.53 

±

0.80 

3.45 

±

0.74 

3.22 

±

0.68 

3.43 

±

0.69 

3.33 

±

0.77 

3

3.45 

±

0.76 

3.41 

±

0.84 

3.50 

±

0.80 

3.49 

±

0.76 

3.57 

±

0.82 

3.42 

±

0.87 

3.55 

±

0.87 

3.59 

±

0.82 

3.39 

±

0.82 

3.57 

±

0.71 

3.51 

±

0.87 

3.50 

±

0.76 

4

3.47 

±

0.69 

3.69 

±

0.69 

3.56 

±

0.77 

3.43 

±

0.83 

3.37 

±

0.77 

3.44 

±

0.92 

3.49 

±

0.82 

3.58 

±

0.78 

3.44 

±

0.81 

3.48 

±

0.72 

3.56 

±

0.84 

3.55 

±

0.82 

5

3.54 

±

0.78 

3.71 

±

0.76 

3.41 

±

0.81 

3.53 

±

0.90 

3.44 

±

0.80 

3.52 

±

0.78 

3.47 

±

0.87 

3.49 

±

0.87 

3.66 

±

0.83 

3.48 

±

0.80 

3.41 

±

0.78 

3.50 

±

0.81 

6 3.44 

±

3.49 

±

3.49 

±

3.50 

±

3.52 

±

3.48 

±

3.43 

±

3.52 

±

3.42 

±

3.38 

±

3.49 

±

3.61 

±Table 4  Summary of Mean and Standard Deviation for Question Categories 

Questio 

ns 

Source 

Actual Exam (Mean  ± STD)  ChatGLM  Generated (Mean  ± STD) 

Question  Categories 

Scoring 

Criteria  1 2 3 4 5 6 1 2 3 4 5 6

0.80  0.76  0.82  0.81  0.78  0.77  0.71  0.66  0.73  0.86  0.76  0.83 

7

3.52 

±

0.70 

3.49 

±

0.81 

3.50 

±

0.73 

3.71 

±

0.76 

3.59 

±

0.77 

3.53 

±

0.81 

3.47 

±

0.87 

3.42 

±

0.81 

3.51 

±

0.86 

3.55 

±

0.82 

3.52 

±

0.75 

3.47 

±

0.83 

8

3.58 

±

0.84 

3.51 

±

0.80 

3.46 

±

0.83 

3.71 

±

0.77 

3.53 

±

0.90 

3.55 

±

0.81 

3.50 

±

0.72 

3.53 

±

0.73 

3.58 

±

0.79 

3.52 

±

0.81 

3.47 

±

0.81 

3.54 

±

0.81 

9

3.55 

±

0.77 

3.48 

±

0.78 

3.50 

±

0.77 

3.55 

±

0.82 

3.40 

±

0.77 

3.60 

±

0.90 

3.52 

±

0.76 

3.58 

±

0.81 

3.52 

±

0.87 

3.56 

±

0.74 

3.33 

±

0.73 

3.56 

±

0.84 

10 

3.62 

±

0.75 

3.37 

±

0.81 

3.44 

±

0.84 

3.49 

±

0.75 

3.50 

±

0.78 

3.43 

±

0.74 

3.55 

±

0.87 

3.54 

±

0.76 

3.54 

±

0.72 

3.51 

±

0.77 

3.51 

±

0.75 

3.51 

±

0.81 

11 

3.47 

±

0.86 

3.56 

±

0.83 

3.43 

±

0.84 

3.60 

±

0.82 

3.45 

±

0.85 

3.55 

±

0.82 

3.62 

±

0.78 

3.51 

±

0.95 

3.60 

±

0.78 

3.45 

±

0.83 

3.64 

±

0.76 

3.59 

±

0.71 

12 

3.46 

±

0.78 

3.53 

±

0.77 

3.46 

±

0.81 

3.49 

±

0.83 

3.42 

±

0.83 

3.53 

±

0.81 

3.48 

±

0.80 

3.47 

±

0.82 

3.52 

±

0.66 

3.54 

±

0.77 

3.40 

±

0.84 

3.43 

±

0.76 

13 

3.38 

±

0.63 

3.54 

±

0.86 

3.49 

±

0.72 

3.45 

±

0.87 

3.55 

±

0.74 

3.53 

±

0.74 

3.40 

±

0.79 

3.54 

±

0.85 

3.47 

±

0.80 

3.60 

±

0.88 

3.48 

±

0.70 

3.66 

±

0.83 

14 

3.48 

±

0.83 

3.55 

±

0.78 

3.58 

±

0.83 

3.43 

±

0.74 

3.52 

±

0.73 

3.42 

±

0.77 

3.58 

±

0.71 

3.47 

±

0.77 

3.42 

±

0.81 

3.58 

±

0.73 

3.33 

±

0.74 

3.53 

±

0.81 

15 

3.65 

±

0.86 

3.36 

±

0.80 

3.56 

±

0.80 

3.39 

±

0.76 

3.54 

±

0.77 

3.51 

±

0.78 

3.59 

±

0.85 

3.52 

±

0.88 

3.51 

±

0.76 

3.51 

±

0.77 

3.54 

±

0.86 

3.54 

±

0.72 

Table 5 One -Way ANOVA Analysis (Welch's Method) 

Based on Question Source 

Scoring 

Criteria  f-value  df 1 df 2 p-

value 

Actual Exam 

(Mean  ± STD) 

ChatGLM Generated 

(Mean  ± STD) 

1 0.06596  1 1195  0.797  3.52 ±0.81  3.51 ±0.77 

2 5.82557  1 1196  0.016  3.51 ±0.77  3.40 ±0.74 

3 0.93142  1 1198  0.335  3.47 ±0.81  3.52 ±0.81 

4 0.25958  1 1198  0.611  3.49 ±0.79  3.52 ±0.80 Table 5 One -Way ANOVA Analysis (Welch's Method) 

Based on Question Source 

Scoring 

Criteria  f-value  df 1 df 2 p-

value 

Actual Exam 

(Mean  ± STD) 

ChatGLM Generated 

(Mean  ± STD) 

5 0.24410  1 1197  0.621  3.52 ±0.81  3.50 ±0.83 

6 0.06799  1 1197  0.794  3.49 ±0.79  3.48 ±0.76 

7 2.11844  1 1192  0.146  3.56 ±0.76  3.49 ±0.82 

8 0.51744  1 1193  0.472  3.56 ±0.83  3.52 ±0.78 

9 0.00131  1 1198  0.971  3.51 ±0.80  3.51 ±0.79 

10  1.31931  1 1198  0.251  3.48 ±0.78  3.53 ±0.78 

11  1.51894  1 1196  0.218  3.51 ±0.84  3.57 ±0.80 

12  0.03339  1 1196  0.855  3.48 ±0.80  3.47 ±0.77 

13  0.59210  1 1194  0.442  3.49 ±0.76  3.52 ±0.81 

14  0.06834  1 1197  0.794  3.50 ±0.78  3.48 ±0.76 

15  0.51777  1 1198  0.472  3.50 ±0.80  3.54 ±0.81
