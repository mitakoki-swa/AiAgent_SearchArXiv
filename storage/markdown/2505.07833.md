Title: Patchwork: A Unified Framework for RAG Serving

URL Source: http://arxiv.org/pdf/2505.07833v1

Published Time: Wed, 14 May 2025 00:00:38 GMT

Markdown Content:
# Patchwork : A Unified Framework for RAG Serving 

## Bodun Hu ∗

UT Austin 

## Luis Pabon ∗

UT Austin 

## Saurabh Agarwal 

UT Austin 

## Aditya Akella 

UT Austin 

Abstract 

Retrieval Augmented Generation (RAG) has emerged as a new paradigm for enhancing Large Language Model relia-bility through integration with external knowledge sources. However, efficient deployment of these systems presents sig-nificant technical challenges due to their inherently heteroge-neous computational pipelines comprising LLMs, databases, and specialized processing components. We introduce Patch-work , a comprehensive end-to-end RAG serving framework designed to address these efficiency bottlenecks. Patchwork ’s architecture offers three key innovations: First, it provides a flexible specification interface enabling users to implement custom RAG pipelines. Secondly, it deploys these pipelines as distributed inference systems while optimizing for the unique scalability characteristics of individual RAG com-ponents. Third, Patchwork incorporates an online sched-uling mechanism that continuously monitors request load and execution progress, dynamically minimizing SLO viola-tions through strategic request prioritization and resource auto-scaling. Our experimental evaluation across four dis-tinct RAG implementations demonstrates that Patchwork 

delivers substantial performance improvements over com-mercial alternatives, achieving throughput gains exceeding 48% while simultaneously reducing SLO violations by ∼24%. 

1 Introduction 

With their rapid advancements, Large Language Models (LLMs) have become indispensable tools across a range of applications, including question answering [ 59 , 80 ], code generation [ 40 , 87 ], task automation [ 27 ], and search assis-tants. Despite major progress in underlying machine learning (ML) techniques, LLMs still face several fundamental limita-tions—such as hallucinations [ 20 ], outdated knowledge [ 37 ], and confinement to the snapshot of training data. To address these issues, Retrieval-Augmented Generation (RAG) systems [ 37 ] have emerged as a widely adopted so-lution, particularly for applications where factual accuracy is critical. Figure 1 provides an overview of a typical RAG architecture. As illustrated, a RAG system supplements an LLM with a knowledge base—often an external, indexed data-base—whose contents are retrieved and incorporated into user queries before being passed to the LLM. Some RAG variants [ 6 , 32 , 68 , 69 ] go further by enabling recursive inter-actions between the LLM and the knowledge base, allowing 

> ∗Both authors contributed equally to this research.

iterative refinements of the response. This integration has been shown to significantly improve both the factual ac-curacy and generation quality of LLM outputs. Moreover, RAG systems offer a practical alternative to costly model re-training, providing a scalable and extensible mechanism for keeping LLMs up to date with new information. These advan-tages have spurred rapid research and real-world adoption of RAG techniques [17, 18, 25, 46, 58, 84]. Given the growing ubiquity of RAG systems, optimizing their performance is becoming increasingly critical. However, three key challenges make this far from straightforward: 

1. Rapid Evolution of the Stack : The RAG ecosystem is evolv-ing at a breakneck pace, with new components, execution strategies, and architectural patterns emerging frequently. This constant change compounds the complexity faced by users in designing and deploying RAG pipelines. More impor-tantly, the resulting fragmentation—a patchwork of bespoke solutions—makes it difficult to develop and apply general, reusable performance management techniques. 

2. Heterogeneity of Components : RAG consist of diverse components—including vector databases, query augmenters, and LLM serving engines—each with unique computational characteristics and scaling behaviors. This makes perfor-mance control via resource management and workload par-titioning significantly more challenging than in monolithic LLM deployments. In this work we call this collection of diverse components a RAG pipeline. 

3. Unpredictable Execution Time : Whereas conventional neural inference exhibits stable and predictable latency [ 22 ,50 ], RAG pipelines introduce wide variability. Query-dependent branching, conditional execution paths, and recursive calls to retrievers or LLMs cause highly unpredictable execu-tion times, complicating RAG systems’ ability to meet tight service-level objectives (SLOs). This unique combination of attributes is un-studied in the computer systems resource allocation space, making the problem we target novel and interesting. To this end, our work introduces Patchwork , a system designed for the scalable, resource-efficient deployment of RAG pipelines. 

Patchwork ’s ground-up design directly addresses the above challenges enabling robust, high-performance operation of modern RAG systems. The key insight powering Patch-work ’s design is that enabling efficient RAG systems while both embracing the above complexities and hiding them from users requires a high-level, cross-component approach. To this end, Patchwork is designed as an intent-driven end-to-end 

> 1
> arXiv:2505.07833v1 [cs.DC] 1 May 2025 Large Language Model
> Indexed Knowledge Base
> Query Indexer
> RAG Logic
> LLM Query

Figure 1. Overview of RAG Pipeline: The above figure shows a schematic of RAG pipeline with details in Section 2. RAG logic can vary dramatically from simple concatenation operation to a com-plicated execution logic with conditional and recursive operations. 

framework. Users simply specify the RAG pipeline. Patch-work then automatically orchestrates the entire pipeline, conducting holistic performance-driven component-level resource management, resource allocation, and sharing de-pending on component-specific scalability patterns. This is in contrast to prior RAG approaches [ 60 , 65 ] that are point solutions – they optimize specific components of a RAG pipeline, and as such, they cannot offer improvements as the workload and component characteristics change. Specifically, Patchwork introduces three key ideas. First, to support rapidly evolving RAG workloads, Patchwork ’s 

light-weight client library provides an interface for users to specify their pipelines using a set of simple, high-level mod-ular abstractions. Through this interface, pipelines can be implemented in standard Python, requiring users only to annotate their compute nodes using a lightweight decora-tor provided by Patchwork . The client library also allows users to easily create new RAG components or extend exist-ing ones. Patchwork then automatically captures the full computational graph using the Python frame evaluation API (introduced in Python 3.6 via PEP 523 [ 56 ]). This enables deep inspection and orchestration of the pipeline without requiring changes to user logic. To further support the con-tinual evolution of RAG systems, Patchwork also allows easy extension of these abstractions, enabling seamless in-tegration of new components as they emerge. Additionally, 

Patchwork ’s interface allows users to write the high-level declarative execution pipeline, at time of deployment Patch-work automatically distributes this computation akin to prior data processing systems [16, 82]. Next, to support heterogeneous resource requirements and scaling patterns of RAG components, the Patchwork sched-uler offline component leverages the client library-provided computation graph to perform component-level resource allo-cation and batching . Here, Patchwork abstracts the details of a RAG system’s underlying components by modeling both problems using a novel max-flow formulation aimed at max-imizing pipeline throughput. Finally, to minimize SLO violations despite unpredictable execution times, Patchwork scheduler’s online component 

continuously tracks the run time of requests being served, estimates the likelihood of a particular request violating the SLO, and, correspondingly adjusts scheduling and resource allocations in a performance-aware fashion . Specifically, upon observing that a request is likely to miss its SLO, the online component takes proactive measures including: (i) prioritiz-ing such requests, (ii) temporarily pausing accepting new requests, and (iii) auto-scaling resources where possible. To show the efficacy of Patchwork , we implement four different RAG algorithms - and show that Patchwork can improve the throughput of RAG serving between 48% and 20 ×. Further, Patchwork also reduces SLO violation by up to 24%. In Summary with Patchwork we make following contri-butions -

• To the best of our knowledge we provide the first end to end system for deploying RAG pipelines. 

• We introduce a modular interface allowing users to implement any RAG components and control logic in Python without requiring custom DSLs. 

• Patchwork ’s scheduler automatically and holistically determines component-level resource allocation and batching that maximizes throughput. 

• Patchwork ’s online scheduler tracks request execu-tion to predict and prevent SLO violations despite high execution-time variability. 

2 Background and Motivation 

We first provide an overview of RAG systems and the struc-ture of a typical RAG pipeline. 

2.1 Retrieval-Augmented Generation 

Retrieval-Augmented Generation (RAG) [36] is a technique that enhances the capabilities of a Large Language Model (LLM) by supplementing it with an external, indexed knowl-edge base. In a typical RAG setup, a user query triggers the retrieval of relevant documents or data from this external source. The retrieved content is incorporated into a prompt and passed to the LLM (or another generative model, such as a multi-modal model) to generate a response. This aug-mentation improves accuracy and mitigates hallucinations. Unlike standard LLM serving [ 4 , 34 , 47 ], RAG systems involve multiple interconnected components. In this paper, we refer to the ordered sequence of these components as the RAG pipeline . Figure 2 illustrates a schematic of typical components a query might go through in a RAG pipeline and their configuration parameters. 

Problem statement. The goal of a RAG serving system is to allocate resources to different pipeline components (which includes replicating some of them) and partition work (across replicas) so as to maximize throughput and minimize SLO violations for requests while operating under bounded RAG pipeline resources (e.g., CPUs, GPUs, and memory).       

> 2Query Rewriter
> User Query Retriever Retrieval Post Processing Augmentor Candidate Analysis LLM Generator Output Analysis
> Rewriting Parameters, Models Encoding Size, Database choice Retrieval Logic Post Processing logic, Metric Query size, Augmentation Logic Query size, Augmentation Logic Model Choice, Augmented Data Size Quality Metric

Figure 2. Components of a RAG pipeline: The above provides a schematic of RAG execution pipeline. The box with solid colors represent the mandatory components, while the the box with hatch pattern represents the optional pattern. The text below the boxes represent a subset of parameters on which the scalability depends. 5000 10000 20000 40000 80000 

> Number of documents
> 0.00000
> 0.00002
> 0.00004
> 0.00006
> 0.00008
> 0.00010
> 0.00012
> 0.00014
> 0.00016
> Per query latency (s)  Qdrant
> Chroma

Figure 3. Scaling with documents: In the above figure, we show the time to find five nearest neighbors using L2 distance metrics scale with number of documents for two popular vector databases-Qdrant and ChromaDB. We observe that the time taken per query by Qdrant scales faster than ChromaDB, highlighting different scalability patterns. 5 10 100 200 

> Number of nearest neighbors
> 0.00
> 0.01
> 0.02
> 0.03
> 0.04
> 0.05
> 0.06
> Per-query latency (s)  Qdrant
> Chroma

Figure 4. Scaling with candidates: In the above figure, we show how the time for ChromaDB and Qdrant scales with different num-bers of nearest neighbors. We observe that these two databases behave differently: ChromaDB remains largely constant as the num-ber of nearest neighbors increases, while Qdrant scales at a high rate. 

Achieving this goal is challenging for three reasons – evo-lution, heterogeneity, and unpredictability – which we dis-cuss in the following subsections. 

2.2 RAG Evolution 

The RAG ecosystem is evolving rapidly with new compo-nents and refinements to existing ones. In the following paragraphs, we survey the most common components of the RAG pipeline and their evolution. We also provide a brief overview of new components that are being added. 

Retriever. The retriever is responsible for fetching rel-evant information from an indexed data store. This store typically contains data that is domain-specific and deemed useful for responding to queries. For instance, a RAG system deployed in a legal firm [ 57 ] might use a database indexed with legal briefs and case records. Retrievers are evolving rapidly. One dimension of evolu-tion is the indexing approach, which is tied to the retrieval strategy. Early RAG systems used plain-text indices [ 33 , 36 ,72 ], but newer approaches to index documents are using vec-tor representations [ 25 , 58 ], graphs [ 17 , 26 ], trees [ 19 , 39 , 43 ], or bitmaps [ 11 ]. More recently, dynamic retrieval [ 38 ] is see-ing adoption; here, for a given query, during the retrieval stage, multiple retriever steps are performed iteratively to achieve better quality of retrieved texts [ 38 ] and the retrieved output is post-processed. 

Augmenter. The augmenter determines how the retrieved documents are incorporated into the original user query. This process can range from simple concatenation [ 6] to more sophisticated recent methods that employ LLMs or classifiers to summarize, rank, or filter the retrieved content before it is added to the query [62]. 

Generator. The generator is typically an LLM that pro-duces the final response based on the augmented query. Re-cent research [ 6 , 71 ] focuses on training models specifically fine-tuned for RAG scenarios. A given generator model can be used alongside of several LLM serving engines, such as vLLM[ 34 ], Ollama[ 54 ], and llama.cpp [ 44 ] are frequently used. The choice of the model and serving engine depends on the desired trade-offs between latency, accuracy, and re-source availability. 

Additional Components. As RAG systems mature, ad-ditional components are being introduced to enhance qual-ity, efficiency, and safety. One common addition is query pre- and post-processing, where models [ 36 , 46 ] either trans-form the user query before retrieval or rephrase it after augmentation to better suit generation. Other notable ex-amples are components that: improve safety [ 41 , 49 , 76 ]; enhance computational efficiency [ 23 , 66 , 78 ], and boost ac-curacy [7, 8, 38, 45, 51]. The large number of components and execution logic which change how these components are interacting make it extremely challenging for developers to implement. Fur-ther, abstractions built by commercial organizations like LangChain/LangGraph [ 35 ] force developers to use multi-ple layer of abstractions making even simple changes hard, and forcing developers to [ 75 ] to write their own monoliths. Additionally, existing abstractions (e.g., those developed by Langchain) often require RAG developers to wait for the framework developers if they want access to a new feature; 

e.g. , , a new augmentation scheme which has been released in a publication requires significant amount of effort to provide an implementation. Thus, we arrive at the following: 

> 3

Requirement 1. A RAG serving system should allow users to incorporate new components or adapt them while abstracting the complexity of the underlying pipeline resource allocation. 

2.3 Heterogeneity in RAG Pipelines Resource heterogeneity. Different components of a RAG pipeline rely on different types of hardware resources. Re-trievers, typically implemented as vector databases, are CPU-bound and memory- or disk-intensive, while generators, which run large language models (LLMs), are GPU-intensive. Augmenters may rely solely on CPUs or use GPUs as well. A RAG serving system should be able to handle the hetero-geneous resource requirements of different components. 

Performance heterogeneity. Different components in RAG pipelines show disparate performance scaling prop-erties. We observe multiple sources that contribute to this heterogeneity across the components of the RAG pipeline. These first is the choice of components at a given RAG stage. To illustrate this, in Figure 3, we compare the scaling patterns of two retrievers, Qdrant and Chroma [ 14 ], as a function of the number of indexed documents: the per-query latency for Qdrant scales much faster, while for ChromaDB it remains constant. Similarly, in Figure 4, Qdrant’s per-query latency scales at a significantly higher rate than ChromaDB’s as the number of nearest neighbors increases. The second source of heterogeneity is a component’s static configuration. Each RAG component has several different options that can be configured; e.g. retrievers can choose the distance metric, number of candidates to return, the encoding size, etc. Similarly, generators can choose the model size and the context length to use. These specific choices impact performance [12]. Another source is a component’s dynamic configuration choices, such as batch size. Figure 5 illustrates the batch-size dependencies of different retrievers’ performance: compared to Qdrant, Chroma benefits more from larger batch sizes. Similarly, Figure 6 compares the scaling patterns for different LLM serving systems: vLLM’s throughput scales much more dramatically compared to the others. These heterogeneities have a direct impact on resource allocation and work partitioning in our problem statement. 

Example of impact. Consider the following simple RAG pipeline, where the retriever takes 2 units of time to process a batch, and the time per query remains constant regardless of batching (similar to Qdrant). The augmenter takes 1 unit of time, and the time per item remains constant regardless of the batch size. Finally, the generator takes 10 units of time per item, and this time decreases linearly with batch size (similar to vLLM). Assume we have 5 CPUs and 2 GPUs available. Using the formulation presented later in this paper, we determine that the throughput-maximizing configuration for the above pipeline is achieved with a batch size of 70, allocating 3 CPUs to the retriever, 2 CPUs to the augmenter, and 1 GPU to the generator. Notably, the retriever serves as the primary bottleneck in this setup. Now, suppose the developer updates the pipeline to use a different retriever, such as ChromaDB, which scales better; everything else remains the same. Here, the throughput-maximizing configuration would give 1 CPU to the retriever, 4 CPUs to the augmenter (to which the bottleneck now shifts), and 1 GPU to the generator. Further, we should use the maximum batch size possible which fits in the memory, as this will maximize the system’s throughput. It is easy to construct similar scenarios showing that changes in the static configuration of chosen components can also lead to changes in ideal resource allocations. 

On choosing configurations. Today, component choices are often driven by developer convenience. For instance, de-ploying ‘llama.cpp‘ is significantly simpler than setting up ‘vLLM‘. As a result, real-world RAG pipelines tend to use a di-verse set of components. Likewise, RAG developers fine-tune static configuration parameters to maximize query accuracy. We argue that serving systems should not override these developer-specified component choices and configurations (as advocated by prior work [ 60 ]) because doing so can ob-scure developers’ understanding of their system’s achievable or target accuracy and impede system manageability. Thus, our work places the following requirement: 

Requirement 2. The serving system must optimize the per-formance of the pipeline as chosen and configured by the de-veloper. As highlighted above, to do so effectively, the system must have a holistic understanding of how component choices and configurations affect scalability and performance. 

2.4 Unpredictability in Execution Time 

Several variants of RAGs incorporate conditional execution ,where the path taken by the execution pipeline depends on the input. For example, Corrective RAG [ 79 ] includes a conditional block that performs accuracy verification on the retrieved candidates. If the verification fails, the retriever is rerun with a modified configuration. Such runtime decisions can cause significant variation in RAG throughput. As shown in Figure 7, unlike standard LLM inference—where context length is a good predictor of execution time—the runtime of RAG pipelines is not easily estimated using the user-provide input query. In particular, we observe that RAGs with conditional or recursive control flow, such as IRCoT, exhibit highly unpredictable inference performance. This unpredictability poses challenges for serv-ing RAG systems. In particular, prior works [ 15 , 22 ] assume predictability in model serving times, thus enabling them to do better scheduling. 

Requirement 3. The variable running time of RAG queries suggests that active query management at runtime is essential to prevent SLO violations.     

> 414816 32
> Batch size
> 0.000
> 0.002
> 0.004
> 0.006
> 0.008
> 0.010
> 0.012
> 0.014
> Per-query latency (s)  Qdrant
> Chroma

Figure 5. Scaling behavior of Qdrant and ChromaDB with batch size, using cosine sim-ilarity as the distance metric. 1 2 4 8 16 

> Batch size
> 0
> 200
> 400
> 600
> 800
> 1000
> 1200
> Tokens per second
> Ollama
> Llama.cpp
> vLLM

Figure 6. Scaling of LLM engines: 

Throughput scaling for Ollama, Llama.cpp, and vLLM with a context length of 8192. 5 10 15 20 25 30 35   

> Query length (tokens)
> 2
> 4
> 6
> 8
> 10
> 12
> Per-query latency (s)
> IRCoT MemoRAG HippoRAG

Figure 7. RAG Scatter Plot: Query latency of IRCoT, MemoRAG, and HippoRAG across query lengths. User RAG pipeline 

> Hetrogenous Resource
> Harmonia Graph Capture
> Harmonia Scheduer
> orchestrator
> Retriever
> Retriever
> Augmenter
> Generator
> Generator

Figure 8. Patchwork overview: Patchwork consists of Patch-work ’s client library and Patchwork scheduler. The user’s RAG pipeline is written in Patchwork ’s client library. 

Resource allocation problems at the intersection of the above requirements – brought to the fore by RAGs - have not been well studied. The unique interplay of factors introduces a layer of complexity that current systems struggle to handle. Addressing this problem opens a key problem space with implications for efficient resource management in future compound AI systems. 

3 Patchwork 

Next, we introduce Patchwork , an end-to-end serving sys-tem for optimized serving of generic RAG pipelines that meets the three requirements outlined above. We start by briefly introducing the key components of Patchwork in Section 3.1. 

3.1 Patchwork Overview 

Figure 8 provides an overview of Patchwork . Patchwork 

consists of three major components - (i) client library, (ii) scheduler, and (iii) orchestrator. The Patchwork client li-brary provides a modular interface that allows users to eas-ily specify the components of their RAG pipeline and de-fine the pipeline’s execution logic. Once a user has specified the pipeline, the client library captures the execution graph and sends it to the Patchwork scheduler. The Patchwork 

scheduler has two modes - In the offline mode, it allocates resources and determines the run-time pipeline-wide config-uration to maximize throughput by crafting a max-flow atop the compute graph. At run-time, the Patchwork ’s scheduler continuously tracks resources and performs query routing and dynamic query scheduling to minimize SLO violations. 

Patchwork ’s scheduler deploys the RAG pipeline for infer-ence, with the aid of an orchestrator that automatically adds communication primitives to each orchestrated node. 

3.2 RAG Specification 

Before introducing Patchwork ’s programming interface, we want to highlight the goals of our intent-driven pro-gramming API. - (i) Offer a convenient way to specify RAG execution pipeline, (ii) Enable users to easily add new com-ponents in RAG pipeline as well as create variants of existing components, and (iii) Avoid high barrier-to-entry by staying close to familiar APIs and interfaces. To specify compute graphs, prior works [ 5 , 48 , 55 , 82 ]typically used Domain Specific Languages (DSLs). However, custom DSL can be inhibitive as users need to learn a new programming interface. Further, RAG components are com-posed of several external library components, for e.g. , , a retriever may be built using one of the several choices of databases. Adding a custom DSL to support new external library components demands significant maintenance effort; 

i.e. , , for each new evolving component, we will have to make changes to the DSL and add a new function. This is akin to PyTorch needing to implement new types of modules as ML evolves. To build Patchwork ’s interface, we took a different ap-proach. We let users write custom execution components within individual Python classes; the execution pipeline is also written in regular Python. Figure 9 shows a snippet of a RAG execution pipeline specified using Patchwork ’s interface. Lines 5-8 show the creation of a user-defined com-ponent abstraction called Grader . This component is respon-sible for evaluating the relevance of retrieved documents by leveraging an LLM. By marking the RAG component with 

Patchwork ’s decorator (line 6), users enable Patchwork to distinguish calls to RAG compute nodes from regular Python conditional logic. Next, in lines 10-15, the user defines the fundamental RAG components to use, supplying the necessary initialization parameters for each. Patchwork automatically captures these parameters and initializes the components at deploy-ment time. Next, in lines 18-27, the user implements the                                                                                                           

> 51import harmonia
> 2from harmonia import Augmenter , Retriever , VLLM , WebSearch , Rewriter
> 34# ########## 1. Initializing harmonia ###########
> 5class Grader ( VLLM ):
> 6@harmonia . make
> 7def custom_inference ( self , input ):
> 8return has_relevant_doc ( call_vllm ( input ))
> 910 augmenter =Augmenter ( num_threads =32)
> 11 rewriter =Rewriter ( writer_prompt = 'Add more context to query . ')
> 12 retriever =Retriever ( topk =3)
> 13 grader =Grader ( sys_prompt = 'Does retrieved docs have relevant info ? ')
> 14 web =WebSearch ( output_format = list )
> 15 llm =VLLM ( temp =0.7)
> 16 17 # ########## 2. Building computational graph ###########
> 18 query ='Which Linux distro suits beginners best ? '
> 19 with harmonia . capture ():
> 20 retrieved_data =retriever . retrieve ( query )
> 21 has_relevant_doc =grader . generate ( retrieved_data )
> 22 if not has_relevant_doc :
> 23 better_query =rewriter . custom_inference ( query )
> 24 doc =web . search ( better_query )
> 25 return llm . generate ( better_query , doc )
> 26 else :
> 27 return llm . generate ( query , retrieved_data )
> 28 29 # ########## 3. Automatic RAG deployment ###########
> 30 inference_endpoint =harmonia . deploy ()

Figure 9. Patchwork flow: The code above shows an example of how to leverage abstraction to efficiently construct a RAG pipeline within Patchwork .

logic of the RAG pipeline within a single-node environment, where Patchwork captures the computational graph. Fi-nally, Patchwork seamlessly deploys this graph into a dis-tributed environment and returns an inference API endpoint that accepts queries (line 30). To capture the execution pipeline, Patchwork leverages PEP 523 [ 56 ], a frame evaluation API introduced in Python 3.6. This API enables interception of Python object frames and the code being executed. Using this mechanism, Patch-work can observe arbitrary function calls and extract the corresponding Python bytecode to transfer to a remote ma-chine for inference. Figure 9 demonstrates the simplicity of Patchwork ’s spec-ification interface. This design enables users to define their RAG execution pipelines as though operating under single-node inference. 

3.3 Resource Allocation and Configuration Selection 

In Section 2, we discussed how different RAG components exhibit varying scalability patterns based on the choice of component, its configuration, and the batch size. These di-verse performance characteristics demand careful resource allocation, as the system bottleneck can shift significantly across different pipeline implementations or over time as the pipeline evolves. Since users often invest considerable effort in tuning certain component configurations (such as the number of documents retrieved or the generator model used) to optimize accuracy, it is undesirable for a frame-work to alter these configurations as they directly impact model quality. Instead, Patchwork focuses on improving performance by optimizing resource allocation and batch sizing to maximize serving throughput, without modifying accuracy-critical settings. To maximize throughput, Patchwork jointly optimizes the batch size and resource allocation by formulating it as maximizing concurrent-flow through a compute graph. The maximizing concurrent flow problem assumes that each node in the graph can concurrently send and receive batches to process. Next, we discuss the problem formulation. 

Problem Formulation Let 𝐺 be a directed graph repre-senting the RAG pipeline with 𝑁 components. We assume that there are 𝐾 different node types and a total 𝑅 nodes available for allocation, with 𝑟 𝑘 nodes of type 𝑘 ∈ 𝐾 , such that Í𝐾 𝑘 𝑟 𝑘 = 𝑅 . Next, we use 𝑚 𝑖,𝑘 to denote the maximum batch size which can run on a node of type 𝑘 for a component 

𝑖 ∈ 𝑁 ; the maximum batch size is determined by memory or storage constraints. Our goal is to determine 𝑏 𝑖,𝑘 and 𝑎 𝑖,𝑘 , where 𝑏 𝑖,𝑘 is the batch size to be processed by component 𝑖 ∈ 𝑁 on node type 𝑘 ∈ 𝐾 , and 𝑎 𝑖,𝑘 is the number of resources of compute node type 𝑘 allocated to component 𝑖 . Our objective is to maximize the overall processing throughput. We assume we can estimate the time to process a sin-gle query by component 𝑖 of the RAG execution pipeline given a batch size 𝑏 𝑖,𝑘 and resource allocation 𝑎 𝑖,𝑘 , denoted as 𝑇 𝑖 (𝑏 𝑖,𝑘 , 𝑎 𝑖,𝑘 ). We provide details of how we estimate 𝑇 𝑖 later in the section. Our maximum throughput objective can be rewritten as maximizing the lowest throughput across all components in the graph. In other words: 

max min 

∀𝑖 ∈𝑁 𝐾 ∑︁ 

𝑘 =1

𝑏 𝑖,𝑘 

𝑇 𝑖 (𝑏 𝑖,𝑘 , 𝑎 𝑖,𝑘 ) (1) We now introduce the constraints in our optimization problem. The first natural constraint is that the number of resources allocated across components should not exceed the total resources available: 

𝑁 ∑︁ 

𝑖 𝐾 ∑︁ 

𝑘 

𝑎 𝑖,𝑘 ≤ 𝑅 (2) The second is that the net batch size of a component across all assigned compute nodes is less than the sum of the batch sizes processed by its immediate predecessors: 

𝐾 ∑︁ 

𝑘 =1

𝑏 𝑖,𝑘 ≤

𝐾 ∑︁ 

𝑘 =1∑︁ 𝑔 

𝑏 𝑔,𝑘 ∀𝑖 ∈ 𝑁 , 𝑔 ∀( 𝑔, 𝑖 ) ∈ 𝐺, (3) Our final constraint ensure that the minimum batch size allocated to a component is at least one, as is the number of resources allocated. 

𝑏 𝑖,𝑘 ≥ 1 ∀𝑖 ∈ 𝑁 , ∃𝑘 ∈ 𝐾 (4) 

𝑟 𝑖,𝑘 ≥ 1 ∀𝑖 ∈ 𝑁 , ∃𝑘 ∈ 𝐾 (5) Finally, the batch size allocated to a compute node should not exceed the maximum batch size the node can support. 

𝑏 𝑖,𝑘 ≤ 𝑚 𝑖,𝑘 ∀𝑖 ∈ 𝑁 , 𝑘 ∈ 𝐾 (6) 

> 6

Estimating 𝑇 𝑖 . Patchwork uses scalable profiling to es-timate 𝑇 𝑖 (the time taken by a configuration choice) in the formulation above. Specifically, we leverage the insight that exact measurements for every batch size are unnecessary — we only need to understand how execution time scales with batch size. To achieve this, we start from an acceptable maximum batch size and use binary search to evaluate the scaling behavior. If a profiled point yields a throughput im-provement below a fixed threshold, we continue the binary search in the lower half. To further enhance scalability, we model 𝑇 𝑖 (𝑏 𝑖 , 𝑟 𝑖,𝑘 ) as a piecewise linear function. We apply linear curve fitting to au-tomatically determine the breakpoints and linear segments. This piecewise linear approximation allows us to formu-late and solve the overall optimization problem as a mixed-integer linear program (MILP). We use Gurobi [ 3 ] to solve the resulting optimization problem. It is important to note, however, that 𝑇 𝑖 remains an approx-imation, as execution time per batch can still vary based on factors such as query size and configuration parameters. Ad-ditionally, RAG pipelines often involve conditional execution paths, causing further variability in component runtimes. As a result, despite careful resource allocation and optimiza-tion, SLO violations would still occur due to the inherently unpredictable nature of execution times. Next, we describe how we mitigate SLO violations. 

3.4 Minimizing SLO violations 

We begin by discussing the sources of SLO violations in ML systems, how RAG serving systems differ, and why RAG systems require a rethinking of SLO violation management. Finally, we discuss Patchwork ’s approach to the issue. In "traditional" ML systems, SLO violations generally stem from two main causes. First, under high load, a large queue of incoming requests can build up, leading to delayed re-sponses for subsequent requests [ 22 ]. Second, poor sched-uling decisions can assign requests to straggler nodes or nodes experiencing high interference, resulting in latency spikes [61]. Recently, in the context of LLM serving—where request runtimes can vary significantly due to differing computa-tional demands, a new class of SLO violations has emerged, driven by variability in request completion times. Recent work [ 9, 52 ] has proposed runtime-aware scheduling strate-gies that estimate execution time at scheduling time and preemptively drop requests that are unlikely to meet their SLOs. These systems typically assume that a request’s run-time can be predicted based on its input query length. While this assumption may hold for standard LLM workloads, Fig-ure 7 shows that it does not hold for RAG pipelines. Thus, when serving RAG pipelines, a different strategy is needed to minimize SLO violations. 8 12 16 20 24 32 48 64 

> Request Rate (req/s)
> 0
> 10
> 20
> 30
> 40
> 50
> 60
> 70
> 80
> SLO Violation (%)
> Running Average Estimator
> XGBoost

Figure 10. Scaling with request rate: The figure above shows the percentage of requests that result in SLO violations at various request rates when running on HippoRAG, with SLO enforcement enabled. We observe that performance is largely similar between XGBoost and the running average method. 

Goal. Prior systems such as [ 22 ], which aim to minimize SLO violations, typically drop requests that are predicted to miss their SLO. However, this approach is suboptimal in the context of RAG systems. Dropping a request not only wastes the compute already spent on its partial execution but may also lead to the user resending the request, further contributing to system congestion. Motivated by these chal-lenges and the unpredictability of query runtimes, we design 

Patchwork ’s SLO minimization mechanism. We begin by discussing how Patchwork detects whether a query is likely to violate its SLO in subsequent stages. 

Detecting potential SLO violations To detect potential SLO violations, we require a prediction model. We explore two lightweight classification approaches: First, a running av-erage estimator that maintains an estimate of query runtimes based on their current state; and second, an XGBoost classi-fier trained on the following features: initial query length, maximum intermediate query length, time spent in previous components, time since scheduling, as well as the minimum and maximum number of nodes remaining after the current one. A request is flagged as likely to violate its SLO if the sum of its expected remaining processing time and the time already spent exceeds the specified SLO. We evaluate the effectiveness of both approaches in the following section. 

SLO violation mitigation strategy. When the classifier flags a request as likely to violate its SLO, Patchwork ini-tiates a series of mitigation strategies. First, it grants the request prioritized access to resources through out-of-order scheduling, allowing it to bypass the standard queue. Sec-ond, to avoid further overloading the system, Patchwork 

temporarily stops accepting new requests until no pending requests are at risk of violating their SLOs. Optionally, if ad-ditional resources are available, Patchwork can also trigger autoscaling to allocate more compute to overloaded compo-nents, further reducing the likelihood of SLO violations. 

Choosing Request Completion Time Estimator We evaluate two potential estimators for predicting request com-pletion time as part of our SLO mitigation strategy. As shown 

> 7

in Figure 10, both the XGBoost model [ 10 ] and the run-ning average estimator result in similar SLO violation rates. Given their comparable performance, we choose the run-ning average estimator due to its significantly lower com-putational overhead and its natural adaptability to different RAG pipelines and workload patterns—without requiring additional training or tuning. 

3.5 Deploying RAGs with Patchwork 

By illustrating the deployment a RAG pipeline, we highlight the components of our Patchwork implementation. 

Patchwork ’s client library. Users interact with Patch-work exclusively through its client library. As shown in Fig-ure 9, they define their execution logic in Python. Creating new components or extending existing ones is simple—users only need to define a standard Python class and annotate the execution function using Patchwork ’s decorator. When the user invokes the deploy function, the client library captures the current frame object, which contains both the initializa-tion parameters for each component and the execution order. With the help of the decorator, the client library can distin-guish between conditional logic and independent execution nodes. The deploy call then launches Patchwork ’s sched-uler and supplies it with the captured execution pipeline. Upon launch, the call returns the IP address and port of the scheduler, which users can use to submit RAG inference re-quests. We now turn to a detailed discussion of Patchwork 

’s scheduler. 

Patchwork ’s scheduler and orchestrator. Patchwork ’s scheduler is a central component of its architecture, oper-ating in two distinct modes: an offline mode, which runs before the RAG system is deployed, and an online mode, which handles runtime execution after deployment. 

Offline Mode: In offline mode, the scheduler receives the execution graph from the Patchwork client library. Upon receiving the graph, Patchwork automatically profiles the scalability characteristics of each component and constructs a piecewise linear approximation of inference time as a func-tion of batch size. Using this profiling information, Patch-work solves the optimization problem described in Sec-tion 3.3 to determine the optimal resource allocation and batch size for each component to maximize throughput. Finally, with the help of the orchestrator, the scheduler deploys the RAG components across available machines. The orchestrator sets up the necessary communication interfaces and launches each component of the RAG pipeline as an in-dependent service. This design allows users to write the com-putation graph as if it were a single-node inference, while the orchestrator handles the distributed execution transparently. 

Online Mode: Once the RAG system is deployed, Patch-work ’s scheduler serves as the front-end for RAG inference. It handles user request admission and returns the final output. Beyond simple request routing, the scheduler performs intel-ligent load balancing across components, tracks per-request completion times, and performs scheduling and autoscaling to minimize SLO violations, as discussed in Section 3.4. Lever-aging Patchwork ’s orchestrator, the scheduler can scale individual components independently, with each component running as a standalone service. 

3.6 Discussion 

Next, we highlight salient features of Patchwork ’s design and performance. 

Accuracy Effects. Patchwork serves the exact same pipeline as provided by the user, using components with-out modification. The performance improvements in Patch-work stem entirely from more efficient component-level scaling and an optimized scheduler—not from altering the pipeline itself. 

Scheduler Scalability. The scheduler is central to Patch-work ’s ability to handle high load and manage SLO vio-lations. At peak throughput, the scheduler could become a potential bottleneck. To address this, we designed it to be highly scalable, leveraging vectorized operations and multi-threading. In our ablation studies, we show that even at a high request rate of 1024 requests per second, Patchwork ’s scheduling latency remains below 2.3ms. 

Profiling Overhead. Patchwork requires a one-time profiling step before deployment. In our experiments, pro-filing a RAG system with five different components took approximately four minutes. However, this cost is quickly amortized over time, as inference systems typically remain online for extended periods. 

3.7 Implementation 

Building on Section 3.5, we provide a few additional imple-mentation details. Patchwork is implemented in 8K lines of Python code. Its scheduler coordinates worker processes responsible for executing various RAG components using gRPC [ 1 ] as the networking backbone to efficiently handle requests and responses between worker nodes, while also monitoring their status. The modular design of Patchwork 

simplifies the implementation of new abstractions, particu-larly for users aiming to extend its functionality beyond the default capabilities. Users are only required to (1) specify the name of the RAG component and (2) annotate the desired functions they wish to include (shown in Figure 9). Patch-work then automatically detects and manages dependencies among these components. All underlying computations are handled by existing frameworks, ensuring efficiency and ease of integration. This streamlined approach allows Patchwork 

to be effortlessly incorporated into existing LLM ecosystems. 

> 8

Table 1. RAG Applications and their components: We provide a brief overview of RAG applications we implemented in Patch-work and the components used by them.                          

> Name Retriever Model Inference Engine Conditional Recursive
> CRAG Chroma, DuckDuckGo Llama3.2-3B-Instruct vLLM OpenAI Server Yes No MemoRAG DenseVector Qwen2.5-7B-Instruct, Mistral-7b-Instruct Huggingface No No IRCoT ElasticSearch Llama-3-8B-Instruct vLLM Yes Yes HippoRAG igraph Llama-3-8B-Instruct vLLM No No

4 Evaluation 

We evaluate Patchwork by implementing and studying four popular RAG pipelines. We compare Patchwork against both author-provided deployment pipelines and commercial deployment engines like LangGraph [ 35 ]. Our evaluation shows that Patchwork at high load provides a gain of ≥

1.48 × in throughput against all baselines. 

Cluster Setup. We conduct our evaluation on four servers, each equipped with two 16-core Intel Xeon Silver 4314 CPUs and 256 GiB of RAM, alongside eight NVIDIA A100 GPUs with 80 GiB of on-device memory. The servers run Ubuntu 22.04.1 with Linux kernel ver. 5.15, using CUDA ver. 12.3. 

Dataset. We evaluate Patchwork on a a dataset of 3000 chats sampled from the LMSYS-Chat-1M dataset [ 86 ]. Each sample includes a conversation ID, model name, conversation text, language tag, and OpenAI moderation API tag. We simulate arrival rates as a Poisson arrival process. Further, we use a subset of 50,000 real news documents in the C4 dataset to create documents for retrieval. 

RAG Applications and Baselines. We implement four representative RAG applications to test the effectiveness of 

Patchwork . Table 1 provides a summary of these applica-tions. We describe their execution pipelines below. 

• Corrective-RAG (CRAG) [ 79 ] enhances RAG by incorpo-rating self-reflection and self-grading of retrieved doc-uments. The workflow involves: (1) retrieving relevant documents from a vector database and segmenting them into "knowledge strips"- vectors with similar information (2) refining the query with an LLM if irrelevant documents are found, optionally supplementing with web search; and (3) using an LLM and the retrieved documents and web search results to generate answers. Corrective RAG can be implemented in LangGraph, therefore we use LangGraph as baseline for comparison. 

• MemoRAG [ 58 ] uses a context memory model to achieve a global understanding of the database. Its workflow in-volves: (1) ingesting a long-context document into a mem-ory LLM to prefill the key-value (KV) cache with relevant context; (2) processing a query with the memory model to generate an answer clue and draft an answer; (3) re-trieving additional information with the answer clue via a vector retriever; and (4) combining the retrieved contexts and original query for final processing by an LLM. For MemoRAG we use the author-provided baseline. 

Table 2. Lines of Code needed to build each RAG abstraction and specify its execution.            

> HippoRAG MemoRAG CRAG IRCoT
> Abstraction Implementation 349 722 317 202 Execution Specification 6610 5

• Interleaving Retrieval with Chain-of-Thought Reasoning (IRCoT) [ 70 ] is a multi-step Question Answering approach that interleaves retrieval with Chain-of-Thought (CoT) steps [ 74 ], using CoT to guide retrieval and leveraging retrieved results to refine CoT. It iteratively retrieves ex-ternal information at each CoT step until the final answer is generated. We use the author-provided baseline. 

• HippoRAG [ 24 ] efficiently integrates new knowledge, in-spired by the hippocampal indexing theory [ 67 ]. It consists of two main phases: offline indexing and online retrieval. In the indexing phase, documents are transformed into a knowledge graph, where nodes represent entities and edges are derived either from relation tuples - consisting of a subject, a relation, and an object - or from synonym relationships between entities. A matrix 𝑃 tracks how of-ten each entity appears in each document. In the retrieval phase, entities are extracted from the query and embed-ded. The most similar nodes are selected, and personalized PageRank is run on the nodes. Final passage scores are calculated by multiplying the PageRank scores by 𝑃 , and the top 𝑘 passages are retrieved and used for generation. For HippoRAG, we use the author-provided baseline. Next, in the following sections, we evaluate various as-pects of Patchwork . Section 4.2 examines the throughput performance improvements and their underlying causes. In Section 4.3, we assess the effectiveness of Patchwork ’s strategies for mitigating SLO violations. Finally, Section 4.4 presents a sensitivity analysis of Patchwork ’s components to evaluate its scalability. 

4.1 Patchwork ’s Ease of Deployment 

Patchwork is designed for developer ease of use. Once users have implemented their RAG pipeline components. Users can deploy their RAG system for distributed inference. Patch-work provides a mechanism to deploy a distributed inference system with less than 10 lines of code. Table 2 summarizes the footprint, showing the lines of code required to create a RAG application abstraction as well as the lines needed to assemble a complete RAG pipeline using those abstractions. Moreover, these abstractions are designed for effortless shar-ing across RAG applications and pipelines, further enhancing ease-of-use. Further, we will like to highlight that the users write their RAG execution pipeline assuming single node inference, Patchwork automatically adds communication components and handles deployment.      

> 924816 32 64
> Request Rate (req/s)
> 0
> 20
> 40
> 60
> 80
> 100
> 120
> Tokens/s
> Baseline
> Harmonia

(a) CRAG 400        

> 600
> 800 Baseline
> Harmonia
> 24816 32 64
> Request Rate (req/s)
> 0
> 25
> 50
> 75
> 100
> 125
> 150
> Tokens/s

(b) MemoRAG 500        

> 750
> 1000 Baseline
> Harmonia
> 24816 32 64
> Request Rate (req/s)
> 0
> 50
> 100
> 150
> 200
> Tokens/s

(c) HippoRAG 2 4 8 16 32 64  

> Request Rate (req/s)
> 0
> 20
> 40
> 60
> 80
> Tokens/s
> Baseline
> Harmonia

(d) IRCoT 

Figure 11. Comparing Patchwork : We compare Patchwork with baseline implementation for four different RAG systems. For LangGraph we observe speedups of atmost 1.48 ×. For other baselines we see speedups between 15 × to 22 ×.

4.2 Comparing Patchwork ’s Throughput 

We begin by evaluating the throughput of Patchwork by comparing the RAG applications described above. We then conduct a detailed analysis to identify the sources of Patch-work ’s throughput advantages. 

Patchwork ’s throughput. Figure 11 compares the through-put of RAG applications implemented in Patchwork with their respective baseline implementations. Across all sce-narios, Patchwork consistently delivers higher throughput than the baselines. In the CRAG case (Figure 11a), Patch-work performs similarly to the baseline at low request ar-rival rates, as the baseline (built using LangGraph [ 35 ]) sup-ports batching at the level of individual RAG components. However, under higher request loads, Patchwork outper-forms the baseline by 1.48 ×, thanks to its finer-grained re-source allocation. For MemoRAG [ 58 ], HippoRAG [ 25 ], and IRCoT [ 69 ], Patchwork achieves substantially higher through-put—up to 15 ×, 14 ×, and 22 × more tokens per second, re-spectively. 

Understanding Patchwork ’s performance gain. We begin by investigating why Patchwork outperforms CRAG. While CRAG is implemented using LangGraph, which sup-ports both batching and pipelining, it lacks component-level resource optimization. In the CRAG workload, the Grader becomes the primary bottleneck, as it must process inputs that include both the original queries and the retrieved con-texts—resulting in a significantly higher computational bur-den. LangGraph’s uniform resource allocation does not ac-count for this imbalance, thereby limiting overall perfor-mance. In contrast, Patchwork dynamically allocates re-sources based on the specific demands of each component — assigning four GPUs to the Grader and one GPU each to the Transformer and Generator. This targeted allocation allevi-ates the Grader bottleneck, reduces overall pipeline latency, and improves resource utilization. Figure 12a shows the time breakdown for each component in CRAG under varying load. In the LangGraph implementation, the Grader dominates the total execution time. In comparison, Patchwork signifi-cantly reduces the Grader’s share of execution time through its component-level resource allocation. Next, we examine why Patchwork outperforms Mem-oRAG. The baseline used in MemoRAG lacks support for 2 4 8 16 32 64  

> Request Rate (req/s)
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Proportion of time
> Chroma
> Augmenter
> Grader
> Transformer
> Web
> Generator
> CRAG Harmonia

(a) CRAG component level breakdown. We observe that CRAG is bottlenecked by the Grader component. Patchwork 

is able to alleviate it by some extent due to better resource allocation. Baseline No  

> Optimization
> Batching Batching+
> Pipeline
> Harmonia
> Request Rate (req/s)
> 0
> 100
> 200
> 300
> 400
> 500
> 600
> 700
> Tokens/s

(b) Ablation with Memorag: 

We start with MemorRAG baseline and incrementally add 

Patchwork ’s optimizations. We observe Patchwork primarily benefits from batching and component level resource allocation. Red bar shows the vanilla MemoRAG baseline. 

Figure 12. Understanding Reason for Patchwork ’s perfor-mance gains. 

component-level batching, pipelining, and resource alloca-tion. To understand how Patchwork ’s optimizations en-hance performance, we conduct an ablation study. As shown in Figure 12b, we begin with a version of Patchwork with all optimizations disabled and then incrementally enable them. First, we add component-level batching, which improves throughput by 9.6×. Next, we introduce pipelining, yield-ing an additional 1.5× improvement. This gain is modest, as MemoRAG is primarily bottlenecked by the Generator component. Finally, we enable component-level resource allocation, which further boosts throughput by 1.78 × by re-distributing compute resources to the Generator, thereby addressing the bottleneck. This indicates that the performance benefit in Patchwork 

primarily come for two contributors: (i) Component Level Resource allocation and batching, enabled by solving an underlying optimization problem; (ii) Pipelining, achieved through Patchwork ’s scheduler, which continuously routes new requests to the next component in the pipeline without waiting for previous requests to complete. 

4.3 Comparing Patchwork ’s SLO violations 

Next, we study Patchwork ’s impact on SLO violations.      

> 10 24816 32 64
> Request Rate (req/s)
> 0
> 20
> 40
> 60
> 80
> 100
> SLO Violation (%)  Baseline
> Harmonia

(a) CRAG 2 4 8 16 32 64  

> Request Rate (req/s)
> 0
> 20
> 40
> 60
> 80
> 100
> SLO Violation (%)  Baseline
> Harmonia

(b) MemoRAG 2 4 8 16 32 64  

> Request Rate (req/s)
> 0
> 20
> 40
> 60
> 80
> 100
> SLO Violation (%)  Baseline
> Harmonia

(c) HippoRAG 2 4 8 16 32 64  

> Request Rate (req/s)
> 0
> 20
> 40
> 60
> 80
> 100
> SLO Violation (%)  Baseline
> Harmonia

(d) IRCoT 

Figure 13. SLO violation minimization: We evaluate Patchwork ’s impact on SLO violations and observe that it can reduce violations by up to 24%. However, under high load, Patchwork is unable to perform SLO mitigation due to limited resources, which leads to all requests violating their SLOs. 

Table 3. Batch Size configuration: We show the resource allo-cation and batch size configuration chosen by Patchwork for a 6GPU and 4 CPU setup serving CRAG.                                  

> Component Default Batch Batch Size Type Num. Resource Input Token Length
> Chroma 32 16 CPU 288 Grader 32 8GPU 42950 Augmenter 32 8CPU 12Transformer 32 8GPU 188 Web 32 8CPU 136 Generator 32 8GPU 1314

Evaluating SLO violation We now evaluate how effec-tively Patchwork can minimize violations of developer-defined SLOs using the strategy discussed in Section 3.4. For these experiments, we define the SLO threshold as 2x the average request latency when serving with Patchwork un-der low load conditions. In Figure 13, we plot the percentage of SLO violations against the number of incoming requests. Our results show that under medium load, Patchwork con-sistently lowers the number of SLO violations—achieving up to a 24% reduction. However, under very high load, Patch-work ’s ability to minimize violations diminishes. In these scenarios, prioritizing one request often results in another missing its deadline, due to the high arrival rate and limited system capacity. 

Impact of SLO mitigation on throughput. Next, we ex-amine the effect of SLO mitigation on overall system through-put. Our approach to minimizing SLO violations works by prioritizing requests that are at risk of missing their deadlines. However, this out-of-turn scheduling disrupts the carefully optimized execution pipeline, creating idle periods ("bub-bles") and resulting in suboptimal resource utilization—which in turn can lower throughput. To quantify this trade-off, we measure end-to-end throughput with SLO mitigation enabled and disabled. As shown in Figure 14, when serving CRAG, enabling SLO violation mitigation can lead to a throughput reduction of up to 16% .

SLO mitigation through auto-scaling. As discussed in Section 3.4, one key advantage of Patchwork ’s end-to-end design is its ability to support automatic resource scal-ing. Figure 15 illustrates the impact of Patchwork ’s auto-scaling mechanism on throughput. Around the 18-second mark, while serving CRAG, Patchwork detects a high like-lihood of an impending SLO violation. Leveraging the in-sights from its optimization process, Patchwork identifies the bottleneck component—in this case, the ’Grader’—which requires additional GPU resources. Patchwork then auto-matically provisions a new GPU and configures it for serving the ’Grader’ component, all with zero downtime. 

4.4 Sensitivity Analysis Impact of batch size selection. Patchwork provides both resource allocation and optimal batch size selection at the component level. For a serving setup with 6 GPUs and 4 CPUs running CRAG, Table 3 presents the batch sizes recom-mended by the optimizer. Figure 16 compares the throughput of CRAG with and without using these suggested batch sizes. When batch sizes are smaller than the recommended values, the performance difference is minimal. However, as batch sizes increase, Patchwork configured with the suggested batch sizes begins to outperform the version using only the available batch size. In particular, it achieves up to 31.6% higher throughput. Using the optimizer-recommended batch sizes helps maintain a more balanced pipeline, leading to improved overall efficiency. 

Scalability of the scheduler The scheduler is a core component of Patchwork , responsible for resource alloca-tion, routing, request scheduling, completion tracking, and monitoring. As such, its performance can become a bottle-neck when scaling Patchwork . We evaluate scheduler’s scalability,by measuring its latency under varying request rates. As shown in Figure 17, our results demonstrate that the scheduler’s query processing latency remains stable across different loads, consistently staying around 2 milliseconds. This is possible because due to careful design of Patchwork 

scheduler’s data structure. Further, the serving threads only performs the absolutely necessary tasks while delegating non-critical components to background threads. 

Scalability of solving the optimization problem Patch-work executes the optimization routine when the inference service is launched and re-runs it occasionally during each auto-scaling event to determine updated batch sizes and resource allocations. The optimization is implemented us-ing Gurobi. As shown in Figure 18, we evaluate its runtime      

> 11 24816 32 64
> Batch Size
> 0
> 20
> 40
> 60
> 80
> 100
> Tokens/s
> w/o SLO Mitigation
> w/ SLO Mitigation

Figure 14. Evaluating loss in throughput due to SLO mitigation: We evaluate the throughput loss caused by SLO mitigation strategies at different batch sizes. 0 10 20 30 40 50 60 

> Time (s)
> 60
> 80
> 100
> 120
> 140
> Tokens/s  SLO violation
> detected
> New machine
> available

Figure 15. Visualizing the impact of auto scaling: In the above figure, we visualize the effect of auto-scaling on throughput. We ini-tiate the auto-scaling process upon detecting an SLO violation. 2 4 8 16 32 64 

> Batch Size
> 0
> 20
> 40
> 60
> 80
> 100
> 120
> Tokens/s
> w/o Batching
> w/ Batching

Figure 16. Evaluating Batching Opti-mization: CRAG throughput, with and with-out batching optimization applied. 8 16 32 64 128 256 512 1024  

> Request Rate (req/s)
> 0.0
> 0.5
> 1.0
> 1.5
> 2.0
> 2.5
> Latency (ms)
> Network Compute

Figure 17. Scheduler Processing Latency: We observe that processing latency per request remains constant for Patchwork ’s scheduler as the load increases. This indicates that the scheduler is unlikely to become a bottleneck. 16 32 64 128 256 512 

> Maximum Nodes
> 0.0
> 0.5
> 1.0
> 1.5
> 2.0
> 2.5
> 3.0
> 3.5
> 4.0
> Average Latency (s)

Figure 18. Scalability of Optimization Problem: Optimizer latency versus number of nodes for a RAG application with 16 components. 

performance and find that, even on a cluster with 512 hetero-geneous nodes, the optimization completes in around three seconds — demonstrating its efficiency and suitability for dynamic environments. The reason we are are able to rapidly solve this problem is due to our formulation. The piecewise linear estimation of runtimes (Section 3.3 enables us to keep it linear). 

5 Related Work 

System for RAG Several prior systems aim to improve the scalability of RAG systems. RALMSpec [ 85 ] extends spec-ulative execution to RAG systems to improve latency. Spec-ulativeRAG [ 73 ] introduces a draft LLM to perform spec-ulative decoding for the generator. Chameleon [ 30 ] pro-poses a resource heterogeneous architecture for accelerating the retrieval process. PiperRAG [ 31 ] introduces a software-hardware co-design for querying structure of the retriever to minimize latency. Recently, TeleRAG [ 42 ] has been pro-posed which performs speculative retrieval. These systems either change the underlying RAG pipeline to extract addi-tional efficiency or only optimize specific bottleneck compo-nents of the RAG pipeline. However, since these bottleneck components constantly change with each RAG pipeline, the optimizations are not widely applicable. In RAGO [ 29 ] a concurrent work, authors study systematic optimization of RAG systems, by categorizing different types of RAGs. The authors in [ 29 ] also try to build representative abstractions to better study the RAG workload. However, the authors did not address the challenges in deploying and active manage-ment of resources when deploying RAG pipelines. To the best of our knowledge Patchwork is the first end-to-end RAG serving system which enables component level batching and resource allocation, and runtime SLO mitigation. 

ML inference Several systems have been built for per-forming distributed ML inference [ 15 , 22 , 61 , 63 ]. However, prior systems targeted single monolithic ML models with predictable latency. Existing systems are not designed to serve complex heterogeneous pipelines like RAG systems. [ 4 ,34 , 53 , 64 , 81 ] explored domain-specific optimizations for LLM inference. [ 2 ] leveraged request arrival metrics to en-able proactive KV cache management. [ 13 , 28 , 61 , 77 , 83 ]dynamically adjust resource allocations to ensure that SLOs are met. However, these techniques focus on improving to-ken generation on a per-request basis, ignoring the intricate interaction between LLM and external RAG components. 

Heterogeneous resource allocation. Addressing het-erogeneous resource requirements of applications has re-ceived significant interest. DRF [ 21 ] provides a solution of fair resource allocation in face of heterogeneous resource requirements. Gavel [ 50 ] address scheduling of ML training jobs in heterogeneous clusters. 

6 Conclusion 

In this work, we present Patchwork , an end-to-end sys-tem for efficient serving of Retrieval-Augmented Generation (RAG) pipelines. Patchwork offers modular and extensi-ble abstractions that allow users to define RAG workflows 

> 12

using a high-level, intent-driven Python framework. It auto-matically captures the user-defined pipeline and determines optimal resource allocation at the component level. At run-time, Patchwork employs strategies to minimize SLO viola-tions. These optimizations enable Patchwork to achieve a 1.48 × speedup over the best-performing commercial serving systems, while reducing SLO violations by ∼24%. 

> 13

References 

[1] grpc–an rpc library and framework. https://grpc.io/ , 2025. Accessed: 2025-04-17. [2] Reyna Abhyankar, Zijian He, Vikranth Srivatsa, Hao Zhang, and Yiying Zhang. Infercept: efficient intercept support for augmented large language model inference. In Proceedings of the 41st International Conference on Machine Learning , ICML’24. JMLR.org, 2024. [3] Tobias Achterberg. What’s new in gurobi 9.0. Webinar Talk url: https://www. gurobi. com/wp-content/uploads/2019/12/Gurobi-90-Overview-Webinar-Slides-1. pdf , 5(9):97–113, 2019. [4] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, and Ramachandran Ramjee. Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills. arXiv preprint arXiv:2308.16369 , 2023. [5] Alex Aiken and Michael Bauer. Programming with legion. 2022. [6] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving lan-guage models by retrieving from trillions of tokens. In International conference on machine learning , pages 2206–2240. PMLR, 2022. [7] Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. Rq-rag: Learning to refine queries for retrieval augmented generation. arXiv preprint arXiv:2404.00610 , 2024. [8] Chengfeng Chen, Xiaodong Huang, Yangzhen Xu, Runfeng Lin, Gan-gliang Li, and Shouqiang Liu. Lire: Efficient query rewriting for re-trieval augmented generation systems. In 2024 4th International Con-ference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI) , pages 27–31. IEEE, 2024. [9] Siyuan Chen, Zhipeng Jia, Samira Khan, Arvind Krishnamurthy, and Phillip B. Gibbons. Slos-serve: Optimized serving of multi-slo llms. 2025. [10] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Confer-ence on Knowledge Discovery and Data Mining , KDD ’16, page 785–794. ACM, August 2016. [11] Xinyue Chen, Pengyu Gao, Jiangjiang Song, and Xiaoyang Tan. Hiqa: A hierarchical contextual augmentation rag for multi-documents qa. 

arXiv preprint arXiv:2402.01767 , 2024. [12] Krishna Teja Chitty-Venkata, Siddhisanket Raskar, Bharat Kale, Farah Ferdaus, Aditya Tanikanti, Ken Raffenetti, Valerie Taylor, Murali Emani, and Venkatram Vishwanath. Llm-inference-bench: Inference benchmarking of large language models on ai accelerators. In SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis , pages 1362–1379. IEEE, 2024. [13] Yujeong Choi, Yunseong Kim, and Minsoo Rhu. Lazybatching: An sla-aware batching system for cloud machine learning inference, 2020. [14] Chroma - the open-source embedding database. https://github.com/ chroma-core/chroma .[15] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E Gonzalez, and Ion Stoica. Clipper: A {Low-Latency } online prediction serving system. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17) , pages 613–627, 2017. [16] Jeffrey Dean and Sanjay Ghemawat. Mapreduce: simplified data pro-cessing on large clusters. Communications of the ACM , 51(1):107–113, 2008. [17] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130 , 2024. [18] Jinyuan Fang, Zaiqiao Meng, and Craig Macdonald. Kirag: Knowledge-driven iterative retriever for enhancing retrieval-augmented genera-tion. arXiv preprint arXiv:2502.18397 , 2025. [19] Masoomali Fatehkia, Ji Kim Lucas, and Sanjay Chawla. T-rag: lessons from the llm trenches. arXiv preprint arXiv:2402.07483 , 2024. [20] Robert Friel and Atindriyo Sanyal. Chainpoll: A high efficacy method for llm hallucination detection. arXiv preprint arXiv:2310.18344 , 2023. [21] Ali Ghodsi, Matei Zaharia, Benjamin Hindman, Andy Konwinski, Scott Shenker, and Ion Stoica. Dominant resource fairness: Fair allocation of multiple resource types. In 8th USENIX symposium on networked systems design and implementation (NSDI 11) , 2011. [22] Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kauf-mann, Ymir Vigfusson, and Jonathan Mace. Serving {DNNs } like clockwork: Performance predictability from the bottom up. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20) , pages 443–462, 2020. [23] Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning , pages 3887–3896. PMLR, 2020. [24] Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neurobiologically inspired long-term memory for large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems , 2024. [25] Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neurobiologically inspired long-term memory for large language models, 2025. [26] Haoyu Han, Yu Wang, Harry Shomer, Kai Guo, Jiayuan Ding, Yongjia Lei, Mahantesh Halappanavar, Ryan A Rossi, Subhabrata Mukherjee, Xianfeng Tang, et al. Retrieval-augmented generation with graphs (graphrag). arXiv preprint arXiv:2501.00309 , 2024. [27] Kunal Handa, Alex Tamkin, Miles McCain, Saffron Huang, Esin Dur-mus, Sarah Heck, Jared Mueller, Jerry Hong, Stuart Ritchie, Tim Be-lonax, et al. Which economic tasks are performed with ai? evidence from millions of claude conversations. arXiv preprint arXiv:2503.04761 ,2025. [28] Bodun Hu, Le Xu, Jeongyoon Moon, Neeraja J Yadwadkar, and Aditya Akella. MOSEL: Inference serving using dynamic modality selec-tion. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, 

Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , pages 8872–8886, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [29] Wenqi Jiang, Suvinay Subramanian, Cat Graves, Gustavo Alonso, Amir Yazdanbakhsh, and Vidushi Dadu. Rago: Systematic performance op-timization for retrieval-augmented generation serving. arXiv preprint arXiv:2503.14649 , 2025. [30] Wenqi Jiang, Marco Zeller, Roger Waleffe, Torsten Hoefler, and Gus-tavo Alonso. Chameleon: a heterogeneous and disaggregated acceler-ator system for retrieval-augmented language models. arXiv preprint arXiv:2310.09949 , 2023. [31] Wenqi Jiang, Shuai Zhang, Boran Han, Jie Wang, Bernie Wang, and Tim Kraska. Piperag: Fast retrieval-augmented generation via algorithm-system co-design. arXiv preprint arXiv:2403.05676 , 2024. [32] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 7969–7992, 2023. [33] Jiajie Jin, Yutao Zhu, Guanting Dong, Yuyao Zhang, Xinyu Yang, Chenghao Zhang, Tong Zhao, Zhao Yang, Zhicheng Dou, and Ji-Rong Wen. Flashrag: A modular toolkit for efficient retrieval-augmented generation research. arXiv preprint arXiv:2405.13576 , 2024. [34] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles , pages 611–626, 2023. 

> 14

[35] Langgraph platform: Design and deploy your agents at scale. https: //www.langchain.com/langgraph .[36] Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke Zettlemoyer. Pre-training via paraphrasing. 

Advances in Neural Information Processing Systems , 33:18470–18481, 2020. [37] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems ,33:9459–9474, 2020. [38] Zhicong Li, Jiahao Wang, Zhishu Jiang, Hangyu Mao, Zhongxia Chen, Jiazhen Du, Yuanxing Zhang, Fuzheng Zhang, Di Zhang, and Yong Liu. Dmqr-rag: Diverse multi-query rewriting for rag. arXiv preprint arXiv:2411.13154 , 2024. [39] Zihang Li, Yangdong Ruan, Wenjun Liu, Zhengyang Wang, and Tong Yang. Cft-rag: An entity tree based retrieval augmented generation algorithm with cuckoo filter. arXiv preprint arXiv:2501.15098 , 2025. [40] Qingyuan Liang, Zeyu Sun, Qihao Zhu, Wenjie Zhang, Lian Yu, Yingfei Xiong, and Lu Zhang. Lyra: A benchmark for turducken-style code generation. arXiv preprint arXiv:2108.12144 , 2021. [41] Xun Liang, Simin Niu, Zhiyu Li, Sensen Zhang, Hanyu Wang, Feiyu Xiong, Jason Zhaoxin Fan, Bo Tang, Shichao Song, Mengwei Wang, et al. Saferag: Benchmarking security in retrieval-augmented genera-tion of large language model. arXiv preprint arXiv:2501.18636 , 2025. [42] Chien-Yu Lin, Keisuke Kamahori, Yiyu Liu, Xiaoxiang Shi, Madhav Kashyap, Yile Gu, Rulin Shao, Zihao Ye, Kan Zhu, Stephanie Wang, et al. Telerag: Efficient retrieval-augmented generation inference with lookahead retrieval. arXiv preprint arXiv:2502.20969 , 2025. [43] Hao Liu, Zhengren Wang, Xi Chen, Zhiyu Li, Feiyu Xiong, Qinhan Yu, and Wentao Zhang. Hoprag: Multi-hop reasoning for logic-aware retrieval-augmented generation. arXiv preprint arXiv:2502.12442 , 2025. [44] Llama cpp. https://github.com/ggml-org/llama.cpp .[45] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. Query rewriting in retrieval-augmented large language models. In 

Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 5303–5315, 2023. [46] Lionel Martis. Luminirag: Vision-enhanced graph rag for complex multi-modal document understanding. Authorea Preprints , 2024. [47] Yixuan Mei, Yonghao Zhuang, Xupeng Miao, Juncheng Yang, Zhi-hao Jia, and Rashmi Vinayak. Helix: Serving large language models over heterogeneous gpus and network via max-flow. arXiv preprint arXiv:2406.01566 , 2024. [48] Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman, Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen, et al. Mllib: Machine learning in apache spark. Journal of Machine Learning Research , 17(34):1–7, 2016. [49] Manisha Mukherjee and Vincent J Hellendoorn. Sosecure: Safer code generation with rag and stackoverflow discussions. arXiv preprint arXiv:2503.13654 , 2025. [50] Deepak Narayanan, Keshav Santhanam, Fiodar Kazhamiaka, Amar Phanishayee, and Matei Zaharia. {Heterogeneity-Aware } cluster scheduling policies for deep learning workloads. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20) , pages 481–498, 2020. [51] Jinming Nian, Zhiyuan Peng, Qifan Wang, and Yi Fang. W-rag: Weakly supervised dense retrieval in rag for open-domain question answering. 

arXiv preprint arXiv:2408.08444 , 2024. [52] Chengyi Nie, Rodrigo Fonseca, and Zhenhua Liu. Aladdin: Joint placement and scaling for slo-aware llm serving. arXiv preprint arXiv:2405.06856 , 2024. [53] NVIDIA. Fastertransformer. https://github.com/NVIDIA/ FasterTransformer , 2023. GitHub repository, accessed on April 18, 2025. [54] Ollama llm-rag. https://github.com/digithree/ollama-rag .[55] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017. [56] Pep 523 – adding a frame evaluation api to cpython. https://peps. python.org/pep-0523/ .[57] Nicholas Pipitone and Ghita Houir Alami. Legalbench-rag: A bench-mark for retrieval-augmented generation in the legal domain. arXiv preprint arXiv:2408.10343 , 2024. [58] Hongjin Qian, Peitian Zhang, Zheng Liu, Kelong Mao, and Zhicheng Dou. Memorag: Moving towards next-gen rag via memory-inspired knowledge discovery. arXiv preprint arXiv:2409.05591 , 2024. [59] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250 , 2016. [60] Siddhant Ray, Rui Pan, Zhuohan Gu, Kuntai Du, Ganesh Anantha-narayanan, Ravi Netravali, and Junchen Jiang. Ragserve: Fast quality-aware rag systems with configuration adaptation. arXiv preprint arXiv:2412.10543 , 2024. [61] Francisco Romero, Qian Li, Neeraja J Yadwadkar, and Christos Kozyrakis. {INFaaS }: Automated model-less inference serving. In 

2021 USENIX Annual Technical Conference (USENIX ATC 21) , pages 397–411, 2021. [62] Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettlemoyer, and Pang Wei W Koh. Scaling retrieval-based language models with a trillion-token datastore. Advances in Neural Information Processing Systems , 37:91260–91299, 2024. [63] Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram. Nexus: A gpu cluster engine for accelerating dnn-based video analysis. In Pro-ceedings of the 27th ACM Symposium on Operating Systems Principles ,pages 322–337, 2019. [64] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: high-throughput generative inference of large language mod-els with a single gpu. In Proceedings of the 40th International Conference on Machine Learning , ICML’23. JMLR.org, 2023. [65] Yunxiao Shi, Xing Zi, Zijing Shi, Haimin Zhang, Qiang Wu, and Min Xu. Enhancing retrieval and managing retrieval: A four-module synergy for improved quality and efficiency in rag systems. arXiv preprint arXiv:2407.10670 , 2024. [66] Ishneet Sukhvinder Singh, Ritvik Aggarwal, Ibrahim Allahverdiyev, Muhammad Taha, Aslihan Akalin, Kevin Zhu, and Sean O’Brien. Chunkrag: Novel llm-chunk filtering method for rag systems. arXiv preprint arXiv:2410.19572 , 2024. [67] Timothy J Teyler and Pascal DiScenna. The hippocampal memory indexing theory. Behavioral neuroscience , 100(2):147, 1986. [68] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reason-ing for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509 , 2022. [69] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions, 2023. [70] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In Anna Rogers, Jor-dan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol-ume 1: Long Papers) , pages 10014–10037, Toronto, Canada, July 2023. Association for Computational Linguistics. [71] Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi, and Bryan Catanzaro. Instructretro: Instruction tuning post retrieval-augmented pretraining. arXiv preprint arXiv:2310.07713 , 2023. 

> 15

[72] Shuting Wang, Xin Yu, Mang Wang, Weipeng Chen, Yutao Zhu, and Zhicheng Dou. Richrag: Crafting rich responses for multi-faceted queries in retrieval-augmented generation. arXiv preprint arXiv:2406.12566 , 2024. [73] Zilong Wang, Zifeng Wang, Long Le, Huaixiu Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang, et al. Speculative rag: Enhancing retrieval augmented generation through drafting. arXiv preprint arXiv:2407.08223 , 2024. [74] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompt-ing elicits reasoning in large language models. Advances in neural information processing systems , 35:24824–24837, 2022. [75] why we no longer use langchain for building our ai agents. 

https://www.octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents .[76] Junde Wu, Jiayuan Zhu, Yunli Qi, Jingkun Chen, Min Xu, Filippo Meno-lascina, and Vicente Grau. Medical graph rag: Towards safe medical large language model via graph retrieval-augmented generation. arXiv preprint arXiv:2408.04187 , 2024. [77] Xiaorui Wu, Hong Xu, and Yi Wang. Irina: Accelerating dnn inference with efficient online scheduling. In Proceedings of the 4th Asia-Pacific Workshop on Networking , APNet ’20, page 36–43, New York, NY, USA, 2020. Association for Computing Machinery. [78] Wei Xiao, Yu Liu, XiangLong Li, Feng Gao, and JinGuang Gu. Tkg-rag: A retrieval-augmented generation framework with text-chunk knowl-edge graph. In 2024 25th International Arab Conference on Information Technology (ACIT) , pages 1–9. IEEE, 2024. [79] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. Corrective retrieval augmented generation. 2024. [80] Yi Yang, Wen-tau Yih, and Christopher Meek. Wikiqa: A challenge dataset for open-domain question answering. In Proceedings of the 2015 conference on empirical methods in natural language processing ,pages 2013–2018, 2015. [81] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributed serving system for 

{Transformer-Based } generative models. In 16th USENIX Sympo-sium on Operating Systems Design and Implementation (OSDI 22) , pages 521–538, 2022. [82] Matei Zaharia, Mosharaf Chowdhury, Michael J Franklin, Scott Shenker, and Ion Stoica. Spark: Cluster computing with working sets. In 2nd USENIX workshop on hot topics in cloud computing (HotCloud 10) , 2010. [83] Hong Zhang, Yupeng Tang, Anurag Khandelwal, and Ion Stoica. SHEP-HERD: Serving DNNs in the wild. In 20th USENIX Symposium on Net-worked Systems Design and Implementation (NSDI 23) , pages 787–808, Boston, MA, April 2023. USENIX Association. [84] Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E Gonzalez. Raft: Adapting language model to domain specific rag. In First Conference on Language Modeling , 2024. [85] Zhihao Zhang, Alan Zhu, Lijie Yang, Yihua Xu, Lanting Li, Phitchaya Mangpo Phothilimthana, and Zhihao Jia. Accelerating retrieval-augmented language model serving with speculation. arXiv preprint arXiv:2401.14021 , 2024. [86] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric. P Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. Lmsys-chat-1m: A large-scale real-world llm conversation dataset, 2023. [87] Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. 

arXiv preprint arXiv:1709.00103 , 2017.
