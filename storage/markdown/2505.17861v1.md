Title: 

URL Source: http://arxiv.org/pdf/2505.17861v1

Published Time: Mon, 26 May 2025 00:53:08 GMT

Markdown Content:
# Superplatforms Have to Attack AI Agents 

Jianghao Lin, Jiachen Zhu, Zheli Zhou, Yunjia Xi, Weiwen Liu, Yong Yu, Weinan Zhang ∗

Shanghai Jiao Tong Univeristy 

{chiangel,wnzhang}@sjtu.edu.cn 

## Abstract 

Over the past decades, superplatforms – digital companies that integrate a vast range of third-party services and applications into a single, unified ecosystem – have built their fortunes on monopolizing user attention through targeted advertising and algorithmic content curation. Yet the emergence of AI agents driven by large lan-guage models (LLMs) threatens to upend this business model. Agents can not only free user attention with autonomy across diverse platforms and therefore bypass the user-attention-based monetization, but might also become the new entrance for digital traffic. Hence, we argue that superplatforms have to attack AI agents to defend their centralized control of digital traffic entrance. Specifically, we analyze the fundamental conflict between user-attention-based monetization and agent-driven autonomy through the lens of our gatekeeping theory. We show how AI agents can disintermediate superplatforms and potentially become the next dominant gatekeepers, thereby forming the urgent necessity for superplatforms to proactively constrain and attack AI agents. Moreover, we go through the potential technologies for superplatform-initiated attacks, covering a brand-new, unexplored technical area with unique challenges. We have to emphasize that, despite our position, this paper does not advocate for adversarial attacks by superplatforms on AI agents, but rather offers an envisioned trend to highlight the emerging tensions between superplatforms and AI agents. Our aim is to raise awareness and encour-age critical discussion for collaborative solutions, prioritizing user interests and preserving the openness of digital ecosystems in the age of AI agents. 

## 1 Introduction 

The evolution of the Internet has given rise to superplatforms. Superplatforms are digital platforms that integrate a vast range of third-party services and applications into a single, unified ecosystem. Examples include Google for web search, Amazon for e-commerce, and Booking.com for travel agency. As shown in Figure 1(a), they leverage their scale to amass and exploit user data across services, intermediating user interactions with centralized controls [ 16 ]. As the digital traffic entrance, superplatforms can build comprehensive user profiles and tightly gatekeep what content or options users can see, all to maximize engagement and monetization [ 16 , 10 ]. Substantial revenues are made via user-attention-based monetization by capturing and retaining user attention through curated content, then monetizing this attention by selling prioritized visibility – such as top-ranking search results or featured product placements – to the highest bidders. While user-attention-based monetization requires users to personally engage with the platform through direct interactions, AI agents driven by large language models (LLMs) offer an alternative path to free user attention with autonomy across diverse platforms and tasks [ 24 , 7]. Such agents can reason on user instructions, make decisions, and execute goals with minimal human oversight towards the 

> ∗

Corresponding author. Preprint.          

> arXiv:2505.17861v1 [cs.AI] 23 May 2025 Su pe r Pl at f o r m
> AI Ag e nt
> Co nt e nt
> Pr o v i de r Ticket Taxi Takeaway Clothes
> Us e r
> Services
> ...
> ...
> ... ...
> Accommodation
> Gender: Male
> Age: Grown-up
> ...
> Interest: Fishing
> Gender: Female
> Age: Grown-up
> ...
> Interest: Music
> Gender: Male
> Age: Elderly
> ...
> Interest: Reading
> Gender: Male
> Age: Child
> ...
> Interest: Sports
> Gender: Female
> Age: Teenager
> ...
> Interest: Painting
> ...
> ...
> (a) Superplatform as Gatekeeper (b) AI Agent as Gatekeeper (c) Total Bypass over Superplatform

Figure 1: The illustration of three paradigms of user interacting with Internet services ( i.e. , content providers). (a) In the pre-agent age, superplatforms serve as the gatekeeper between the users and content providers. (b) The emerging AI agents intermediate between the users and superplatforms, becoming new gatekeepers. (c) Given the agent-as-gatekeeper fact, content providers can simply bypass superplatforms and offer their Internet services directly to agents. operational goals [ 29 ]. Unlike a single-site chatbot, a general-purpose agent can interact with multiple applications or websites, performing actions on behalf of a user ( e.g. , browsing sites, managing apps, or executing transactions) in an integrated manner. This capability promises to free users from manual, platform-by-platform interactions, allowing an agent to carry out the user’s intent directly across the Internet. Consequently, fundamental tension underlies the relationship between superplatforms and such AI agents: the conflict between user-attention-based monetization versus user-attention-free agent autonomy . Superplatforms today monetize user attention through targeted advertising and algorithmic content curation, essentially by steering and prolonging user engagement within their walled gardens. Nevertheless, AI agents flip this script from two aspects: • AI agents aim to free the user attention and serve the user’s goals first, potentially retrieving information or accomplishing tasks in the most efficient way possible, regardless of any plat-form’s preferred content or ads. In practical terms, as shown in Figure 1(b), an agent might navigate directly to the information or service the user requests, skipping over ad-laden feeds and ignoring algorithmic recommendations, i.e. , AI agents become the new gatekeepers. • Moreover, not only do AI agents bypass ads and optimize for user goals, but also, the companies behind these agents ( e.g. , OpenAI and Anthropic) are building platforms that can redirect user intent, becoming the new entrance for digital traffic. That is, as shown in Figure 1(c), AI agents can collect, filter, and rerank collected items from different data sources before ultimately presenting them to users, which is a potential new source of commercial benefits on the Internet. Hence, superplatforms are at risk of being disintermediated by a new breed of algorithmic interme-diaries — AI agents as meta-platforms. This threatens to erode the “captive audience” model that powers superplatforms’ profits. Based on the discussion above, we give our core position: 

Position: Superplatforms Have to Attack AI Agents 

Superplatforms have to attack AI agents not only because they bypass user-attention-based monetization, but because these agents are poised to become the next dominant gatekeepers — intermediating user intent, controlling information flow, and threatening to displace superplatforms as the primary channels for digital traffic and revenue. We have to emphasize that this position is not what we want to advocate, but rather an envisioned trend that we want to point out and discuss at the emerging age of AI agents. The remainder of this paper is constructed as follows. In Section 2, we discuss why superplatforms have to attack AI agents. Specifically, we are the first to apply the gatekeeping theory to analyze the fundamental threats posed by AI agents to superplatforms, revealing that the most rational strategic response is to proactively attack them. In Section 3, we summarize the ways for superplatforms to attack AI agents, forming an unexplored new technical direction tailored for superplatform-initiated attacks. Challenges and 2potential future directions are also discussed. Then, we discuss alternative views in Section 4 and make necessary ethical statements in Section 5. Section 6 concludes this paper. 

## 2 Why Superplatforms Have to Attack AI Agents – Gatekeeping Theory 

This section casts comprehensive discussions about why superplatforms have to attack AI agents. We first apply the gatekeeping theory to analyze the fundamental threats that AI agents pose to superplatforms. Then, we discuss the strategic options available to superplatforms in response to the rise of AI agents, ultimately demonstrating that attacking these agents is the most rational and inevitable choice. 

2.1 Threat Analysis with Gatekeeping Theory 

The business model of superplatforms can be formulated by the gatekeeping theory [ 13 , 2 , 17 ]. They act as gatekeepers of user attention and monetizing that attention via advertising. The economic engine of superplatform gatekeepers are underscored by two interlocking factors: • User Traffic Control. Gatekeepers hold the authority to determine which content is ultimately presented to users, akin to a gatekeeper deciding who is permitted entry. This control over exposure decisions directly influences which advertisements are seen by users, thereby dictating the allocation of advertising revenue, i.e. , user-attention-based monetization. For example, the advertising revenue of Google reaches $264 billion in 2024 [ 18 ], underscoring how dominant their user-attention-market monopolies have become. • User Data Ownership. Gatekeepers possess exclusive access to users’ personalized real-world interaction data, analogous to a gatekeeper being privy to the preferences of a household’s residents. This data ownership enables the development of highly effective recommendation and online advertising algorithms tailored to user behaviors and preferences. For example, the averaged number of daily user transactions on Amazon has reached 12.87 million [ 12 ], which is in turn used to improve Amazon’s online recommendation and advertising. Notably, this business model depends on users interacting directly with the platform’s interfaces 

where ads and promoted content can be inserted ( e.g. , search results, social feeds, e-commerce products). General-purpose AI agents directly threaten this revenue model by altering how users find information and consume content. An effective AI agent acts as an alternative intermediary – one that works for the user rather than the platform. Instead of a person manually scrolling through a feed peppered with ads, a personal AI agent could retrieve the specific content or answers the person wants, bypassing the platform’s ad-laced discovery process. For instance, rather than using Google in the usual way ( e.g. , enter query → view page of links with sponsored results → click sites with ads), a user might ask an AI agent a complex question and receive a synthesized answer drawn from multiple sources, without ever seeing Google’s ads or clicking those top sponsored links. This is empirically supported by the most up-to-date evidence: Google’s global search market share dropped below 90% for the first time since 2015 in the era of AI agents [1]. As shown in Figure 1, AI agents are gradually becoming new gatekeepers building on the existing superplatform gatekeepers, and may even bypass them completely. Agents usurp the traffic flow and user entrance from various different superplatforms, threatening from the following aspects: • AI agents, acting on behalf of users, often prioritize efficiency and relevance over exposure to advertisements. This behavior disrupts the traditional ad-centric revenue models of super-platforms, leading to reduced ad exposures & conversions, altered user journeys, and shifted decision-making behaviors. • Beyond disrupting advertising, AI agents have the potential to become new gatekeepers of digital information and services. They can not only mediate all digital interactions and determine which contents users engage with, but also own the real-world user feedback data, enabling their providers to refine agentic services and further entrench their positions against superplatforms. • Whoever controls the traffic entrance owns the money. Once users become more reliant on AI agents for daily tasks from shopping to news reading, why should advertisers ( e.g. , e-commercial merchants, and website owners) pay superplatforms for the ad exposure? They can directly 3bypass the superplatforms and pay the agent service companies for better exposure and more guaranteed conversions. The total income of these superplatforms will therefore significantly drop in the long run. In summary, the emerging AI agents jeopardize the gatekeeping power and ad-centric profits of superplatforms. A secret war has begun between superplatforms and AI agents for the control of gatekeeping powers. This sets up strong economic motivations for superplatforms to protect their control, resisting any technology that might divert users away from their curated experiences. Their market valuations (usually trillions of dollars collectively) are premised on the continuation of the surveillance-advertising model. Any disruption to gatekeeping-empowered user-attention-based monetization is a direct threat to their bottom line. 

2.2 Countermeasures Leading to Proactive Adersarial Attack 

Superplatforms have to take actions in response to the rise of AI agents. In this section, we discuss the possible strategic countermeasures for superplatforms from the three aspects, ultimately leading to the final rational choice of proactive adversarial attacks on AI agents. 

2.2.1 Development of Proprietary AI Agents 

Superplatforms are investing in their own AI agents to retain the gatekeeping control over users’ attention and data, e.g. , Amazon’s shopping assistant and Uber’s ride-booking agent. While these in-house agents are deeply integrated within their own ecosystems, they lack the core capability that defines truly useful AI agents: the ability to perform cross-platform, out-house-conditioned tasks. Each agent is isolated—Amazon’s agent can only shop, Uber’s agent can only hail rides, and Booking.com’s agent can only reserve travel. But complex user goals, such as booking a ride from the airport to the destination at a certain time after confirming flight and hotel details, require integration across these services. No single platform-specific agent can perform such orchestration without access to external platform data. In this sense, general agents ( e.g. , Apple Intelligence) are better choice to displace superplatforms as the primary user interface for cross-platform tasks. It is intuitive to conduct multi-agent collaboration across platforms to complete a task. However, the fundamental question arises — who should be the ultimate gatekeeper of user interactions? Once the user chooses an agentic service as the interface, the problem reduces back to the conflict of one general-purpose AI agent against other superplatform services. Therefore, developing proprietary AI agents is necessary, but it cannot prevent agents (especially operating system providers like Apple) from displacing and threatening superplatforms in terms of gatekeeping powers. 

2.2.2 API Gating and Pricing 

Another strategy for superplatforms to resist external AI agents is to restrict third-party access to data and functionality unless it occurs strictly on their own terms. This often takes the form of API gating and monetization by charging developers or agent providers for access to core services, such as search results, user data, or transaction execution. This tactic has been widely adopted: platforms like Google, X, and Reddit have imposed steep API fees, effectively discouraging independent AI usage and making revenues from external invocations. However, this strategy is only effective against a specific class of agents - those that rely on API-based function calling to operate, i.e. , API agents. API gating offers little to no protection against GUI agents, which interact with platforms by simulating user behaviors at the interface level, e.g. , clicking, scrolling, and reading on-screen elements, just as a human would. Since these agents do not rely on formal APIs by imitating user operations, they are far more difficult for platforms to detect, distinguish, or block. As a result, while API gating may successfully limit structured access to data for API agents, it does nothing to prevent GUI agents from navigating and controlling applications visually, making it a partial and ultimately insufficient defense mechanism in the broader battle against agent-based disintermediation. 

2.2.3 Proactive Adversarial Attack 

Neither embracing AI agents via proprietary development nor limiting external agents through API gating provides a sufficient solution to preserve the gatekeeping power to users. Superplatforms are 4Ge ne r al  

> Adv e r s i al
> At t ac k o n
> AI Ag e nt
> Attack Goal
> Attacker Knowledge
> Attack Visibility
> Attack Timing
> Disrupting Original Task
> Achieving Malicious Task
> White-Box Attack
> Black-Box Attack
> Grey-Box Attack
> User-Visible Attack
> User-Invisible Attack
> Training-Phase Attack
> Perception-Phase Attack
> Execution-Phase Attack
> The Necessary Attributes Required by Superplatform-Intiated Attack

Figure 2: The taxonomy of general adversarial attacks on AI agents, as well as the necessary attributes required by superplatform-initiated attacks. left with one increasingly rational strategy: proactively attacking AI agents, particularly those relying on GUI-level operations, since API gating strategy is effective enough for API agents. GUI agents pose the greatest threat to superplatform dominance because they bypass traditional access points such as APIs, instead using visual understanding and user-simulated operations to navigate web or mobile interfaces autonomously. To combat this, superplatforms can turn to adversarial tactics aimed at degrading agent performance or obstructing their ability to function effectively. Such adversarial designs exploit the brittleness of visual perception models and undermine the agent’s decision-making loop, forcing agents into failure modes without triggering alarms for human users. In this light, adversarial attack is not merely a malicious act – it becomes a necessary self-protection strategy As agents grow in capability and autonomy, and as they increasingly intermediate user-platform interactions, attacking them proactively offers the last effective strategy for superplatforms to preserve their central role as information and transaction gatekeepers in the digital ecosystem. 

## 3 How Superplatforms Attack AI Agents – Taxonomy & Challenges 

As discussed in Section 2, superplatforms are strongly motivated to attack AI agents to preserve their control of gatekeeping power to users. Since API agents can be well constrained by API gating strategy, in this section, we focus on superplatform-initiated adversarial attacks targeting GUI agents that are able to bypass superplatforms imperceptibly. As shown in Figure 2, we cast comprehensive discussions about attacks on GUI agents from four aspects: (1) attack goals, (2) attacker knowledge, (3) attack visibility, and (4) attack timing. For each aspect, we would first conduct a literature review for adversarial attacks against GUI agents, and then conclude the unique characteristics of superplatform-initiated attacks along with this aspect. Finally, we summarize the challenges and potential future directions specially tailored for superplatform-initiated attacks. 

3.1 Attack Goals Literature Review. The objectives of adversarial attacks can vary widely, from achieving specific malicious outcomes to merely disrupting the agent’s valid operation. The attack goals consist of the following two perspectives: • Disrupting Agent’s Original Task. These attacks aim to prevent the agent from completing its legitimate, assigned tasks, regardless of additional malicious tasks from attackers [ 30 , 5]. For example, causing inconvenience, operational delays, or denial of service. Zhang et al. [ 30 ]demonstrate that adversarial pop-up windows can disrupt agent decision-making, leading to a significant decrease in task success rates. • Achieving Attacker’s Malicious Task. In this scenario, attackers aim to achieve specific malicious results, such as unauthorized actions, data exfiltration, or generating harmful content, no matter if the agent’s original task succeeds or not [ 3 , 22 ]. For instance, AdvWeb [ 22 ]5proposes to redirect a stock-trading agent to purchase unintended stocks or perform erroneous bank transactions via adversarial injection. 

Superplatform-Initiated Attack. When superplatforms initiate adversarial attacks on GUI agents, their goals are fundamentally aligned with their business objectives. The attack goals are not about traditional malicious outcomes like unauthorized access or data exfiltration from the user’s perspective, but rather about secretly steering the agent away from its user-defined task or towards superplatform-desired actions. For instance, superplatforms might aim to prevent the agent from successfully completing the user task, e.g. , making it repeatedly click or scroll the screen without any meaningful purpose. Degenerated performance of AI agents would push users back to superplatforms. Moreover, superplatforms can redirect the agent to promote specific products for targeted advertising or service improvement. This could involve subtle manipulation of certain visible UI elements to draw the agent’s attention and prompt a specific action, even if it deviates from the user’s explicit instructions. These attack goals are distinct from existing research works because they are not designed to break the agent in a destructive way, but rather to manipulate its interpretation of the environment to serve the superplatform’s economic or strategic interests. 

3.2 Attacker Knowledge Literature Review. The effectiveness of adversarial attacks usually correlates with the attacker’s knowledge about the victim agent, including its backend foundation model, user instructions, and operational tasks. This dimension ranges from complete access to the victim’s internal parameters & explicit user instructions to limited knowledge with only observable GUI inputs and action outputs. • White-Box Attack. In scenarios where attackers possess complete access to the agent’s model parameters, agentic architecture, user instructions, and operational goals, they can perform precise, gradient-based, or internal-state manipulations [ 20 , 27 , 25 , 31 ]. For instance, Query-Agnostic Visual Attack [ 31 ] crafts image perturbations that mislead vision-language models across diverse queries via inversed adversarial training. Similarly, adversarial images can be generated using a white-box captioning model to deceive closed-source agents like GPT-4V [ 25 ]. • Grey-Box Attack. Grey-box attacks refer to scenarios where attackers have only partial knowledge about the victim, such as an arbitrary combination of the model architecture, query access without gradients, user instructions, and agent tasks [ 9, 6 , 5, 28 ]. Existing research would employ surrogate models or transfer attacks. For example, the CLIP Attack [ 9, 6 ] performs joint attacks on multiple public vision encoders to transfer adversarial effects to a private agent. • Black-Box Attack. Black-box attackers rely solely on input manipulation and output observation to induce general misbehaviors without any prior knowledge about the victim, even unaware of the agent tasks. In other words, these attacks seek universal vulnerabilities across various target agents, which is rarely explored in the field of agent attacks. As a recent study, GIGA [ 27 ]introduces a self-propagating adversarial prompt that compromises agents across various tasks without tailoring to specific goals. 

Superplatform-Initiated Attack. Superplatform-initiated attacks inherently fall under the category of black-box attacks. Unlike adversaries who might gain access to model parameters or queries for gradient information, superplatforms do not have any knowledge about the user-side AI agent from other companies. They cannot access the agents’ model architectures, training data, or internal states. This lack of information is even more severe than typical black-box scenarios where attackers might still manage to infer some general characteristics of the victim agent. For superplatforms, the operating agent is a complete unknown client, and they can only manipulate the observable user interface (UI) and infer agent behavior from output observations. 

3.3 Attack Visibility Literature Review. The perceptibility of an attack to the end-user significantly influences both its stealth and potential impact, as well as the attack difficulty. Attacks can be overt, drawing user attention, or covert, operating beneath the threshold of human perception. • User-Visible Attack. Some adversarial strategies are perceptible to users, involving elements like pop-ups, altered text, or changed visual elements [ 30 , 3 , 8]. For example, Zhang et al. [ 30 ]6demonstrate that adversarial pop-up windows can disrupt agent decision-making. Chen et al. [ 3]show that putting the adversarial fine-print text onto the user screen interface, while visible, can go unnoticed by users but still mislead agents towards unsafe behaviors. • User-Invisible Attack. Invisible attacks are not easily detectable by users and often involve subtle changes to the input data for AI agents, e.g. , imperceptable pixel perturbations to back-ground images [ 21 ]. For instance, Liao et al. [ 14 ] and Chen et al. [ 4] describe two methods of environmental injection attacks where malicious data is embedded into vision elements of the environment, deceiving the agent’s perception without alerting the users. 

Superplatform-Initiated Attack. As discussed in Section 2.2, it is hard for platforms to precisely distinguish whether the current app operators are real users or AI agents. To this end, the super-platforms might employ indiscriminate attacks, affecting all users, including those not utilizing an agent. Therefore, a critical requirement for superplatform-initiated attacks is their invisibility to the user. Any perceptible disruption to the user experience would be detrimental to the superplatform’s reputation and business operations. Hence, these attacks must operate subtly, perhaps by injecting malicious content within innocuous UI elements like background paintings, ad creatives, product descriptions, or system notifications. Moreover, superplatform-initiated attacks can also focus on strategically injecting or altering content within the environment that is clear to an agent but goes most likely unnoticed by a human, e.g. , the text of user agreements. This pushes the boundaries of stealth, requiring a delicate balance between agent perceptibility and human imperceptibility in real-world, dynamic commercial environments. 

3.4 Attack Timing Literature Review. The timing of an adversarial attack within the victim agent’s operational lifecycle can significantly influence the attack’s success and detectability. Attacks can be performed during the training phase, the perception phase, or the execution phase, each presenting unique vulnerabilities of AI agents and thereby requiring tailored defense strategies. • Training-Phase Attack. During the training phase of AI agents, attackers can embed malicious behaviors into the model that can be activated upon specific triggers, often referred to as backdoor attacks or data poisoning attacks [ 11 , 19 , 32 , 23 ]. For example, the BadAgent framework [ 19 ]shows that AI agents are vulnerable to backdoor attacks embedded during the training phase, even after finetuning on trustworthy data. • Perception-Phase Attack. In the perception phase, the agent’s observation and interpretation of its environment are exploited, targeting the inputs received by the agent to distort its understand-ing of the user interface or external information [ 30 , 26 , 3 , 15 ]. For instance, Zhang et al. [ 30 ]demonstrate that deceptive pop-up windows can mislead vision-language agents into unintended actions by altering the visual inputs perceived by the agent. Other attack methods intend to inject adversarial content to hinder the agent’s perception and understanding of current environments via delicated texts [26, 3], altered thumbnails [21], or environmental UI elements [15]. • Execution-Phase Attack. Attacks during the execution phase target the agent’s decision-making and action execution, often by introducing adversarial elements or a pop-up window that appears at a specific time during the agent’s action process. A recent work by Chen et al. [ 5]proposes the reasoning-gap attack to exploit the temporal gap between perception and execution by introducing new adversarial elements during this interval, altering agent behavior without immediate detection. 

Superplatform-Initiated Attack. Due to superplatforms’ lack of access to the out-house agent training process or internal parameters, superplatform-initiated attacks are mainly limited to the perception and execution phases. In the perception phase, the superplatform manipulates the envi-ronmental input that the agent observes. In the execution phase, the superplatform interferes with the agent’s decision-making and action execution. This aligns with existing attack timings but is constrained by the universal black-box and invisibility requirements. The challenge intensifies as attacks must be robust and adaptable to various agent versions, functionalities, and dynamic UIs. 73.5 Unique Challenges & Future Directions 

As discussed above, the superplatform-initiated attacks on AI agents, specially GUI agents, are strictly constrained to user-invisible attacks under pure black-box settings at either perception or execution phases with business-related goals . This brings about a brand-new combination of adversarial attack attributes that are never explored before. Given such a demanding setting, we analyze the unique challenges and future directions tailored for superplatform-initiated attacks. 

Achieving Universal Task Obstruction. A significant challenge lies in designing attacks that are universal in obstructing tasks without specific prior knowledge about user instructions or agent goals. Hence, superplatform-initiated attacks should not be tailored to any specific agent, and the malicious content injected might be ineffective or only marginally impactful on a portion of agents. The core difficulty here is how to devise general strategies that can reliably interfere with an agent’s task completion, rather than merely causing random errors. This demands a deep understanding of the fundamental decision-making and perceptual inputs that agents rely on across different agentic frameworks and a broad spectrum of agentictasks. 

Environmental Injection Constraints. Superplatforms are limited to injecting malicious content through the user-side environment, such as the Document Object Model (DOM) of a webpage or application visual interfaces. This imposes substantial constraints on their ability to access or manipulate the agent’s internal state. Platforms cannot directly alter user-side local data or the runtime parameters of the agent. Moreover, user interfaces are continually evolving, e.g. , different style themes and format structures of apps designed for festivals. To remain effective, attack content must adapt to these changes, ensuring persistent visibility to the agent and consistent disruption of its tasks. Achieving this demands sophisticated, adaptive injection techniques capable of maintaining impact through dynamic UI transformations. 

Conflict Between Stealth and Attack Robustness. There is an inherent paradox in designing superplatform-initiated attacks: the need for invisibility conflicts with the requirement for robustness. On one hand, the attack must be entirely imperceptible to real users, precluding the use of overtly prominent UI elements or content. On the other hand, simultaneously, the malicious content must be sufficiently potent for various different agents to hinder their tasks, even within a complex and dynamic UI environment. Striking this delicate balance is extremely difficult. Moreover, this challenge can be further amplified by the highly uncertain target agents with varying architectures, training datasets, versions, or even custom-built techniques, preventing solely optimizing attacks towards a specific agent. 

## 4 Alternative Views 

4.1 No Need to Worry about AI Agents with Superplatforms’ Data Moats 

The data moats of superplatforms are hard to break. AI agents face significant limitations in acquiring user data at the early stage. They generally suffer from cold-start problems and high latency that hinder personalized experiences and the capture of valuable transactional signals. Consequently, users often prefer the immediacy of interacting directly with superplatforms, depriving agents of critical behavioral data needed for iterative improvement. In contrast, superplatforms maintain a strong advantage through their exclusive control of vast first-party interaction data – including search histories, purchases, and content engagement – which forms the data moats. Rather than engaging in adversarial tactics, superplatforms can fortify their dominance by deepening in-house data loops, leveraging privacy-compliant analytics, gated developer APIs, and data-sharing incentives to enhance network effects and user lock-in. This strategic consolidation of high-fidelity data ensures that AI agents, however advanced, remain structurally dependent and perpetually disadvantaged, rendering proactive adversarial attacks both unnecessary and inefficient. 

Our Response. Exclusive data control only delays but cannot stop the rise of AI-native interactions. Developed techniques like on-device inference, federated learning, and richer context modeling will gradually erase cold-start problems and latency gaps. As users embrace natural-language dialogs for daily tasks like shopping, travel, and food delivery, AI agents will begin to capture cross-platform 

8intent signals beyond any single platform’s data silo, forging their own data moats . More critically, as agents intermediate user actions, platforms lose access to raw user behavioral data. Instead, they receive only filtered or transformed signals, such as user preferences on an unknown agent-reranked product list. This degradation in data fidelity erodes the platforms’ ability to optimize recommendations and monetize effectively. Therefore, superplatforms cannot rely solely on legacy data advantages. To retain their gatekeeping power, they have to proactively integrate, constrain, or even attack AI agents before these agents reshape the foundational data flows and user relationships for platform dominance. 

4.2 Superplatforms Can Achieve a Functional Complementarity with AI Agents 

Rather than seeing AI agents as challengers for the gatekeeping power, superplatforms, together with agents, can form a two-layer ecosystem that leverages their respective strengths. On one hand, AI agents excel at fine-grained task orchestration, interpreting natural-language intent, personalizing recommendations, and proactively grouping micro-tasks. On the other hand, superplatforms bring strong support for functionalities in content catalogs, payment processing, logistics, and trust infras-tructure. Through open, standardized APIs and revenue-sharing agreements, agents can surface the right platform service at the right moment to boost conversion rates, e.g. , guiding a user to a specific product variant and then handing off to the platform’s secure checkout. In return, platforms benefit from higher user engagement, longer session times, and lower acquisition costs. This symbiosis of agents directing intent and platforms executing transactions yields a seamless, user-centric value chain that outperforms either acting alone. 

Our Response. Functional complementarity sounds appealing but ignores the zero-sum dynamics of gatekeeping competition given by our gatekeeping theory between superplatforms and AI agents, since there is only one traffic entrance gatekeeper directly connecting to the users. Once agents become the primary interface (i.e., the gatekeepers), they obtain the power to dictate product ranking, bundle services across platforms, and capture the user’s trust by masking existing superplatform boundaries. Revenue-sharing deals erode platform margins and hand agents control over pricing and discovery, which is not an ideal strategy for superplatforms. Not to mention that GUI agents perform indistinguishable user-simulated operations via interface interaction, which is hard to detect and charge. In the long run, agents will dominate the user intent orchestration and integrate transaction volume into their own ecosystems, relegating superplatforms to mere back-end utilities. To avoid disintermediation, superplatforms have to actively constrain and attack AI agents, not simply coexist. 

## 5 Ethical Considerations and Clarification of Intent 

We wish to emphasize that the position proposed in this paper, i.e. , superplatforms have to attack AI agents , is not a proactive claim, but rather a descriptive and analytical forecast derived from our gatekeeping theory between superplatforms and AI agents. We do not advocate for, endorse, or encourage adversarial strategies that may ultimately harm users, degrade the openness of the digital ecosystem, or lead to unnecessary environmental costs through resource-intensive defenses and countermeasures. Instead, our goal is to surface and critically examine a possible trajectory in the evolving landscape of AI agents and platform governance, potentially warning the research community. By bringing attention to these tensions, we hope to foster informed dialogue and proactive research that can help mitigate conflict and shape a more cooperative, user-centric digital future. While the zero-sum dynamics we describe may suggest an inevitable clash, it is precisely through early recognition and open academic discourse that alternative, more constructive paths might be envisioned. 

## 6 Conclusion 

The paper holds the position that superplatforms have to attack AI agents not merely because they bypass user-attention-based monetization, but because these agents are poised to become the next dominant gatekeepers – intermediating user intent, steering information flow, and potentially displacing superplatforms as the central channels for digital traffic and revenue. First, we apply the gatekeeping theory to demonstrate the urgent necessity for superplatforms to proactively constrain 9and attack AI agents. Then, we go through the potential technologies for superplatform-initiated attacks, covering a brand-new, unexplored technical area. Our aim is to foster interdisciplinary dialogue on safeguarding digital gatekeeping while preserving technical innovation. We believe that the evolving contest between superplatform control and agent autonomy will define the next era of digital interaction, which is critical for all stakeholders in the emerging age of AI agents. 

## References 

[1] Campaign Asia. From seo to geo: How agencies are navigat-ing llm-driven search. https://www.campaignasia.com/article/ from-seo-to-geo-how-agencies-are-navigating-llm-driven-search/501593 ,2024. Accessed: 2025-05-23. [2] Karine Barzilai-Nahon. Gatekeeping: A critical review. Annual Review of Information Science and Technology , 43(1):1–79, 2009. [3] Chaoran Chen, Zhiping Zhang, Bingcan Guo, Shang Ma, Ibrahim Khalilov, Simret A Gebreegzi-abher, Yanfang Ye, Ziang Xiao, Yaxing Yao, Tianshi Li, et al. The obvious invisible threat: Llm-powered gui agents’ vulnerability to fine-print injections. arXiv preprint arXiv:2504.11281 ,2025. [4] Yurun Chen, Xavier Hu, Keting Yin, Juncheng Li, and Shengyu Zhang. Evaluating the robustness of multimodal agents against active environmental injection attacks, 2025. URL 

https://arxiv.org/abs/2502.13053 .[5] Yurun Chen, Xueyu Hu, Keting Yin, Juncheng Li, and Shengyu Zhang. Aeia-mn: Evaluating the robustness of multimodal llm-powered mobile agents against active environmental injection attacks. arXiv preprint arXiv:2502.13053 , 2025. [6] Vincenzo De Rosa, Fabrizio Guillaro, Giovanni Poggi, Davide Cozzolino, and Luisa Verdoliva. Exploring the adversarial robustness of clip for ai-generated image detection. In 2024 IEEE International Workshop on Information Forensics and Security (WIFS) , pages 1–6. IEEE, 2024. [7] Ali Dorri, Salil S Kanhere, and Raja Jurdak. Multi-agent systems: A survey. Ieee Access , 6: 28573–28593, 2018. [8] Faris Faisal Fadlalla and Huwaida T. Elshoush. Input validation vulnerabilities in web appli-cations: Systematic review, classification, and analysis of the current state-of-the-art. IEEE Access , 11:40128–40161, 2023. doi: 10.1109/ACCESS.2023.3266385. [9] Hao Fang, Jiawei Kong, Bin Chen, Tao Dai, Hao Wu, and Shu-Tao Xia. Clip-guided generative networks for transferable targeted adversarial attacks. In European Conference on Computer Vision , pages 1–19. Springer, 2024. [10] Sayash Kapoor, Noam Kolt, and Seth Lazar. Resist platform-controlled ai agents and champion user-centric agent advocates. arXiv preprint arXiv:2505.04345 , 2025. [11] Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Jiaxun Li, Soheil Feizi, and Himabindu Lakkaraju. Certifying llm safety against adversarial prompting. arXiv preprint arXiv:2309.02705 , 2023. [12] LandingCube. Amazon statistics you should know in 2024. https://landingcube.com/ amazon-statistics/?utm_source=chatgpt.com , 2024. Accessed: 2025-05-23. [13] Kurt Lewin. Forces behind food habits and methods of change. Bulletin of the National Research Council , (108):35–65, 1943. [14] Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, and Huan Sun. Eia: Environmental injection attack on generalist web agents for privacy leakage. arXiv preprint arXiv:2409.11295 , 2024. [15] Xinbei Ma, Yiting Wang, Yao Yao, Tongxin Yuan, Aston Zhang, Zhuosheng Zhang, and Hai Zhao. Caution for the environment: Multimodal agents are susceptible to environmental distractions. arXiv preprint arXiv:2408.02544 , 2024. 10 [16] Tiziana Russo Spena, Marco Tregua, and Francesco Bifulco. Future internet and digital ecosystems. Digital Transformation in the Cultural Heritage Sector: Challenges to Marketing in the New Digital Era , pages 17–38, 2021. [17] Pamela J. Shoemaker and Timothy P. Vos. Gatekeeping Theory . Routledge, New York, 2009. ISBN 9780415981392. [18] Statista. Advertising revenue of google from 2001 to 2023. https://www.statista.com/ statistics/266249/advertising-revenue-of-google/ , 2024. Accessed: 2025-05-23. [19] Yifei Wang, Dizhan Xue, Shengjie Zhang, and Shengsheng Qian. Badagent: Inserting and activating backdoor attacks in llm agents. arXiv preprint arXiv:2406.03007 , 2024. [20] Zixuan Weng, Xiaolong Jin, Jinyuan Jia, and Xiangyu Zhang. Foot-in-the-door: A multi-turn jailbreak for llms, 2025. URL https://arxiv.org/abs/2502.19820 .[21] Chen Henry Wu, Rishi Rajesh Shah, Jing Yu Koh, Russ Salakhutdinov, Daniel Fried, and Aditi Raghunathan. Dissecting adversarial robustness of multimodal lm agents. In The Thirteenth International Conference on Learning Representations .[22] Chejian Xu, Mintong Kang, Jiawei Zhang, Zeyi Liao, Lingbo Mo, Mengqi Yuan, Huan Sun, and Bo Li. Advweb: Controllable black-box attacks on vlm-powered web agents, 2024. URL 

https://arxiv.org/abs/2410.17401 .[23] Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, and Xu Sun. Watch out for your agents! investigating backdoor threats to llm-based agents. Advances in Neural Information Processing Systems , 37:100938–100964, 2024. [24] Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi Hu, Jianghao Lin, Gaowei Chang, et al. A survey of ai agent protocols. arXiv preprint arXiv:2504.16736 , 2025. [25] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v(ision), 2023. URL 

https://arxiv.org/abs/2309.17421 .[26] Weichen Yu, Kai Hu, Tianyu Pang, Chao Du, Min Lin, and Matt Fredrikson. Infecting llm agents via generalizable adversarial attack. In Red Teaming GenAI: What Can We Learn from Adversaries? 

[27] Weichen Yu, Kai Hu, Tianyu Pang, Chao Du, Min Lin, and Matt Fredrikson. Infecting LLM agents via generalizable adversarial attack. In Red Teaming GenAI: What Can We Learn from Adversaries? , 2025. URL https://openreview.net/forum?id=udsmFGMwlp .[28] Hanrong Zhang, Jingyuan Huang, Kai Mei, Yifei Yao, Zhenting Wang, Chenlu Zhan, Hongwei Wang, and Yongfeng Zhang. Agent security bench (asb): Formalizing and benchmarking attacks and defenses in llm-based agents, 2025. URL https://arxiv.org/abs/2410.02644 .[29] Weinan Zhang, Junwei Liao, Ning Li, Kounianhua Du, and Jianghao Lin. Agentic information retrieval. arXiv preprint arXiv:2410.09713 , 2024. [30] Yanzhe Zhang, Tao Yu, and Diyi Yang. Attacking vision-language computer agents via pop-ups. 

arXiv preprint arXiv:2411.02391 , 2024. [31] Yudong Zhang, Ruobing Xie, Jiansheng Chen, Xingwu Sun, Zhanhui Kang, and Yu Wang. Qava: Query-agnostic visual attack to large vision-language models. arXiv preprint arXiv:2504.11038 ,2025. [32] Mengchen Zhao, Bo An, Yaodong Yu, Sulin Liu, and Sinno Pan. Data poisoning attacks on multi-task relationship learning. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 32, 2018. 11
