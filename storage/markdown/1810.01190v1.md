Title: Inference Over Programs That Make Predictions

URL Source: http://arxiv.org/pdf/1810.01190v1

Published Time: Sun, 22 Jan 2023 22:39:43 GMT

Markdown Content:
> arXiv:1810.01190v1 [cs.PL] 2 Oct 2018

# Inference Over Programs That Make Predictions 

## Yura Perov 

Babylon Health London, United Kingdom 

yura.perov@babylonhealth.com 

## Abstract 

This abstract extends on the previous work [21, 22] on pro-gram induction [16] using probabilistic programming. It de-scribes possible further steps to extend that work, such that, ultimately, automatic probabilistic program synthesis can generalise over any reasonable set of inputs and outputs, in particular in regard to text, image and video data. 

Keywords Probabilistic Programming, Program Induction, Program Synthesis 

## 1 Introduction 

Probabilistic programming provides a natural framework for program induction, in which models can be automatically generated in the form of probabilistic programs given a spec-ification. The specification can be expressed as input-output pairs, e.g. as “23-year male has lower right abdominal pain (LRAP) and vomiting” => “Appendicitis”; “34-year female has stiff neck, severe headache” => “Meningitis”, etc. We ex-pect that given a set of such input-output examples, a pro-gram induction system will generate a model, which is capa-ble of predicting an output for a new input. In the ideal sce-nario, the prediction will be a distribution over values rather than a specific deterministic value, as in “23-year male has LRAP and vomiting” can be mapped to both “Appendicitis” and “Gastroenteritis” with probabilities p1, p2.In the previous work [21, 22] it was shown how to define an adaptor [6], strongly-typed grammar as a probabilistic program, which can generate other probabilistic programs given a specification in the form of a set of observations: ( assume model ( run −grammar ' a r r a y ' word ) ) ( n o i s y − o b s e r v e ( a p p l y model i n p 1 ) o u t p 1 ) . . . ( n o i s y − o b s e r v e ( a p p l y model inpN ) outpN ) ( p r e d i c t i n p 0 ) By performing inference over model -s, it was possible to infer simple probabilistic programs (specifically, samplers from one dimensional distributions, e.g. Bernoulli, Poisson, etc.) that can generalise over any available training data (in-put and output pairs) and predict new outputs given new inputs. The results were comparable to another state-of-the-art approach of program induction, genetic programming. 

> ICPP, 2018, Boston, MA, USA
> 2018.

## 2 Further Extensions 

To facilitate the research into program induction using prob-abilistic programming, the follow improvements into the methodology are suggested: 

2.1 Using non-parametric, hierarchical distributions for the grammar 

In the previous work, the adaptor, strongly-typed grammar was used with an enhancement of adding an option of draw-ing variables from local environments. For example, if we were looking for a model that takes two integers as argu-ments and outputs another integer, we would define an ini-tial local environment scope with two variables x1, x2 and 

N -predefined constants (like π, e1, etc.), and sample a can-didate program from the grammar, i.e. as from (grammar ( localscope (typed ' x1 ' int ) (typed ' x2 ' int ) ) ' int ). The 

(grammar . . . ) method randomly generates an expression that produces an integer (or any other type): 

• either it will an integer constant (hence expr int − > c), 

• an integer variable (hence expr int − > xi ), 

• a predefined method f , which returns an integer, with 

M arguments (for each of which grammar will be called recursively to construct the full expression (f expr -

arg 1 . . . exprargM )), 

• a new extended local scope (via (let . . . )) with one more variable of any supported type (including func-tions themselves), such that the returned expression of that (let ... ) is still of integer type (hence expr int − >

(let (newsymb expr ∗) expr ′ 

> int

)), 

• short-circuit i f such that expr int − >

(i f expr bool expr int expr int ),

• recursive call to the current function expr int − > (recur . . . )

assuming the type signature of the current function is integer. To make the priors over programs more flexible, we sug-gest to use non-parametric [6, 11], hierarchical [9] priors. That is, instead of adding a local environment with a vari-able of arbitrary type and randomly grammar-generated ex-pression of that type, we suggest to instead draw expres-sions from a non-parametric distribution defined as a mem oized function with a base function being the grammar itself. The arguments of a call to that function might be another ex-pression generated using the same grammar, which ensures that the “keys” of that mem oized function will be generated based on the program induction inputs. The hierarchical 

> 1

ICPP, 2018, Boston, MA, USA Yura Perov 

property of such a prior might be achieved by ensuring that the same mem oized function might be call in its body; by do-ing so, we allow this function to decide whether to return an expression or to make another call to itself with different arguments, hence going deeper in the hierarchy. 

2.2 Extending types supported, including higher-order typing 

Another improvement over the previous work can be achieved by extending the types which can be used by the grammar. This includes adding more types like dictionaries, lists, ma-trices, sets, queues, etc. Also, ideally we would like to sup-port “recursive” type definitions such that the grammar not just produces the expressions to be evaluated, but is also ca-pable of producing expressions that generate other expres-sions to be evaluated. 

2.3 Using discriminative model proposals 

To facilitate inference in a probabilistic program induction framework, we can use modern advances in probabilistic programming inference. In particular, we can use discrim-inative models, such as neural networks, to facilitate the in-ference [1, 4, 8, 23]. 

2.4 Incorporating the compressed knowledge in the form of embeddings 

The set of functions that can be used by the grammar also can be extended. Specifically, we believe one of the most in-teresting additions into that set might be the pre-trained em-beddings. For example, we can incorporate word 2vec [15] functions which would map a type “word” to a type “float”. This should allow the program induction to benefit from the compressed knowledge which the word 2vec and similar em-bedding models represent. 

2.5 “Ultimate” task for the induction 

In the previous work, the probabilistic program induction was performed over a simple one dimensional distribution. We believe that the most effective and cheap way to pro-vide as much training data as possible is to set a task of pre-dicting 1-20 words given previous 20-500 words for an “ar-bitrary piece of text”. These pieces might be extracted from any source, e.g. from Wikipedia, news web-sites, books, etc. The observational likelihood might be a Binominal distribu-tion Bernoulli (N , p), where N is the number of words to pre-dict for that particular input-output pair, and p is the prob-ability of “success”. This approach follows the methods of noisy Approximate Bayesian Computation [14]. Parameter 

p also might be varied in the process of inference, hence we might be performing simulated annealing. We believe that with enough computational power and with rich enough pri-ors, the inference will be able to find models that predicts reasonably well what the next word or list of few words should be. Once a good model that can predict next words is well trained, this task can be extended to predicting: audio and video [23] sequences, image reconstruction [7, 13], text-to-image and image-to-text tasks, as well as then ultimately performing actions in environments like OpenAI Gym. 

2.6 Distributing the computations 

The inference over such a gigantic set of input-output pairs will require a massive amount of computations which needs to be distributed. One approach to run the inference in par-allel might be running multiple N >> 1 Markov chains (e.g. using Metropolis-Hastings algorithm) where each chain is given some subset of observations (i.e. it would be similar to 

stochastic gradient descent approach), as well as those chains “lazily” share the hidden states of the non-parametric, hier-archical, adaptor grammar. By “lazy” sharing we mean that the hidden states of the non-parametric components of the grammar are to be updated from time to time. 

2.7 Discussion over proposals 

While this abstract has focused on possible enhancements to improve priors over models as well as possible ways of set-ting the inference objective, it is also important to allow the proposal over a new probabilistic program candidate (e.g., as in X → X ′ in Metropolis-Hastings) to be flexible, ide-ally by sampling the proposal function from the grammar as well. In that case, it will be “inference over inference”, i.e. nested inference [24] over the grammar and over the pro-posal. Another way of improving the process of inducing a new program candidate is the merging of two existing pro-grams as in [5]. 

2.8 Conclusion 

This short abstract extends the previous work by suggest-ing some enhancements to allow more practical probabilis-tic program induction. Implementing a system which is capable of such com-plex probabilistic program induction will require a lot of resources, with the computational resource and its distribu-tion being the most expensive one, presumably. Another careful consideration should be made to the choice of three languages: 

• the language in which the system is to be written, 

• the language in which the grammar is defined, 

• the language of the inferred probabilistic programs (models). It might be beneficial if it is the same language altogether, such that the system can benefit from recursive use of the same grammar components (e.g. for doing inference over the inference proposal itself as suggested before). Also, ide-ally it is a “popular” language (or a language that can be easily transformed into such), such that all publicly avail-able source code made be incorporated [10] into the priors. 

> 2

Inference Over Programs That Make Predictions ICPP, 2018, Boston, MA, USA 

Examples of such language candidates are Church [2], Ven-ture [12], Anglican [25, 26], Probabilistic Scheme [17] or WebPPL [3]. 

## Acknowledgments 

This abstract is the extension to the work [21], as well as in-corporates ideas which had been published online [18–20]. 

## References 

[1] Laura Douglas, Iliyan Zarov, Konstantinos Gourgoulias, Chris Lucas, Chris Hart, Adam Baker, Maneesh Sahani, Yura Perov, and Saurabh Johri. 2017. A Universal Marginalizer for Amortized Inference in Gen-erative Models. arXiv preprint arXiv:1711.00695 (2017). [2] Noah Goodman, Vikash Mansinghka, Daniel M Roy, Keith Bonawitz, and Joshua B Tenenbaum. 2012. Church: a language for generative models. arXiv preprint arXiv:1206.3255 (2012). [3] Noah D Goodman and Andreas Stuhlmüller. 2014. The design and implementation of probabilistic programming languages. [4] Shixiang Gu, Zoubin Ghahramani, and Richard E Turner. 2015. Neural adaptive sequential monte carlo. In Advances in Neural Information Processing Systems . 2629–2637. [5] Irvin Hwang, Andreas Stuhlmüller, and Noah D Goodman. 2011. In-ducing probabilistic programs by Bayesian program merging. arXiv preprint arXiv:1110.5667 (2011). [6] Mark Johnson, Thomas L Griffiths, and Sharon Goldwater. 2007. Adap-tor grammars: A framework for specifying compositional nonpara-metric Bayesian models. In Advances in neural information processing systems . 641–648. [7] Tejas D Kulkarni, Pushmeet Kohli, Joshua B Tenenbaum, and Vikash Mansinghka. 2015. Picture: A probabilistic programming language for scene perception. In Proceedings of the ieee conference on computer vision and pattern recognition . 4390–4399. [8] Tuan Anh Le, Atilim Gunes Baydin, and Frank Wood. 2016. Inference compilation and universal probabilistic programming. arXiv preprint arXiv:1610.09900 (2016). [9] Percy Liang, Michael I Jordan, and Dan Klein. 2010. Learning pro-grams: A hierarchical Bayesian approach. In Proceedings of the 27th International Conference on Machine Learning (ICML-10) . 639–646. [10] Chris Maddison and Daniel Tarlow. 2014. Structured generative mod-els of natural source code. In International Conference on Machine Learning . 649–657. [11] Vikash Mansinghka, Charles Kemp, Thomas Griffiths, and Joshua Tenenbaum. 2012. Structured priors for structure learning. arXiv preprint arXiv:1206.6852 (2012). [12] Vikash Mansinghka, Daniel Selsam, and Yura Perov. 2014. Ven-ture: a higher-order probabilistic programming platform with pro-grammable inference. arXiv preprint arXiv:1404.0099 (2014). [13] Vikash K Mansinghka, Tejas D Kulkarni, Yura N Perov, and Josh Tenenbaum. 2013. Approximate bayesian image interpretation using generative probabilistic graphics programs. In Advances in Neural In-formation Processing Systems . 1520–1528. [14] Paul Marjoram, John Molitor, Vincent Plagnol, and Simon Tavaré. 2003. Markov chain Monte Carlo without likelihoods. Proceedings of the National Academy of Sciences 100, 26 (2003), 15324–15328. [15] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Ef-ficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013). [16] Stephen H Muggleton, Ute Schmid, and Rishabh Singh. [n. d.]. Ap-proaches and Applications of Inductive Programming. ([n. d.]). [17] Brooks Paige and Frank Wood. 2014. A compilation target for probabilistic programming languages. arXiv preprint arXiv:1403.0504 

(2014). [18] Yura Perov. [n. d.]. AI1 project draft. 

hp://yuraperov.com/docs/ai/SolvingAI.pdf . [Online; accessed 01-August-2018]. [19] Yura Perov. [n. d.]. AI1 proposal draft. 

hp://yuraperov.com/docs/ai/AI1Proposal.pdf . [Online; accessed 01-August-2018]. [20] Yura Perov. [n. d.]. AI1 proposal draft (two additional pieces). 

hp://yuraperov.com/docs/ai/AI1Proposal_TwoAdditionalPieces.pdf .[Online; accessed 01-August-2018]. [21] Yura Perov. 2015. Applications of probabilistic programming . Ph.D. Dissertation. University of Oxford. [22] Yura Perov and Frank Wood. 2016. Automatic sampler discovery via probabilistic programming and approximate bayesian computation. In Artificial General Intelligence . Springer, 262–273. [23] Yura N Perov, Tuan Anh Le, and Frank Wood. 2015. Data-driven se-quential Monte Carlo in probabilistic programming. arXiv preprint arXiv:1512.04387 (2015). [24] Tom Rainforth. 2018. Nesting Probabilistic Programs. arXiv preprint arXiv:1803.06328 (2018). [25] David Tolpin, Jan-Willem van de Meent, and Frank Wood. 2015. Prob-abilistic programming in Anglican. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases . Springer, 308–311. [26] Frank Wood, Jan Willem Meent, and Vikash Mansinghka. 2014. Anew approach to probabilistic programming inference. In Artificial Intelligence and Statistics . 1024–1032. 3
