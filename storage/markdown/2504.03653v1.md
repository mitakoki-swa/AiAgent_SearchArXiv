Title: 

URL Source: http://arxiv.org/pdf/2504.03653v1

Published Time: Tue, 08 Apr 2025 00:00:28 GMT

Markdown Content:
# A Survey on Heterogeneous Computing Using SmartNICs and Emerging Data Processing Units (Expanded Preprint) 

# Nathan Tibbetts ∗

# , Sifat Ibtisum ∗† 

# , and Satish Puri Department of Computer Science, Missouri University of Science and Technology, Rolla, Missouri 3 March 2025 

Abstract 

The emergence of new, off-path smart network cards (SmartNICs), known generally as Data Processing Units (DPU), has opened a wide range of research opportunities. Of particular interest is the use of these and related devices in tandem with their host’s CPU, creating a heteroge-neous computing system with new properties and strengths to be explored, capable of accelerating a wide variety of workloads. This survey begins by providing background information to this new field, such as discussing its origins, its motivations and challenges, listing a few of the current market offerings for DPUs, and providing some brief information about the major programming languages and frameworks for using them. Then, we review and categorize a number of recent works in the field, covering a wide va-riety of studies, benchmarks, and application areas such as in data center infrastructure, commercial uses, and AI and ML acceleration. 

# 1 Introduction 

The computational hardware landscape is changing rapidly with the advent of new processors and networking equipment. In this survey, we focus on academic and industrial research on heterogeneous computing using an emerging class of network computing devices known as data processing units (DPU), in a way that is more accessible to those less familiar with networking devices, in contrast with expert-oriented surveys such as the recent work of Kfoury et al. [49]. As network interface cards (NIC) led to the development of smart network interface cards 

> ∗

Contributed equally. 

> †

Currently affiliated with Rochester Institute of Technology. 

1

> arXiv:2504.03653v1 [cs.DC] 3 Mar 2025

(SmartNIC), we see similar advancements in the field of SmartNICs. Recent developments in SmartNICs by companies like Nvidia, Amazon, Microsoft (fun-gible acquisition) and AMD (Pensando acquisition) have led to the new “DPU” nomenclature. A distinguishing aspect of a DPU is that it integrates a general-purpose CPU, capable of running an operating system, with DRAM and storage, into the high-speed NIC datapath of a SmartNIC. Thus, modern DPUs have a higher data throughput of 100 Gbps/400 Gbps compared to traditional SmartNICs, although the CPU cores in a DPU are generally less powerful than the host’s CPU cores. To provide an example, NVidia’s Mellanox Bluefield-2 is considered a DPU. Note that while we use the term ‘DPU’ to refer to a class of advanced SmartNICs, as described, these are also known by other names, such as “Dis-tributed Processing Unit” and “Infrastructure Processing Unit” (not to be con-fused with an “Intelligence Processing Unit”, which is a fundamentally different device); nomenclature is inconsistent, and depends upon the vendor and prod-uct specialization. Furthermore, the term ‘Data Processing Unit’ itself has become overloaded, as it has also been used to describe other devices, such as Processing-in-Memory (PIM) devices, which are not discussed in this work, and Deep-Learning Processing Units, which are not SmartNICs. DPU SmartNICs are typically used in place of a server’s standard Smart-NICs, which connect it to the network, and constitute an integral component of the conventional data center’s infrastructure resources, along with the CPU, memory, storage, and networking constituents. The physical hardware imple-mentation of a DPU can consist of either System-on-chip, FPGA, or hybrid architectures. The development of advanced SmartNICs, or DPUs, was started by large cloud service providers (“hyperscalers”) such as Amazon who realized that around 30% of processor cores were being utilized for infrastructure process-ing [90]. This motivated cloud vendors to develop advanced SmartNICs to offload infrastructure-related tasks related to networking, security, and stor-age from host CPUs, so that host CPU cores can focus on users’ application workloads. According to the projection of Dell’Oro Group, the traditional NIC market share will be overtaken by SmartNICs by 2027 [79]. There are many different reasons for developers and professionals to use SmartNICs, the most obvious of which is its intended use as a network infras-tructure support device. For example, one such use case is to perform network offload, where networking tasks, such as packet processing, are delegated to SmartNICs by host CPUs. Their utility extends beyond basic networking func-tions and is also capable of encompassing security offloading, which is achieved by accelerating cryptographic operations such as those common in a Secure Sockets Layer (SSL), Transport Layer Security (TLS), and encryption. Simi-larly, SmartNICs are used to offload storage-related operations, as well as to support virtualization tasks. However, data center infrastructure is not the sole application domain in which SmartNICs are applied, which is the main motivation of this paper. This 2survey focuses on works exploring the use of DPUs for in-network computing and applications acceleration, as well as infrastructure acceleration, though we highlight the usage of the DPU as a computational and communication acceler-ator for non-infrastructure-related applications. Within each of these domains, such accelerations are made possible primarily by two things: 1) acceleration and offload of computations and networking functions, and 2) isolation between host and DPU. Both these and the evolution of SmartNICs will be discussed in Section 2. Thereafter, the remainder of the survey will be organized as follows: Sec-tion 3 discusses the motivation behind the development of DPUs and the chal-lenges encountered in leveraging this emerging class of networking hardware; Section 4 presents the variety of DPU offerings by technology vendors; Section 5 presents works regarding programming tools and frameworks for developing ap-plication software targeted to DPUs; and Section 6 presents works regarding various studies, benchmarks, and applications of DPUs, ranging from infras-tructure to offloading commercial applications, including AI and ML. 

# 2 Background 

For those unfamiliar with SmartNICs, a certain amount of background infor-mation will be helpful in understanding the works presented later on. 

# 2.1 Evolution of NICs and DPUs 

Figure 1: The 2021 ServeTheHome NIC Continuum, a useful classification of existing NIC hardware, and which doubles as a representation of its evolution over time [47]. The NIC class distinctions shown in Figure 1 are not universally recognized, but nevertheless represent a useful categorization created by ServeTheHome (STH) [47], which we will use. This progression evolved quite naturally over time, beginning with companies such as Amazon designing in-house solutions (namely AWS Nitro) to meet a growing need for more secure and efficient system management and virtualization services. These pioneers of the field represented 3a reimagining of network and data center infrastructure, and others followed who built upon these concepts in search of more generalizable solutions [48, 97]. The ‘Foundational NIC’ shown in Figure 1 represents the fundamental level of a network interface. Nearly all contemporary NICs possess certain rudi-mentary offloads, such as IPv4/IPv6 and TCP/UDP checksum offloads. Foun-dational NICs are specifically engineered to facilitate low-cost network ports, eschewing many of the advanced offload features that contribute to increased cost of more sophisticated devices. Foundational NICs retain their significance within the industry, but are insufficient in many cases, particularly at higher data rates where the escalation in data processing necessitates enhanced com-putational capabilities. Consequently, the prevailing NICs are predominantly mid-tier ‘Offload NICs’, which generally provide networking speeds of about 100Gbps and above [47]. In order to design NICs with such speeds, it becomes necessary to offload more network support functions, generally in the form of on-board hardware accel-erators (ASICs, or application-specific integrated circuits) [47] - otherwise the host CPU would be quite busy trying to keep up. ‘SmartNICs’, however, go beyond simple offloads. These are NICs that pos-sess programmable pipelines, allowing systems and applications to tweak and configure offloads more flexibly. The primary purpose of a SmartNIC, as being part of a server device, is to reduce the burden imposed on the host CPU [47]. “Smart”, in the name, of course suggests a certain programmability; at the very least, an increased ability to handle complex tasks - both current, and being adaptable to future needs [87]. A ‘DPU’ differs from this paradigm, however, in that it can hardly be con-sidered part of the host server - it is, in and of itself, a mini network endpoint [47, 4], providing ‘off-path’ capabilities (we will define off-path later). DPUs rep-resent a more sophisticated iteration of SmartNICs. The concept encompasses the former functionality and extends beyond it, possessing not only ASIC of-fload capabilities and the flexible programmable pipelines of a SmartNIC, but also its own memory, storage, and CPUs. This design enables them to not only perform network tasks, but to participate in data processing tasks - further freeing up the host’s compute resources, all while consuming less power than the host [4]. While ServeTheHome further differentiates devices incorporating large Field-Programmable Gate Arrays (FPGA), we lump these in with DPUs for the purposes of this survey. 

# 2.2 Computational Acceleration and Offload 

DPUs are designed to address the increasing burden on CPUs in data centers caused by the massive overhead of infrastructure processing [20, 66]. A DPU, being ‘close to’ the network, of course boasts better network I/O processing performance compared to the host, making it ideal for accelerating network stack and packet processing, session management, distributed storage and file system client execution, erasure coding, network security layers, and even acting as a virtual switch [28, 58, 21]. DPUs have also been leveraged in network-edge 4scenarios for applications such as network awareness, power-efficient edge-to-cloud continuum, etc. [5, 63]. The goal of such offloads, of course, is to free up the host’s resources to focus on applications processing tasks and improve system-wide performance by reducing network and data center overhead [82]. And, while DPU acceleration typically refers to offloading these types of network and infrastructure tasks using on-board hardware accelerators (ASICs) [66, 63], this survey especially points out those works which instead show that one can offload generic computations and data manipulation to a DPU, including work in the applications layer - moving virtually any kind of processing from the host to the DPU’s CPUs. Some notable examples from the papers we review include big data processes, machine learning functions and inference, molecular dynamics, and various other forms of scientific computing. A simple illustration of what offloading to a DPU might look like is provided in Figure 2. Host Server Node 

> Memory

# Host CPU DPU 

> Memory

# CPU Work Result DMA 

Figure 2: Illustration of offloading work to DPU hardware within a compute node. Offload methods can take advantage of a variety of tools, such as Direct Memory Access (DMA), illustrated here. 

# 2.3 Data Path 

Table 1: Characteristics of on-path and off-path SmartNICs. 

Characteristic / Offload Ability On-Path Off-Path 

Communication Overhead Low High Operating System ✓

Low-Level Control ✓

ASIC Offloads ✓ ✓

App. Offloads & Co-Processing If CPU/FPGA ✓

When discussing ‘data paths’ in terms of advanced SmartNICs such as DPUs, the term may be more specific than the typical usage. In general a data path describes the steps through which data travels as it is moved, analyzed, trans-formed, and processed within a workflow. In a DPU, however, the data path usually traverses both the host and the DPU, and typically can be described 5one of two ways: ‘on-path’ or ‘off-path’, as shown in Table 1. Understanding the difference between the two is helpful in understanding the various use-cases of SmartNICs and DPUs. On-path SmartNICs have CPU cores that are situated on the communica-tion path of the network packets traveling to the host, and possess the ability to manipulate each incoming or outgoing packet rapidly, but too much ad-ditional processing assigned to these cores can bottleneck the communication path, because all network traffic must pass through them. Examples of on-path SmartNICs include LiquidIO II CN2350 and CN2360. By contrast, off-path SmartNICs (typically DPUs) have cores that are not in the direct line of traffic to the host. Their internal NIC can use customizable network forwarding rules to sort between packets that should be sent to the DPU cores first for pre-processing, and those that should be forwarded directly to the host instead. However, direct control over packets and memory will be limited, and have overheads - any off-path processing which occurs lengthens the data path, and thus time, for packets to make it to the host [81]. Examples of off-path SmartNICs include the Mellanox BlueField and the Broadcom Stingray [60]. In short, a ‘DPU’ is a class of device with certain advanced capabilities, which is usually an ‘off-path’ device, a term which describes both the physical network path within the device, and a paradigm for how the data flows through it. Many research projects take advantage the flexibility of off-path systems, analyzing the data path and data flow of a given application and experimenting with using a SmartNIC to optimize its efficiency, or save on data pre-processing steps. 

# 2.4 Isolation 

By DPU isolation, we are actually referring to two separate concepts, perfor-mance isolation and security isolation, both of which make these devices valuable in data center ecosystems. First, amid the many services, or tenants, which a server node often has to support, being able to separate, or isolate, many of the infrastructure tasks from the tenant or application workload, as well as separating those tenant workloads from each other, has the capacity to improve performance, not only because it redistributes compute resources, but also because it reduces interference between tasks; CPU context switching can have a high overhead cost, particularly due to cache pollution, and task segregation can help reduce that cost. This problem only worsens when you consider the myriad of additional ASIC resources over which there might be contention [30]. Secondly, a DPU can be used as an extra security layer. Because it func-tions as a separate device which can process network packets before forwarding them to the host, it provides an added layer of hardware isolation between the network and the host server, effectively isolating infrastructure and other func-tions performed on the SmartNIC from those performed on the host [93], which usually amounts to separating control plane operations from data plane oper-6ations, or infrastructure operations from tenant operations, or network access from user applications, thus further protecting the data center from nefarious tenants. And, with more security operations offloaded to ASICs, one could rea-sonably say that the more computation one does in silicon, instead of software, the more secure the system will be [89]. 

# 3 Motivation and Challenges 

Approximately 30% of the work done in a data center is in handling network infrastructure tasks (colloquially called an “infrastructure tax”), meaning that it requires almost half again as many servers to get the intended work done; this severely bloats the size and cost of data centers. DPUs are increasingly seen as a solution to this problem, and CPUs as insufficient [94]. Heterogeneous computing using emerging Data Processing Units (DPUs) is driven in particular by the necessity for specialized acceleration, enhanced performance, and energy efficiency. DPUs, which are specifically designed for particular tasks, optimize the effectiveness of the system by transferring special-ized workloads from the host CPU, providing a more energy-efficient solution for data center infrastructure workloads, which also improves cost-efficiency. However, since DPUs have a general-use processor that is also more energy ef-ficient [21], they are thus capable of accommodating a wide range of workloads and enhancing the overall utilization of resources. The adaptability and flexibil-ity of heterogeneous computing with DPUs is intended to address the evolving requirements of modern applications in this way [15]. The motivation behind the creation of these smart network devices, however, goes deeper. As the world turns to cloud technologies and flexibly hosted web services, there is an increasing need for software-defined networking (SDN) - sys-tems capable of not only specialized, but customizable networking configurations within data centers and supercomputers [12] - perhaps for both infrastructure and tenant applications alike. DPUs will be part of the future infrastructure powering services such as augmented reality, smart cities, edge computing, big-data analytics, and more [89]. A smarter NIC, of course, comes with a higher price point, and whether or not that cost comes out to be worth it is a matter of balance, requiring the consideration of many factors, for a given data center - not least of which, the potential for millions of dollars worth of electricity and capital savings for a large data center [21]. However, the desire for software-reconfigurable optimization within the data center may be king, given the directions in which technology is advancing - so, DPU adoption is expected to keep increasing [41]. 

# 3.1 Challenges 

Of course, DPUs and SmartNICs are not flawless solutions, with plenty of chal-lenges of their own, regarding their adoption and usage. Here are a few of the issues one will face when working with these devices [16]: 7• Computational Power: DPUs generally have energy-efficient proces-sors like ARM processors that are not as powerful as modern multi-core processors from Intel, AMD, and NVIDIA. The number of processing cores is also less than mainstream server devices. These design choices are mo-tivated by the need to build energy-efficient processors in DPUs, but they increase the difficulty of applying DPUs to some problem domains. 

• Memory: DPUs often have limited on-chip memory compared to general-purpose CPUs [96]. For instance, a Bluefield-2 DPU has 16 GB of main memory [67]. This forces developers and companies using them to carefully consider how data is managed and accessed, especially for workloads with large datasets. Since DPUs also have limited memory bandwidth, high-throughput workloads can also pose a challenge. Efficient data transfer between the DPU and system memory is crucial for preventing bottle-necks; to this end, some DPUs support direct memory access (DMA) between host and DPU. 

• Programming: Developing applications and configuring SmartNICs to take advantage of their capabilities can be challenging, requiring that de-velopers be familiar with the individual programming models and APIs of each specific SmartNIC they’re working with [9]. Furthermore, their troubleshooting capabilities are often quite limited [16]. 

• Network Overhead: Off-path devices, specifically, incur a higher net-work overhead, even if they have a higher overall capacity [81]. 

• Power and Cooling: Energy efficient as they might be, it is still difficult to add any kind of power-hungry device to pre-existing servers in a data center with power and cooling systems that were not designed for the extra load per-rack, requiring additional considerations for any would-be mass-adopters [41]. 

• Expertise: These devices are more complex to use, and will often require considerable device-specific hardware, IT, and administration expertise. 

• Absence of Standardization: As with any new technology, until reg-ular standards are developed (in the realms of both hardware and soft-ware), the variety of SmartNICs and DPUs is broadly heterogeneous [16], meaning that their usage represents a large learning curve with little-to-no flexibility between brands. 

• Financial Cost: SmartNICs and DPUs, being more complex than regular NICs, are of course going to be more costly. 

# 4 DPUs on the Market 

There are currently a variety of SmartNICs on the market which can be classified as DPUs; we review a few important players briefly, here, and in even more in 8Vendor Product Line Product Example Core Type                         

> NVIDIA (Mellanox) BlueField BlueField-2, 3 CPU Marvell OCTEON, ARMADA LiquidIO III, etc. CPU Intel IPU Mount Evans, etc. FPGA + CPU AMD (Pensando) Alveo, Elba, Capri (several) FPGA Broadcom Stingray (several) CPU Netronome Agilio CX (several) CPU Achronix Speedster7t FPGA (several) FPGA Fungible Fl F1 DPU, etc. CPU

Table 2: A few major players in the DPU market, both vendor, product line, and processor technology [3]. Table 4. 

NVIDIA BlueField : These DPUs are powerful, and provide an optimized infrastructure designed to accommodate a wide variety of workloads in environ-ments such as cloud computing and data centers, achieved through the offload-ing, accelerating, and segregating of a vast array of workloads. The BlueField-3 is pioneering line-rate packet processing at 400Gb/s, being the first to boast this capability [3]. 

Marvell OCTEON: These DPUs are designed with both 5G wireless in-frastructure and networking in mind, and are intended to optimize not only cloud workloads, but also machine learning inference on the edge, and other ap-plications. They’re even developing their own in-house ML engine to work with these devices. Such hardware-based (ASIC) ML/AI accelerators have enormous potential to accelerate AI algorithms, by as much as 100x over software-based implementations [3, 61]. 

Intel IPU: The IPU is part of Intel’s vision for end-to-end programmable networks, and their target customers are both cloud and communication service providers, and enterprises. IPUs incorporate a mixture of ASICs, Xeon CPUs, and FPGAs, and generally have similar goals to other such devices, such as reducing infrastructure overhead, and enhancing security and isolation for both data centers and tenants [14, 15]. 

AMD Alveo: These devices, developed by AMD, are based on FPGAs, making it possible to use them to hardware-accelerate even custom, compu-tationally demanding tasks such as machine learning inference, data analysis, video transcoding, and more, all while boasting up to 90x performance com-pared to a CPU. Given the swifter evolution of algorithms than chip design cycles, the hope is that devices such as this will be able to keep up with algo-rithmic changes flexibly. The Alveo is considered a pioneer in the respect that this makes it one of the first smartNICs capable of offloading literally any types of tasks, all under the same platform [3]. We also note, briefly, the importance of the AWS Nitro System and Azure’s Catapult within the evolution and space of DPU technologies [3, 97, 95, 36], but as they are somewhat more proprietary systems, we will consider them less relevant to the current research around using and improving DPUs. 9Table 4 provides a list of a few company whitepapers regarding DPU devices, which the reader may find helpful. 

# 5 Programming Languages and Frameworks 

The selection of programming languages, frameworks, and development tools used for SmartNICs varies based on the distinct requirements and functional-ities of the SmartNIC, and the requirements of the intended application. The decision regarding the programming languages and tools one will use is often directly influenced by the specific SmartNIC vendor and model; different Smart-NICs may possess distinct capacities, and vendors often furnish custom-made unique software development kits and tools that are meticulously tailored to their hardware platforms - and not others. Furthermore, the programmability attributes of SmartNICs, such as P4 support, are undergoing continual evolu-tion, which of course also impacts the selection. Table 3 shows a categorization of various publications based upon the soft-ware tools that they use, evaluate, or expand upon in relation to SmartNICs. 

Software Related Works 

DOCA [60, 86, 11, 96] DPDK / SPDK [24, 40, 73, 30, 59, 70, 19, 62, 78, 58, 91] MPI [72, 6, 43, 29, 86, 38, 37, 13, 82, 11] OpenMP [88, 103, 64] gRPC [46, 34] P4 [99, 100, 98] Table 3: Classification of publications based on software used (frameworks, libraries, and languages). Some simply make use of the software for their work on SmartNICs, while others evaluate it in SmartNIC applications, or go so far as to modify or expand upon the software for improved use with SmartNICs. Note: Many other works may use libraries we were unaware of, and so are not included. Although other works, such as Kfoury et al. [49], go more in-depth, we at least list a number of programming models and toolsets available for program-ming DPUs: 

• DOCA, and other platform-specific SDKs: DOCA is NVIDIA’s ex-tensive software development kit (SDK), and more, for BlueField DPUs, and contains virtually all of the important tools and software packages needed for developing applications to work with their hardware, both on the device and its host, aiming to facilitate and streamline setup and de-velopment. It is the go-to for working with NVIDIA DPUs, and is even intended to improve working with AI and ML on these devices [7, 68]. Other DPU brands often offer similar, extensive platform-specific toolsets, such as AMD Pensando’s SSDK [1], which are not discussed in this work. 10 • DPDK: Data Plane Development Kit (DPDK) can be used in conjunction with DPUs to optimize the performance of high-speed network data packet processing, by offloading network-related tasks from the CPU to DPUs or SmartNICs. This more efficient and rapid handling of network data packets helps achieve both lower latency and higher throughput. DPDK is used in applications such as edge computing, 5G networks, and data centers where speed and efficiency are paramount. 

• SDPK: The Storage Performance Development Kit (SPDK) is an open-source framework primarily designed for high-performance storage appli-cations in general, not necessarily for DPUs or SmartNICs. It focuses on optimizing storage operations, especially for SSD and NVMe devices. However, there is some potential overlap with DPUs. Since DPUs or SmartNICs are hardware accelerators that can handle various network and data processing tasks, in certain scenarios SPDK could be employed with them to enhance the performance of storage-related operations in a net-work context, particularly when dealing with high-speed data storage and retrieval. The integration and customization of SPDK for use with DPUs or SmartNICs would depend on specific application requirements [80]. 

• MPI: Libraries implementing the Message Passing Interface (MPI) pro-vide general-purpose parallelism without shared memory. MPI defines interfaces for inter-process communication. Communication in MPI are basically of two types - 1) peer-to-peer communication (sending and re-ceiving messages) and 2) collective communication (e.g., broadcasting a message). NVidia Bluefield DPUs have been used to accelerate MPI im-plementations for collective communication functions only [6, 72]. This is because collective communication functions like all-to-all personalized communication function require more computational work among MPI processes in order to do communication buffer management. These collec-tive communication functions often used to accelerate scientific applica-tions in High-Performance Computing (HPC) clusters. By leveraging the parallel processing capabilities of MPI and the offloading capabilities of DPUs, researchers can significantly enhance data processing, simulations, and scientific research in general in HPC, offering the potential for break-throughs in fields like molecular dynamics, genomics, climate modeling, astrophysics, etc [6]. 

• OpenMP and related models: OpenMP is another programming model widely used in parallel computing, but unlike MPI, OpenMP uses a shared memory paradigm. It allows for the decomposition of sequential programs into parallel components, using compiler directives [88]. Other models and libraries extend this concept of shared memory to work over a distributed system, such as PGAS models, OpenSHMEM, and OpenSNAPI [34]. 11 • gRPC: Google has developed an open-source Remote Procedure Call (RPC) framework known as gRPC, which boasts high performance capa-bilities across widely distributed systems. gRPC’s remote procdure calls work over standard network connections with a binary protocol, and are language-agnostic. This enables applications to establish seamless com-munication across diverse systems and languages through a clearly de-fined interface. While gRPC itself is not specifically tailored for DPUs, it can be deployed alongside them to enhance particular aspects of net-work communication and data handling. Essentially, it becomes another communication method by which one can offload responsibilities to the SmartNIC, thus decreasing CPU utilization. It is suitable for applications needing rapid data processing and low-latency communication, particu-larly in data centers and networking infrastructure. Nevertheless, using DPUs and gRPC together will require customized development and con-figuration in order to make full use of their capabilities [34]. 

• P4 : P4 is a low-level programming language tailored for packet proces-sors, enabling programmers to modify the packet processing routines of switches post-deployment, rendering them adaptable in real-world scenar-ios. P4 also enables switches to be independent of any specific network protocols. The language has been put forward as a strawman proposal for the future evolution of OpenFlow, a widely used SDN control protocol. P4 has been implemented and evaluated in various scenarios, including dynamic advanced Active Queue Management (AQM) schemes and cus-tomized flow tables. It has shown promising results in terms of feasibil-ity, performance improvement, and flexibility for network customization [99, 9]. 

• Hardware Description Languages: It is important to note also that FPGAs are typically programmed very differently from CPUs and ASICs, requiring various Hardware Description Languages (HDLs), such as Ver-ilog, VHDL, etc. [35], making the more exotic FPGA-centric smartNICs likely more difficult to program than general-purpose CPU-based DPUs, although some systems (such as Alveo Vitis HLS) provide abstractions to remove this barrier [2]. 

# 5.1 Extending Existing Tools 

Current NIC software development frameworks are typically optimized for ASIC NICs, and do not take into consideration the type of flexibility inherent in off-path SmartNICs’ ability to dynamically offload a broader variety of tasks than those defined by a few ASIC accelerators [98]. Some researchers are addressing this problem by adding onto the existing languages and frameworks, creating modifications or compilers specifically for DPU applications, or to re-implement standards-based frameworks, such as MPI, with SmartNICs in mind. 

MPI: In one such example, Sarkauskas et al. [72] redesign MPI’s non-blocking broadcast and all-gather collectives to make use of DPU offloading, 12 since these primitives are difficult to synchronize efficiently. They test their de-signs with microbenchmarks on BlueField DPUs, reducing the execution time of the osu ibcast microbenchmark by up to 54%, and the osu iallgather 

microbenchmark by up to 43%. Additionally, Mohammadreza et al. [6] pro-posed a framework named ‘BluesMPI’ that demonstrates a significant enhance-ment in the overall execution time of the OSU Micro Benchmark when utilising the MPI Ialltoall function, with improvements of up to 44%. The use of BluesMPI results in a notable enhancement in the overall execution duration of the P3DFFT application, with improvements of up to 30%. Turalija et al. [86] also intend to extend MPI more fully into the DPU realm with a library they are developing. Chakraborty et al [13] work towards improving MPI’s bulk-synchronous failure recovery for HPC applications, by improving upon the global-restart model, resulting in a 4x speedup compared to other solutions. 

OpenMP: Usman et al. [88] introduced support for using OpenMP to treat DPUs as network co-processors for offloading, presenting a design in LLVM that supports OpenMP’s standard offloading semantics. Their proposed approach offers productivity advantages for programmers working with DPUs, compared to standard OpenMP. Similarly, the work of Zhao et al. [103] is towards flexi-bly allocating network resources for OpenMP, using load prediction and fuzzy control, and it very effectively improves resource utilization. Furthermore, ad-ditional compiler tools, such as OpenMP Advisor [64], may also be used to improve OpenMP’s usefulness within the realm of heterogeneous architectures; although it currently only enables code offloading to GPUs (using OpenMP and machine learning), it may be extended to other devices. 

P4: Expanding P4 has also been explored. Jiarong Xing et al. [98] have cre-ated a new optimization engine for the P4 compiler which prepares its programs to dynamically rearrange network pipelines during runtime. Their experiments evidenced various successful optimizations in througput or latency, with im-provements ranging from small percentages up to 49% in one case, with only a small overhead for managing the live optimization software. Their experi-ments were performed on Bluefield-2 and Agilio CX DPUs, as well as a software SmartNIC emulator. These few examples alone bespeak enormous room for future work in software-level optimization for SmartNICs, and they are just the tip of the iceberg. 

# 5.2 Creating New Tools 

A few works appear to create new libraries or APIs. Of course, there are a number of other works producing a variety of code bases for more specific things, so we will only mention a few, more general communication libraries here. Some time ago, Hoefler et al. introduced ‘sPIN’ [33], a portable packet-processing acceleration programming model that offloads packet processing func-tions to network cards and enabling the acceleration of application and system services, including for redundant in-memory file systems. This acceleration is analogous to compute acceleration using CUDA or OpenCL. They followed more recently by introducing ‘PsPIN’ [26], an open-source implementation of the sPIN 13 model for doing computations in the NIC. Their experiments with PsPIN boast 400 Gbit/s packet-processing speeds, with only 26 ns latencies for 64 B packets, and low power consumption. Suresh et al. [82] present additional API primitives for point-to-point and group communication patterns, for offloading any type of communication pat-tern to a DPU, in an attempt to combat degraded latency, bottle-necking, and limitations on communication patterns present in other state-of-the-art ap-proaches, such as MPI and OpenSHMEM. They test their work with Alltoall micro-benchmarks, as well as a P3DFFT application. Other libraries of note, which are detailed in later sections, include Accel-Net, AlNiCo, IO-TCP, PEDAL, INEC, FairNIC, iPipe, and Runway, among others [24, 53, 51, 54, 77, 30, 60, 70]. 

# 6 Application Areas and Benchmarking of DPUs 

In this section, we categorize the main body of papers we’ve reviewed. Initially we identify those which are broad studies, or which perform benchmarks of DPUs, then continue on with works applying SmartNICs to various applications, by which we attempt to group them. Finally we have separated out those works connecting these devices with various forms of machine learning (ML) and artificial intelligence (AI) applications, as an area of interest. As DPUs continue to evolve, their application areas and benchmarks, and the focal points of the research community, will play a pivotal role in shaping the landscape of high-performance computing and data processing. 

# 6.1 Broad Studies and Benchmarks 

Benchmarking these devices is essential for assessing their performance and efficiency in different applications. Such benchmarks may measure factors such as CPU or network throughput, latency, and energy efficiency, providing insights into the DPU’s capabilities under various workloads. These benchmarks help in optimizing hardware and software configurations for specific use cases, ensuring they deliver optimal performance in their target applications. While a few of the following works are broad studies, and others are in-depth general device benchmarks, other works only measure their specific contribution’s effect; since most works do this to some extent, we will identify only a few contribution-specific works of note here. Table 4 includes a mixture of selections of both. Note that, due to the relatively small number of works performing thorough benchmarks, we have not specified which SmartNIC type each work is using, but instead have categorized by benchmark topic. 

Broad Studies and Advice: In [81], Sun et al. use deeper evaluations of compute power and communication overhead, performed on a BlueField, to recommend four experiment-supported pieces of advice for using off-path SmartNICs to improve performance: 1. Use the built-in ASIC accelerators for their intended offloads 14 Table 4: Works performing noteworthy studies, or benchmarks of DPU hard-ware, organized by benchmarking criteria. Works in the far right column simply measure the effects of their own specific contribution.                      

> Topic or Benchmark Category General Contribution-Specific
> Works Creating Benchmark Suites [63, 91] Non-Benchmark Studies [49, 65, 19, 12, 29, 5, 24, 62, 75, 96, 34] Whitepapers & Specs [21, 11, 67, 36, 61, 96, 14, 1] Compute Power [81, 59, 60] [40] Communication (Throughput, Latency) [49, 81, 92, 59, 60, 43, 24, 11] [73, 43, 98, 51, 25, 72, 40, 30, 74, 53, 26] Memory Access (Local, Distributed) [81, 59, 60] [102] Storage (Local, Distributed) [59, 11] [92, 104, 77, 45, 17, 18, 50, 73, 33, 26] Compression [49, 58, 55, 56, 59] [54, 70, 17, 18] Other ASIC Accelerators [81, 59] Applications, System Services [11] [32, 43, 8, 60, 85, 45, 57, 83, 46, 13] Engergy Savings [21] [69, 105, 100, 84] Scalability [43, 23, 32, 31, 27, 37, 38]

2. In choosing other things to offload, focus on high-latency tasks 3. Treat the SmartNIC as a separate, independent endpoint, but one with additional resources for the server host to use 4. Do not simply use the same system design as you would for an on-path SmartNIC, as it often won’t perform as well as an on-path device in those areas. Finally, they discuss possible refutations of performance gains shown by a few other papers, demonstrating the difficulty of creating any effective offloads for off-path SmartNICs. Xingda Wei et al. [92] also perform a comprehensive study of the Bluefield-2, focusing on file system and key-value store (KV-store) com-munication paths, and using multiple paths concurrently, to discover substantial performance improvements. They then give advice on finding such potential op-timizations, which they, too, have validated through their studies. Jianshen Liu et al. [59] also evaluate the capabilities of the network and computing aspects of the NVIDIA BlueField-2, and showed that while the device is flexible, it is easy to bottleneck and overwhelm it. They recommend a selection of operations for offloading, which this DPU performs particularly well at. 

A Big-Picture View on Cloud Architecture: Some years back, Caulfield et al. [12] published an invited work which is not so much a broad study or benchmark, as it is an analysis of the current state of the research regarding fully programmable networks. They provide an in-depth discussion towards ap-plying FPGA SmartNICs to create a more flexible cloud infrastructure, going beyond the use of SmartNICs for offloading specific tasks, and discussing the architecting of highly dynamic data center systems to meet the growing needs for abstractions to power infrastructure, applications, and developers alike. We find this discussion interesting, as it appears to reflect how network hardware and software have developed since its publication; for example, lacking reference to proper DPUs, which now possess some of the abstractions they call for. The reader may find its big-picture view enlightening. In a more recent review, Ryan E. Grant et al. [29] discuss the design of SmartNICs and how they can be used to offload current runtime software, as 15 well as enable future in-network runtime systems. D¨ oring et al. [19] discuss the demand for these devices, as well as their creation and use in attempting to meet the growing needs surrounding Software-Defined Networking (SDN). They highlight how SmartNICs can be used to tackle the lack of network flexibility, making virtualized networks more feasible. The paper also aims to provide a universal definition for the term “SmartNIC”. Luca Barsellotti et al. [5] go on to discuss three different use cases for DPUs at the edge: network monitoring for 5G networks, power efficiency in edge-to-cloud, and added network security. Horany et al. [34] describe a recommended model for offloading and allocating SmartNIC resources across a network, using gRPC, among other tools. On a related note, Justine Sherry [75] makes an argument that instead of considering DPUs only for compute acceleration and offload, these devices can develop to become the overall data movement controller (DMC) for a compute node with a multi-core CPU and other accelerators like GPUs. This is a new way to see the role of emerging DPUs, when compared to traditionally making the host responsible for both computations and orchestrating data movement. The paper presents challenges and open questions in realizing this vision in the data center environment. Finally, recent detailed surveys can be very instructive, such as those pre-sented by Kfoury et al. [49] for general information about SmartNICs, and Nickel et al. [65] for information on in-network computing with SmartNICs. 

Compression Studies: Recently, there have been a number of works study-ing, more deeply, the benefits of using DPUs in order to accelerate data com-pression tasks, utilizing the compression related hardware building blocks in NVidia’s Bluefield SmartNICs. One particularly relevant work is that of Yuke Li et al. [55, 56], who first examine the performance of lossy and lossless compres-sion on NVIDIA BlueField DPUs (particularly SZ3, DEFLATE, and zlib) using seven real-world datasets, underlining the potential of offloading compression tasks from host CPUs to DPUs to enhance the performance in data-intensive applications. Later, they move to accelerate such compression tasks, which we will describe in Section 6.2, along with a number of other works on compression. 

Creating New Benchmark Suites: And finally, where there are those using benchmarks, there must also be those creating useful new benchmarks. Wang et al. [91] proposed ‘DPUBench’ an application-driven benchmark suite, covering three common categories of DPU applications: network, storage, and security. The suite includes a scalable framework for benchmarking how well DPUs handle tasks relating to those categories. Similarly, Michalowicz et al. [63] created a micro-benchmark suite, also called ‘DPU-Bench’, for helping HPC researchers to determine how many processes can be offloaded to a DPU without degrading performance, given various factors. 

# 6.2 DPUs in Commercial and Infrastructure Applications 

The commercial application of DPUs has become increasingly prevalent in mod-ern computing environments. Thus, much of the current research around DPUs is in using them to improve the infrastructure itself of the data centers they 16 belong to, since their ability to accelerate network communication and offload tasks relating to packet processing, security, infrastructure processing, storage, virtualization, cloud computing etc. can provide an important advantage to the companies that use them. As these are the primary familiar and intended applications of SmartNICs, these areas of research tend to show many positive results, as described in the following papers. However, as the line is often very blurred between these and more unconven-tional applications of SmartNICs, in this section we will also present those works which experiment with using SmartNICs for various other kinds of offloads and co-processing tasks, reaching far into the realm of applications as well as in-frastructure. The flexibility of DPUs has led to their adoption in a surprising range of domains, beyond simply accelerating data center infrastructure and security; while they are quickly becoming crucial in that realm, their surpris-ing application to unexpected problem domains has also skyrocketed. From blockchain technologies to molecular dynamics simulations [43, 86], they are be-ing treated as co-processors, data management and analytics devices, machine learning accelerators, and more. Table 5 gives an attempted breakdown of the many overlapping topics within this category. Table 5: Works using SmartNICs in various commercial applications and net-work or data center infrastructure, organized by subdomain or topic. Works in each of our tables may appear in multiple categories.                    

> Subdomain or Offload References
> Cloud Architecture [12, 65, 85, 5, 75, 101, 21, 36, 61, 29, 11, 34] Network Communication [53, 82, 40, 43, 86, 24, 73, 85, 26, 63, 60, 59, 32, 31, 27, 25, 33] Message Proc., TCP/IP [51, 102, 45] Pipelines, P4 [98, 99, 74] Content Delivery [51] Hardware Implementation, FPGAs [102, 53, 24, 10, 74] (See also Section 6.3) Distributed Storage [102, 63, 28] File Systems [50, 92, 104] Compression [102, 17, 18, 54, 55, 56, 58, 40, 70] Databases & KV-Stores [92, 17, 18, 77, 30, 84, 83] Big-Data Processes [57, 60] 5G Networks [99, 8, 5] Virtualization, Containers [30, 52, 24, 74, 23, 66] Security [23, 5, 62, 63, 36] Kernel [40] Scientific HPC [43, 86, 58, 87, 63, 70, 54, 37, 38, 46, 13] Blockchain [42, 39] AI, ML (See Table 6)

Network Offloading and Load-Balancing Frameworks: One of the earlier pioneers in the area of SmartNIC packet-processing offloading is the work of Antoine Kaufmann et al. [45], a direct memory access (DMA) interface called ‘FlexNIC’. Since then, other works have continued to forge ahead. Ming Liu et al. [60] presented ‘iPipe’, an “actor-based” framework to aid in offloading varied, distributed applications to SmartNICs in a way that maximally uses the SmartNICs’ cores, despite differences in task execution costs. Evaluations of 17 iPipe show significant core savings (up to 3.1 decent Intel cores in a 10Gbps scenario) and latency reductions by 23.0 μs when offloading to SmartNICs for data analytics, transaction systems, and KV-stores. However, [81] questions the efficacy of iPipe’s design for off-path SmartNICs. J. Ravi et al. [70] introduced the ‘Runway’ framework, which uses an object storage abstraction to allow for computation on data while it’s in-transit, and offloading compression to a heterogeneous device. This framework is specifically designed to be adaptable in order to execute user-defined functions during runtime. They perform tests on both CPUs, DPUs, and GPUs. Finally, Fuhrer et al. [25] use on-NIC reinforcement learning for network congestion control, but we will discuss this work in Section 6.3. A potential way to use SmartNICs to improve disk and network I/O op-erations is by rearchitecting the TCP stack itself, as done in [51] by Taehyun Kim et al. They present ‘IO-TCP’, which improves content delivery by cleverly splitting its stack of operations between the host CPU and the SmartNIC. Since offloading the whole stack to the SmartNIC would leave data inaccessible to the CPU, and the SmartNIC isn’t fast enough to compete for some operations, they differentiate between “control plane” operations to be handled by the CPU, and “data plane” operations to be managed by the SmartNIC. This reduces cache pollution between operations with large amounts of disk I/O. Their work is tested on the Bluefield-2 DPU, and demonstrates approximately double the throughput in cases with high CPU congestion, even compared to systems using far more CPU cores. We note that many of the works below also bear domain-specific improve-ments in networking and communication efficacy, but we have chosen to cate-gorize them by their particular application domain where sensible, rather than by their application to the very broad category of “networking”. 

Hardware and FPGA-Based; Network, Packets, & Transactions: 

Some works, such as that of Jie Zhang et al. in [102], aim to improve the de-sign of SmartNICs themselves. Their efforts demonstrate ‘SmartDS’, an FPGA prototype of a SmartNIC with the ability to split message headers and bodies between the host and SmartDS, improving throughput for distributed storage systems, with potential for accelerating and reducing cloud storage infrastruc-ture by offloading compression, reducing middle-tier servers required for dis-agregated block storage, etc. They test their prototype in the middle-tier of a distributed system, and show up to 4.3x throughput compared to CPU-based middle-tier management, and also reducing the average latency by 2.6x. Simi-larly, Li et al. develop ‘AlNiCo’ [53], an on-NIC network transaction scheduling system which aims to offload the scheduling of transaction requests and re-sponses to specific host CPUs, in a manner which reduces both overhead and resource contention, and which adapts to to feedback during dynamic work-loads. Their implementation involves both hardware and software solutions, using an FPGA-based SmartNIC to program their own hardware acceleration which they feed feature vectors comprised of request information. Their solu-tion also involves RDMA (Remote Direct Memory Access), and the develop-ment of their own “scheduling-enabled” RPC (Remote Procedure Call). They 18 show improvements to both throughput and latency. Also, Shan Yizhou et al. [74] fill a niche by presenting the first SmartNIC that is both multi-tenant for virtualization needs, and programmable but hardware-based, called ‘Super-NIC’. They focused on optimizing execution of entire connected groups (flows or DAGs-Directed acyclic Graphs) of network tasks, and included various opti-mization techniques resulting in low overhead and high throughput compared to the baseline. Furthermore, the researchers proposed a fair sharing mechanism for hardware resources in their SuperNIC. As for traffic analysis, Siracusano et al. [27] use a hardware-implemented binary neural network, described in sec-tion 6.3), to speed things up by two orders of magnitude. Finally, Brunella et al.’s ‘Hyperion’ [10], described below, is a more direct example of modifying DPU hardware, in pursuit of better distributed storage systems. In a slightly older work, Daniel et al. [24] introduced Azure’s ‘AccelNet’, a comprehensive framework comprising a hardware/software codesign model. Furthermore, they presented performance outcomes on significant workloads, along with their acquired experiences and valuable insights pertaining to the development and deployment of AccelNet on FPGA-based Azure SmartNICs. Notably, the experimental findings indicate a considerable reduction in latency, from 50 μs to 17 μs , as well as a substantial increase in network capacity of up to 32Gbps for that time period. 

Distributed File Systems: Others have also been able to achieve substan-tial speedups by focusing on distributed file systems (DFS). LineFS, a persistent memory (PM) distributed file system (DFS), was first presented by Jongyul Kim et al. in [50], a paper describing its design, implementation, and evalua-tion. LineFS resides within the SmartNIC itself to offload DFS operations and reduce CPU contention via in-network processing, while still taking advantage of new client-local persistent memory (PM) to create speedups by preferring client-local behavior. They test their work on a Mellanox Bluefield and com-pare it to Assise (a current PM DFS), showing improvement in DFS availability, latency of LevelDB operations by up to 80%, and throughput in Filebench by as much as 79%. However, [81] notes a limited range of situations in which LineFS improves performance. Xingda Wei et al. [92] perform case studies using both LineFS (an on-SmartNIC DFS) and DrTM-KV (an RDMA-based disagregated KV-store), focusing on the management of such systems, and the idea of us-ing multiple communication paths concurrently, achieving up to 30% and 25% improvements, respectively, using a Bluefield-2 DPU. Gootzen et al. [28] propose ‘DPFS’, which represents a decoupling of the host from its file system and using virtio-fs to virtualize it onto the DPU. The host’s computation is freed up significantly, with little latency cost for tenants. Kan Zhong et al. likewise proposed ‘DPC’ [104], a high-performance, DPU-accelerated, distributed/standalone file system client, designed to offload com-putationally intensive tasks from host CPUs to DPUs. DPC optimizes file oper-ations through ‘nvme-fs’, an enhanced NVMe protocol that enables low-latency and high-performance interactions. Additionally, they introduce ‘KVFS’, a key-value-based file system that replaces underutilized local disks with disaggregated storage to provide efficient standalone file services. Experimental results show 19 significant improvements, including host-DPU transmission latency as low as 20 μs, over 80% CPU usage savings in high-concurrency scenarios, and a 90% CPU usage reduction in distributed file systems while maintaining high perfor-mance. The concept of a DPU has also grown in an interesting direction with Brunella et al.’s ‘Hyperion’, which replaces host CPU-based devices as the pri-mary manager of distributed NVMe storage [10], thus reimagining typical ar-chitectures to eliminate the need for many CPU server devices. 

Compression: Yuke Li et al. [54]. They introduced a library ‘PEDAL’ that optimizes data compression designs utilizing the DPU’s hardware capabilities. They also highlight the challenges of existing compression methods due to high computational demands, and present a co-design with the MPICH MPI library which achieves a speedup up to 101x in compression time, and reducing the communication latency by up to 88x, compared to not using ASICs. Lastly, they focused on enhancing the data compression in communication-oriented HPC scenarios. Chen Ding et al. [17, 18] also choose to offload compression to the DPU with D 2Comp, in order to accelerate LSM-tree compaction; Ravi et al. [70] do so with Runway for in-transit computation; and Zhang et al. [102] with SmartDS (all detailed in other parts of this survey). Others, such as Jianshen [58], of course make use of the compression ASICs as well. 

KV-Stores, Databases, & Big Data Systems: In [57], Jiaxin Lin et al. focus their efforts on the shuffle processes (map-reduce, etc.) of data-intensive applications, and more generally on dynamically partially offloading operations requiring intermediate data exchange between compute nodes. They present ‘SmartShuffle’ for coordinating offload to maximize usage of both the CPU and SmartNIC simultaneously. They attach it to Spark and test it on a Broadcom Stingray, achieving up to 40% faster performance on TPC-H benchmarks than both Spark alone and Spark RDMA. Somewhat related, Henry N. Schuh et al. [73] introduce ‘Xenic’, a system that employs an asynchronous, aggregated exe-cution paradigm with adaptable point-to-point communication between Smart-NICs, improving both both network and core efficiency in performing sharded data store transactions. Their experimental findings demonstrate that Xenic achieves notable improvements on three different benchmarks, at least doubling throughput and significantly reducing latency. However, [81] questions Xenic’s use in off-path Smart-NICs. As for more specific aspects of such systems, Chen Ding et al. [17, 18] proposed another improvement, ‘DComp’ and then ‘D ²Comp’, wherein they in-tegrate DPU-offloaded LSM-tree compaction with RocksDB. This approach was evaluated using NVIDIA’s Bluefield-2 DPU, the results of which demonstrated the effectiveness of the DPU solution, accelerating compaction performance by up to 4x, write and read throughput by up to 3.2x and 1.4x respectively, and boasting a reduction in host CPU contention. Also, Haiyang Shi et al. [77] proposed a set of coherent in-network erasure coding EC primitives, named ‘INEC’ (building on their previous work named ‘TriEC’ [76]), to more easily allow offloading of EC to SmartNICs despite the variety of schemes EC follows. The researchers conducted experiments and demonstrated that NIC implemen-20 tations of INEC primitives, for various EC schemes, with a KV store co-designed with INEC, results in a substantial improvement at the 50th, 95th, and 99th percentiles in end-to-end throughput, write performance, and degraded read performance, achieving enhancements of up to 99.57%, 47.30%, and 49.55% re-spectively Works belonging to other sections, such as [92], may also deal with KV-stores. Lastly, when dealing with network data flows through SmartNICs, Jianshen Liu et al. [58] recommend the use of Apache Arrow at the foundation, because its data format can save data transformation time. The authors share their experience in adjusting a partitioning algorithm for particle data to work with Apache Arrow. 

5G Network Infrastructure: Yan yan et al. [99] introduced a SmartNIC solution that is P4-enabled and FPGA-based. This solution was specifically developed to cater to the networking requirements of web-scale cloud and the 5G / beyond-5G era. The authors demonstrated the application of this solution in a 5G environment, with a particular focus on network slicing from edge data center to core data center. The experimental outcomes showed that the system can achieve a throughput of up to 84.8 Gbps, even using only a single CPU core. Additionally, it was observed that the incorporation of hardware-offloaded segment routing with P4 can achieve up to 30% higher bandwidth. Later, Justine Cris Borromeo et al. [8] add a 5G DU Low-PHY layer to an FPGA SmartNIC using the OpenCL framework. They demonstrate that incorporating these 5G functionalities into a SmartNIC as an offload, as opposed to other current solutions for a 5G network, can result in notable reductions in both processing time and power consumption. 

Multiple Tenants, Virtualization, and SDN: Applications running on SmartNICs are currently limited regarding tenants, because they require a level of mutual performance isolation which is unsupported by current software [30]; when the available resources include ASIC hardware accelerators in addition to the CPU, this complicates potential sharing mechanisms. ‘FairNIC’, created by Grant et al. [30], provides this isolation between tenants, enabling multiple applications to co-habitate with fair resource sharing on a single NIC without impacting each other’s performance, and demonstrating that sharing SmartNICs among virtual tenants is feasible, though it will require appropriate security mechanisms in the future. We also note in this section Shan Yizhou et al.’s [74] work on SuperNIC, also towards multi-tenancy and virtualization, which we have described previously. Partially related, Njavro et al. [66] apply DPU offloading to container overlay networks to decrease congestion in the host. In an older work, Yanfang et al. [52] put forward an architecture, named UNO, to provide Software-Defined Networking (SDN) controlled network func-tion offloading through a virtual management plane, using multiple switches within the host, without interfering with data-center-wide data- and control-planes. The experimental findings reduce control-plane overhead, and demon-strate a potential reduction in power consumption by a factor of 2. 

Security: Mounir El Kiraf et al. [23] conducted a study in which they in-vestigated the potential of DPUs as a scalable and transparent security solution. 21 This solution incorporates Intrusion Detection Systems / Intrusion Prevention Systems (IDS/IPS) and supports multiple VPN connections. The study pri-marily focused on two aspects: first, the ability of the DPU to process, filter, and route all data to and from the operating system, thus reducing its workload and enhancing security; second, the scalability of the solution. The experimen-tal results demonstrated the BlueField-1 DPU effectively ensures a transparent and scalable security solution in terms of VPN connections. Also, Sebastiano Miano et al. [62] directs the reader through the process of using a SmartNIC to offload mitigation of Distributed Denial of Service (DDoS) attacks, taking advantage of on-NIC hardware filtering. 

Kernel: Houxiang Ji et al. [40] proposed STYX, a framework that offloads memory optimization features from the Linux kernel to SmartNICs. It reduces the disruption of application execution, improving data center efficiency. STYX utilizes the SmartNIC’s RDMA capability to copy memory regions, and its compute capability to perform intensive operations, resulting in a significant decrease (55-89%) in 99th-percentile application latency[40]. 

HPC & Scientific Applications: DPUs have also been experimented with in the realm of high-performance computing (HPC) and scientific simulations - in particular, for tasks which require heavy communication patterns, such as halo exchanges, as found in molecular dynamics (MD) and particle simulations. In one such example, Sara Karamati et al. [43, 44] evaluated the benefits of using NVIDIA BlueField-2 DPUs as compute accelerators for MiniMD molec-ular dynamics simulations (proxy for LAMMPS). The BlueField-2 provided up to 20% speedup, with no real loss of simulation accuracy. However, perhaps of most interest with this work, they demonstrate the value of re-thinking current algorithms, highlighting the necessity of rearranging some algorithmic patterns to better take advantage of parallelism and DPUs’ strengths and communication patterns, and provide valuable advice thereon. Additionally, Matea Turalija et al. [86] also proposed DPUs for innovative methods of MD simulation acceler-ation, inspired by the Anton supercomputer because of its specially designed ASICS, and how DPUs also allow access to networking ASICs, which can be used to accelerate MD simulations in similar ways. This opens the door for other systems to potentially reach the same microsecond timescale needed for many real applications. In a similar vein, Ulmer et al. [87] extend data services onto SmartNICs in order to leverage resource isolation, also for the benefit of HPC applications, such as particle simulations. The paper addresses how to construct software for implementing services on SmartNICs, and seeks to learn how useful they will be. Experimental results from a 100-node cluster using BlueField-2s indicate utility in data management tasks, though they are of course less performant than their hosts. Likewise, Liu et al. [58], described previously, perform experiments regarding particle data. Finally, in the realm of geospatial computing, Kaymak et al. [46] experiment with offloading the ‘filter’ step in spatial join operations, or polygon intersection queries, leaving the heavier ‘refine’ step computations to the host devices. They use a cluster containing a number of BlueField DPUs for their experiment, and 22 show that the offload provides a performance improvement. 

Blockchain: While the use of blockchain technology has become widespread across various industries, it is widely acknowledged that blockchains have the po-tential to have a significant environmental impact due to their high energy con-sumption and hardware requirements [42]. As a partial solution, Eish Kapoor et al. [42], building on their previous work [39], presented a blockchain infras-tructure that runs completely on SmartNICs, called ‘BlockNIC’. Their findings point toward a somewhat more ecologically conscious way of building blockchain systems into future, under-utilized infrastructure [39]. 

DPU Management: Wang et al. [101] introduced a firmware upgrade sys-tem based on WAN to facilitate the secure over-the-air upgrading of DPUs, re-ducing operation and maintenance expenses. Empirical findings have indicated a nearly perfect success rate for DPU upgrades, alongside exceptional perfor-mance, in their particular application of simulating a Marine Engine Room. 

# 6.3 DPUs in AI and ML Applications 

Due to the current importance of artificial intelligence (AI) and machine learning (ML) in both research and industry, it is of particular interest that we note how DPUs have been used in conjunction with these technologies. Often, this means taking advantage of FPGAs to create hardware implementations of neural network architectures or operations, which can sometimes boast over two orders of magnitude faster inference [25, 27, 105], though it can also simply be done via computational offloading. See Table 6 for a breakdown of the types of neural networks etc. the research is exploring. Table 6: Works relating to using DPUs for AI & ML, organized by subtopic or neural network type. 

AI/ML Topic References 

DPUs + AI / ML [96, 84, 31, 83] Deep Learning [105, 38, 37, 40] Convolutional NN (Non-SmartNIC: [71, 106, 22]) Binary NN [27] Graph NN [31] Reinforcement Learning [25] Federated Learning [78] Computer Vision [69, 84] In-Hardware / FPGA Impl. [105, 31, 25], (Non-SmartNIC: [71, 106, 22]) DPUs + GPUs [85, 96, 70, 32, 100] 

Making Deep Learning (DL) Scalable: Anqi Guo et al. [31] proposed a framework called ‘FCsN’ for performing neural network inference on FPGA-based SmartNICs, aiming in general to improve the performance of HPC and data center processing. Their experimental results, tested on both DNNs and Graph Neural Networks (GNN), boast up to 10x speedups compared with a 23 baseline of using MPI on CPUs. Later, Anqi Guo et al. [32] continue their work, proposing a heterogeneous SmartNIC system with both hardware and software co-designed, for distributed deep learning recommendation models (DLRMs). Their intent is to resolve the all-to-all communication bottleneck these models suffer from, and to increase the scalability of these models. Their approach en-hances locality and computational efficiency, achieving a 2.1 × inference latency speedup, and a 1.6 × training throughput speedup. Both these works measure how their contributions affect the scalability of such systems. Omar Zawawi [100] identifies a “data stall” issue, where extensive data pre-processing at the CPU during DL training causes host devices to struggle to keep up with the training GPUs, causing the whole pipeline to stall and GPUs to spend time idle. He proposes offloading some of this pre-processing to the DPU in order to help mitigate the problem, as well as to improve energy ef-ficiency within the data center by nature of the lighter-weight DPU energy requirements. On a similar note, Maroun Tork et al. [85] presented ‘Lynx’, a new network server architecture for offloading both data and control planes of network tasks to SmartNICs. The design is accelerator-centric, capable of run-ning without CPU help by enabling more direct networking to the GPUs with lightweight I/O mechanisms. This allows for network servers to be hardware-accelerated, and attains greater than 4x throughput for their example GPU task of face-verification, though only 25% increase for an accelerated inference task, compared to host-centric architectures. However, [81] questions the efficacy of running Lynx on a SmartNIC versus the host. 

AI/ML for Network Traffic Management : Benjamin Fuhrer et al. [25] introduce a lightweight congestion control solution for datacenters using a rein-forcement learning algorithm, RL-CC, which is converted into decision trees to achieve a 500x reduction in inference time. Deployment on NVIDIA ConnectX-6Dx SmartNICs demonstrated superior performance in managing bandwidth, latency, and packet loss in a balanced way, compared to existing algorithms like DCQCN and Swift across various benchmarks. On a related note, Hardware-based, on-NIC Binary Neural Networks (BNNs) are introduced to the data plane of FPGA SmartNICs by Siracusano et al. [27], in order to do classification and analysis of network traffic at a line rate of 40Gbps using machine learning. This hardware-accelerated neural network drastically improves latency compared to software implementations of similar networks (by up to two orders of magni-tude), and appears to achieve decent accuracy, while freeing up the host’s CPU for other work. Their work is proof of the value of researching hardware imple-mentations of neural networks. As for software implementations, Kasim Tasdemir et al. [83] investigate deploying Machine Learning (ML) algorithms on BlueField-3 DPUs for SQL injection detection. They test 20 different ML models on an SQL dataset, achieving near-real-time detection with a Passive Aggressive Classifier, with an accuracy of 99.78%. 

Co-Compute Offloading: Arpan Jain [37, 38] conducted a study to in-vestigate the potential benefits of using the ARM cores of BlueField-2 DPUs to expedite the training of DNN models by offloading phases of the training 24 to them; their study may represent the first such attempt. MPI is employed despite the heterogeneity of devices, and the researchers’ approach ultimately demonstrates up to 17.5% improvement in training duration on cutting-edge HPC clusters. They try several designs, and test them on multiple DL model types. They also measure scalability. Likewise, Shibahara et al. [78] point out the heavy network processing load for Federated Learning schemes, which require aggregating local weights from distributed learners and then sending global weights back out. They make use of the DPU’s cores to offload this aggregation, with a 1.39x speedup. In another offloading effort, Marina Perea-Trigo et al. [69] conducted a study to investigate the potential advantages of employing a DPU in order to mitigate the burden experienced by a continuously operating server. They proceeded to present the findings of their research, which involved quantifying the extent to which a CCTV system for weapon detection can alleviate workload, and evaluating its performance across various scenarios. As a result, they observed a remarkable reduction in workload of 43,123% over a 24-hour period, with savings exceeding 98% during nighttime intervals [69]. Also, Tootaghaj et al. [84] proposed a new architecture, ‘Spike-Offload’, that strategically offloads workload spikes from microservices (important for edge ML workloads), when the SmartNIC has available compute power, reducing service level agreement (SLA) violations and vaunting better performance, lower energy consumption, and 40% potential reduction in capital expenditure. 

Hardware Convolutional Neural Networks (CNN): Three works of note dealing with FPGA-based CNNs are those of Saidi et al., Jiang Zhu et al., and Zelin Du et al. [71, 106, 22]. However, it must be noted in our current con-text that these experiments, though effective, and relevant because their work may be somewhat transferrable to SmartNICs, were not performed on actual SmartNICs. Rather, these used ”Deep-Learning Processing Units” (which are not network cards), or a type of Xilinx FPGAs which are often included in SmartNICs. 

Optical-Electronic Technologies: Zhong et al. [105] proposed ‘Light-ning’, a pioneer in reconfigurable SmartNICs with photonic computing cores capable of doing DNN inference in real-time, a feat that outpaces NVIDIA A100 GPUs by over 300x, while using 300x less power, with similarly strong results for A100X DPUs and Brainwave SmartNICs. Their hardware-based approach avoids creating bottlenecks in packet processing and data movement. Their work demonstrates reasonably accurate photonic computation operations. 

# 7 Overview Comments 

Over the course of our research for this survey, a couple of points have become clear, so that we are able to draw a few broad conclusions from this body of work. First, this is a very active field of research, with a large number of perfor-mance improvements being achievable with SmartNICs, in almost any type of 25 work offloading. However, we note that many of these potential solutions are aimed at eating up the spare processing power and other resources of the DPU, meaning that they are not simultaneously applicable. Whether they are or are not, we recommend that the members of the research community be attentive to means of compatibly combining their methods with others’, in order to maximize the potential impact of their work and broaden its scope beyond their particu-lar use-cases. Where methods are simultaneously applicable, such collaboration has the potential to result in impressively compounding improvements. Second, it is clear from the research that the most impressive speedups and performance improvements shown are usually those which come from taking advantage of hardware acceleration; be it by properly making use of the on-board ASICs, by specialized FPGA programming, or by the advent of photonic computation, these continue to be the results which most often make orders of magnitude of difference, compared to current solutions. We recommend search-ing for every possible opportunity to apply the built-in accelerators, at least, to your target task. Other offloading is, of course, still worthwhile, and likely energy-saving, though it is likely capped by the computational capacity of the DPU, and the effects of reducing cache pollution and CPU congestion. 

# 8 Conclusion 

Much research is currently being done in exploring DPUs and SmartNICs as co-processors and accelerators, rather than simply as data center infrastructure offload and management devices, and this research is yielding favorable results. Therefore, interest in these heterogeneous computing systems has even spread to diverse fields outside of computer science and IT, such as the natural sciences and engineering, among others. New frameworks are being developed to provide the abstractions needed to bring these devices into wider usage, making them increasingly useful and effective. Despite this progress, several obstacles persist, but we believe that future research will yield resolutions to these challenges, and the niche for DPUs will continue to both develop and evolve. 

# 9 Acknowledgment 

This work was supported by National Science Foundation CAREER Grant num-ber 2344578. 

# References 

[1] Amd pensando software-in-silicon development kit (ssdk). https://ww w.amd.com/content/dam/amd/en/documents/pensando-technical-d ocs/product-briefs/pensando-ssdk-product-brief.pdf , 11 2023. AMD Pensando. 26 [2] Amd vitis hls. https://www.amd.com/en/products/software/adapt ive-socs-and-fpgas/vitis/vitis-hls.html , 2024. Advanced Micro Devices, Inc. [3] Asterfusion . The most comprehensive dpu/smartnic vendors with its product line summary. https://cloudswit.ch/blogs/the-most-com plete-dpu-smartnic-vendors-with-its-product-line-summary/ , 7 2022. [Accessed: 2025-02-06]. [4] Asterfusion data technologies . What is smartnic?difference of dpu and smartnic? https://medium.com/@Asterfusion/what-is-sma rtnic-difference-of-dpu-and-smartnic-7928c93ecf7f , 10 2023. [Accessed: 2025-02-06]. [5] Barsellotti, L., Alhamed, F., Vegas Olmos, J. J., Paolucci, F., Castoldi, P., and Cugini, F. Introducing data processing units (dpu) at the edge [invited]. In 2022 International Conference on Computer Communications and Networks (ICCCN) (2022), pp. 1–6. [6] Bayatpour, M., Sarkauskas, N., Subramoni, H., Maq-bool Hashmi, J., and Panda, D. K. Bluesmpi: Efficient mpi non-blocking alltoall offloading designs on modern bluefield smart nics. In 

High Performance Computing: 36th International Conference, ISC High Performance 2021, Virtual Event, June 24 – July 2, 2021, Proceedings 

(Berlin, Heidelberg, 2021), Springer-Verlag, p. 18–37. [7] Betz, J. Demystifying nvidia doca. https://developer.nvidia.com/b log/demystifying-doca/ , 9 2021. [Accessed: 03-11-2023]. [8] Borromeo, J. C., Kondepu, K., Andriolli, N., and Valcarenghi, L. Fpga-accelerated smartnic for supporting 5g virtualized radio access network. Computer Networks 210 (2022), 108931. [9] Bosshart, P., Daly, D., Izzard, M., McKeown, N., Rexford, J., Talayco, D., Vahdat, A., Varghese, G., and Walker, D. P4: Pro-gramming protocol-independent packet processors. CoRR abs/1312.1719 

(2013). [10] Brunella, M. S., Bonola, M., and Trivedi, A. K. Hyperion: Acase for unified, self-hosting, zero-cpu data-processing units (dpus). ArXiv abs/2205.08882 (2022). [11] Burstein, I. Nvidia data center processing unit (dpu) architecture. In 

2021 IEEE Hot Chips 33 Symposium (HCS) (2021), pp. 1–20. [12] Caulfield, A., Costa, P., and Ghobadi, M. Beyond smartnics: To-wards a fully programmable cloud: Invited paper. In 2018 IEEE 19th International Conference on High Performance Switching and Routing (HPSR) (2018), Institute of Electrical and Electronics Engineers Inc., pp. 1–6. 27 [13] Chakraborty, S., Laguna, I., Emani, M., Mohror, K., Panda, D. K., Schulz, M., and Subramoni, H. Ereinit: Scalable and efficient fault-tolerance for bulk-synchronous mpi applications. Concurrency and Computation: Practice and Experience (2020). e4863 cpe.4863. [14] Corporation, I. Intel unveils infrastructure processing unit roadmap; 2nd generation to ship in 2022. https://download.intel.com/newsr oom/2022/corporate/vision/Intel-IPU-Roadmap-Fact-Sheet.pdf ,May, 2022. [15] Creating a new intel ipu-based computing platform for optimal cloud plat-form management and cost-effectiveness powering infrastructure to help shape the data center of the future overview. https://www.intel.com/ content/www/us/en/products/details/network-io/ipu.html . Intel Corporation. [16] Detko, A. SmartNIC–what it is and what are the types? - CodiLime, 12 2021. [Accessed: 2025-02-06]. [17] Ding, C., Zhou, J., Lu, K., Li, S., Xiong, Y., Wan, J., and Zhan, L. D2comp: Efficient offload of lsm-tree compaction with data processing units on disaggregated storage. ACM Trans. Archit. Code Optim. 21 , 3 (Sept. 2024). [18] Ding, C., Zhou, J., Wan, J., Xiong, Y., Li, S., Chen, S., Liu, H., Tang, L., Zhan, L., Lu, K., and Xu, P. Dcomp: Efficient offload of lsm-tree compaction with data processing units. In Proceedings of the 52nd International Conference on Parallel Processing (New York, NY, USA, 2023), ICPP ’23, Association for Computing Machinery, p. 233–243. [19] D¨ oring, T., Stubbe, H., and Holzinger, K. SmartNICs: Cur-rent trends in research and industry. In Proceedings of the Seminar Innovative Internet Technologies and Mobile Communications (IITM), Winter Semester 2020/2021 (Munich, Germany, May 2021), G. Carle, S. G¨ unther, and B. Jaeger, Eds., vol. NET-2021-05-1 of Network Archi-tectures and Services (NET) , Chair of Network Architectures and Ser-vices, Department of Computer Science, Technical University of Munich, pp. 19–23. [20] Dpu-based acceleration. https://www.vmware.com/topics/glossary /content/dpu-based-acceleration.html . Broadcom. [Accessed: Dec. 2023]. [21] Dpu power efficiency. https://resources.nvidia.com/en-us-acceler ated-networking-resource-library/nvidia-dpu-power-efficienc y-white-paper , 10 2023. [22] Du, Z., Zhang, W., Zhou, Z., Shao, Z., and Ju, L. Accelerating dnn inference with heterogeneous multi-dpu engines. In 2023 60th ACM/IEEE Design Automation Conference (DAC) (2023), pp. 1–6. 28 [23] El Kirafi, M., Rahimi, I., and Both, C. Dpu implementation of a scalable and transparent security solution for numerous vpn connections. Tech. rep., University of Amsterdam, 4 2021. [24] Firestone, D., Putnam, A., Mundkur, S., Chiou, D., Dabagh, A., Andrewartha, M., Angepat, H., Bhanu, V., Caulfield, A., Chung, E., et al. Azure accelerated networking: SmartNICs in the public cloud. In 15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18) (2018), pp. 51–66. [25] Fuhrer, B., Shpigelman, Y., Tessler, C., Mannor, S., Chechik, G., Zahavi, E., and Dalal, G. Implementing reinforcement learn-ing datacenter congestion control in nvidia nics. In 2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid) (2023), pp. 331–343. [26] Girolamo, S. D., Kurth, A., Calotoiu, A., Benz, T. E., Schnei-der, T., Ber´ anek, J., Benini, L., and Hoefler, T. Pspin: Ahigh-performance low-power architecture for flexible in-network compute. 

ArXiv abs/2010.03536 (2020). [27] Giuseppe, S., Galea, S., Sanvito, D., Malekzadeh, M., Antichi, G., Costa, P., Haddadi, H., and Bifulco, R. Re-architecting traffic analysis with neural network interface cards. In 19th USENIX Symposium on Networked Systems Design and Implementation (2022), pp. 513–533. [28] Gootzen, P.-J., Pfefferle, J., Stoica, R., and Trivedi, A. Dpfs: Dpu-powered file system virtualization. In Proceedings of the 16th ACM International Conference on Systems and Storage (New York, NY, USA, 2023), SYSTOR ’23, Association for Computing Machinery, p. 1–7. [29] Grant, R. E., Schonbein, W., and Levy, S. Radd runtimes: Radical and different distributed runtimes with smartnics. In 2020 IEEE/ACM Fourth Annual Workshop on Emerging Parallel and Distributed Runtime Systems and Middleware (IPDRM) (2020), pp. 17–24. [30] Grant, S., Yelam, A., Bland, M., and Snoeren, A. C. Smart-nic performance isolation with fairnic: Programmable networking for the cloud. In Proceedings of the Annual Conference of the ACM Special In-terest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication (New York, NY, USA, 2020), SIGCOMM ’20, Association for Computing Machinery, p. 681–693. [31] Guo, A., Geng, T., Zhang, Y., Haghi, P., Wu, C., Tan, C., Lin, Y., Li, A., and Herbordt, M. A framework for neural network infer-ence on fpga-centric smartnics. In 2022 32nd International Conference on Field-Programmable Logic and Applications (FPL) (2022), pp. 01–08. 29 [32] Guo, A., Hao, Y., Wu, C., Haghi, P., Pan, Z., Si, M., Tao, D., Li, A., Herbordt, M., and Geng, T. Software-hardware co-design of heterogeneous smartnic system for recommendation models inference and training. In Proceedings of the 37th ACM International Conference on Supercomputing (New York, NY, USA, 2023), ICS ’23, Association for Computing Machinery, p. 336–347. [33] Hoefler, T., Girolamo, S. D., Taranov, K., Grant, R. E., and Brightwell, R. spin: High-performance streaming processing in the network. In SC17: International Conference for High Performance Com-puting, Networking, Storage and Analysis (2017), pp. 1–16. [34] Horany, M., and Margolin, A. Remote programmability model for smartnics in HPC workloads. In OpenSHMEM and Related Technologies. OpenSHMEM in the Era of Exascale and Smart Networks (2021), S. W. Poole, O. R. Hernandez, M. B. Baker, and T. Curtis, Eds., vol. 13159 of 

Lecture Notes in Computer Science , Springer, pp. 178–186. [35] How to program your first fpga device. https://www.intel.com/cont ent/www/us/en/developer/articles/training/how-to-program-you r-first-fpga-device.html , 3 2017. Intel. [36] Introducing the security design of the aws nitro system whitepaper. ht tps://aws.amazon.com/blogs/security/introducing-the-securit y-design-of-the-aws-nitro-system-whitepaper/ , oct 2023. AWS Security Blog. [37] Jain, A., Alnaasan, N., Shafi, A., Subramoni, H., and Panda, D. K. Accelerating cpu-based distributed dnn training on modern hpc clusters using bluefield-2 dpus. In 2021 IEEE Symposium on High-Performance Interconnects (HOTI) (2021), pp. 17–24. [38] Jain, A., Alnaasan, N., Shafi, A., Subramoni, H., and Panda, D. K. Optimizing distributed dnn training using cpus and bluefield-2 dpus. In IEEE Micro (2022), vol. 42, IEEE, pp. 53–60. [39] Jampani, Gavin; Kapoor, E., and Ponnala, V. Smartnic compatible blockchain. Tech. rep., Santa Clara University, 2023. [Accessed: 06-10-2023]. [40] Ji, H., Mansi, M., Sun, Y., Yuan, Y., Huang, J., Kuper, R., Swift, M. M., and Kim, N. S. STYX: Exploiting SmartNIC capa-bility to reduce datacenter memory tax. In 2023 USENIX Annual Tech-nical Conference (USENIX ATC 23) (Boston, MA, July 2023), USENIX Association, pp. 619–633. [41] Judge, P. The dpu dilemma: life beyond smartnics. https://www.data centerdynamics.com/en/analysis/the-dpu-dilemma-life-beyond-s martnics/ , 1 2022. Data Centre Dynamics Ltd. (DCD). 30 [42] Kapoor, E., Jampani, G., and Choi, S. Blocknic: Smartnic assisted blockchain. In 2023 Silicon Valley Cybersecurity Conference (SVCC) 

(2023), pp. 1–8. [43] Karamati, S., Hughes, C., Hemmert, K., Grant, R. E., Schon-bein, W., Levy, S., Conte, T. M., Young, J., and Vuduc, R. W. 

“smarter” nics for faster molecular dynamics: a case study. In 2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS) 

(Los Alamitos, CA, USA, jun 2022), IEEE Computer Society, pp. 583–594. [44] Karamati, S., Young, J., Conte, T., Hemmert, K. S., Grant, R., Hughes, C., and Vuduc, R. Computational offload with bluefield smart nics. Tech. rep., Sandia National Laboratories, 10 2021. [45] Kaufmann, A., Peter, S., Sharma, N. K., Anderson, T. E., and Krishnamurthy, A. High performance packet processing with flexnic. 

Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems (2016). [46] Kaymak, D., and Puri, S. Geospatial Filter and Refine Computations on Nvidia Bluefield Data Processing Units. In The International Confer-ence for High Performance Computing, Networking, Storage, and Analysis 

(2023), p. 3. [47] Kennedy, P. DPU vs SmartNIC and the STH NIC Continuum Frame-work. https://www.servethehome.com/dpu-vs-smartnic-sth-nic -continuum-framework-for-discussing-nic-types/ . [Accessed: 2025-02-06]. [48] Kennedy, P. What is a dpu a data processing unit quick primer. https: //www.servethehome.com/what-is-a-dpu-a-data-processing-uni t-quick-primer/ , 9 2020. ServeTheHome. [49] Kfoury, E. F., Choueiri, S., Mazloum, A., AlSabeh, A., Gomez, J., and Crichigno, J. A comprehensive survey on smartnics: Architec-tures, development models, applications, and research directions. IEEE Access 12 (2024), 107297–107336. [50] Kim, J., Jang, I., Reda, W., Im, J., Canini, M., Kosti´ c, D., Kwon, Y., Peter, S., and Witchel, E. Linefs: Efficient smartnic offload of a distributed file system with pipeline parallelism. In SOSP 2021 -Proceedings of the 28th ACM Symposium on Operating Systems Principles 

(10 2021), Association for Computing Machinery, Inc, pp. 756–771. [51] Kim, T., Ng, D. M., Gong, J., Kwon, Y., and Yu, M. Rearchitecting the tcp stack for i/o-offloaded content delivery. In Proceedings of the 20th USENIX Symposium on Networked Systems Design and Implementation 

(2023). 31 [52] Le, Y., Chang, H., Mukherjee, S., Wang, L., Akella, A., Swift, M. M., and Lakshman, T. V. Uno: Unifying host and smart nic offload for flexible packet processing. In Proceedings of the 2017 Symposium on Cloud Computing (New York, NY, USA, 2017), SoCC ’17, Association for Computing Machinery, p. 506–519. [53] Li, J., Lu, Y., Wang, Q., Lin, J., Yang, Z., and Shu, J. Al-nico: Smartnic-accelerated contention-aware request scheduling for trans-action processing. In 2022 USENIX Annual Technical Conference (2022), pp. 951–966. [54] Li, Y., Kashyap, A., Chen, W., Guo, Y., and Lu, X. Accelerating lossy and lossless compression on emerging bluefield dpu architectures. In 

2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS) (2024), pp. 373–385. [55] Li, Y., Kashyap, A., Guo, Y., and Lu, X. Characterizing lossy and lossless compression on emerging bluefield dpu architectures. In 2023 IEEE Symposium on High-Performance Interconnects (HOTI) (2023), pp. 33–40. [56] Li, Y., Kashyap, A., Guo, Y., and Lu, X. Compression analysis for bluefield-2/-3 data processing units: Lossy and lossless perspectives. 

IEEE Micro 44 , 2 (Dec. 2023), 8–19. [57] Lin, J., Ji, T., Hao, X., Cha, H., Le, Y., Yu, X., and Akella, A. Towards accelerating data intensive application’s shuffle process us-ing smartnics. Proceedings of the ACM on Measurement and Analysis of Computing Systems 7 (5 2023). [58] Liu, J., Maltzahn, C., Curry, M. L., and Ulmer, C. Process-ing particle data flows with smartnics. In 2022 IEEE High Performance Extreme Computing Conference (HPEC) (2022), pp. 1–8. [59] Liu, J., Maltzahn, C., Ulmer, C. D., and Curry, M. L. Perfor-mance characteristics of the bluefield-2 smartnic. Tech. rep., Sandia Na-tional Lab. (SNL-CA), Livermore, CA (United States); Sandia National Lab. (SNL-NM), Albuquerque, NM (United States), 5 2021. [60] Liu, M., Cui, T., Schuh, H., Krishnamurthy, A., Peter, S., and Gupta, K. Offloading distributed applications onto smartnics using ipipe. In Proceedings of the ACM Special Interest Group on Data Communication 

(New York, NY, USA, 2019), SIGCOMM ’19, Association for Computing Machinery, p. 318–333. [61] Marvell liquidio iii. https://www.marvell.com/content/dam/marvell/ en/public-collateral/embedded-processors/marvell-liquidio-I II-solutions-brief.pdf , 9 2020. Marvell. 32 [62] Miano, S., Doriguzzi-Corin, R., Risso, F., Siracusa, D., and Sommese, R. Introducing smartnics in server-based data plane process-ing: The ddos mitigation use case. IEEE Access 7 (2019), 107161–107170. [63] Michalowicz, B., Kandadi Suresh, K., Subramoni, H., Panda, D., and Poole, S. Dpu-bench: A micro-benchmark suite to measure offload efficiency of smartnics. In Practice and Experience in Advanced Research Computing (New York, NY, USA, 2023), PEARC ’23, Association for Computing Machinery, p. 94–101. [64] Mishra, A., Malik, A. M., Lin, M., and Chapman, B. Openmp advisor, 2023. [65] Nickel, M., and G¨ ohringer, D. A survey on architectures, hard-ware acceleration and challenges for in-network computing. ACM Trans. Reconfigurable Technol. Syst. 18 , 1 (Dec. 2024). [66] Njavro, A., Tau, J. F., Groves, T., Wright, N. J., and West, R. 

A dpu solution for container overlay networks. arXiv.org (2022). [67] Nvidia bluefield-2 dpu data center infrastructure on a chip. https://re sources.nvidia.com/en-us-accelerated-networking-resource-lib rary/bluefield-2-dpu-datasheet , 11 2021. [68] Ozery, I., and Ciccone, S. Transform the data center for the ai era with nvidia dpus and nvidia doca. https://developer.nvidia.com/b log/transform-the-data-center-for-the-ai-era-with-nvidia-d pus-and-nvidia-doca/ , 3 2023. [Accessed: 11-12-2024]. [69] Perea-Trigo, M., L´ opez-Ortiz, E. J., Salazar-Gonz´ alez, J. L., ´Alvarez Garc´ ıa, J. A., and Vegas Olmos, J. J. Data processing unit for energy saving in computer vision: Weapon detection use case. 

Electronics 12 , 1 (2023). [70] Ravi, J., Byna, S., and Becchi, M. Runway: In-transit data compres-sion on heterogeneous hpc systems. In 2023 IEEE/ACM 23rd Interna-tional Symposium on Cluster, Cloud and Internet Computing (CCGrid) 

(2023), pp. 229–239. [71] Saidi, A., Othman, S. B., Dhouibi, M., and Saoud, S. B. Cnn in-ference acceleration on limited resources fpga platforms epilepsy detection case study. Int J Inf & Commun Technol 12 , 3 (2023), 251–260. [72] Sarkauskas, N., Bayatpour, M., Tran, T., Ramesh, B., Subra-moni, H., and Panda, D. K. Large-message nonblocking mpi iallgather and mpi ibcast offload via bluefield-2 dpu. 2021 IEEE 28th International Conference on High Performance Computing, Data, and Analytics (HiPC) 

(2021), 388–393. 33 [73] Schuh, H. N., Liang, W., Liu, M., Nelson, J., and Krishna-murthy, A. Xenic: Smartnic-accelerated distributed transactions. In 

Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles (New York, NY, USA, 2021), SOSP ’21, Association for Com-puting Machinery, p. 740–755. [74] Shan, Y., Lin, W., Kosta, R., Krishnamurthy, A., and Zhang, Y. 

Supernic: A hardware-based, programmable, and multi-tenant smartnic, 2021. [75] Sherry, J. The i/o driven server: From smartnics to data movement controllers. ACM SIGCOMM Computer Communication Review 53 , 3 (2024), 9–17. [76] Shi, H., and Lu, X. Triec: tripartite graph based erasure coding nic offload. In Proceedings of the International Conference for High Perfor-mance Computing, Networking, Storage and Analysis (New York, NY, USA, 2019), SC ’19, Association for Computing Machinery. [77] Shi, H., and Lu, X. Inec: Fast and coherent in-network erasure cod-ing. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis (2020), pp. 1–17. [78] Shibahara, N., Koibuchi, M., and Matsutani, H. Performance im-provement of federated learning server using smart nic. 2023 Eleventh In-ternational Symposium on Computing and Networking Workshops (CAN-DARW) (2023), 165–171. [79] Smart nic market to approach $2 billion by 2027, according to dell’oro group. https://www.delloro.com/news/smart-nic-market-to-appro ach-2-billion-by-2027/ . Dell’Oro. [Accessed: 2025-02-06]. [80] Storage Performance Development Kit . SPDK Documentation: Event Framework , Nov 2023. [81] Sun, S., Huang, C., Zhang, R., Chen, L., Huang, Y., Yan, M., and Wu, J. A comprehensive study on optimizing systems with data processing units, 1 2023. [82] Suresh, K. K., Michalowicz, B., Ramesh, B., Contini, N., Yao, J., Xu, S., Shafi, A., Subramoni, H., and Panda, D. A novel framework for efficient offloading of communication operations to bluefield smartnics. In 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS) (2023). [83] Tasdemir, K., Khan, R., Siddiqui, F., Sezer, S., Kurugollu, F., and Bolat, A. An investigation of machine learning algorithms for high-bandwidth SQL injection detection utilising bluefield-3 DPU tech-nology. In 36th IEEE International System-on-Chip Conference, SOCC 

34 2023, Santa Clara, CA, USA, September 5-8, 2023 (2023), J. Becker, A. Marshall, T. Harbaum, A. Ganguly, F. Siddiqui, and K. McLaughlin, Eds., IEEE, pp. 1–6. [84] Tootaghaj, D. Z., Mercian, A., Adarsh, V., Sharifian, M., and Sharma, P. Smartnics at edge for transient compute elasticity. In 

Proceedings of the 3rd International Workshop on Distributed Machine Learning (New York, NY, USA, 2022), DistributedML ’22, Association for Computing Machinery, p. 9–15. [85] Tork, M., Maudlej, L., and Silberstein, M. Lynx: A smartnic-driven accelerator-centric architecture for network servers. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (New York, NY, USA, 2020), ASPLOS ’20, Association for Computing Machinery, p. 117–131. [86] Turalija, M., Petrovi´ c, M., and Kovaˇ ci´ c, B. Towards general-purpose long-timescale molecular dynamics simulation on exascale super-computers with data processing units. In 2022 45th Jubilee International Convention on Information, Communication and Electronic Technology (MIPRO) (2022), pp. 300–306. [87] Ulmer, C., Liu, J., Maltzahn, C., and Curry, M. L. Extending composable data services into smartnics. In 2023 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW) 

(2023), pp. 953–959. [88] Usman, M., Iserte, S., Ferrer, R., and Pe˜ na, A. J. Dpu offloading programming with the openmp api. In Proceedings of the SC ’23 Work-shops of The International Conference on High Performance Computing, Network, Storage, and Analysis (New York, NY, USA, 2023), SC-W ’23, Association for Computing Machinery. [89] VMware . Exploring the advantages of deploying dpus in the data center. 

https://www.cio.com/article/401773/exploring-the-advantage s-of-deploying-dpus-in-the-data-center.html , 6 2022. [Accessed: 2025-02-06]. [90] Vural, S. What is a smartNIC and how is the technology shaping modern data centres?, 12 2023. [Accessed: 2025-02-06]. [91] Wang, Z., Wang, C., and Wang, L. Dpubench: An application-driven scalable benchmark suite for comprehensive dpu evaluation. BenchCouncil Transactions on Benchmarks, Standards and Evaluations (2023), 100120. [92] Wei, X., Chen, R., Yang, Y., Chen, R., and Chen, H. Charac-terizing off-path smartnic for accelerating distributed systems. In 17th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2023, Boston, MA, USA, July 10-12, 2023 (2023), R. Geambasu and E. Nightingale, Eds., USENIX Association, pp. 987–1004. 35 [93] What is a data processing unit (dpu)? https://www.supermicro.com/e n/glossary/data-processing-unit , 2024. Super Micro Computer, Inc. [94] What is a smartnic? www.xelera.io/post/what-is-a-smartnic , 2023. [Accessed: 2024-12-09]. [95] What is aws nitro. https://www.virtana.com/glossary/what-is-aws -nitro/ , 2023. Virtana. [Accessed: 2024-12-09]. [96] Wheeler, B. Dpu-based hardware acceleration: A software perspective. Tech. rep., The Linley Group, Oct. 2023. [97] Why aws nitro system? https://aws.amazon.com/ec2/nitro/ , 2024. Amazon Web Services, Inc. [98] Xing, J., Qiu, Y., Hsu, K. F., Sui, S., Manaa, K., Shabtai, O., Piasetzky, Y., Kadosh, M., Krishnamurthy, A., Ng, T. S., and Chen, A. Unleashing smartnic packet processing performance in p4. In 

SIGCOMM 2023 - Proceedings of the ACM SIGCOMM 2023 Conference 

(9 2023), Association for Computing Machinery, Inc, pp. 1028–1042. [99] Yan, Y., Beldachi, A. F., Nejabati, R., and Simeonidou, D. P4-enabled smart nic: Enabling sliceable and service-driven optical data cen-tres. Journal of Lightwave Technology 38 , 9 (2020), 2688–2694. [100] Zawawi, O. Resource-efficient data pre-processing for deep learning. ht tps://repository.kaust.edu.sa/handle/10754/692272 , 4 2023. [101] Zeng, H., Liu, H., Zhang, J., Sun, M., and Wang, T. Design of remote upgrade system for data processing unit in marine engine room simulator. Applied Sciences (2022). [102] Zhang, J., Huang, H., Zhu, L., Ma, S., Rong, D., Hou, Y., Sun, M., Gu, C., Cheng, P., Shi, C., and Wang, Z. Smartds: Middle-tier-centric smartnic enabling application-aware message split for disag-gregated block storage. In Proceedings - International Symposium on Computer Architecture (6 2023), Institute of Electrical and Electronics Engineers Inc., pp. 592–604. [103] Zhao, J., Gao, X., and Li, Y. Research on elastic extension of multi type resources for openmp program. In 2022 IEEE 24th Int Conf on High Performance Computing & Communications; 8th Int Conf on Data Science & Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys) (2022), pp. 971–978. [104] Zhong, K., Yu, Z., Li, Q., Luo, X., Long, L., Tan, Y., Ren, A., and Liu, D. Dpc: Dpu-accelerated high-performance file system client. In Proceedings of the 53rd International Conference on Parallel Process-ing (New York, NY, USA, 2024), ICPP ’24, Association for Computing Machinery, p. 63–72. 36 [105] Zhong, Z., Yang, M., Lang, J., Williams, C., Kronman, L., Sludds, A., Esfahanizadeh, H., Englund, D., and Ghobadi, M. 

Lightning: A reconfigurable photonic-electronic smartnic for fast and energy-efficient inference. In Proceedings of the ACM SIGCOMM 2023 Conference (New York, NY, USA, 2023), ACM SIGCOMM ’23, Associa-tion for Computing Machinery, p. 452–472. [106] Zhu, J., Wang, L., Liu, H., Tian, S., Deng, Q., and Li, J. An efficient task assignment framework to accelerate dpu-based convolutional neural network inference on fpgas. IEEE Access 8 (2020), 83224–83237. 37
