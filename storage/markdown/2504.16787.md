Title: Credible Plan-Driven RAG Method for Multi-Hop Question Answering

URL Source: http://arxiv.org/pdf/2504.16787v2

Published Time: Mon, 04 Aug 2025 00:36:44 GMT

Markdown Content:
# CREDIBLE PLAN -D RIVEN RAG M ETHOD FOR MULTI -H OP 

# QUESTION ANSWERING 

Ningning Zhang 1, Chi Zhang 1, Zhizhong Tan 1, Xingxing Yang 1, Weiping Deng 1, and Wenyong Wang ∗11Macau University of Science and Technology August 4, 2025 

## ABSTRACT 

Multi-hop question answering (QA) presents significant challenges for retrieval-augmented generation (RAG), particularly in decomposing complex queries into reliable reasoning paths and managing error propagation. Existing RAG methods often suffer from deviations in reasoning paths and cumulative errors in intermediate steps, reducing the fidelity of the final answer. To address these limitations, we propose PAR-RAG (Plan-then-Act-and-Review RAG), a novel framework inspired by the PDCA (Plan-Do-Check-Act) cycle, to enhance both the accuracy and factual consistency in multi-hop question answering. Specifically, PAR-RAG selects exemplars matched by the semantic complexity of the current question to guide complexity-aware top-down planning, resulting in more precise and coherent multi-step reasoning trajectories. This design mitigates reasoning drift and reduces the risk of suboptimal path convergence, a common issue in existing RAG approaches. Furthermore, a dual-verification mechanism evaluates and corrects intermediate errors, ensuring that the reasoning process remains factually grounded. Experimental results on various QA benchmarks demonstrate that PAR-RAG outperforms existing state-of-the-art methods, validating its effectiveness in both performance and reasoning robustness. 

Keywords Retrieval-Augmented Generation · Multi-Hop Question Answering · Large Large Model 

## 1 Introduction 

Multi-hop question answering [ 1] entails synthesizing information from multiple sources and executing multi-step reasoning to produce an accurate response. Consequently, it has been widely applied in domains that require complex decision-making and deep knowledge inference [ 2]. In recent years, Retrieval-Augmented Generation (RAG) [ 3 ] has demonstrated significant progress in addressing multi-hop QA tasks [ 4, 5, 6]. Employing techniques such as iterative question decomposition (e.g., Chain-of-Thought prompting) and improved data structuring to reduce reasoning path length, these approaches have substantially mitigated the limitations of traditional RAG systems in managing complex queries. Nevertheless, RAG continues to encounter key challenges in multi-hop question answering tasks. First, existing studies have shown that once reasoning paths deviate during question decomposition—such as through the selection of an incorrect trajectory or an error in the initial hop [ 7, 8]—subsequent correction becomes increasingly difficult. Second, during the resolution of intermediate sub-questions, the reasoning process is highly susceptible to noise and interference from irrelevant information, stance bias, or conflicting knowledge sources [ 9, 10 , 11 , 12 ], often resulting in flawed intermediate outputs. More critically, in multi-step reasoning, these initial deviations and intermediate errors can be progressively propagated and amplified, culminating in a butterfly effect [ 13 ] that significantly degrades the accuracy of answers to complex queries. Inspired by the PDCA (Plan–Do–Check–Act) cycle [ 14 ], which is widely used to address complex problems through systematic strategies, we propose a novel RAG framework, PAR-RAG. This framework adapts the traditional PDCA 

> ∗

corresponding author 

> arXiv:2504.16787v2 [cs.CL] 1 Aug 2025

A PREPRINT - A UGUST 4, 2025 

Figure 1: Comparison of PAR-RAG to other RAG paradigms. (a) The standard RAG paradigm follows a Retrieve-then-Read approach, where information is directly retrieved based on the input question, and the answer is generated from the retrieved data. (b) The iterative reasoning RAG paradigm progressively decomposes the question, retrieves data based on dynamically generated sub-questions, and produces intermediate results in a cyclic process until the final answer is obtained. (c) The proposed Plan-then-Act-and-Review (PAR-RAG) paradigm first formulates a global reasoning plan, then sequentially executes the steps in the plan while performing timely verification and revision of intermediate outputs, ultimately generating the final answer through global integration. process into a streamlined Plan–then-Act-and–Review paradigm, tailored to optimize multi-step reasoning in complex question answering tasks. In the planning phase, PAR-RAG retrieves exemplar cases that exhibit a similar level of semantic complexity to the input question. These examples are used as contextual references to enhance the stability and consistency of plan generation. By leveraging the semantic and reasoning capabilities of large language models (LLMs) [ 15 , 16 , 17 ], the system generates a structured, multi-step reasoning plan aligned with the complexity of the original query. In the acting phase, PAR-RAG follows a typical retrieve-then-read process to generate an initial response for each reasoning step, using a citation-based evidence filtering mechanism to ensure contextual relevance. During the review phase, a dual-verification mechanism jointly evaluates the accuracy and factual consistency of each intermediate answer. If the evaluation falls below predefined thresholds, the system re-initiates retrieval and revises the response accordingly. Finally, a holistic analysis of the complete reasoning trace is conducted to synthesize the final answer. Compared to existing RAG paradigms (as illustrated in Figure 1), PAR-RAG adopts a top-down planning strategy and rigorous verification process that enable precise task decomposition, reliable execution, and systematic refinement. This substantially reduces error accumulation and propagation caused by reasoning deviations and intermediate inaccuracies in multi-hop question answering tasks. Extensive evaluation of the PAR-RAG framework across multiple multi-hop question answering datasets demonstrates its superiority over existing state-of-the-art methods in key metrics, including EM and Accuracy. Furthermore, experiments on single-hop datasets verify PAR-RAG’s strong adaptability, highlighting its applicability beyond multi-hop scenarios. Our main contributions are summarized as follows: • This study proposes PAR-RAG, a plan-driven and trustworthy RAG framework. By integrating global reasoning path planning with a multi-stage verification and revision mechanism, PAR-RAG effectively mitigates reasoning path drift and intermediate result errors commonly observed in traditional methods. The 2A PREPRINT - A UGUST 4, 2025 framework thus provides an interpretable and progressive reasoning paradigm tailored for multi-hop question answering tasks. • Distinct from existing planning methods, this study trains a multi-dimensional semantic complexity classi-fication model to dynamically retrieve exemplars exhibiting similar complexity to the current query during the planning phase, thereby enhancing the robustness of the planning strategy. Furthermore, unlike prior approaches that emphasize only result accuracy, we introduce a joint accuracy-trustworthiness evaluation mechanism that facilitates real-time correction of low-quality intermediate outputs, effectively ensuring the reliability of results throughout the reasoning process. • Experiments on multiple benchmark datasets demonstrate that PAR-RAG significantly outperforms existing state-of-the-art methods in multi-hop question answering tasks. These results validate the effectiveness of the plan-and-verify collaborative mechanism in enhancing accuracy for complex knowledge reasoning. 

## 2 Related Work 

Recent advances in RAG have produced a rich taxonomy of methods that attempt to overcome the limitations of standard Retrieve-then-Read paradigm, especially in the context of multi-hop question answering. These methods can be broadly categorized into structure-enhanced, reasoning-enhanced, reflection-enhanced, and planning-enhanced RAG paradigms. However, most approaches optimize only a subset of the reasoning process (e.g., retrieval accuracy or response fluency), often lacking a global view of reasoning trajectory construction and verification. This section systematically surveys related work and highlights how the proposed PAR-RAG fills critical methodological and architectural gaps. 

2.1 Structure-Enhanced RAG 

Structure-aware approaches improve retrieval granularity and contextual richness by altering the data representation used during retrieval. RAPTOR[ 18 ] constructs recursive hierarchical trees for dense index formation, while GraphRAG [ 19 ] converts unstructured text into entity-relation graphs to enable semantic retrieval at multiple abstraction levels. StructRAG[ 20 ] adaptively selects document structures (e.g., flat vs. hierarchical) during inference, and HippoRAG [ 21 ]mimics long-term memory patterns inspired by neuro-cognitive systems. Despite enhanced input structuring, these methods operate without explicit multi-step plan scaffolding or reasoning path controllability, leading to fragile response generation under multi-hop constraints. 

2.2 Reasoning-Enhanced RAG 

This line of work integrates LLM-based reasoning with iterative retrieval processes. For example, IRCoT [ 22 ] uses Chain-of-Thought [ 23 ] prompting with round-based retrieval. IterRetGen [ 24 ] enhances this synergy between retrieval and generation, and Adaptive-RAG [25] adds complexity-aware retrieval adjustment through classification. However, these methods often lack a global reasoning structure and are prone to early-hop bias—where an incorrect first step derails subsequent reasoning. 

2.3 Reflection-Enhanced RAG 

Reflection-based models aim to verify LLM outputs through introspective mechanisms[ 26 ]. Self-RAG [ 27 ] performs post-answer verification to validate factual consistency, while Reflexion [ 28 ] employs meta-cognitive evaluation of CoT paths. CRAG [29] introduces self-corrective steps into the retrieval and generation loop. However, these approaches assume that language models can reliably critique themselves, an assumption challenged by recent findings [ 30 , 31 ]. In complex reasoning, LLMs often hallucinate or accept invalid logical transitions without external signals. 

2.4 Planning-Enhanced RAG 

Planning-oriented frameworks incorporate structured intermediate steps before answer generation. RAP [ 32 ] frames reasoning as tree search via dual-role LLM agents. LPKG [33] extracts knowledge-graph-guided paths and fine-tunes LLMs on generated reasoning templates. Self-Ask [ 34 ] addresses complex question answering by iteratively generating and answering sub-questions via external knowledge sources, achieving strong performance on benchmarks like HotpotQA and MuSiQue. 3A PREPRINT - A UGUST 4, 2025 Method Planning Retrieval Step Verification Complexity Adaptation Path Stability Standard RAG × ✓ × × ×

RAPTOR × ✓ × × ×

Self-Ask △ ✓ × × △

ReAct △ ✓ × × △

Adaptive-RAG △ ✓ × ✓ △

RAP ✓ ✓ × × ✓

PAR-RAG ✓ ✓ ✓ ✓ ✓

Table 1: A comprehensive comparison of capabilities across multiple RAG methods. ✓=full support, ×=none, 

△=incomplete. While valuable, these methods typically rely on either static exemplars or handcrafted knowledge structures, and they seldom incorporate dynamic verification mechanisms during plan execution. As our error analysis reveals, even well-structured plans can fail if not continuously evaluated and refined. In contrast to the existing planning methods, PAR-RAG’s design is grounded in the Plan–Do–Check–Act (PDCA) loop, a cognitive model of systematic human problem solving. We reinterpret this cycle as a Plan–then-Act–and-Review (PAR) structure within LLM reasoning, where: • Planning serves as a bias prior, anchoring the reasoning trajectory; • Acting retrieves evidence and generates local answers; • Reviewing functions as a meta-cognitive evaluator, correcting deviations in real time. The design philosophy of PAR-RAG aligns with dual-process theory in cognitive science, where System 2 (slow, reflective, plan-based) overrides System 1 (fast, intuitive) when problem complexity is high. Existing reflection-based RAG methods simulate System 2 reactively, whereas PAR-RAG proactively engages it through semantic complexity-aware planning. Facing the critical challenges posed by the deviation of reasoning paths and intermediate errors, PAR-RAG directly targets this dual failure pattern through semantic complexity-aware plan generation, which reduces planning bias, and dual verification that incorporates both accuracy and factual consistency, thereby limiting semantic hallucination and inference collapse. To contextualize PAR-RAG within existing paradigms, we present a structured comparison in Table 1. Through this unified Plan–then-Act–and-Review paradigm, semantic complexity awareness, and dual-verification mechanism, PAR-RAG delivers a comprehensive solution that integrates structure, plan controllability, inference adaptability, and meta-cognitive feedback—setting a new direction for robust multi-hop reasoning systems. 

## 3 Methodology 

As illustrated in Figure 2, PAR-RAG comprises three core stages: (a) the Plan Phase, (b) the Act Phase, and (c) the Review Phase. In the Plan Phase, a multi-hop question is systematically decomposed into a structured reasoning plan consisting of multiple steps, guided by exemplars with similar semantic complexity. Next, during the Act Phase, each reasoning step is executed sequentially. In parallel, the Review Phase evaluates the accuracy and factual consistency of the intermediate answers generated at each step. Based on this evaluation, PAR-RAG determines whether revisions are necessary and updates the reasoning trajectory accordingly. This iterative process continues until all planned steps are completed. Finally, PAR-RAG conducts a holistic assessment of the entire reasoning trajectory to derive the final answer. 

3.1 Semantic Complexity-Aware Plan Generation 

Existing planning approaches typically adopt two strategies to generate solution plans for multi-hop questions: zero-shot direct generation or prompt learning based on fixed exemplars. However, these methods neglect the alignment of semantic complexity between the target question and the exemplars used. Recent research indicates that exemplars closely matching the semantic complexity of the target question substantially improve LLMs’ reasoning performance on complex tasks [ 35 , 36 ]. Therefore, in multi-hop question solving, using exemplars with comparable semantic complexity can enhance the robustness of plan generation. In contrast, exemplars with substantial complexity mismatches may introduce noise, disrupt the LLM’s reasoning process, and ultimately reduce the stability of the generated plans. 4A PREPRINT - A UGUST 4, 2025 

Figure 2: The PAR-RAG architecture consists of three core components: the Plan Module for designing multi-hop question-answering strategies, the Act Module for executing retrieval and generation, and the Review Module for error detection and correction. Since the semantic complexity of multi-hop questions is challenging to measure directly, this study employs the number of reasoning hops as a proxy measure. A classifier is trained to estimate the semantic complexity of a multi-hop question by predicting the number of reasoning hops involved. It is worth noting that traditional information entropy measures alone cannot adequately capture the semantic complexity of multi-hop questions. To address this limitation, we construct a comprehensive evaluation framework based on multiple features related to semantic complexity: • Sentence word sense entropy: H(X)

• Number of named entities: N C (X)

• Dependency tree depth: Depth (X)

• Sentence length: Len (X)

• Embedding vector: EM (X)

Here, X denotes a multi-hop question. Sentence word sense entropy H(X) quantifies the semantic complexity of a sentence by measuring the uncertainty in the distribution of word senses for each word in the sentence. It is calculated as: 

H(X) = −

> n

X

> i=1

P (xi) log P (xi) (1) where P (xi) denotes the probability that the random variable X takes the value xi.A dependency tree is a syntactic structure derived from dependency parsing that models the syntactic relationships between words in a sentence. In this tree, each node corresponds to a word, and each edge represents a dependency relation between two words. The depth of a dependency tree is defined as the length of the path (measured by the number of edges) from the root node to the most distant leaf node—that is, a word with no children. In other words, the depth corresponds to the longest path from the root to any leaf node in the tree. In summary, for a given multi-hop question, its semantic complexity can be approximated as: 

F (X) = H(X) + N C (X) + Depth (X) + Len (X)

Complexity (X) = EM (X) + F (X) (2) 5A PREPRINT - A UGUST 4, 2025 Meanwhile, by applying the pretrained semantic complexity classifier to the target instance, its predicted hop count can be obtained as Hop (X), that is: 

Hop (X) = Classif ier (Complexity (X)) (3) The predicted hop count can serve as a key retrieval condition to match successfully solved examples with similar characteristics from the example repository, thus enhancing the robustness and stability of plan generation. 

3.2 Dual Confidence Verification Mechanism 

Existing RAG methods typically evaluate performance solely based on answer accuracy, often neglecting the factual consistency of generated results, specifically whether answers are genuinely supported by the retrieved evidence. To address this limitation, this study proposes a comprehensive evaluation mechanism that integrates both accuracy and factual consistency. This mechanism quantifies answer quality at each intermediate step in multi-hop question answering, enabling dynamic decisions regarding further retrieval or answer revision, thus improving the reliability of the final output. Specifically, this study adopts the LLM-as-a-judge approach [ 37 ], leveraging the LLM itself to score the accuracy of the generated answer, as shown in Equation (4). 

Accuracy = LLM (Question, Answer, Evidences ) (4) The output is a normalized score within the range [0, 1]. The factual consistency of the answer is evaluated based on its coverage by the cited data, measured using AttrScore [ 38 ]. The three values returned by AttrScore—[Attributable, Contradictory, Extrapolatory]—are mapped to corresponding numerical scores [1, 0, 0.5], respectively, as follows: 

Credibility = AttrScore (Question, Answer, Evidences ) (5) The final confidence score is calculated as the weighted geometric mean of two factors: accuracy and credibility. Let 

x1 and x2 denote the accuracy and credibility values, respectively. Corresponding weights w1=α and w2=1 − α are assigned to each factor, where α is empirically set to 0.5 in our experiments to balance both components equally. The weighted geometric mean is formally defined as follows: 

¯xwg =

> n

Y

> i=1

xwi

> i

! 1Pni=1 wi

(6) Where xi is the i-th data point, wi is the weight of the i-th data point, and Qni=1 wi is the sum of all weights used for normalization. When evaluating answer quality, the confidence score of the current answer is first calculated, then compared against a predefined threshold Ct, which is set to 0.75 by default: ConfidenceScore (X) = 

1 if ¯xwg ≥ CT

0 if ¯xwg < C T

(7) When the confidence score equals 1, the current answer is considered to meet both accuracy and credibility requirements. Otherwise, PAR-RAG treats the current answer as a retrieval query to conduct additional retrieval and revise the response for the current sub-question, thereby improving the quality of intermediate results. 

## 4 Experiments 

The experiments utilize GPT-4o as the default language model, with the Top-K parameter empirically set to 10. 

4.1 Datasets 

We conducted experiments on three multi-hop question answering datasets—2WikiMultiHopQA[ 39 ], HotpotQA[ 2], and MuSiQue[40]—as well as one single-hop dataset, TriviaQA[41]. 6A PREPRINT - A UGUST 4, 2025 Name 2Wiki HotpotQA MuSiQue TriviaQA EM Acc EM Acc EM Acc EM Acc Standard RAG 0.37 0.38 0.45 0.52 0.08 0.08 0.55 0.58 RAPTOR 0.16 0.22 0.26 0.35 0.06 0.12 0.34 0.44 IRCoT 0.49 0.61 0.58 0.7 0.31 0.35 0.35 0.62 IRCoT+HippoRAG 0.71 0.76 0.57 0.7 0.3 0.42 0.33 0.65 ReAct 0.61 0.69 0.32 0.39 0.15 0.36 0.54 0.62 Self-Ask 0.37 0.43 0.53 0.62 0.13 0.24 0.45 0.63 PAR-RAG 0.71 0.78 0.61 0.72 0.33 0.43 0.6 0.67 

PAR-RAG(w/o Plan Module ) 0.51 0.53 0.5 0.63 0.16 0.23 0.51 0.54 PAR-RAG(w/o Review Module) 0.68 0.73 0.53 0.67 0.29 0.37 0.52 0.59 Table 2: Comparison of results across multiple datasets for various RAG methods. Bold and underline indicate the best and the second-best results. 

4.2 Baselines 

To comprehensively assess performance, we compared our method with representative baselines covering key RAG paradigms: 1) Standard RAG [ 42 ], embodying the classic Retrieve-then-Read approach; 2) reasoning-enhanced methods such as ReAct [ 43 ] and IRCoT; 3) RAPTOR, a structure-enhanced RAG method, and HippoRAG, which can be combined with IRCoT to enhance QA capabilities, offering a unique advantage in complex reasoning tasks; 4) Self-Ask, the representative of planning-enhanced RAG method. 

4.3 Evaluation Metrics 

We used the following metrics to evaluate the performance of the methods: 1) EM [ 44 ]; 2) Accuracy (Acc) evaluates the semantic alignment between the generated answer and the gold reference using LLMs [45]. 

4.4 Implementation Details 

We used bert-base-uncased as the backbone to train a multi-dimensional BERT classifier that integrates semantic entropy and other complexity-related features. This classifier serves as the question complexity predictor within the PAR-RAG framework. Implemented in PyTorch, the model was trained with a learning rate of 3.05e-5 and a batch size of 8, using 4,684 samples evenly distributed across 1- to 4-hop questions from the MuSiQue and TriviaQA training sets. 

4.5 Main Results 

To comprehensively assess the generalizability and robustness of PAR-RAG across diverse multi-hop question answering scenarios, we benchmarked its performance on three widely adopted datasets: 2WikiMultiHopQA, HotpotQA, and MuSiQue. As presented in Table 2, PAR-RAG consistently outperforms state-of-the-art RAG baselines on nearly all metrics, including EM and overall answer accuracy. The only exception occurs on the EM score for 2WikiMultiHopQA, where PAR-RAG ties with the best-performing method (IRCoT+HippoRAG). Quantitatively, compared to the strongest baseline per dataset, PAR-RAG yields absolute EM gains of +5.17% on HotpotQA, and +6.45% on MuSiQue. Additionally, PAR-RAG achieves consistent improvements in overall accuracy by +2.38% to +2.7% across the three benchmarks. These improvements are non-trivial, especially on MuSiQue, where question decomposition and distractor interference are particularly challenging. Furthermore, PAR-RAG significantly surpasses competing methods on the single-hop benchmark TriviaQA, with improvements of 11%, and 3.08% in EM and Acc metrics, respectively. These findings demonstrate that PAR-RAG is not confined to complex reasoning tasks; its core mechanism—adaptive plan generation based on semantic complexity—is equally effective for single-hop questions. 7A PREPRINT - A UGUST 4, 2025 2Wiki HotpotQA MuSiQue 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

> Accuracy

0.66 0.63 

0.38 

0.74 0.7 

0.4 

0.78 

0.72 

0.43 

Zero Shot Mode Random Mode Similar Complexity Mode 

Figure 3: QA Performance across various sampling strategies. 

4.6 Analysis 4.6.1 Ablation Study 

To assess the impact of the Plan Module and Review Module of PAR-RAG, we conducted ablation experiments involving two variants. The variant without the Plan Module treats the original question as a single-step plan, foregoing multi-step decomposition, while the variant without the Review Module disables the verification process, accepting intermediate answers without validation or revision. All other components remain unchanged. As shown in Table 2, both ablation variants significantly underperform compared to the full PAR-RAG model across all metrics and datasets. The variant lacking the Plan Module shows a more substantial decline, highlighting the essential function of global planning in multi-step reasoning. These results underscore the importance of the full PAR-RAG framework. 

4.6.2 Effect of Sampling Strategies 

This section analyzes how different example selection strategies affect the quality of multi-hop question plan generation. Acc (Accuracy) is employed as the primary evaluation metric to ensure comparability. Figure 3 illustrates that selecting examples with semantic complexity closely matching the target question significantly improves plan generation performance compared to Zero-Shot or random selection strategies. We attribute this performance improvement primarily to the beneficial effects of semantic relevance and coherent logical structure among examples on the reasoning capabilities of language models [ 46 ]. Semantically similar examples more effectively guide the model in capturing the latent reasoning structure of the target question, thereby improving the overall quality and consistency of plan generation. These findings confirm the effectiveness of the semantic complexity classifier in PAR-RAG. By accurately selecting examples closely aligned in semantic complexity with the target question, the model gains richer contextual support during plan generation, significantly enhancing the stability and accuracy of the produced plans. 8A PREPRINT - A UGUST 4, 2025 Question 

Plan  Review         

> Who burned down the city where Keeping Up with the Joneses is set, during the war marking the first time that the number of b lac k people living in
> NYC declined?
> Step 1
> •Thought: I need to identify the city where 'Keeping
> Up with the Joneses' is set.
> •Question: What is the setting of 'Keeping Up with
> the Joneses’?
> Evaluation Result: {'Status': 'PASS', 'Score': 1.0,
> 'Correctness': 1.0, 'Credibility': 1}
> Evaluation Result: {'Status': 'PASS', 'Score': 0.9,
> 'Correctness': 0.8, 'Credibility': 1}
> Evaluation Result: {'Status': 'UNCONFIDENT ', 'Score':
> 0, 'Correctness': 0, 'Credibility': 0}
> Review Result: Status': 'REVISED' , 'Answer': ' Confederate
> Gen. John Bell Hood ordered the destruction of Atlanta,
> including public buildings and possible Confederate assets,
> during the Civil War. ', 'Reason': 'Sources 1 and 2 explicitly
> state that Confederate Gen. John Bell Hood evacuated
> Atlanta on September 1, 1864, and ordered all public
> buildings and possible Confederate assets destroyed [1].
> This aligns with historical accounts of the fall of Atlanta
> during the Civil War.'}
> Current question:Who burned down Atlanta
> during the Civil War?
> Answer :I don't know
> Current question:Which war marked the first time
> that the number of Black people living in New York
> City declined?
> Answer:The Civil War marked the first time that
> the number of Black people living in New York
> City declined. For the first time since the Civil War,
> the number of blacks declined over a decade in
> New York City, as noted in the population data
> from 2000 to 2010 [1].
> Current question:What is the setting of 'Keeping Up
> with the Joneses’?
> Answer:The setting of *Keeping Up with the
> Joneses* is Atlanta, where Jeff Gaffney works at a
> defense contractor company called MBI, and where
> he and his wife Karen live in a nice cul -de -sac [ 1].

Act     

> Step 3
> •Thought: I need to identify who burned down the city
> determined in the first step during the war identified in the
> second step.
> •Question: Who burned down [city from step 1] during [war
> from step 2]?
> Step 2
> •Thought: I need to determine the war during which the
> number of black people living in NYC first declined.
> •Question: Which war marked the first time that the number
> of black people living in NYC declined?

Figure 4: This example illustrates how PAR-RAG employs its planning mechanism to develop a multi-hop question answering plan and leverages its review mechanism to promptly identify and rectify errors in intermediate results during execution. 

4.6.3 Case Study 

To illustrate the operational mechanism of PAR-RAG, we analyze a multi-hop question answering example presented in Figure 4. This example, sourced from a standard multi-hop dataset, exemplifies the model’s reasoning performance. From this analysis, we identify the following key insights: • Global planning guides PAR-RAG to produce logically coherent and well-structured multi-step reasoning plans for complex problems. By strictly adhering to these pre-generated steps, the model consistently derives accurate answers. • The review mechanism maintains the accuracy of reasoning outcomes. During plan execution, PAR-RAG employs a Review Module to continuously detect and assess intermediate results. Upon detecting potential errors (marked in yellow in the figure), the system corrects them using contextual information (marked in green), effectively halting error propagation and enhancing the accuracy and reliability of the final output. 

4.6.4 Error Analysis 

To gain deeper insights into PAR-RAG’s performance, we performed a systematic error analysis on 30 failure cases drawn from four benchmark datasets. The findings, summarized in Table 3, reveal several distinct categories of errors: • Planning Errors (27%): These errors mainly arise from the LLM’s inadequate understanding of the semantic structure of complex questions, which produces reasoning plans misaligned with the intended query, thereby causing incorrect answers. • Retrieval Errors (13%): These errors generally stem from insufficient relevant information in the knowledge base, causing the retrieval stage to provide inadequate evidence to support effective reasoning. • Reasoning and Generation Errors (60% in total): Reasoning errors by the LLM constitute 43%, making them the primary cause of failure. These arise when the retrieved information lacks explicit answers, and the LLM fails to integrate implicit clues for logical inference. Generation errors, accounting for 17%, occur when the LLM produces incorrect answers during response generation despite retrieving relevant evidence. Collectively, these errors highlight the ongoing challenges faced by LLMs in semantic integration and response accuracy. 9A PREPRINT - A UGUST 4, 2025 Error Category Count Percentage Planning Error 8 27% Retrieval Error 4 13% Inference Error 13 43% Deviation in reasoning paths 0 0% Generation Error 5 17% Table 3: Failure types and their distribution. Model 2Wiki HotpotQA MuSiQue EM Acc EM Acc EM Acc GPT-4o 0.71 0.78 0.61 0.72 0.33 0.43 

GPT-4o-mini 0.51 0.59 0.45 0.55 0.26 0.31 Llama3-8B 0.24 0.25 0.3 0.36 0.07 0.15 Table 4: Analysis for the impact of model size on multi-hop datasets. • Reasoning Path Deviation Errors (0%): Notably, this analysis revealed no occurrences of reasoning path deviation. This outcome indicates that PAR-RAG’s global planning framework effectively preserves coherence in the reasoning trajectory, successfully preventing the path drift commonly seen in conventional iterative reasoning methods. In summary, the analysis reveals that the primary limitation of current multi-hop question answering systems is the LLM’s capacity for profound language comprehension and effective cross-passage reasoning when addressing complex queries. 

4.6.5 Effect of Model Size 

As detailed in the section above, due to the substantial reasoning demands of knowledge-based tasks, systematically evaluating language models of varying parameter sizes within complex multi-hop contexts is essential to elucidate the relationship between model capacity and task effectiveness. To this end, we conducted comparative experiments involving several mainstream LLMs and pretrained language models (PLMs). Table 4 illustrates that models with smaller parameter sizes, such as GPT-4o-mini and Llama3-8B, perform notably worse on complex reasoning tasks compared to the more capable GPT-4o. These findings underscore that reasoning ability is a critical factor in addressing complex problems, reinforcing the necessity of utilizing models with robust inferential capacities in such contexts. 

## 5 Conclusion and Future Direction 

In this study, we proposed PAR-RAG, a plan-driven RAG framework designed to address key challenges in multi-hop question answering. By incorporating a semantic complexity–aware planning strategy, PAR-RAG generates structured reasoning trajectories tailored to the difficulty of each query. During execution, a dual-verification mechanism jointly evaluates answer accuracy and evidential credibility, enabling dynamic correction of intermediate outputs and significantly reducing reasoning path deviations and error propagation. Extensive evaluations on benchmark datasets demonstrate that PAR-RAG outperforms existing state-of-the-art methods in both Exact Match and semantic accuracy. Ablation studies further validate the complementary roles of the planning and review modules. While PAR-RAG achieves notable improvements in robustness and interpretability, its reliance on iterative validation and large language models introduces additional computational cost. Future work will focus on optimizing inference efficiency, scaling to broader knowledge-intensive tasks, and exploring lightweight planning and verification strategies for real-world deployment. 10 A PREPRINT - A UGUST 4, 2025 

## Declaration of competing interest 

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. 

## References 

[1] V. Mavi, A. Jangra, and A. Jatowt, “Multi-hop question answering,” 2024. [Online]. Available: https://arxiv.org/abs/2204.09140 [2] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning, “Hotpotqa: A dataset for diverse, explainable multi-hop question answering,” in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 2018. [Online]. Available: https://aclanthology.org/D18-1259/ [3] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel 

et al. , “Retrieval-augmented generation for knowledge-intensive nlp tasks,” vol. 33, 2020, pp. 9459–9474. [Online]. Available: https://arxiv.org/abs/2005.11401 [4] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, H. Wang, and H. Wang, “Retrieval-augmented generation for large language models: A survey,” 2023. [Online]. Available: https://arxiv.org/abs/2312.10997 [5] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana, and S. Nanayakkara, “Improving the domain adaptation of retrieval augmented generation (rag) models for open domain question answering,” 

Transactions of the Association for Computational Linguistics , vol. 11, 2023. [Online]. Available: https://aclanthology.org/2023.tacl-1.1/ [6] Z. Jiang, M. Sun, L. Liang, and Z. Zhang, “Retrieve, summarize, plan: Advancing multi-hop question answering with an iterative approach,” 2024. [Online]. Available: https://arxiv.org/abs/2407.13101 [7] S. Yang, E. Gribovskaya, N. Kassner, M. Geva, and S. Riedel, “Do large language models latently perform multi-hop reasoning?” in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 2024, p. 10210–10229. [Online]. Available: https://aclanthology.org/2024.acl-long.550/ [8] Z. Li, G. Jiang, H. Xie, L. Song, D. Lian, and Y. Wei, “Understanding and patching compositional reasoning in llms,” in Findings of the Association for Computational Linguistics ACL 2024 . Association for Computational Linguistics, 2024, p. 9668–9688. [Online]. Available: https://aclanthology.org/2024.findings-acl.576/ [9] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-augmented language models robust to irrelevant context,” 2023. [Online]. Available: https://arxiv.org/abs/2310.01558 [10] T. Li, G. Zhang, Q. D. Do, X. Yue, and W. Chen, “Long-context llms struggle with long in-context learning,” 2024. [Online]. Available: https://arxiv.org/abs/2404.02060 [11] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Schärli, and D. Zhou, “Large language models can be easily distracted by irrelevant context,” in Proceedings of the 40th International Conference on Machine Learning , ser. Proceedings of Machine Learning Research, A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, Eds., vol. 202. PMLR, 23–29 Jul 2023, pp. 31 210–31 227. [Online]. Available: https://proceedings.mlr.press/v202/shi23a.html [12] R. Xu, Z. Qi, Z. Guo, C. Wang, H. Wang, Y. Zhang, and W. Xu, “Knowledge conflicts for llms: A survey,” 2024. [Online]. Available: https://arxiv.org/abs/2403.08319 [13] E. Lorenz, “Predictability: Does the flap of a butterfly’s wing in brazil set off a tornado in texas?” 1972. [Online]. Available: https://www.ias.ac.in/article/fulltext/reso/020/03/0260-0263 [14] W. E. Deming, Out of the Crisis, reissue . MIT press, 2018. [15] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large language models are zero-shot reasoners,” 2023. [Online]. Available: https://arxiv.org/abs/2205.11916 [16] S. S. Raman, V. Cohen, E. Rosen, I. Idrees, D. Paulius, and S. Tellex, “Planning with large language models via corrective re-prompting,” in NeurIPS 2022 Foundation Models for Decision Making Workshop , 2022. [Online]. Available: https://openreview.net/forum?id=cMDMRBe1TKs [17] Z. Wang, S. Cai, G. Chen, A. Liu, X. Ma, and Y. Liang, “Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents,” 2024. [Online]. Available: https://arxiv.org/abs/2302.01560 11 A PREPRINT - A UGUST 4, 2025 [18] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D. Manning, “Raptor: Recursive abstractive processing for tree-organized retrieval,” in The Twelfth International Conference on Learning Representations ,2024. [Online]. Available: https://arxiv.org/abs/2401.18059 [19] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt, D. Metropolitansky, R. O. Ness, and J. Larson, “From local to global: A graph rag approach to query-focused summarization,” 2025. [Online]. Available: https://arxiv.org/abs/2404.16130 [20] Z. Li, X. Chen, H. Yu, H. Lin, Y. Lu, Q. Tang, F. Huang, X. Han, L. Sun, and Y. Li, “Structrag: Boosting knowledge intensive reasoning of llms via inference-time hybrid information structurization,” arXiv preprint arXiv:2410.08815 , 2024. [Online]. Available: https://arxiv.org/abs/2410.08815 [21] B. Jimenez Gutierrez, Y. Shu, Y. Gu, M. Yasunaga, and Y. Su, “Hipporag: Neurobiologically inspired long-term memory for large language models,” Advances in Neural Information Processing Systems , vol. 37, pp. 59 532–59 569, 2024. [Online]. Available: https://arxiv.org/abs/2405.14831 [22] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions,” in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , 2023, pp. 10 014–10 037. [Online]. Available: https://aclanthology.org/2023.acl-long.557/ [23] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al. , “Chain-of-thought prompting elicits reasoning in large language models,” Advances in neural information processing systems ,vol. 35, pp. 24 824–24 837, 2022. [Online]. Available: https://arxiv.org/abs/2201.11903 [24] Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and W. Chen, “Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy,” in Findings of the Association for Computational Linguistics: EMNLP 2023 , 2023, pp. 9248–9274. [Online]. Available: https://aclanthology.org/2023.findings-emnlp.620/ [25] S. Jeong, J. Baek, S. Cho, S. J. Hwang, and J. C. Park, “Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity,” in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , 2024, pp. 7029–7043. [Online]. Available: https://aclanthology.org/2024.naacl-long.389/ [26] Z. Wu, Q. Zeng, Z. Zhang, Z. Tan, C. Shen, and M. Jiang, “Large language models can self-correct with key condition verification,” in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing . Miami, Florida, USA: Association for Computational Linguistics, 2024, pp. 12 846–12 867. [Online]. Available: https://aclanthology.org/2024.emnlp-main.714/ [27] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, “Self-rag: Learning to retrieve, generate, and critique through self-reflection,” in The Twelfth International Conference on Learning Representations , 2024. [Online]. Available: https://arxiv.org/abs/2310.11511 [28] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan, and S. Yao, “Reflexion: Language agents with verbal reinforcement learning,” 2023. [Online]. Available: https://arxiv.org/abs/2303.11366 [29] S.-Q. Yan, J.-C. Gu, Y. Zhu, and Z.-H. Ling, “Corrective retrieval augmented generation,” arXiv preprint arXiv:2401.15884 , 2024. [Online]. Available: https://arxiv.org/abs/2401.15884 [30] R. Kamoi, Y. Zhang, N. Zhang, J. Han, and R. Zhang, “When can llms actually correct their own mistakes? a critical survey of self-correction of llms,” Transactions of the Association for Computational Linguistics , vol. 12, pp. 1417–1440, 2024. [Online]. Available: https://aclanthology.org/2024.tacl-1.78/ [31] J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou, “Large language models cannot self-correct reasoning yet,” arXiv preprint arXiv:2310.01798 , 2023. [Online]. Available: https://arxiv.org/abs/2310.01798 [32] S. Hao, Y. Gu, H. Ma, J. Hong, Z. Wang, D. Wang, and Z. Hu, “Reasoning with language model is planning with world model,” in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing ,2023, pp. 8154–8173. [Online]. Available: https://aclanthology.org/2023.emnlp-main.507/ [33] J. Wang, M. Chen, B. Hu, D. Yang, Z. Liu, Y. Shen, P. Wei, Z. Zhang, J. Gu, J. Zhou et al. ,“Learning to plan for retrieval-augmented large language models from knowledge graphs,” in Findings of the Association for Computational Linguistics: EMNLP 2024 , 2024, pp. 7813–7835. [Online]. Available: https://aclanthology.org/2024.findings-emnlp.459/ [34] O. Press, M. Zhang, S. Min, L. Schmidt, N. A. Smith, and M. Lewis, “Measuring and narrowing the compositionality gap in language models,” 2023. [Online]. Available: https://arxiv.org/abs/2210.03350 [35] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp, “Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity,” 2022. [Online]. Available: https://arxiv.org/abs/2104.08786 12 A PREPRINT - A UGUST 4, 2025 [36] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen, “What makes good in-context examples for GPT-3?” in Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures , E. Agirre, M. Apidianaki, and I. Vuli´ c, Eds. Dublin, Ireland and Online: Association for Computational Linguistics, May 2022, pp. 100–114. [Online]. Available: https://aclanthology.org/2022.deelio-1.10/ [37] D. Li, B. Jiang, L. Huang, A. Beigi, C. Zhao, Z. Tan, A. Bhattacharjee, Y. Jiang, C. Chen, T. Wu, K. Shu, L. Cheng, and H. Liu, “From generation to judgment: Opportunities and challenges of llm-as-a-judge,” 2025. [Online]. Available: https://arxiv.org/abs/2411.16594 [38] X. Yue, B. Wang, Z. Chen, K. Zhang, Y. Su, and H. Sun, “Automatic evaluation of attribution by large language models,” 2023. [Online]. Available: https://arxiv.org/abs/2305.06311 [39] X. Ho, A.-K. D. Nguyen, S. Sugawara, and A. Aizawa, “Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps,” in Proceedings of the 28th International Conference on Computational Linguistics .Barcelona, Spain (Online): International Committee on Computational Linguistics, Dec. 2020, pp. 6609–6625. [Online]. Available: https://aclanthology.org/2020.coling-main.580/ [40] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Musique: Multihop questions via single-hop question composition,” Transactions of the Association for Computational Linguistics , vol. 10, pp. 539–554, 2022. [Online]. Available: https://aclanthology.org/2022.tacl-1.31/ [41] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer, “Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension,” in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , 2017, pp. 1601–1611. [Online]. Available: https://aclanthology.org/P17-1147/ [42] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, M. Wang, and H. Wang, “Retrieval-augmented generation for large language models: A survey,” 2024. [Online]. Available: https://arxiv.org/abs/2312.10997 [43] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, “React: Synergizing reasoning and acting in language models,” 2023. [Online]. Available: https://arxiv.org/abs/2210.03629 [44] A. Mallen, A. Asai, V. Zhong, R. Das, D. Khashabi, and H. Hajishirzi, “When not to trust language models: Investigating effectiveness of parametric and non-parametric memories,” in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , 2023, pp. 9802–9822. [Online]. Available: https://aclanthology.org/2023.acl-long.546/ [45] H. Yu, A. Gan, K. Zhang, S. Tong, Q. Liu, and Z. Liu, “Evaluation of retrieval-augmented generation: A survey,” in Big Data . Singapore: Springer Nature Singapore, 2025, pp. 102–120. [46] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, “Language models are few-shot learners,” ser. NIPS ’20. Red Hook, NY, USA: Curran Associates Inc., 2020. 

## A Experiments 

All experiments were conducted on a single MacBook Pro equipped with an Apple M4 chip (16-core CPU, 40-core GPU, 128GB unified memory). 

A.1 Datasets 

To comprehensively evaluate the performance of PAR-RAG on multi-hop question answering, we selected three multi-hop QA datasets commonly used by other RAG methods: • 2WikiMultiHopQA. It is built upon Wikipedia content and knowledge graph paths, specifically designed to evaluate models’ ability to perform two-hop reasoning tasks. • HotpotQA. It challenges models with questions that bridge multiple Wikipedia articles, necessitating the integration of information from different sources. • MuSiQue. It introduces complexity by combining multiple single-hop queries into multi-hop questions, requiring models to navigate through 2-4 logical steps. 13 A PREPRINT - A UGUST 4, 2025 We selected 500 instances from each dataset as test entities. Additionally, to validate whether PAR-RAG performs similarly on single-hop question-answering tasks, we selected the following single-hop dataset as evaluation source, with 500 instances chosen from the dataset: • TriviaQA. It draws upon trivia questions from diverse online quiz platforms, offering a challenging testbed for open-domain question-answering systems. 

A.2 Baselines 

We compared multiple RAG methods with PAR-RAG: • Standard RAG: Standard RAG directly retrieves relevant evidence based on the question, selecting the top-k retrieved results as context data. These results, along with the question, are passed to the LLM through a prompt to generate the answer. • Structure-Enhanced RAG 

– RAPTOR: It recursively organizes document fragments into hierarchical summaries, constructing a tree where each node represents the summary of its child nodes. During retrieval, RAPTOR evaluates the relevance of each node in the tree and identifies the most relevant nodes, with the number of nodes limited by the top-k criterion. • Reasoning-Enhanced RAG 

– ReAct is an enhanced generation model framework that integrates reasoning and acting by alternating between reasoning steps and action steps, enabling the model to dynamically plan and execute in complex tasks. Compared to traditional methods, ReAct demonstrates higher flexibility and accuracy in scenarios such as question answering and task planning, especially excelling in tasks that require multi-step reasoning and environmental interaction. 

– IRCoT is a framework that enhances language model performance through iterative reasoning and chain-of-thought prompting. By performing multiple rounds of iterative reasoning combined with chain-of-thought prompts, it progressively improves the model’s ability to answer complex questions. 

– IRCoT+HippoRAG: HippoRAG simulates the long-term memory mechanism of the human brain by extracting entities and their relationships from documents to build a knowledge graph. It establishes an index and efficiently retrieves relevant data using the Personalized PageRank (PPR) algorithm. It can be combined with IRCoT to enhance QA capabilities, offering a unique advantage in complex reasoning tasks. • Planning-Enhanced RAG 

– Self-Ask addresses complex question answering by iteratively generating and answering sub-questions via external knowledge sources, achieving strong performance on benchmarks like HotpotQA and MuSiQue. Its effectiveness, however, is contingent on the quality of sub-question decomposition and the reliability of retrieved information. 

A.3 Evaluation Metrics 

We used the following metrics to evaluate the performance of the methods, but only chosen EM and Acc as the representative metrics in the result analysis and discussion: • Exact Match (EM): It measures whether the predicted answer precisely matches the expected result, focusing on exact matches. • F1: It measures the overlap between the predicted answer and the expected result, focusing on partial matches. • Accuracy (Acc): This metric evaluates the semantic consistency between the generated answer and the reference answer using LLMs, thereby measuring the accuracy of the output. Compared to traditional surface-level matching methods (such as Exact Match or F1), this approach focuses more on semantic alignment, offering a more faithful reflection of answer quality in complex question answering tasks. 14 A PREPRINT - A UGUST 4, 2025 Model Accuracy Best Parameters (a) BERT Classifier 80.83% lr=3.1e-5 epoch=5 (b) VAE Model 67.52% lr=2e-5 epoch=300 (c) BERT Classifier 81.32% lr=3.05e-5 epoch=5 Table 5: A comprehensive comparison of performance across multiple models. 

## B Implementation Details 

B.1 Semantic Complexity-Aware Classifier B.1.1 Training Data Construction 

In this study, multi-hop question answering samples with hop counts ranging from 1 to 4 were selected from the training datasets of MuSiQue and TriviaQA. Specifically, 1,171 samples were chosen for each hop level, resulting in a total of 4,684 training instances. To evaluate model performance, a validation set comprising 3,400 samples was constructed by applying the same sampling strategy to the validation datasets of MuSiQue and SQuAD, with 850 samples selected for each hop level (1 to 4). 

B.1.2 Model Training 

To evaluate the semantic complexity of multi-hop questions, this study compares three different construction methods: • (a) Bert Classifier: A BERT classifier based on information entropy; • (b) VAE Model: A VAE model incorporating information entropy and other dimensions; • (c) Bert Classifier: A BERT classifier based on semantic distribution entropy and additional dimensions. The BERT backbone used is the bert-base-uncased version. We implemented our model using PyTorch 2.1.0, and the model was trained on a single NVIDIA RTX 4090 GPU with 24GB VRAM and 128GB of system RAM. After multiple rounds of iterative training and cross-validation, as illustrated in Table 5, the BERT-based classi-fier—enhanced with multi-dimensional features such as semantic entropy—achieved an accuracy of 81.32% in pre-dicting the semantic complexity (i.e., hop count) of multi-hop questions. However, as shown in Figure 5, a number of off-diagonal entries (e.g., 104 instances of 1-hop questions misclassified as 2-hop, and 90 instances of 3-hop predicted as 2-hop) reveal persistent confusion between adjacent hop levels. These results underscore the promise of semantic complexity-aware classification in guiding plan generation for multi-hop reasoning, while also highlighting the need for more fine-grained differentiation—an important direction for future research. 

B.2 Settings of baselines and PAR-RAG 

GPT-4o is adopted as the default language model across all experimental settings. For retrieval-based meth-ods—including Standard RAG, HippoRAG, and PAR-RAG—ColBERTv2 is employed as the default retrieval encoder. Additionally, the Top-K parameter is empirically set to 10 throughout all retrieval stages. 

## C Workflow of PAR-RAG 

In the following, we provide a detailed description of the PAR-RAG workflow. 

C.1 Problem Formulation 

Given a multi-hop question answering task Q, which can be decomposed into a set of sub-questions: Q =(q1, q 2, . . . , q m), and a document collection D = ( D1, D 2, . . . , D k), the goal of a plan-driven RAG method is to find a plan P that facilitates sequential retrieval of information from the document collection D and generates and combines intermediate answers A1, A 2, . . . , A n, such that 

Af inal = fcombine (A1, A 2, . . . , A n)

, where fcombine is a LLM or a specific method that integrates multi-hop information to produce the final answer Af inal .Each intermediate step in the plan can be formally expressed as 

Pi = fread (fretrieve (Qi, D 1, D 2, ..., D i)) 

15 A PREPRINT - A UGUST 4, 2025 

Figure 5: Confusion Matrix of the BERT classifier based on semantic entropy and other multi-dimensional features. , where fretrieve is the information retrieval method or LLM responsible for retrieving relevant information, and fread 

is the LLM tasked with generating the answer for the intermediate step. 

C.2 Workflow 

Based on the proposed model architecture and problem definition outlined above, the specific workflow of PAR-RAG consists of the following steps: • In the planning stage, PAR-RAG employs a BERT-based classifier to assess the semantic complexity of a given multi-hop question. Based on this assessment, it retrieves in-context exemplars with similar complexity levels to prompt the LLM to decompose the original question into an executable multi-step reasoning plan, denoted as P = ( P1, ..., P n). Each step in the plan is defined as: Pi = ( T hought i, Q i), where T hought irepresents the LLM’s reasoning trace or chain of thought for the current step. Qi is the sub-question derived from the decomposition of the original complex query.This structured representation ensures that each reasoning step is both interpretable and executable, enabling downstream retrieval and verification stages to operate with higher precision and controllability. • Next, in the execution stage, PAR-RAG sequentially processes each reasoning step defined in the plan. For each step, the sub-question Qi is used as a retrieval query to obtain relevant documents, Di = Retrieve (Qi),and the LLM generate an intermediate answer using a citation filtering mechanism to choose only those pieces of information that are most relevant to the current reasoning context, Ai = Read (Qi, D i); Then, the cited evidences are extracted from the retrieved documents , Ei = Citation (Di).• To enhance reliability, PAR-RAG evaluates the accuracy and factual consistency of each intermediate answer 

Ai. If the answer quality is insufficient, it re-initiates retrieval and evidence selection using the current sub-question, and generates a revised version of the answer via: Ai = Rectif y (Qi, D i).16 A PREPRINT - A UGUST 4, 2025 

Algorithm 1 Credible Plan-Driven RAG 

Input : Question Q

Parameter : Confidence Threshold Ct, Prompt Template P T 

Output : Af inal 

1: Let i = 1 .

2: Let Hcomplexity = P redictComplexity (Q).

3: Let Eexemplar = F indM ostSimilarCase (Q, H complexity ).

4: Let P = GenerateP lan (Eexemplar , Q, P T ).

5: repeat 

6: Pi ← P

7: Qi ← Pi

8: Di = Retrieve (Qi)

9: Ai = Read (Qi, D i)

10: Ei = Citation (Ai, D i)

11: Ci = EvaluateConf idenceScore (Qi, A i, E i)

12: if Ci < C t then 

13: Di = Retrieve (Ai)

14: Ai = Rectif y (Qi, D i)

15: Ei = Citation (Ai, D i)

16: end if 

17: Qnext ← Pi+1 

18: Qnext = Ref ineN extQuestion (Ai, Q next )

19: T = GenerateAndConcatT rajectory (Qi, A i, E i)

20: i = i + 1 

21: until i > n 

22: Af inal = GenerateAnswer (Q, T )

23: return Af inal 

• Based on the answer Ai, PAR-RAG refines the next sub-question to improve coherence in reasoning: Qnext =

Ref ineN extQuestion (Ai, Q next ). It also constructs a trajectory record of the current step and appends it to the trajectory chain, Ti = GenerateAndConcatT rajectory (Qi, A i, E i), T = ( Ti, ..., T i).• Once all reasoning steps have been completed, PAR-RAG uses the initial question Q and the full trajectory chain T as context to prompt the LLM to generate the final answer: Af inal = GenerateAnswer (Q, T ).Its algorithmic description is shown in Algorithm 1. 17
