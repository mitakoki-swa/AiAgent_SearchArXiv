Title: Position Paper: Bounded Alignment: What (Not) To Expect From AGI Agents

URL Source: http://arxiv.org/pdf/2505.11866v1

Published Time: Sat, 14 Jun 2025 00:06:05 GMT

Markdown Content:
> arXiv:2505.11866v1 [cs.AI] 17 May 2025

# Position Paper: Bounded Alignment: What (Not) To Expect From AGI Agents 

Ali A. Minai, Senior Member IEEE 

Dept. of Electrical and Computer Engineering University of Cincinnati 

Cincinnati, USA minaiaa@ucmail.uc.edu 

Abstract —The issues of AI risk and AI safety are becoming critical as the prospect of artificial general intelligence (AGI) looms larger. The emergence of extremely large and capable generative models has led to alarming predictions and created a stir from boardrooms to legislatures. As a result, AI alignment has emerged as one of the most important areas in AI research. The goal of this position paper is to argue that the currently dominant vision of AGI in the AI and machine learning (AI/ML) community needs to evolve, and that expectations and metrics for its safety must be informed much more by our understanding of the only existing instance of general intelligence, i.e., the intelligence found in animals, and especially in humans. This change in perspective will lead to a more realistic view of the technology, and allow for better policy decisions. 

I. I NTRODUCTION 

The most successful AI systems today, such as large lan-guage models (LLMs) [1]–[5], are based on a computation-alist, statistical, and decision-theoretic paradigm rather than a biological one. As these systems scale up in size, they are improving their performance in areas such as reasoning [6]– [9], and becoming more multimodal [10]–[14]. AI agents [15]– [17], including physical ones [18]–[20], are also becoming increasingly capable. With these rapid advances, there is an expectation that powerful systems with artificial general intelligence (AGI) may soon be at hand. Through all this, there is a general desire that AGI must remain subject to human control and intervention, and must exist only to serve human needs (see, for example, the discussion in [21]). There is also great concern that increasingly powerful AGI systems with autonomous agency might pose serious risks, including existential ones [22]–[27], which has led to a focus on AI alignment , i.e., making AI systems consistent with human norms and preferences [28], [29]. The main position argued in this paper is that: 1) General intelligence should be seen in terms of its archetype: The intelligence of living agents ; and 2) The goal of building powerful AGI agents is fundamentally inconsistent with the expectation of complete alignment or near-total control of AGI agents by humans even in principle . The best that can be achieved is bounded alignment , defined by analogy with 

bounded rationality [30] as follows: The agent’s behavior is almost always acceptable – though not necessarily optimal – for almost all humans who interact with it or are affected by it . The degree of alignment expected from AGI should be no more than that expected from well-behaved humans and trained animals. II. W HY ALIGNMENT IS HARD 

Broadly, the goal of alignment is to make AI agents safe by constraining them to follow acceptable norms of behavior. However, this goal conceals a great deal of complexity because it is difficult to turn the abstract problem of alignment into a concrete task that can be accomplished computationally. Alignment aims to constrain two things in particular: Goals and behavior (including expression), and to do so by aligning the values of the agent with human values – or, at least with human preferences . A great deal of thinking [23], [31], [32] and methodological research [33]–[39] has gone into how this can be achieved. However, the task of alignment faces several challenges, including the following. 1. Values are vague : Values and preferences are extremely hard to define quantitatively and consistently across the entire spectrum of possible behaviors and objectives, and across human differences in culture and ideology [29], [40]. Whose values should the agent align with? And how should it make sure that it acts appropriately in all situations, not just those it was trained in? In fact, “alignment” is not quite the right term for the goal of creating safe AGI, since it implies that there is something to align with , and no such unique set of values or preferences can be defined rigorously. The values that the agent needs to acquire, instead, are those embodied in a personal code of ethics that it applies to all its goals and actions. And, while these values may be universal from the agent’s viewpoint, they will still not be universally acceptable to all those it interacts with, or in all contexts. The possibility of a universally aligned, explainable, trustworthy AGI agent is no more realistic than that of a universally aligned, explainable, trustworthy human being. 2. Intelligence is inherently risky : Attempts to build increasingly powerful AGI agents will necessarily require the agents to have open-ended behavioral complexity, autonomy, self-motivation, life-long learning, and other attributes of general intelligence even if the ultimate goal is only to serve human needs, and these attributes will inevitably give such agents the capacity for inappropriate or dangerous behavior. Accepted for Proceedings of IJCNN 2025, Rome, Italy. Copyright IEEE 3. The agent and the environment are both complex dynamical systems : The most fundamental problem for AGI alignment is the nature of general intelligence itself. Any AGI agent will necessarily be an autonomous complex adaptive system 

(ACAS) operating in an extremely complex dynamic environment. The behavior of the agent will emerge from its interaction with the environment in the context of its 

available affordances and its internal perceptual, cognitive, and motivational states , which, in turn, will influence the state of the environment in highly complex ways. This creates several problems: 1) It will be impossible to anticipate all the situations that the agent might encounter, and to predict whether the behavior that emerges will be aligned; 2) The values and preferences that underlie this behavior will also be emergent [22], [41]; 3) Any values and preferences that the agent may initially have could change as it experiences and learns from its complex environment; and 4) Since any AGI agent will have affordances and state spaces very different from those of humans, they will typically not share a common representational framework in which they can agree on values any more than humans do with other animals. The essential lesson of these impediments is that the goal of perfect alignment and perfectly safe AGI misunderstands the nature of general intelligence and underestimates its com-plexity. III. S-A GENTS AND P-A GENTS 

Since its inception, there has been a conceptual paradox at the core of the AI enterprise: Is the goal of building AI: a) to provide tools to achieve human goals, or b) to create autonomous intelligent entities with their own motivations and goals? The implicit assumption behind most work in AI has been that (a) is the right answer. This is the view motivating everything from the so-called Laws of Robotics proposed by Asimov [42] to the current work on AI safety [40], [43]–[45] and alignment through post-training reinforcement learning [33]–[39]. But is it logical to assume that AI systems will remain captives of human goals even as they progress towards achieving general intelligence? Some crucial attributes that we would expect in a submissive agent include obedience, reliability, veracity, honesty, trans-parency, prosociality, etc. Together, these and other similar attributes may be termed safety attributes , or S-attributes ,and an agent with these S-attributes designated an S-agent .While S-attributes would make the agent highly aligned, they would not give it the ability to solve complex real-world problems or perform complex real-world tasks. Even assuming that the agent’s only purpose is to serve human needs, if it seeks to satisfy goals across the entire spectrum of human needs and desires, it must confront the reality that: a) No objective can ever be specified by a human so completely that it excludes all harmful possibilities, as demonstrated by the famous paperclip task [46]; and b) The world is an extremely complex, unpredictable, nonlinear, nonstationary dynamical system with emergent phenomena, an infinite number of unexpected situations, and an untold number of other active agents – living and artificial – with their own goals and behaviors. This means that much of the agent’s own behavior must be generated on the fly rather than being pre-planned. While a human user may specify the ultimate goal for an agent at some level, it is up to the agent to come up with the necessary sub-goals and the means to accomplish them in real time. Indeed, the entire purpose of building an AGI servant would be to free humans from this responsibility. Meeting this challenge will require the agent to have a set of performance attributes , or P-attributes , including: 1) Auton-omy to act without continuous guidance; 2) Self-motivation to pursue internally-generated goals; 3) Creativity to solve hard problems; 4) Imagination to consider hypotheticals and coun-terfactuals; 5) Introspection for making deliberative choices; 6) Versatility across a wide range of domains; 7) Perceptual breadth to make sense of multimodal real-world situations; 8) Cognitive depth to create, store, recall, and use concepts and ideas across a hierarchy from concrete to abstract; 9) 

Cognitive control to attend selectively to internal mental states and make wise choices; 10) Cognitive agility to revise plans continuously in real time as needed; 11) Behavioral flexibility 

to adapt behavior in real time as needed; 12) Exceptional capabilities to do things that humans cannot do; and 13) Open-ended, continuous, autonomous, life-long learning to retain and improve capabilities in a dynamic world. Of course, this list is not exhaustive, but it attempts to encapsulate the things that would give an agent the kind of general intelligence needed to behave intelligently in the real world. An agent with these attributes is termed a P-agent .It is easy to see that any generally intelligent agent we build to serve even quite specific human needs – say, a robot restaurant waiter or nursing home attendant – will need all 

these attributes if it is to be useful. Anything less would be worse than useless because the agent would keep making bad choices, not know what to do in unexpected situations, and require constant instruction or approval. The question at the heart of alignment is whether a P-agent can also be an S-agent. An autonomous agent is, by definition, beyond total human monitoring and control. If it is creative, imaginative, and has complex behavioral and mental capabilities, it will be able to generate complex ideas, plans, and strategies, and to execute difficult tasks across many domains, which is exactly what we would like it to do. But these very attributes would, in principle, allow the agent to deceive, disobey, and inflict harm. Indeed, these negative abilities are features, not bugs for any intelligent agent in a complex and dangerous world. But the AGI agent will go beyond that: Its vast computational resources would enable it to out-think humans, and to learn continuously across what could be a lifespan far longer than that of a human. Any “factory settings” for good behavior could thus be erased over time by dilution or by deliberate choice. The inescapable dilemma is that the very things that make AGI useful also make it unsafe. There is a safety-utility tradeoff between S-attributes and P-attributes, and an ideal AGI agent will have to compromise on S-attributes to find the right balance, precluding the possibility of perfect 2alignment. This tradeoff is qualitatively different than the risk-reward tradeoff for other high-risk technologies such as nuclear, biological, and chemical weapons for several reasons: a) Unlike AGI, the underlying science in those cases is clear-cut and the risks are well-understood; b) Those technologies have narrow applications, providing feasible regulatory targets, whereas AGI will be applied to virtually everything; c) Those technologies are passive and require human agency for their use whereas AGI will be autonomous and self-motivated with the ability to act on its own; and d) Even a relatively low level of P-attributes will give an agent the potential to bootstrap itself to ever higher capability levels, thus increasing risk in an open-ended way. In summary, building an AGI agent with P-attributes is fundamentally incompatible with perfect alignment. Exist-ing generally intelligent agents, i.e., animals (including hu-mans), demonstrate this limitation, and we do not expect fellow humans, pets, or domesticated animals to be perfectly well-behaved even after extensive training. Recognizing this dilemma, we should accept that the alignment problem will never be “solved” completely. The goal should instead be to achieve robust, long-lasting bounded alignment, and that will require a deeper understanding of general intelligence itself. IV. R ETHINKING AGI 

A. General Intelligence 

Defining AGI has proved to be quite challenging due to the breadth of issues involved and an emphasis on computational formalisms [47]–[49]. Some have defined it as AI that can perform almost all economically useful tasks that humans can perform at a human level of competence or better, which is a rather anthropocentric way of looking at intelligence. This position paper argues for a broader, more agent-centric and biologically grounded view, defining general intelligence as follows: 

General Intelligence: The ability of an autonomous agent to exploit its environment productively, actively, creatively, and opportunistically in almost all the situations it encounters in its native environment – including extremely novel ones. 

Thus, general intelligence is not defined as the ability of the agent to perform certain tasks to serve its users, but as its ability to exploit the possibilities of its environment pervasively on its own behalf . A key point is that the utility function implicit in this exploitation may be arbitrarily complex and inaccessible even to the agent itself. Thus, the potential behavior that emerges in the course of the exploitation is open-ended and unknowable a priori . It is possible to be generally intelligent in an infinite number of ways for different agents in different environments. 

B. Natural General Intelligence 

To understand how general intelligence arises and how it works, it is useful to look at the only general intelligence that already exists, i.e. the natural general intelligence (NGI) of animals with brains. The intelligence of NGI agents emerges from the interaction between their specific form and their environment, and as they experience that environment, they reorganize themselves continually in ways that correspond to acquiring new knowledge and the emergence of new skills. A generally intelligent agent embedded in a complex dynamic environment is necessarily one that learns and evolves through-out its life; otherwise it would lose the ability to keep exploit-ing its environment. Here, the term “specific form” is used deliberately instead of the more common term “embodiment” because the latter has come to refer mainly to an agent’s external, macroscopic form, whereas intelligence arises from the configuration of structures and processes from the macro to the cellular and molecular level in physical agents, and across a similarly wide range of scales in virtual agents. This definition of general intelligence extends the concept of AGI to all kinds of broadly competent artificial intelligent agents – present and future – and allows research on issues within AI to connect with the increasingly unified understand-ing of intelligence and agency in living systems from the simplest to the most complex [50]–[52]. 

C. Intelligent Agents 

Intelligence is ultimately a basis for generating behavior in the agent’s specific environment. The agent’s sensors and effectors represent its external embodiment (or body), but a particular mental architecture is also necessary. The en-tire evolutionary process can be seen as the co-evolution of increasingly complex external embodiments and mental architectures, resulting in increasingly complex general intelli-gence. Several key components are essential to the functional configuration of an intelligent autonomous agent’s mental architecture, and they are briefly discussed below. Of these, the first four – perception, cognition, behavior, and drives – form the core, while the other three represent more general aspects of intelligence. As we think of AGI agents, it is useful to ask how they map on to this framework. 

Perception: A huge amount of redundant multimodal sen-sory information flows into the agent in real-time. It must be processed into a form that gives the agent a consistent, integrated, and actionable view of its environment. This is the role of perception . The result of the perceptual process is the creation of a perceptual space in terms of which the agent experiences its world. 

Behavior: Formal decision frameworks such as reinforce-ment learning (RL) typically model behavior as sequences of individual actions within an action space [53], but behavior in agents with general intelligence is far more complex in several ways: It is real-time, continuous, high-dimensional [54], [55], hierarchically organized [56], and multiscale in space and time [57], [58]. Based on these factors, it is more appropriate to think of an intelligent agent as operating in an extremely complex, multi-level, multiscale, open-ended, self-organizing behavioral space rather than a predefined discrete or continuous action space. 3Cognition: Linking perception to behavior is arguably the primary competence of an agent, and cognition is the process mediating this linkage – creating the so-called sense-think-act cycle . Conceptually, it plays the role of a very complex and deep, multi-recurrent “hidden network” linking the perceptual space to the behavioral space bidirectionally. Like perception and behavior, cognition too is an active process with complex, multi-scale dynamics rather than the set of feed-forward trans-formations found in most deep neural networks today. Most importantly, it provides a rich, hierarchical representation of the world at levels ranging from concrete features grounded in sensorimotor experience to abstract ones [59]. The depth of this hierarchy – the agent’s cognitive space – plays a central role in determining its degree of intelligence. 

Intrinsic Drives: Unlike passive AI models such as LLMs, animals possess internal drives including hunger, thirst, the mating urge, curiosity, emotions, etc., to motivate their behav-ior. These drives are a critical component of any generally intelligent agent because they lie at the root of its agency – its role as an autonomous, self-motivated actor in the world. An agent without curiosity, the urge to explore and exploit its en-vironment, a sense of self-preservation, the motivation to learn, and the need to achieve its own goals, could not be successful in a complex, dynamic, hazardous, ever-changing world. In any generally intelligent agent, drives form a hierarchy that is rooted in a fundamental set of primal drives – notably self-preservation. These give rise to more specific drives such as risk avoidance, dominance-seeking, etc. In humans, this hierarchy is very deep with complex drives such as creativity, ambition, and a desire for social approval or self-expression. The drives of the agent also change with time due to factors such as aging, injury, change of context, and learning. 

Affordance Space: Affordances are the possibilities for per-ception and action that an environment offers to the agent. The productive exploitation of the environment by the agent lies in effective use of these affordances, and this is precisely what intelligence enables. The key point is that these affordances are 

relative , not absolute: They are relations induced emergently between particular aspects of the environment and specific perceptual and behavioral capabilities of the agent [60], or, in Gibson’s memorable phrase, they imply “the complementarity of the animal and the environment” [61]. Thus, for an agent with a specific form, a specific envi-ronment induces a specific but infinite affordance space that circumscribes the agent’s perceptual and behavioral possibili-ties as well as the time-varying, context-dependent availability 

of these possibilities. While the agent’s mental experience of its world is defined by its four core components, it is the affordance space that implicitly generates its self-perception of agency in that world. Affordances may be discovered by the agent through experience, lost due to injury, or become available through practice or via the use of tools. 

World Model: The concept of a world model is widely used in AI to refer to an agent’s cognitive model for prediction and planning in its environment [62]–[65], often in a reinforcement learning framework [53], [66]. A more general definition comes from LeCun [67], who equates world models with “common sense” that “can be seen as a collection of models of the world that can tell an agent what is likely, what is plausible, and what is impossible” in its world. This, as LeCun points out, is the reason an intelligent agent can apply its intelligence to everything in its world, including completely novel situations. Essentially, the world model must represent the deep structure and causal relationships in the world it models at a level commensurate with the agent’s affordances. Importantly, the world model is not a passive oracle – a resource to be queried by a higher-level decision-maker as is assumed in RL – but a deep, richly multi-scale, real-time active mediator between perception and behavior, continuously gen-erating hypotheses, predictions, and recommendations in the context of the agent’s internal state and affordances as well as the changing state of the environment, which has its own complex dynamics. The implicit goal of life-long learning for the agent is the improvement of its world model to expand its ability to exploit more of its affordances, and possibly recognize new ones. 

Memory: A critical element in any intelligent agent is mem-ory, i.e., the ability to retain information from past experiences to influence future behavior. Memory in biological agents is pervasive through all aspects of mental function. Broadly, it can be divided into types such as short-term working memory 

for storage of currently needed information [68]; long-term 

declarative memory , including episodic and semantic memory ;and implicit memory , which includes procedural and emotional memory among other types. As the locus of learning and the basis of behavior, memory of all types is especially important for a functioning world model. It is also the key integrator of the perceptual, cognitive, motivational, and behavioral compo-nents of the agent, forming representations across modalities and at every hierarchical level. In animals, memory involves both the brain and the body, and this will be the case for embodied AGI agents as well. To summarize, any AGI agent will need to have all the com-ponents of intelligence listed above – explicitly or implicitly. Whatever its specific form – physical or virtual – it will have sensors and effectors, and will interact with its environment to induce an affordance space. It will learn a world model grounded in its perceptual, behavioral, and affordance spaces in the context of its internal state and drives. Any such agent will have preferences across all these modalities, and these will implicitly define its values, biasing how it perceives the world and how it chooses to act in it. V. T HE NEED FOR A P ARADIGM SHIFT 

A. Contrast with the Current AI Paradigm 

The model of AGI proposed above is quite different from the dominant paradigm in machine learning-based AI today which is focused on computational mechanisms, optimizing specific objectives, accomplishing specific tasks, and building towards a more general version of AI by adding tasks and modali-ties gradually while maintaining the same general paradigms (typically, autoregressive generation). The architectures and 4learning algorithms of these models are regular, stereotypical, and fundamentally different from those seen in biological agents, with generic architectures such as stacked attention layers, supervised (or self-supervised) learning using gradient descent, and severely limited, narrow objectives such as next token generation or pixel updating. The sensory inputs of multimodal LLMs [10]–[14] consist of token sequences repre-senting text, numerical data, code, images, and audio, so their perceptual affordances are limited to these modalities. It can be argued that internal embeddings generated in the early layers of deep neural networks such as LLMs or CNNs correspond to perceptual space representations, and the later layers can be seen as performing cognitive transformations based on a cognitive hierarchy implicit in their weights, producing a cognitive representation in the final embedding layer. This immediately raises the question of whether even very large embedding vector spaces have anywhere near the richness of the perceptual and cognitive spaces of a human or even a fish. While there is a growing body of work showing some correspondence between representations found by deep neural networks and those in the brain [69]–[71], these are typically confined to single modalities. The behavioral spaces of LLMs are typically limited to token generation. Sequences of such actions correspond to generated text or code, which can then be interpreted by other tools to create images or take other actions. Their working memory consists of the context buffer and the signals carried by the residual path in the network, and is thus quite superficial with no ability to store discrete internal states. Their long-term memory is distributed across their weights, with no separation between semantic and procedural memory. Classic LLMs do not have episodic memory, though OpenAI recently added a feature allowing GPT-4 to search through previous conversa-tions. LLMs generate their behavior in a “ballistic” way with no internal recurrence, introspection, or deliberation. Only very recently have there been rather simplistic attempts to add in deliberative, System 2 mechanisms [72], using post-training reinforcement learning and inference-time computation [6]– [8], [38] or latent-space deliberation [73]. LLMs can be seen as learning a world model, but their “world” is the one defined by text, code, numerical data, images, etc., not the physical world. Thus, they internalize the causal and logical relationships underlying grammar, syntax, semantics, and, to some degree, image structure. While such models have some utility for exploration and decision-making in the real world as well [74], this is based on second-hand “book learning” rather than direct experience, and thus often fails on simple tasks. At best, such models represent an important first step towards building AGI agents. One of the most important developments in AI recently is a surge of new work on building AI agents based on LLMs [15]–[17]. These models leverage generative AI capabilities to create a degree of agency, and greatly expand the behavioral space by adding various effectors such as code interpreters and embodied manipulators. This has come to be termed agentic AI . In practice, most of these agents are still focused on specific tasks or domains, though it is possible to define a hierarchy from specificity to generality [16]. Recently, there have also been proposals for more open-ended, creative agents that can define their own environments, goals, and tasks [75]. A key innovation is the development of vision-language-action models (VLAs) [18]–[20], which integrate LLMs and action prediction models to allow fairly complex behavior learning in embodied agents. As AI moves towards AGI, agents will inevitably have to become more natural and incorporate more and more features seen in biological agents [9], but may do so in radically non-biological ways. These agents will differ greatly from those we see today. They will be active, autonomous, self-motivated, introspective, and deliberative. They will have affective states and extremely complex world models. Their behavior spaces too will be extremely rich and complex, but, depending on their particular forms, could be defined on affordance spaces very different from those of humans, just as the behavioral spaces of other animals are. These agents will thus be alter-native intelligences – even alien intelligences [76] – that may inhabit worlds and have thoughts and values incomprehensible to humans, thus posing arbitrary and open-ended risks [27]. AGI alignment must focus primarily on mitigating this risk. VI. A LIGNING ALTERNATIVE INTELLIGENCES 

A. Alternative Intelligence 

In 1974, cognitive science pioneer Thomas Nagel published a seminal paper [77] titled, “What is it like to be a bat?” The paper argued that, given a bat’s specific embodiment and set of mental and physical capacities (i.e., affordances), it is impossible for a human to imagine how the bat experiences the world inwardly as a bat (rather than as a human simulation of a bat). Indeed, every agent with a specific form experiences the world in terms of it own perceptual space, world model, motivational states, and affordances. This poses a fundamental challenge for agents in understanding the experiences of other agents, but the difficulty depends strongly on the types of agents involved. It is relatively easy for us humans to understand the experiences of fellow humans because of shared biology, shared form and affordances (with minor variations), shared experiences, and the brain’s mirror system [78], which allows an individual to represent an observed action by another person in a representational space shared with their own actions. Thus, humans have high-quality theories of mind for other humans. This is much more difficult when trying to understand the mental experiences of other animals such as Nagel’s bat, but even so, these animals – from cats and dogs to bats, bees, and octopuses – share many things with us, including the same fundamental biology; many homologous genes, tissues, and organs; many of the same needs and hazards; and, crucially, similar drives embedded in all animals by evolution [50]. Thus, we can at least describe and comprehend their behaviors and guess their motivations, if only in human terms. When a bird flies away as we approach, or a bee stings us if we swat at it, we may not share their mental experiences, but 5we do understand their motivations because we too run away from danger and fight back when attacked. Similarly, we comprehend the motivations of a spider spinning a web, a mole digging a burrow, and a tiger stalking prey. They are fellow animals – even distant relatives – and are doing things that we can identify with as humans. We also have the technical language to describe these behaviors at many levels using the frameworks developed by biologists, neuroscientists, and psychologists, though decades of research has still only given us limited insight into the minds of intensively-studied animals such as rats and chimpanzees. AGI agents will be far more 

inscrutable .Unlike every other intelligent agent we know, AGI agents will not be biological. They may be virtual or embodied, but, in their material or electronic substrates, their forms, their information processing mechanisms, their sensors and effectors, their affordances, and in terms of their metabolic needs and vulnerabilities, they could be completely different. They will not face the same hazards (e.g., particular chemical toxicities and injuries), nor experience animal-like kinship ties or go through the experience of aging and death on the same timescale as most animals. They will potentially have memory capacities and information processing capabilities far exceeding those of humans, direct access to cyberspace, and, most importantly, the ability to easily replicate their minds, if not their bodies, and to modify and improve them rapidly. With all these differences, AGI agents will inhabit worlds completely different from those of any animal. Indeed, they may not even share worlds with each other because of their radically diverse forms [23]. How, then, could such diverse alternative intelligences possibly be aligned with humans? And are the current methods being used for alignment suitable for this? 

B. Current Approaches 

As discussed above, in the context of AGI safety, values ,refers to a systematic set of implicit principles that modulate the motivations, goals, and behavioral preferences of an agent globally. These, in turn, depend on how the agent sees the world (perception), its internal motivations (drives), its causal understanding of the situation (world model), and its available behavioral possibilities (behavioral affordances). Thus, values in an aligned AGI agent must be implicit in all components of the agent’s mental processing and shape the flow of infor-mation through all of them, biasing the agent towards having an aligned disposition – if only in a bounded sense. The issue is whether current approaches to alignment can achieve this, and whether there is a better way. The dominant practice for value alignment in current AI agents is to take pretrained base models and teach them to prefer acceptable responses through reinforcement learning and/or fine-tuning [33]–[38], thus effectively biasing their weights to implicitly embed human preferences. While sound in principle and consistent with the idea of values as inherent biases, these approaches have serious practical limitations. They are very expensive to scale, rely on uncertain inductive reward models, and assume that the agent is under human control and subject to human training or observation, which will not always be the case with AGI. Another problem is that the training process can lead to the learning of emergent values that may be inappropriate but hard to detect using benchmarks. Mazeika et al. recently reported that, with scal-ing, LLMs aligned using current methods converge emergently to coherent value systems that can be “problematic and often “shocking”, including “cases where AIs value themselves over humans and are anti-aligned with specific individuals” [41] – presumably because of latent biases in the data. They propose a principled utility engineering framework for probing emergent values in LLMs, and using it to train them towards a value system obtained from a simulated citizen community. While insightful, such an approach in its current form is applicable mainly to LLM-like systems and cannot work for AGI agents that are autonomous and are learning continuously on their own outside of human control. Also, the method depends fundamentally on the probing protocol used to estimate the LLM’s values, which is likely to miss latent values that the utility engineers failed to imagine. The same applies to the Constitutional AI principle used by Anthropic [36], where the constitution in which values are grounded is necessarily limited by the imagination of its authors. The main problem with most current alignment methods, however, is their logical structure, where a “wild” but highly capable base model is trained on huge amounts of data, and then “civilized” using feedback from trained evaluation models or self-generated data. This is like trying to civilize an adult human who was raised by wolves – actually worse, because the wild model, unlike a wolf-man, already has vast knowledge and generative capabilities. It is possible – even likely – that this post-facto alignment will leave a lot of harmful atavistic tendencies latent in the agent, which may emerge unexpectedly in the field. Thus, the alignment produced by current methods is typically brittle because it is superficial – a thin veneer of civility over a wild, toxic nature that can be unmasked by jail-breaking methods [79], [80]. An alternative approach to making AI agents safer is to not assume complete alignment, but to detect misalignment and to mitigate it. A useful tool for this is mechanistic analysis , which tries to look at the emergent internal representations in deep models, and to determine if they are meaningful, interpretable, and robust [81]–[84]. This can potentially be used to explain how the model is making its decisions, and thus validate its acceptability. While very useful for understanding how deep networks work, this approach can only provide indirect and incomplete information on alignment. It is also likely to become infeasible in extremely large models with complex internal dynamics, emergent behaviors, self-motivation, and autonomous continual learning. Another mitigation-based approach is to use strategies in-volving continuous monitoring, assessment, and control [43]– [45]. However, these methods still rely on human anticipation of all possible failure modes and detection of failures using explicit observers and built-in verifiers. Thus, they too are 6unlikely to scale to AGI that is capable of out-thinking humans, or to be useful for agents operating in the real world beyond constant human supervision and intervention. Given all these issues, how can robust bounded alignment be achieved in AGI agents? The next section proposes a possible approach for this. VII. A N ATURAL APPROACH TO ALIGNMENT 

A. Principles for Robust Bounded Alignment 

The argument made in this paper is that the following principles can serve as the basis of building AGI agents with the kind of robust bounded alignment seen in humans and trained animals: 1) AGI agents should be designed in ways that allow them and humans to have adequate theories of mind for each other. These can then serve as the basis of mutual com-munication, comprehension, cooperation, and instruction [85], [86], leading naturally to easier alignment and greater likelihood of corrigibility (tolerance for instruc-tion and correction) [87]. 2) AGI agents should be given innate characteristics that make them inherently amenable to alignment, possi-bly including immutable, built-in features that do not compromise P-attributes too much but limit or expose misalignment before it becomes dangerous. 3) Ethical principles should be part of the learning regimen of AGI agents from the beginning rather than being imposed after a base model is trained, thus embedding values deeply into the fabric of the agent’s mind. While there may be many ways to instantiate these prin-ciples, an obvious approach is to make AGI agents more like biological agents in their functional architecture, informa-tion processing mechanisms, representational frameworks, and drives. This will make them intrinsically less alien, and thus allow them to be analyzed, understood, and trained in more familiar ways, leading to greater safety and trust. It would also allow insights from biology, neuroscience, cognitive science, developmental psychology, and even economics and philoso-phy, to be applied more readily to AGI agents. For example, this could make it easier to define innate alignability traits by analogy with humans. The idea of building a more brain-like AI is a central tenet of the emerging field of NeuroAI [88]. A very detailed and insightful study of how NeuroAI methods can be used explicitly to enhance AI safety has recently been published by Mineault et al. [89], and the approach deserves serious consideration. While the methods proposed in the study are theoretically compelling, they seem rather ambitious for the current state-of-the art in both neuroscience and machine learning, e.g., building detailed embodied digital twins of animals and humans, building foundation models for the brain, and fine-tuning AI systems on large amounts of data from the brain. These are becoming feasible for simpler animals like the fruit fly [90], but scaling them up to humans – or even rodents – will be prohibitively difficult (though see [91], [92]). However, the vision behind NeuroAI can be applied at a more abstract level to constrain AGI agents to be more congruent 

with natural ones, allowing the agents and humans to have theories of mind for each other and lead naturally to greater alignability. A key point here is that, in making AGI agents more natural, it will be important to look not only at brains and bodies, but also at the evolutionary and developmental processes that make natural intelligent agents what they are in their nature – an approach that may be called EvoDevoNeuroAI 

or BioAI rather than just NeuroAI [93]. 

B. Developmental Value Learning 

A key component of this approach from the alignment perspective is to use a more developmental method for value learning. If the goal is to make AGI agents robustly aligned, this alignment must become part of their character, not some-thing added as an afterthought on a wild underlying disposition or provided in a reference document to tell the agent right from wrong. Agents must be given a conscience , and to do so, we should look at how humans acquire theirs. The answer is that humans learn values in stages from infancy to adulthood. At each stage, they learn values at a level commensurate with their mental and behavioral capabilities, and these values continue to constrain their choices even as their capabilities grow, e.g., an individual who learns to avoid tantrums as a child is likely to become a more responsible and thoughtful adult. Even a dog or a cat must be house-broken early before their behavioral choices become too complex. For example, prosocial behavior – a key component of alignment – emerges developmentally in humans from infancy to adulthood [94], becoming more complex as the individual’s capacities grow. The same is true of other character traits. This principle should also be used for AGI agents, training agents in stages of increasing perceptual, cognitive, and behavioral complexity, and integrating value learning at an appropriate level into the training process at all stages. The goal is to embed the trained values so deeply and inextricably into the agent’s mind that violating or erasing them becomes mentally intolerable for the agent. At the same time, it should be recognized that even the deep alignment produced by this process may fail under stress, or as a result of toxic influences, as it can in humans. VIII. C ONCLUSION 

It is likely, if not certain, that artificial agents with sig-nificant general intelligence will become feasible in the near future. The risks posed by such agents have been discussed extensively, but much of the recent methodological and policy discussion focuses on large, centralized systems hosted in datacenters controlled by humans – hence the emphasis on continuous testing, detection, control, and updating. Apart from the obvious “Skynet risk” such systems pose if left unconstrained, it just seems likelier that future AGI agents will be smaller, autonomous, self-contained entities loose in the physical world and cyberspace – millions (or billions) of extremely diverse new non-biological living entities added to the world’s ecosystem. They may connect to large AI 7models in the cloud like humans access the Internet to enhance their cognitive power, but they will have their independent minds. Expecting human testing and control to keep them in line will not be feasible, and adding kill-switch mechanisms will be both counterproductive and futile. Unlike all complex artifacts mass produced through engineering, e.g., computers and aircraft, factory settings and tests will be no guarantee of future behavior because the agents will adapt continuously, interact with other human and AGI agents, and change over time. These will be the first major human technology to which the classic engineering paradigm will be fundamentally inapplicable. Given all this, some of the questions the AI community should think about to make the AGI of the future as safe as possible are the following: 1) How should alignment be defined and analyzed in agents with alternative intelligences featuring radically different affordance spaces and internal representations than those of humans? 2) What system archi-tectures, computational mechanisms, and learning processes should be used in AGI agents to make them more inherently alignable? 3) Can we define a finite (if growing) set of well-understood canonical models for AGI agents – like species or genera in animals – rather than an uncontrolled diversity of distinct models? 4) How can we enable AGI agents and humans to have viable theories of mind for each other, and is an LLM-like approach appropriate for this? 5) What charac-teristics must explicitly be avoided in building AGI models, and why? and 6) What useful innate values can feasibly be built into AGI agents, and how? In the end, AGI agents will not be completely alignable with humans – and perhaps not even with each other. The best that can be hoped for is to have mechanisms for being well-behaved by mutual consent, which is the assumption we make naturally in dealing with other humans and animals. Even so, any alignment that is achieved could still fail as the agent or its environment changes over time, so living with AGI will be a continuous process of learning and accommodation between humans and AGI agents. And if AGI agents do become significantly more intelligent than humans, it may no more be possible for us to constrain their attitudes than chimpanzees can constrain ours. They may turn out to be benevolent towards humans, or not. We must either accept to live with this risk or hope that true AGI never comes to pass. REFERENCES [1] OpenAI and Josh Achiam et al. GPT-4 technical report. 

arXiv:2303.08774 , 2023. [2] Gemini Team and Rohan Anil et al. Gemini: A family of highly capable multimodal models. arXiv:2312.11805 , 2024. [3] Anthropic. The Claude 3 model family: Opus, sonnet, haiku. Technical report, Anthropic AI, 2024. [4] Aaron Grattafiori et al. The Llama 3 herd of models. arXiv:2407.21783 ,2024. [5] DeepSeek AI and Aixin Liu et al. DeepSeek-V3 technical report. 

arXiv:2412.19437 , 2024. [6] Haolin Chen et al. Language models are hidden reasoners: Unlocking latent reasoning capabilities via self-rewarding. arXiv:2411.04282 , 2024. [7] Violet Xiang et al. Towards system 2 reasoning in LLMs: learning how to think with meta chain-of-thought. arXiv:2501.04682 , 2025. [8] DeepSeek-AI and Daya Guo et al. DeepSeek-R1: Incentivizing reason-ing capability in LLMs via reinforcement learning. arXiv:2501.12948 ,2025. [9] Bang Liu et al. Advances and challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe systems. arXiv:2504.01990 , 2025. [10] Chunyuan Li et al. Multimodal foundation models: From specialists to general-purpose assistants. arXiv:2309.10020 , 2023. [11] Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Foundations and trends in multimodal machine learning: Principles, challenges, and open questions. arXiv:2209.03430 , 2023. [12] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models. 

arXiv:2306.13549 , 2023. [13] Yizhang Jin et al. Efficient multimodal large language models: A survey. 

arXiv:2405.10739 , 2024. [14] Jiaqi Wang et al. A comprehensive review of multimodal large language models: Performance and challenges across different tasks. 

arXiv:2408.01319 , 2024. [15] Joon Sung Park et al. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology , 2023. [16] Yonadav Shavit et al. Practices for governing agentic AI systems. 

OpenAI White Paper , 2023. [17] Julia Wiesinger, Patrick Marlow, and Vladimir Vuskovic. Agents. 

Google White Paper , 2025. [18] Kevin Black et al. π0: A vision-language-action flow model for general robot control. arXiv:2410.24164 , 2024. [19] Ilija Radosavovic et al. Humanoid locomotion as next token predic-tion. In The Thirty-eighth Annual Conference on Neural Information Processing Systems , 2024. [20] Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, and Irwin King. A survey on vision-language-action models for embodied ai. 

arXiv:2405.14093 , 2024. [21] Ben Pace. Debate on instrumental convergence between LeCun, Russell, Bengio, Zador, and more, October 2019. [22] Stephen M. Omohundro. The nature of self-improving artificial intelli-gence, January 2008. [23] Eliezer Yudkowsky. Artificial intelligence as a positive and negative factor in global risk. In Nick Bostrom and Milan M. Cirkovic, editors, 

Global Catastrophic Risks . Oxford University Press, 2008. [24] Nick Bostrom. The superintelligent will: Motivation and instrumental rationality in advanced artificial agents. Minds and Machines , 22, 2012. [25] Eliza Strickland and Glenn Zorpette. The AI apocalypse: A scorecard. 

IEEE Spectrum , June 2023. [26] Joseph Carlsmith. Is power-seeking AI an existential risk? 

arXiv:2206.13353 , 2024. [27] Yoshua Bengio et al. Superintelligent agents pose catastrophic risks: Can scientist AI offer a safer path? arXiv:2502.15657 , 2025. [28] Jan Leike, John Schulman, and Jeffrey Wu. Our approach to alignment research, 2022. [29] Iason Gabriel et al. The ethics of advanced ai assistants. 

arXiv:2404.16244 , 2024. [30] Herbert Simon. A behavioral model of rational choice. Quarterly Journal of Economics , 69:99–118, 1955. [31] Brian Christian. The Alignment Problem: Machine Learning and Human Values . Norton, 2020. [32] Richard Ngo, Lawrence Chan, and S¨ oren Mindermann. The alignment problem from a deep learning perspective. arXiv:2209.00626 , 2025. [33] Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Neural Information Processing Systems (NeurIPS 2017) ,2017. [34] Long Ouyang et al. Training language models to follow instructions with human feedback. arXiv:2203.02155 , 2022. [35] Yuntao Bai et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv:2204.05862 , 2022. [36] Yuntao Bai et al. Constitutional AI: Harmlessness from AI feedback. 

arXiv:2212.08073 , 2022. [37] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christo-pher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv:2305.18290 ,2024. 

8[38] Melody Y. Guan et al. Deliberative alignment: Reasoning enables safer language models. arXiv:2412.16339 , 2025. [39] Xiangyu Qi et al. Safety alignment should be made more than just a few tokens deep. In Proceedings of ICLR 2025 , 2025. [40] Stuart Russell. Human Compatible: Artificial Intelligence and the Problem of Control . Penguin Books, 2019. [41] Mantas Mazeika et al. Utility engineering: Analyzing and controlling emergent value systems in AIs. arXiv:2502.08640 , 2025. [42] Isaac Asimov. Runaround. In I, Robot . Doubleday, 1950. [43] David ”davidad” Dalrymple et al. Towards guaranteed safe ai: A frame-work for ensuring robust and reliable AI systems. arXiv:2405.06624 ,2024. [44] Ryan Greenblatt, Buck Shlegeris, Kshitij Sachan, and Fabien Roger. AI control: Improving safety despite intentional subversion. 

arXiv:2312.06942 , 2024. [45] Aryan Bhatt et al. Ctrl-Z: Controlling AI agents via resampling. 

arXiv:2504.10374 , 2025. [46] Nick Bostrom. Superintelligence: Paths, Dangers, Strategies . Oxford University Press, 2014. [47] Alan M. Turing. I.—Computing Machinery and Intelligence. Mind ,LIX(236):433–460, 10 1950. [48] Ben Goertzel. Artificial general intelligence: Concept, state of the art, and future prospects. Journal of Artificial General Intelligence , 1, 2014. [49] Meredith Ringel Morris et al. Levels of AGI: Operationalizing progress on the path to AGI, 2024. [50] Kevin J. Mitchell. Free Agents: How Evolution Gave Us Free Will .Princeton University Press, 2023. [51] Rowan Jacobsen. Brains are not required when it comes to thinking and solving problems—simple cells can do it. Scientific American , February 2024. [52] Gabriella Seifert, Ava Sealander, Sarah Marzen, and Michael Levin. From reinforcement learning to agency: Frameworks for understanding basal cognition. Biosystems , 235:105107, 2024. [53] Richard Sutton and Andrew J. Barto. Reinforcement Learning . MIT Press, 1998. [54] Nikolai A. Bernstein. The Co-ordination and Regulation of Movements .Pergamon Press, 1967. [55] Pietro Morasso. A vexing question in motor control: The degrees of freedom problem. Frontiers in Bioengineering and Biotechnology ,9:article 783501, 2022. [56] Josh Merel, Matthew Botvinick, and Greg Wayne. Hierarchical motor control in mammals and machines. Nature Communications , 10:article 5489, 2019. [57] Nima Dehghani. Theoretical principles of multiscale spatiotemporal control of neuronal networks: A complex systems perspective. Frontiers in Computational Neuroscience , 12, 2018. [58] Roland Kays et al. Multi-scale movement syndromes for comparative analyses of animal movement patterns. Movement Ecology , 11:article 61, 2023. [59] Markus Kiefer and Friedemann Pulverm¨ uller. Conceptual representa-tions in mind and brain: Theoretical developments, current evidence and future directions. Cortex , 48:805–825, 2012. [60] Anthony Chemero. An outline of a theory of affordances. Biological Psychology , 15:181–195, 2003. [61] James J. Gibson. The theory of affordances. In R. Shaw and J. Brans-ford, editors, Perceiving, Acting, and Knowing: Toward an Ecological Psychology , pages 67–82. Routledge, 1977. [62] David Ha and J¨ urgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances in Neural Information Processing Systems 31 , page 5379–5390. Curran Associates, 2018. [63] C. Daniel Freeman, Luke Metz, and David Ha. Learning to predict without looking ahead: World models without forward prediction. In Ad-vances in Neural Information Processing Systems 33 , page 2451–2463. Curran Associates, 2019. [64] Lunjun Zhang, Ge Yang, and Bradley Stadie. World model as a graph: Learning latent landmarks for planning. In Proceedings of ICLR 2021 ,2021. [65] Karl Friston, Rosalyn J. Moran, Yukie Nagai, Tadahiro Taniguchi, Hi-roaki Gomi, and Josh Tenenbaum. World model learning and inference. 

Neural Networks , 144:573–590, 2021. [66] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv:2301.04104 ,2024. [67] Yann LeCun. A path towards autonomous machine intelligence, version 0.9.2, 2022-06-27. OpenReview , 2022. [68] Alan Baddeley. Working memory: looking back and looking forward. 

Nature Reviews Neuroscience , 4:829–839, 2003. [69] Edgar Y. Walker et al. Inception loops discover what excites neurons most using deep predictive models. Nature Neuroscience , 20:2260–2265, 2024. [70] Eghbal Hosseini, Colton Casto, Noga Zaslavsky, Colin Conwell, Mark Richardson, and Evelina Fedorenko. Universality of representation in biological and artificial neural networks. bioRxiv , 2024. [71] Ariel Goldstein et al. Alignment of brain embeddings and artificial contextual embeddings in natural language points to common geometric patterns. Nature Communications , 15:2768, 2024. [72] Daniel Kahneman. Thinking Fast and Slow . Penguin Books, 2011. [73] Luyang Liu, Jonas Pfeiffer, Jiaxing Wu, Jun Xie, and Arthur Szlam. Deliberation in latent space via differentiable cache augmentation. 

arXiv:2412.17747 , 2024. [74] Kaige Xie, Ian Yang, John Gunerli, and Mark Riedl. Making large language models into world models with precondition and effect knowl-edge. arXiv:2409.12278 , 2024. [75] Maxence Faldor, Jenny Zhang, Antoine Cully, and Jeff Clune. OMNI-EPIC: Open-endedness via models of human notions of interestingness with environments programmed in code. arXiv:2405.15568 , 2024. [76] Yuval Noah Harari. Nexus: A Brief History of Information Networks from the Stone Age to AI . Random House, 2024. [77] Thomas Nagel. What is it like to be a bat? The Philosophical Review ,83:435–450, 1974. [78] Giacomo Rizzolatti and Laila Craighero. The mirror-neuron system. 

Annual Review of Neuroscience , 27:169–192, 2004. [79] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv:2307.15043 , 2023. [80] Cem Anil et al. Many-shot jailbreaking. In Advances in Neural Information Processing Systems , volume 37, pages 129696–129742, 2024. [81] Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical nlp pipeline. In ACL 2019 , 2019. [82] Eghbal Hosseini, Noga Zaslavsky, Colton Casto, and Evelina Fedorenko. Understanding deep image representations by inverting them. In 2023 Conference on Cognitive Computational Neuroscience , 2023. [83] Ilia Sucholutsky et al. Getting aligned on representational alignment. 

arXiv:2310.13018 , 2024. [84] Robert Huben, Hoagy Cunningham, Logan Riggs Smith, Aidan Ewart, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. In The Twelfth International Conference on Learning Representations , 2024. [85] Jessica Williams, Stephen M. Fiore, and Florian Jentsch. Supporting artificial social intelligence with theory of mind. Frontiers in Artificial Intelligence , 5, 2022. [86] Pranav Gupta, Thuy Ngoc Nguyen Cleotilde Gonzalez, and Anita Williams Woolley. Fostering collective intelligence in human–AI collaboration: Laying the groundwork for COHUMAIN. Topics in Cognitive Science , 2025. [87] Nate Soares, Benja Fallerstein, Eliezer Yudkowsky, and Stuart Arm-strong. Corrigibility. In 2015 AAAI Workshop on Artificial Intelligence and Ethics , 2015. [88] Anthony Zador et al. Catalyzing next-generation artificial intelligence through NeuroAI. Nature Communications , 14:1597, 2023. [89] Patrick Mineault et al. NeuroAI for AI safety. arXiv:2411.18526 , 2025. [90] Kenichi Iwasaki, Charles Neuhauser, Chris Stokes, and Aleksandr Rayshubskiy. The fruit fly, Drosophila melanogaster, as a micro-robotics platform. Proceedings of the National Academy of Sciences ,122(15):e2426180122, 2025. [91] Mackenzie Weygandt Mathis, Adriana Perez Rotondo, Edward F. Chang, Andreas S. Tolias, and Alexander Mathis. Decoding the brain: From neural representations to mechanistic models. Cell , 187(21):5814–5832, 2024. [92] Yizi Zhang et al. Neural encoding and decoding at scale. 

arXiv:2504.08201 , 2025. [93] Ali A. Minai. Deep intelligence: What AI should learn from nature’s imagination. Cognitive Computation , 16:2389–2404, 2024. [94] Keith Jensen, Amrisha Vaish, and Marco F. H. Schmidt. The emergence of human prosociality: aligning with others through feelings, concerns, and norms. Frontiers in Psychology , 5:822, 2014. 

9
