Title: XARP Tools: An Extended Reality Platform for Humans and AI Agents

URL Source: http://arxiv.org/pdf/2508.04108v3

Published Time: Wed, 24 Sep 2025 00:45:25 GMT

Markdown Content:
# XARP: A Human-First and AI Agent-Ready Extended Reality Toolkit 

# Arthur Caetano 

caetano@ucsb.edu University of California Santa Barbara, CA, USA 

# Radha Kumaran 

rkumaran@ucsb.edu University of California Santa Barbara, CA, USA 

# Kelvin Jou 

kelvinjou@ucsb.edu University of California Santa Barbara, CA, USA 

# Tobias HÃ¶llerer 

holl@ucsb.edu University of California Santa Barbara, CA, USA 

# Misha Sra 

sra@ucsb.edu University of California Santa Barbara, CA, USA 

## Abstract 

Artificial intelligence (AI) and extended reality (XR) are increasingly combined in applications such as motor skill training, personal-ized feedback, and embodied task guidance. Yet developing AI-XR systems remains challenging due to fragmented toolchains that push developers into ad hoc integrations, diverting their atten-tion away from essential design concerns such as interactivity and context awareness. To address this issue, we present XARP (XR Agent-ready Remote Procedures), a toolkit for AI-XR development designed for both human developers and AI agents. XARP imple-ments JSON-based remote procedure calls that allow server-side Python to control XR clients, providing a high-level abstraction over low-level integration details. Humans can use XARP as a Python library to write XR applications with reduced implementation over-head. AI agents operate with the same abstraction to dynamically call tools to generate XR applications at runtime in response to context changes and user requests. XARP offers Model Context Pro-tocol (MCP) connectivity that allows third-party agents and tools to leverage XR capabilities, previously unavailable. We conducted three case studies that demonstrate XARP supports a variety of AI-XR applications, including AI-guided fencing, drone assistance, and room layout design. We evaluated XARP in a walkthrough study with 24 AI and XR developers. UTAUT scores indicate high potential for adoption, and participants reported that XARP can reduce authoring time, lower entry barriers for developers unfamil-iar with AI or XR, and enable the implementation of novel AI-XR systems. 

## Keywords 

Toolkit, XR, AI, Agents, Model Context Protocol 

## 1 Introduction 

Extended reality (XR) systems increasingly integrate artificial intelli-gence (AI) to support task execution and skill acquisition in dynamic spatial environments. Recent AI-XR systems, for example, guide users through motor tasks with step-by-step instructions [ 31 , 47 ]and provide personalized sports performance feedback [ 34 , 36 ]. Yet designing such systems is challenging because current tools overburden developers with implementation details that divert at-tention from core design decisions of the AI-XR domain [ 25 , 37 ]. Existing toolchains lack abstractions that bridge the heterogeneous demands of realtime interactivity on lightweight devices and large-scale inference on GPU clusters. This gap increases the risk of ad hoc integrations across rendering engines and AI services, making systems fragile to the rapid turnover of SDKs and frameworks in this domain. As a result, implementation and deployment become laborious and error-prone, draining resources that could instead address essential design challenges of AI-XR systems [ 5, 6 ], such as user interaction and contextual understanding in unseen environ-ments [ 10 , 54 ]. Higher-level abstractions that unify access to XR and AI capabilities could reduce this burden, enabling developers to redirect effort toward rigorous evaluation and innovative AI-XR designs. In this work, we present XARP (pronounced â€œ sharp â€â€” XR Agent-ready Remote Procedures), a toolkit that lowers the development burden of AI-XR development by providing a unified interface for both human developers and AI agents. Informed by design patterns distilled from recent AI-XR literature, XARP abstracts device-specific details into a high-level API delivered through a server-side Python library and platform-specific XR clients. The library issues JSON-based remote procedure calls over WebSocket channels, which clients translate into low-latency, device-level func-tions for realtime interaction. XARP supports three complementary modes of use: (i) as a server-side library that abstracts XR front-end interfaces for backend developers; (ii) as a set of callable tools that AI agents can compose to generate end-user applications on the fly; and (iii) as a Model Context Protocol (MCP) server that exposes XR functionalities to third-party AI services, which is a capability not previously available. Together, these modes make XR environments directly accessible to AI systems while streamlining development across diverse platforms. Consider a fitness application built with XARP. Developers can call high-level XR functions to guide users through calibration, as-sess available training space, and track skill level through body pose estimation. At runtime, AI agents can reuse these functions to adapt exercises dynamically based on user performance, environmental changes, or new user preferences. Through MCP integration, third-party providers could plug in AI coaches with distinct training styles that seamlessly extend the application. Unlike existing so-lutions [ 20 , 31 , 35 , 54 ], XARPâ€™s unified abstraction simultaneously reduces the implementation burden on developers and enables them to delegate design decisions to AI agents at runtime, trading spec-ification effort for adaptive, dynamic behavior. XARP is the first 

> arXiv:2508.04108v3 [cs.HC] 23 Sep 2025 Caetano et al.

Figure 1: Toolkit Architecture: XARP provides a unified abstraction for XR, accessible to both humans and AI agents. The abstraction is implemented as a server-side Python API that issues JSON RPCs over Websockets. Clients translate server calls into platform-specific XR functionalities. Developers can use XARP as a library with BaseXRApp or create agents with 

BaseXRAgent . Agents call methods from XRApp as tools, effectively using the same abstraction humans do. MCP connectivity is supported via CLI or with BaseXRMCP . We offer reference clients for Meta Quest and Web, with a server built with smolagents, fastmcp, and LMStudio. 

system to expose XR capabilities through MCP, simplifying AI-XR integration with an open standard. To demonstrate the feasibility of XARP and its ability to sup-port diverse AI-XR applications, we conducted three case studies: AI-guided fencing, drone assistance, and room layout design. De-sign reflections after each case study helped align XARP with how developers actually build AI-XR systems. These studies surfaced recurring challenges and common development roles, motivating features such as persistent interaction logs visualized in a chat for-mat, rapid prototyping via a web client, and separation of concerns 

in team workflows through clear boundaries between front- and back-end experts. We evaluated adoption potential through a video walkthrough study with 24 AI and XR developers. Participants watched step-by-step demonstrations of seven tasks interleaved with comprehension checks. The tasks covered the toolkit setup, extension, and all three modes of use. Afterward, they completed a survey based on established toolkit evaluation goals [ 28 ] and the UTAUT technology adoption scale [ 48 ]. In open-ended responses, participants described when they would or would not use XARP and posed questions that remained after the walkthrough. Results indicate that participantsâ€™ intention to adopt XARP was caused by a positive perception of performance, particularly in reducing authoring time, empowering new audiences, and enabling replication of existing solutions. Our study also suggests partici-pants perceived less value in the paths of least resistance created by XARP, and the lack of facilitating conditions, such as documen-tation and support channels, might hinder the intention to adopt our toolkit. Overall, this work contributes: (1) XARP , a toolkit that provides a unified abstraction for AI-XR development, designed for both human develop-ers and AI agents. XARP is open source and available at https://github.com/HAL-UCSB/xarp. (2) Evaluation of adoption potential and perceived value 

through a video walkthrough study with 24 developers, using established toolkit evaluation goals and the UTAUT adoption scale. (3) Case studies and Design Pattern that demonstrate the feasibility of our toolkit and inform the design of future AI-XR toolkits. 

## 2 Toolkit Architecture 

XARP is organized around three architectural pillars: backendâ€“frontend separation, high-level XR functions, and AI agent integration. To-gether, these pillars enable XARP to abstract device-specific com-plexity while supporting both developer- and agent-driven work-flows. fig. 1 illustrates the current architecture. 

## 2.1 Backend and Frontend Separation 

XARP adopts a clear architectural separation between backend application logic and frontend XR clients, enabling flexible deploy-ment and scalable team collaboration. This separation allows devel-opers to iterate on backend logic without rebuilding or redeploy-ing client applications, accelerating experimentation and reducing development overhead. The architecture also supports modular, role-specific workflows aligned with Conwayâ€™s Law [ 13 ], which suggests that system designs tend to mirror the communication structures of the organizations that produce them. Teams can divide responsibilities across XR specialists, Python developers, and AI XARP: A Human-First and AI Agent-Ready Extended Reality Toolkit 

engineers, each working within their domain without conflicting dependencies. This architecture design enables multi-platform de-ployment, where any XR client that implements the required set of primitive tools can connect to the same backend application. This architecture also supports multi-client experiences, where different clients connect to the same backend app and share data with a session key. This is useful to support copresence, with multiple users connected from different devices. Another advantage of this design is the cross-device synergy, e.g., pairing a VR headset with a mobile device to enable efficient text entry or multiview video. 

## 2.2 High-level XR Functions 

XARP abstracts common XR features into high-level functions in-formed by our design pattern elicitation study section 4. By stan-dardizing frequently used patterns as pre-built components, the system reduces the need for low-level, engine-specific program-ming and lowers the barrier to entry for developers with limited XR experience. These functions can be invoked through simple Python calls, enabling rapid prototyping and expanding access to AI-XR development beyond XR engine specialists. Standardization also supports reuse and replication of successful solutions across projects and teams. fig. 1 shows the XR functionalities currently available in XARP through the AsyncXRApp class. 

## 2.3 AI Agent Integration 

XARP enables AI agents to engage directly in XR experiences be-yond coding assistance or asset generation, as active participants during runtime. Agents gain access to the same high-level XR functions available to human developers, exposed as callable tools through a shared abstraction layer. To support generalization across platforms, XARP includes a capability discovery mechanism where each connected XR client declares its supported tools to the server, allowing AI agents to adapt their behavior dynamically based on different client capabilities. This mechanism can potentially en-able the same application logic to operate across heterogeneous devices, from high-end XR headsets to mobile AR platforms. By supporting runtime behavior generation, XARP reduces the need for complete system specification at design time with AI-driven adaptiveness. The Model Context Protocol (MCP) [ 2] is an open standard for connecting AI agents to external resources, executable tools, and reusable prompts. While MCP introduces challenges such as stateful session management and external system dependencies, it represents an important step toward standardized AI integration. XARP is the first toolkit to expose XR functionalities via MCP, al-lowing third-party agents to interact seamlessly with users in XR. This capability expands the design space for AI-XR applications and contributes to the convergence of these ecosystems. 

## 2.4 Implementation Details 

The XARP backend is centered around a class that encapsulates WebSocket connections to XR clients and exposes both synchro-nous and asynchronous functions for issuing remote procedure calls for executing XR functions on the client. Messages between server and client follow a JSON-based application protocol over WebSockets. On the frontend, incoming JSON messages are routed through a dispatcher to a factory system, which retrieves singleton executors or instantiates new ones responsible for carrying out XR function calls. While a client executes a function, the backend asynchronously awaits return values or acknowledgment signals, including error codes when no specific return value is required. All messages exchanged between server and client are stored in a chat-log format to support human analysis and integration with conversational AI systems. This architecture enables a turn-based interaction model aligned with agent-driven, chat-like workflows. XARP currently includes two client implementations: (1) a Meta Quest client built in Unity and (2) a Python-based Web client de-veloped with Streamlit, which inspired our systemâ€™s client-server design. The architecture supports extensibility through new remote procedures on the backend paired with corresponding client-side executors. Our reference implementation uses FastAPI as the web server, Hugging Face Smolagents for AI agents, and FastMCP for MCP integration. Language models were run locally using LMStu-dio with Google Gemma-3-27B and OpenAI GPT-OSS. XARP offers wide compatibility with other agentic frameworks through callable tools and MCP. The reference implementation currently uses a local filesystem for storage and authorization, though more robust in-frastructure is recommended for production deployments. XARP is open source and available at https://github.com/HAL-UCSB/xarp. 

## 3 Related Work 

Drawing from seminal work in software engineering, we divide the challenges in AI-XR development into essential and accidental complexity [ 25 , 37 ]. Essential complexity refers to the intrinsic dif-ficulty of conceptualizing a solution, while accidental complexity 

arises from translating that solution into a concrete implementa-tion using the available tools and processes [ 25 , 37 ]. Prior work has tackled both forms of complexity through toolkits, asset generation pipelines, and adaptive systems. 

## 3.1 Reducing Accidental Complexity in AI-XR 

To reduce accidental complexity, vendors integrate their XR plat-forms with AI services, such as Android XR with Gemini and Snap OS with Lens Studio AI. While these solutions lower development friction within proprietary ecosystems, they introduce vendor lock-in. Unity Sentis mitigates this by relying on the ONNX open stan-dard to enable on-device AI, though execution remains bound to the Unity engine. In a different strategy, AI has been used to reduce accidental complexity by automating code generation [ 24 , 40 , 49 ]and accelerating asset creation [ 26 , 52 ]. But these approaches ad-dress only a narrow part of the entire development pipeline and require manual effort to integrate output into XR applications. Spe-cialized middleware such as Estuary [ 35 ], CUIfy [ 8], and NVIDIA ACE further reduce authoring effort costs for conversational em-bodied avatars, but are not designed for general-purpose AI-XR development. While overall these approaches reduce development effort at specific stages such as coding, asset creation, or platform-specific deployment, they do not provide a general-purpose abstraction that unifies XR capabilities for both human developers and AI agents. XARP addresses this gap by exposing high-level functions through a shared interface, simplifying integration across toolchains and supporting more flexible, agent-driven development workflows. Caetano et al. 

## 3.2 Managing Essential Complexity in AI-XR 

Efforts to manage essential complexity have followed two main strategies: design toolkits and adaptive systems. Design toolkits support early-stage analysis and prototyping [ 10 , 55 ], but alone do not produce deployable solutions. Adaptive systems shift decisions from specification time to runtime, allowing systems to respond to the concrete context of use[ 18 , 33 ]. However, most adaptive XR systems remain limited to spatial layout and interface adjustments. More recent systems aim to bridge the gap between code, assets, and deployable XR applications, enabling runtime generation of end-user experiences [ 14 , 20 , 31 ]. Although these approaches can manage essential complexity by deferring design decisions to run-time, they are typically implemented as standalone systems tailored to specific use cases rather than general-purpose toolkits. An excep-tion is XaiR [ 54 ], which offers a pathway for building new adaptive systems but does not yet provide a high-level abstraction usable by both developers and AI agents. XARP complements prior efforts in managing essential complex-ity in AI-XR, offering a novel XR abstraction that is accessible to humans as high-level functions and to agents as callable tools. This allows designers to prototype flexible behaviors without hardcod-ing interaction logic, and enables AI agents to generate runtime actions in response to realtime context. By supporting integration with agentic frameworks and emerging standards like MCP, XARP expands toolkit-level support from authoring to execution, making it possible to manage essential complexity across both design and runtime phases. 

## 4 Design Patterns from Literature 

Inspired by prior work on design pattern elicitation in HCI and software engineering [ 4, 43 , 50 ], we derived recurring interaction patterns from a review of recent AIâ€“XR systems. Our process in-volved: (1) selecting a representative set of published systems across academic and industry sources; (2) identifying common functional features and user-agent interactions; (3) abstracting each feature into a design pattern described in terms of context, problem, and solution; and (4) iteratively refining the patterns. We analyzed 18 recent AI-XR systems published between 2023â€“2025, drawn from academic venues (ISMAR, IEEE VR, CHI, TVCG, UIST). All selected works included a combination of keywords related to AI (e.g., Intelligent, Large Language Models) and XR (e.g., Augmented, Virtual, Mixed) in the title. For each system, we extracted functional components that described how XR environments were manipu-lated, how agents interacted with users, and how runtime behavior was adapted or generated. Using inductive thematic coding, we grouped the codes into recurring patterns of interaction. Two au-thors independently coded each system and resolved discrepancies through discussion. While individual patterns varied in form, they consistently aligned with four higher-level functional stages observed across systems: 

multimodal user input , contextual sensing , contextual data processing ,and adaptive output and feedback . These stages served as organiz-ing principles for XARPâ€™s high-level function set. By implementing these functions as callable tools, XARP enables both human de-velopers and AI agents to compose interactive behaviors at the 

Figure 2: Three in-house case studies used to surface require-ments for AI-XR development: (A.) Fencing coach shows a camera capturing user actions through pose tracking to in-form AI feedback on technique; (B.) Drone assistant shows the system identifying obstacles in the environment to sup-port AI planning of clear drone navigation paths; (C.) Room layout shows generating a 3D model of the space to enable AI verification of furniture arrangements. Insights from these scenarios informed the design of new XARP functions for sensor integration, environment reconstruction, and spatial feedback. 

appropriate level of abstraction. fig. 1 shows the XR functions cur-rently implemented in XARP through the AsyncXRApp class. The four design patterns are: 

Multimodal User Input. AI-XR systems must reliably capture user input that can be processed in real time by the application XARP: A Human-First and AI Agent-Ready Extended Reality Toolkit 

or delegated to an AI agent. Across prior work, three modalities recur most frequently: hand and gesture-based interaction [ 1 , 3, 15 ,16 , 22 , 23 , 29 , 41 , 44 , 56 ], eye and head gaze [ 7 , 11 , 12 , 22 , 29 , 30 ,51 , 56 ], and speech input [ 1 , 3 , 23 , 30 , 31 , 47 ]. Implementing these modalities typically requires juggling platform-specific SDKs and device APIs, creating fragile pipelines that complicate integration with AI models. XARP abstracts these recurring input modalities into a unified set of high-level functions ( read , hand_pointer ), exposing them both to developers and to AI agents. This removes the need for low-level input handling while ensuring that agents can flexibly access and combine gesture, gaze, and speech channels at runtime. 

Contextual Sensing. Most intelligent XR systems aim to support users beyond executing explicit commands, and a key enabler of this functionality is providing AI models or agents with additional contextual information that can be used to predict user needs and offer proactive assistance. Common sources of such context include user position in the environment [ 3 , 11 , 23 , 33 , 47 ], RGB frames or continuous video streams from the device camera [ 3 , 11 , 12 , 33 , 41 ], and depth information aligned with those frames [ 3, 11 , 33 , 44 , 47 ,56 ]. Implementing these inputs often requires stitching together fragmented APIs and sensor pipelines, which increases engineering complexity and makes it difficult for AI agents to access context in a consistent format. XARP abstracts these recurring sources of context into a unified set of functions ( see , depth , head_pose ), providing both developers and AI agents with device-agnostic access to positional tracking, RGB streams, and depth information. This reduces the burden of managing low-level sensing details and enables agents to leverage contextual state for proactive, adaptive assistance. 

Contextual Data Processing. Spatially aware and intelligent XR applications use contextual information not only to capture raw data but also to process it into higher-level representations that enable interactivity or support user goals. For example, systems can recognize objects in the environment [ 16 ] and infer their function-ality, supporting tasks such as room layout rearrangement [ 1] and retrieval of information about shared objects in the space [ 53 ]. Other forms of context processing observed in prior work include measur-ing and modeling the 3D environment [ 3, 12 , 15 , 29 , 44 , 47 , 51 ], as well as generating new 3D objects based on user needs and applica-tion goals [ 1, 15 ]. While these capabilities expand the scope of AI-XR interaction, they often require combining multiple perception pipelines and inference services, creating complex dependencies that are difficult for developers to implement and for AI agents to reuse. XARP abstracts these recurring processing tasks into a high-level function mesh that exposes 3D reconstruction of the environ-ment with semantic labels in a consistent format. This reduces the overhead of accessing multiple perception services and enables AI agents to directly build on processed context for adaptive behaviors. 

Adaptive Output and Feedback. A fundamental component of interactive XR and AI systems is producing feedback that responds to user queries or commands, both to confirm whether actions were successfully executed and to convey additional information when needed. Prior work shows that such feedback spans multiple modal-ities. For non-spatial queries, responses are often delivered through audio [ 1, 7, 11 , 15 , 16 , 29 â€“ 31 , 47 ] or text [ 3 , 7, 11 , 16 , 22 , 31 , 44 , 47 ]. When the feedback relates directly to the spatial environment, 3D annotations such as arrows, highlights, or bounding boxes are more effective at conveying system state and guiding user action [ 1 , 7, 29 , 31 , 47 ]. Implementing these feedback channels typically requires a mix-and-match across separate XR toolkits and UI frameworks, complicating development and limiting how AI agents can dynamically tailor feedback to the situation. XARP provides a unified set of functions ( write , box , display_glb ,

avatar ) for system output and feedback, covering audio, text, and 3D annotations. This abstraction reduces the complexity of man-aging multiple feedback channels and enables AI agents to deliver adaptive, multimodal responses that align with both the userâ€™s query and the spatial context of the task. 

## 5 Case Studies 

We conducted three in-house case studies using XARP to probe toolkit limitations and examine the AI-XR development process. These scenarios revealed recurring challenges, common developer workflows, and role divisions, which guided subsequent system refinements. The fencing coach case study emphasized instruction and feedback on the userâ€™s motor actions. The drone assistance case study emphasized interaction with an embodied agent that paves the way toward robotic embodiments. The room layout case study emphasized virtual augmentations on physical space. Together, these case studies span essential elements of AI-XR design, i.e., the user, AI agent, and environment [10]. 

## 5.1 Fencing Coach 

Scenario focus. This case study explored how XARP could support AI-driven motor feedback in athletic training scenarios where ex-pert coaches are unavailable. Specifically, we developed an AI agent that provides sabre fencing feedback on off-the-line movements (the explosive opening phase of a bout). To ground the feedback in expert practices, we surveyed three fencing coaches to identify key biomechanical features relevant for evaluating form during this phase. 

Challenges and insights. Implementing the feedback system required capturing full-body pose data using sensors beyond the capabilities of standard XR headsets [ 32 ]. The integration of exter-nal RGB video for pose tracking and the calculation of joint angle features revealed two core development challenges: (1) the need for flexible sensor integration in XR environments, and (2) the impor-tance of comprehensive data logging to support both AI grounding and developer troubleshooting. In particular, enabling AI agents to generate grounded feedback using Retrieval-Augmented Genera-tion (RAG) workflows demanded persistent access to temporally indexed interaction traces. 

Impact on XARPâ€™s design. This case directly shaped XARPâ€™s support for sensor extensibility, leading to the addition of inter-faces for external RGB-based pose tracking, currently supported by the XARP web client. It also motivated the implementation of per-sistent logging support, including time-aligned interaction traces and system events stored in structured formats. These logs can Caetano et al. 

be later visualized in a chat-style interface, improving developer transparency and enabling post-hoc analysis for both AI grounding and user feedback review. 

## 5.2 Drone Assistant 

Scenario focus. This case study examined how XARP could sup-port embodied AI agents that operate across physical and virtual space. We developed a drone assistant prototype for an outdoor pick-and-place task, focusing on how XR interfaces can mediate user interaction with autonomous aerial agents. Initial develop-ment used a Wizard-of-Oz prototype to test user expectations and feedback modalities. These early iterations informed the implemen-tation of a high-fidelity version powered by XARP. 

Challenges and insights. 

Throughout three design iterations, end-users expected agents to understand gestures, speech, changes in the physical surroundings, and to provide visual feedforward of planned paths. To generate realistic flight behavior, we relied on AirSim [ 45 ]. Integrating this third-party component was a key challenge, as it uses Unreal as an underlying physics simulator, which brings a cascade of dependen-cies into the development workflow. 

Impact on XARPâ€™s design. In response, XARP was extended with callable tools for natural user interaction, drone avatar control, flight simulation, and environment reconstruction. These abstrac-tions exposed drone capabilities as composable primitives usable by both developers and AI agents. This case study demonstrates how a shared abstraction can simplify integration with complex ex-ternal tools and enable simulator-in-the-loop designs. The need for third-party simulator integration exemplifies the recurring need for integration of AI services outside the XR environment, which fur-ther motivates XARPâ€™s support for Model Context Protocol (MCP). 

## 5.3 Room Layout 

Scenario focus. This case study explored how XR can serve as a medium for collaborative spatial reasoning between users and co-located AI agents. We developed an AI assistant capable of ar-ranging furniture in indoor spaces such as offices and bedrooms, drawing on interior design heuristics to generate layout sugges-tions. In this scenario, we created a VR environment aligned with the basic architectural features of the room, but removing all the furniture. Our AI assistant generates 3D models of pieces of fur-niture using a text-to-3D MCP server 1. The AI assistant places the generated model in the room based on space availability and grounded in interior design heuristics. 

Challenges and insights. Supporting collaborative layout de-sign required that the AI agent understand both the semantics of the environment (e.g., object types and room function) and the userâ€™s referential actions, such as pointing gestures. Developers also needed tools to support dynamic, situated content updates, such as updating layout visualizations in real time as users reacted or provided feedback. This virtual reality case also poses poten-tial safety challenges due to the difficulty of maintaining spatial awareness when furniture currently in the physical space is not adequately represented virtually. 

> 1https://huggingface.co/spaces/hysts/Shap-E

Impact on XARPâ€™s design. This case led to the addition of high-level functions in XARP for pointing gesture detection, semantic scene understanding, and context-aware 3D content placement. These abstractions allowed AI agents to interpret user input in spatially grounded ways and to present dynamic responses that felt co-present and responsive. Together, these capabilities positioned XARP to support tightly coupled spatial collaboration between users and AI agents in shared XR environments. While the design patterns offered high-level insight into recur-ring functional components across AI-XR systems, the case studies grounded those abstractions in concrete development scenarios. In several cases, the challenges observed in implementation mirrored the needs identified through the patterns, reinforcing their rele-vance while revealing additional platform requirements at a finer level of detail. 

## 6 Walkthrough Study 

Our goal in this study was to evaluate the perceived utility and adop-tion potential of XARP. Rather than measuring implementation-specific usability or performance metrics, which could obscure the evaluation of our underlying design strategy [ 21 ], we focused on how developers understand and assess the benefits of XARP for the development of future AI-XR systems. To this end, we conducted video walkthrough demonstrations [ 17 , 28 , 42 ] with 24 potential users. This method mirrors how developers often engage with new tools by watching tutorials before deciding whether to adopt them. It also echoes a long-standing tradition of using demonstrations to convey system potential, as seen in Engelbartâ€™s 1968 â€œMother of All Demos.â€ After the walkthrough, participants completed a survey combining a standardized technology adoption scale [ 48 ]with a custom questionnaire inspired by common goals of HCI toolkits [ 28 ] to assess XARPâ€™s perceived value and potential for adoption in their own workflows. 

## 6.1 Participants 

We recruited 24 adults (ages 18â€“44; 14 male, 10 female) through social media and online AI-XR developer communities. Participants were geographically distributed in five different countries. Edu-cational backgrounds ranged from high school (n=3) to doctoral degrees (n=5), with most trained in computer science, AI, or engi-neering. Professional roles included industry (n=5), academia (n=8), and students (n=11). All participants had at least one year of expe-rience with either AI or XR. XR experience varied from none (n=3) to more than five years (n=5), with Meta Quest, iOS, and HoloLens cited as the most commonly used platforms. Unity with AR Foun-dation and the Mixed Reality Toolkit were frequently mentioned as development tools. In terms of AI experience, two participants reported no prior experience, while five had more than five years. Commonly cited platforms included the OpenAI API, PyTorch, and TensorFlow. 

## 6.2 Measures 

We assessed the potential acceptance of XARP using the Unified Theory of Acceptance and Use of Technology (UTAUT) scale [ 48 ], a widely used instrument with strong psychometric validity for evaluating behavioral intention to adopt a technology. Because XARP: A Human-First and AI Agent-Ready Extended Reality Toolkit 

the Social Influence construct primarily applies in organizational contexts with mandatory use, and XARP adoption is expected to be voluntary, we excluded this construct. We kept the remaining constructs: Performance Expectancy , Effort Expectancy , Facilitating Conditions , and the outcome Behavioral Intention . In addition, we collected open-ended responses asking participants to explain their reasons for adopting or avoiding XARP. To assess the perceived value of the toolkit, we designed a custom survey based on the HCI Toolkit Goals (HCITG) compiled by Ledo et al. [ 28 ]. Since no standardized survey instrument exists to assess the toolkit goals proposed in this framework, we constructed a custom instrument named HCITG. We mapped each toolkit goal compiled by Ledo et al. into a survey construct, further separating creative exploration from solution replication. Each HCITG survey construct was measured with three to six Likert-scale items, includ-ing at least one reverse-coded item. In summary, our customized HCITG instrument included: Reducing Authoring Time , Creating Paths of Least Resistance , Empowering New Audiences , Integrating with Current Practices and Infrastructure .We measured participant comprehension of the walkthrough videos to ensure their judgments of the toolkit were based on in-formed understanding. Each task was followed by three to four comprehension-check items, along with a confidence prompt: â€œThe video provided enough evidence for me to answer the questions in this section.â€ Our post-study questionnaire included â€œWhat ques-tions do you still have about the toolkit?â€ to identify gaps in par-ticipant comprehension. A summary of all measured variables is shown in fig. 3. 

## 6.3 Procedure 

The study was conducted fully online and asynchronously using Qualtrics 2, allowing participants to pause and resume at any point. This format reduced participation barriers and enabled broader geographic reach. The median completion time, including pauses, was 45 minutes. The fastest participant finished the study in 25 minutes, which is consistent with the durations we observed in pilot studies. After providing informed consent and demographic informa-tion, participants completed an eligibility screen requiring them to be at least 18 years old and to have at least one year of expe-rience in either AI or XR development. Eligible participants then proceeded through seven short videos demonstrating XARP, pre-sented in a fixed pedagogical order: Overview , Setup , Visual Q&A ,

Virtual Drone , AI Agent , MCP Integration , and Modifying XARP .Each video was approximately two minutes long, totaling around 15 minutes of content. Except for the overview video, which in-troduced core concepts and the toolkit architecture, each segment followed a consistent structure, including a brief introduction, a step-by-step walkthrough, and an outcome demonstration. After each video, participants answered comprehension-check questions. Videos could be replayed while responding, as the goal was to en-sure informed exposure to the toolkit and its capabilities, rather than testing learning or recall. 

> 2https://www.qualtrics.com/

Figure 3: Video Walkthrough Study Measurements: Perfor-mance Expectancy drove Behavioral Intention [48 ], while XARPâ€™s Effort Expectancy was better rated than its Facili-tating Conditions . A customized survey based on common toolkit goals [ 28 ] showed participants valued XARP for re-ducing authoring time, empowering new audiences, and replicating solutions, but less for providing paths of least resistance. 

Following the videos, participants completed two 7-point Likert-scale surveys (HCITG and UTAUT) and an open-ended question-naire. In this final section, they could ask further questions about the toolkit and provide reasons for why they might adopt or avoid XARP in future AI-XR projects. 

## 6.4 Data Analysis 

We screened all responses for quality based on four criteria: con-sistency on reverse-coded items, a minimum completion time of 20 minutes (to account for 15 minutes of video content and 5 min-utes of responses), open-text entries with more than a single word, and above-chance performance on comprehension checks. All 24 participants met these criteria, and no responses were excluded. For Likert-scale items, we used two-sided Wilcoxon signed-rank tests to verify deviations from the neutral midpoint. We verified the reliability of items within the same construct with McDonaldâ€™s 

ğœ” before aggregating them into composite 0-1 scores. We verified normality and sphericity assumptions and proceeded to analyze the scores using repeated-measures ANOVA. When omnibus tests Caetano et al. 

were significant, we conducted post hoc pairwise comparisons using paired t-tests with Holm correction. When the normality and sphericity assumptions were not met, we proceeded with a nonparametric Friedman test with post hoc Wilcoxon signed-rank tests (also Holm-corrected). Finally, in line with the causal structure of UTAUT, multiple linear regression was used to examine the effects of constructs and moderators (i.e., age, gender, expertise) on Behavioral Intention . This analysis was conducted across all the participants and also by novice and expert splits. Participants with more than three years of experience in either AI or XR were classified as experts, and others as novices. We analyzed participant answers about reasons to adopt or avoid XARP, and their open-ended questions, using a rapid variant of the Framework Method [ 19 ]. This approach provides a transparent, systematic way to code and chart even small corpora, enabling clear comparison across responses while minimizing analytic overhead. 

## 7 Results 7.1 Potential Acceptance 

Reliability of the UTAUT constructs was confirmed by McDonaldâ€™s 

ğœ” : Performance Expectancy (ğœ” = 0.948 , 4 items); Effort Expectancy 

(ğœ” = 0.928 , 4 items); Facilitating Conditions (ğœ” = 0.928 , 4 items); 

Behavioral Intent (ğœ” = 0.916 , 3 items). A Wilcoxon signed-rank test indicated that UTAUT responses were significantly higher than the neutral midpoint ( ğ‘Š = 4760 .5, ğ‘ < .001 ), suggesting overall positive agreement. The mean normalized UTAUT score across participants was ğ‘€ = 0.78 , ğ‘†ğ· = 0.12 , 95% ğ¶ğ¼ [0.73 , 0.84 ], and the mean Behavioral Intention score was ğ‘€ = 0.80 , ğ‘†ğ· = 0.15 ,

95% ğ¶ğ¼ [0.73 , 0.86 ].A repeated-measures ANOVA revealed a significant effect of the UTAUT constructs on agreement scores ( ğ¹ (2, 46 ) = 3.679 , ğ‘ =

.032 ). Post hoc pairwise t-test with Holm correction showed that 

Effort Expectancy was rated significantly higher than Facilitating Conditions (ğ‘¡ = 2.66 , ğ‘ Holm = .041 ). A multiple linear regression in-cluding age, gender, and expertise as moderators accounted for 57% of the variance in Behavioral Intention (ğ‘… 2 

> adj

= .57 , ğ¹ = 2.830 , ğ‘ =

.060 ). UTAUT predictors and moderators did not show a signifi-cant relationship with Behavioral Intention in this model. Another multiple linear regression without the moderators accounted for 62% of the variance in Behavioral Intention (ğ‘… 2 

> adj

= .62 , ğ¹ = 11 .10 ,

ğ‘ < .001 ). In this model, Performance Expectancy was a strong and statistically significant positive predictor of Behavioral Intention 

(ğ›½ = 0.73 , ğ‘ < .001 ), whereas Effort Expectancy was not significant (( ğ›½ = 0.12 , ğ‘ = .47 ). fig. 5 Shows boxplots of the UTAUT scores. 

## 7.2 HCI Toolkit Goals 

Some constructs of our customized HCITG scale had reliability confirmed by McDonaldâ€™s ğœ” : Enabling Creative Exploration (ğœ” =

0.830 , 3 items); Empowering New Audiences (ğœ” = 0.828 , 5 items); 

Integrating with Current Practices and Infrastructure (ğœ” = 0.786 , 3

items). Enabling Replication of Existing Solutions (ğœ” = 0.784 , 3

items); However, others fell bellow the conventional McDonaldâ€™s ğœ” 

of 0.7: Reducing Authoring Time (ğœ” = 0.687 , 3 items); Creating Paths of Least Resistance (ğœ” = 0.655 , 3 items). A Wilcoxon signed-rank test indicated that HCITG responses were significantly higher than the neutral midpoint ( ğ‘Š = 13127 .5, ğ‘ < .001 ), suggesting overall positive agreement. The mean HCITG score across participants was 

ğ‘€ = 0.75 , ğ‘†ğ· = 0.09 , 95% ğ¶ğ¼ [0.71 , 0.79 ].A repeated-measures ANOVA revealed a significant effect of HCITG constructs on agreement scores ( ğ¹ (5, 115 ) = 5.21 , ğ‘ <

.001 ). Post hoc pairwise t-test with Holm correction showed partic-ipants rated Creating Paths of Least Resistance significantly lower than Reducing Authoring Time and Complexity (ğ‘¡ = 3.80 , ğ‘ Holm =

.01 ); Empowering New Audiences (ğ‘¡ = 3.29 , ğ‘ Holm = .041 ) and En-abling Replication of Existing Solutions (ğ‘¡ = 3.57 , ğ‘ Holm = .02 ). fig. 6 Shows boxplots of the HCITG scores. 

## 7.3 Toolkit Comprehension 

Regarding the walkthrough comprehension scores, the average across participants was ğ‘€ = 0.92 , ğ‘†ğ· = 0.09 , 95% ğ¶ğ¼ [0.88 , 0.96 ]. The nu-merically highest walkthrough comprehension score was obtained on the tasks Toolkit Overview (ğ‘€ = 0.97 , ğ‘†ğ· = 0.13 ) and Setup 

(ğ‘€ = 0.97 , ğ‘†ğ· = 0.09 ). The numerically lowest video walkthrough comprehension score was obtained on the tasks MCP Integration 

(ğ‘€ = 0.83 , ğ‘†ğ· = 0.28 ) and Modifying XARP (ğ‘€ = 0.85 , ğ‘†ğ· = 0.27 ). A Friedman test revealed a significant effect of task on comprehen-sion scores ( ğœ’ 2 (6) = 14 .25 , ğ‘ = 0.02 ), although post hoc pairwise t-tests with Holm correction were non-significant. The participant judgment confidence on the UTAUT average across participants was ğ‘€ = 0.80 , ğ‘†ğ· = 0.15 , 95% ğ¶ğ¼ [0.73 , 0.87 ]. A Friedman test revealed a non-significant effect of UTAUT constructs on judgment confidence ( ğœ’ 2 (3) = 4.01 , ğ‘ = .25 ). The participant 

judgment confidence on the HCITG comprehension score average across participants was ğ‘€ = 0.80 , ğ‘†ğ· = 0.12 , 95% ğ¶ğ¼ [0.75 , 0.86 ].A Friedman test revealed a significant effect of HCITG constructs on judgment confidence ( ğœ’ 2 (5) = 13 .53 , ğ‘ = 0.018 ), although post hoc Wilcoxon signed-rank test tests with Holm correction were non-significant. A rapid Framework analysis [ 19 ] of participant responses to the question, â€œWhat other questions do you have that were not answered in the videos?â€ produced seven main FAQ topics. The most frequent topic was Compatibility & Integration (42%), e.g., â€œIs XARP compatible with other AR headsets beyond the Quest?â€ and â€œDoes the [toolkit] have anything for integrating with Unity Sentis and on-device AI models?â€ Another frequent topic was Limita-tions (15%), e.g., â€œ(...) In what circumstances have you found the built-in commands, LLM prompting, or motion/data tracking to be unreliable?â€ 

## 8 Discussion 8.1 Factors of AI-XR Toolkit Adoption 

8.1.1 Performance Expectancy and Authoring Time Reduction. Con-sistent with the causal relationship supported by UTAUT, our re-gression analysis suggests that Behavioral Intention was driven by participant belief that XARP would enhance their performance in AIâ€“XR development. Qualitative accounts reinforced this interpre-tation. P16 noted that XARP helps avoid â€œtediousâ€ aspects of XR app development such as â€œworking with different Prefabs, Packages, as well as the hardware itself.â€ Participants further described three main sources of perceived performance gains. XARP: A Human-First and AI Agent-Ready Extended Reality Toolkit 

Figure 4: Likert-scale responses to the four UTAUT constructs and the six HCITG constructs, aggregated across our subject pool. All responses were significantly higher than the neutral midpoint. 

Figure 5: UTAUT Score: The construct Facilitating Condi-tions was rated significantly lower than Effort Expectancy by participants. 

Figure 6: HCITG Scores: The construct Creating Paths of Least Resistance was rated significantly lower than Reducing Authoring Time and Complexity , Empowering New Audiences ,and Enabling Replication of Existing Solutions 

High-level abstraction. Participants valued the simplicity and clarity of XARPâ€™s functions and its ability to hide unnecessary tech-nical detail. P3 emphasized â€œthe efficiency, organization/simplicity of commands, and ease of integration with external tools,â€ adding their preference to â€œuse Python when possible.â€ P7 highlighted that XARP can â€œexpose the least low-level details to the users who are not experts in AI or XR,â€ while P9 stressed they â€œwould not [be] bogged down with reading up on various concepts or worrying about smaller details.â€ 

Backend and frontend separation. Several participants asso-ciated performance gains with the ability to code server-side apps without redeploying clients. P2 remarked XARP requires â€œno need to build and rebuild Unity Projects.â€ Similarly, P24 appreciated that XARP â€œallows developers to avoid constantly pushing new builds in the development of an XR app.â€ 

AI agent integration. Participants also saw performance bene-fits in XARPâ€™s AI agent integration. P6 valued â€œthe efficiency that the agent understands my need and my current status.â€ P17 em-phasized time savings for prototyping: â€œthe main value add would be the amount of time it would save to come up with quick proto-types (...) I think a strength is the flexibility to swap out models and plug-and-play MCPs.â€ 

8.1.2 Facilitating Conditions and Paths of Least Resistance. Our analysis indicated that Facilitating Conditions was rated lower than 

Effort Expectancy , suggesting that future adoption of the toolkit may be hindered by limited documentation, lack of training resources, and compatibility with available hardware and workflows. When asked about reasons they might avoid using XARP, participants identified reasons aligned with Facilitating Conditions .

Compatibility. Participants expressed concerns about device and client support. P2 noted â€œClient compatibility with different headsets,â€ while P4 remarked, â€œI probably would not use it in my current role, because I do not have access to a Meta Quest device.â€ Caetano et al. 

Similarly, P24 emphasized, â€œI would not use the platform if I did not own a Meta Quest.â€ These concerns highlight that users, especially experts who might specialize in a single platform, may not benefit from the multi-platform capability of the toolkit. Although XARP currently supports only a Meta Quest client and a Web UI, the same backend application can connect to multiple clients. 

Learning Curve and documentation. Several participants highlighted the need for more learning support and documentation. P3 commented, â€œIf there were more fitting alternative technologies in certain circumstances (or technologies with more thorough doc-umentation or other similar benefits), then I might be preferential to those. I donâ€™t think I have enough knowledge/info to respond specifically to the downsides of this platform.â€ Others pointed to the difficulty of learning and using the system effectively: P6 men-tioned â€œThe sharp learning curve or the long running time it takes,â€ P10 said they â€œNeed more time to understand and learn how to use it and design more functions,â€ and P14 noted â€œDifficulty in learning the language syntax.â€ We plan to create comprehensive documenta-tion for XARP once it is expanded to include the full planned range of functionalities, which we anticipate will ease the concerns we observed regarding learning support. 

Resources. Participants also noted constraints related to com-putational resources. P7 observed, â€œIf the token usage is too high, it might stop users from creating more creative or complex tasks, cuz not too many users have a locally deployed gpt-oss model, or they could not afford the cost of using token APIs.â€ Likewise, P12 raised concerns about performance and reliability: â€œPerformance for realtime tasks, not sure how an AI coding agent will be able to optimize an XR app to run in realtime or improve performance. I feel that client-server interaction is unreliable.â€ Performance and token consumption are critical considerations not only for adopting XARP, but also for any agentic AI solution. Use cases that demand real-time performance or routine or narrowly scoped tasks might be better addressed procedurally or with non-agentic AI. 

## 8.2 Threshold and Ceiling at Different Expertise Levels 

Threshold, the effort required to begin productive use of a system, and ceiling, the sophistication of what can be achieved with the system, are guiding themes in toolkit research [ 38 ]. To examine how users perceive XARP along these dimensions, we analyzed responses from participants at two levels of expertise. Less experi-enced participants may emphasize challenges associated with the toolkit threshold, while those with greater expertise can offer criti-cal feedback based on advanced use cases that delimit the toolkit ceiling. Novice participants rated Empowering New Audiences higher and more confidently than Creating Paths of Least Resistance , suggesting they perceived XARP as more accessible to new audiences. Novices also rated Reducing Authoring Time higher than Creating Paths of Least Resistance , highlighting productivity gains at their skill level. Together, these patterns suggest that novices viewed XARP as having a low threshold to entry. Their lower confidence in judging paths of least resistance may reflect the fact that identifying such paths requires familiarity with best practices and common pitfalls of AI-XR development, knowledge that novices may not have yet acquired. Experts rated all constructs in HCITG positively, and there was a greater variance in ratings for Creative Exploration compared to the other constructs. This is consistent with participant comments that highlighted the hyper-specific use cases that most experts engage with, and suggests varied interpretations of â€œcreative exploration" across the experts in this study. For example, P12 valued the â€œEase of prototyping, [allowing me to] benchmark a bunch of API callable models quickly on a specific task.", and P23 liked â€œThe ability to iterate quickly due to the server-first nature". Other experts found XARP limiting in itâ€™s ability to support features such as â€œvisual design and tailored immersive experiences" (P19), raised concerns about limitations due to API token usage (P5, P7) and a general lack of low-level flexibility (P20). This reflects the tradeoff between convenient abstractions that support quick development and fine-grained control [46]. 

## 8.3 A Future Vision for the AI-XR Ecosystem 

Unlike previous strategies to address complexity in software, devel-opers can now delegate design decisions to AI agents, essentially trading control over specification for adaptive behavior at runtime. This capability enables a new scaling mechanism, unconstrained by the burden of upfront human design. Acting as personal co-designers, agents can generate bespoke solutions at runtime. If made efficient, such agents could be deployed directly to end-users, generating unique applications on demand. In this vision, develop-ers do not vanish, they climb the â€œabstraction ladderâ€ [ 25 ]. Their role shifts from delivering finished products to shaping humanâ€“AI co-design workflows, embedding human insight into hybrid design teams, and consolidating patterns that emerge from millions of per-sonalized solutions. Business logic will change accordingly. Rather than competing through standardized products, companies could offer design-as-a-service, deploying AI agents with distinct design styles. A marketplace of such agents may emerge, with interoper-ability standards like MCP enabling both integration and economic exchange. XARP and other similar future toolkits serve as catalysts for this change in the XR domain by enabling agents to interface with both physical and virtual environments. XARP contributes to this vision by providing a convergence point where developers build creative workflows while AI agents generate on-the-fly XR experiences, seamlessly connected to a wider ecosystem of AI services through MCP. Our vision builds on a long-standing effort to empower end-users. Alan Kayâ€™s proposal of the â€œpersonal dynamic mediumâ€ set the precedent for imagining computing as something users could directly shape rather than merely consume [ 27 ]. By the 1990s, the HCI community had begun exploring end-user programming as a path toward broadening participation in software creation [ 39 ]. In the 2000s, this agenda matured into end-user software engineer-ing, articulating the technical and methodological challenges of enabling non-professionals to construct reliable software [ 9]. AI agents extend this trajectory, potentially democratizing software creation even further. XARP: A Human-First and AI Agent-Ready Extended Reality Toolkit 

## 9 Limitations & Future Work 

Our design pattern analysis aimed for representativeness, but our search may have excluded relevant papers. While our walkthrough study recruitment process engaged participants with varied ex-pertise across five countries, the reliance on personal distribution channels introduces potential sampling bias, and the sample size may have constrained statistical power. The walkthrough study was suitable for assessing early acceptance and perceived value of XARP, but further research should evaluate direct use of more mature versions in hybrid workshops and take-home studies [28]. XARP is an early-stage research artifact that currently imple-ments a reduced set of XR functions derived from design pattern case studies. Future versions should broaden functionality and pa-rameterization to further raise the ceiling while keeping the en-try threshold low. Technical limitations also remain: multi-client features require more robust authentication and synchronization mechanisms, and I/O performance would benefit from more effi-cient serialization formats (e.g., Protobuf). As observed in the FAQ, implementing XARP clients compatible with other popular XR plat-forms listed by participants is an important future step for adoption. In its current form, XARP is a research prototype and is not yet suitable for production use. 

## 10 Conclusion 

We introduced XARP, a toolkit that unifies XR abstraction for both human developers and AI agents. Case studies demonstrated its feasibility in real systems, and a study with 24 developers showed that productivity gains were the strongest predictor of adoption intention. While further refinement is needed to create paths of least resistance and facilitating conditions, XARP contributes to an AI-XR ecosystem where agents generate XR apps on demand and developers focus on enhancing userâ€“agent collaboration through open interoperability standards such as MCP. 

## References 

[1] Setareh Aghel Manesh, Tianyi Zhang, Yuki Onishi, Kotaro Hara, Scott Bateman, Jiannan Li, and Anthony Tang. 2024. How people prompt generative ai to create interactive vr scenes. In Proceedings of the 2024 ACM Designing Interactive Systems Conference . 2319â€“2340. [2] Anthropic. 2024. Introducing the Model Context Protocol . https://www.anthropic. com/news/model-context-protocol [3] Dan Bohus, Sean Andrist, Nick Saw, Ann Paradiso, Ishani Chakraborty, and Mahdi Rad. 2024. Sigma: An open-source interactive system for mixed-reality task assistance researchâ€“extended abstract. In 2024 IEEE conference on virtual reality and 3D user interfaces abstracts and workshops (VRW) . IEEE, 889â€“890. [4] Jan O. Borchers. 2000. A pattern approach to interaction design. In Proceed-ings of the 3rd Conference on Designing Interactive Systems: Processes, Prac-tices, Methods, and Techniques (New York City, New York, USA) (DIS â€™00) . As-sociation for Computing Machinery, New York, NY, USA, 369â€“378. https: //doi.org/10.1145/347642.347795 [5] Ingo BÃ¶rsting, Markus Heikamp, Marc Hesenius, Wilhelm Koop, and Volker Gruhn. 2022. Software Engineering for Augmented Reality - A Research Agenda. 

Proc. ACM Hum.-Comput. Interact. 6, EICS, Article 155 (June 2022), 34 pages. https://doi.org/10.1145/3532205 [6] Dibyendu Brinto Bose and Chris Brown. 2024. An Empirical Study on Current Practices and Challenges of Core AR/VR Developers. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering Workshops 

(Sacramento, CA, USA) (ASEW â€™24) . Association for Computing Machinery, New York, NY, USA, 233â€“238. https://doi.org/10.1145/3691621.3694956 [7] Riccardo Bovo, Steven Abreu, Karan Ahuja, Eric J Gonzalez, Li-Te Cheng, and Mar Gonzalez-Franco. 2025. Embardiment: an embodied ai agent for productivity in xr. In 2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR) . IEEE, 708â€“717. [8] Kadir Burak Buldu, SÃ¼leyman Ã–zdel, Ka Hei Carrie Lau, Mengdi Wang, Daniel Saad, Sofie SchÃ¶nborn, Auxane Boch, Enkelejda Kasneci, and Efe Bozkir. 2025. Cuify the xr: An open-source package to embed llm-powered conversational agents in xr. In 2025 IEEE International Conference on Artificial Intelligence and eXtended and Virtual Reality (AIxVR) . IEEE, 192â€“197. [9] Margaret Burnett, Curtis Cook, and Gregg Rothermel. 2004. End-user software engineering. Commun. ACM 47, 9 (2004), 53â€“58. [10] Arthur Caetano, Alejandro Aponte, and Misha Sra. 2025. A design toolkit for task support with mixed reality and artificial intelligence. Frontiers in Virtual Reality 6 (2025), 1536393. [11] Runze Cai, Nuwan Janaka, Hyeongcheol Kim, Yang Chen, Shengdong Zhao, Yun Huang, and David Hsu. 2025. AiGet: Transforming Everyday Moments into Hidden Knowledge Discovery with AI Assistance on Smart Glasses. In 

Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems .1â€“26. [12] Sonia Castelo, Joao Rulff, Erin McGowan, Bea Steers, Guande Wu, Shaoyu Chen, Iran Roman, Roque Lopez, Ethan Brewer, Chen Zhao, et al . 2023. Argus: Visual-ization of ai-assisted task guidance in ar. IEEE Transactions on Visualization and Computer Graphics 30, 1 (2023), 1313â€“1323. [13] Melvin E Conway. 1968. How do committees invent. Datamation 14, 4 (1968), 28â€“31. [14] Fernanda De La Torre, Cathy Mengying Fang, Han Huang, Andrzej Banburski-Fahey, Judith Amores Fernandez, and Jaron Lanier. 2024. LLMR: Real-time Prompting of Interactive Worlds using Large Language Models. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI â€™24) . Association for Computing Machinery, New York, NY, USA, Article 600, 22 pages. https://doi.org/10.1145/3613904.3642579 [15] Fernanda De La Torre, Cathy Mengying Fang, Han Huang, Andrzej Banburski-Fahey, Judith Amores Fernandez, and Jaron Lanier. 2024. Llmr: Real-time prompt-ing of interactive worlds using large language models. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems . 1â€“22. [16] Mustafa Doga Dogan, Eric J Gonzalez, Karan Ahuja, Ruofei Du, Andrea ColaÃ§o, Johnny Lee, Mar Gonzalez-Franco, and David Kim. 2024. Augmented Object Intelligence with XR-Objects. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology (Pittsburgh, PA, USA) (UIST â€™24) . As-sociation for Computing Machinery, New York, NY, USA, Article 19, 15 pages. https://doi.org/10.1145/3654777.3676379 [17] JoÃ£o Marcelo Evangelista Belo, Anna Maria Feit, Tiare Feuchtner, and Kaj GrÃ¸n-bÃ¦k. 2021. XRgonomics: Facilitating the Creation of Ergonomic 3D Interfaces. In 

Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems 

(Yokohama, Japan) (CHI â€™21) . Association for Computing Machinery, New York, NY, USA, Article 290, 11 pages. https://doi.org/10.1145/3411764.3445349 [18] JoÃ£o Marcelo Evangelista Belo, Mathias N LystbÃ¦k, Anna Maria Feit, Ken Pfeuffer, Peter KÃ¡n, Antti Oulasvirta, and Kaj GrÃ¸nbÃ¦k. 2022. Auitâ€“the adaptive user interfaces toolkit for designing xr applications. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology . 1â€“16. [19] Nicola K Gale, Gemma Heath, Elaine Cameron, Sabina Rashid, and Sabi Redwood. 2013. Using the framework method for the analysis of qualitative data in multi-disciplinary health research. BMC medical research methodology 13, 1 (2013), 117. [20] Daniele Giunchi, Nels Numan, Elia Gatti, and Anthony Steed. 2024. Dream-CodeVR: Towards Democratizing Behavior Design in Virtual Reality with Speech-Driven Programming. In 2024 IEEE Conference Virtual Reality and 3D User Inter-faces (VR) . 579â€“589. https://doi.org/10.1109/VR58804.2024.00078 [21] Saul Greenberg and Bill Buxton. 2008. Usability evaluation considered harmful (some of the time). In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Florence, Italy) (CHI â€™08) . Association for Computing Ma-chinery, New York, NY, USA, 111â€“120. https://doi.org/10.1145/1357054.1357074 [22] Amal Hashky, Benjamin Rheault, Ahmed Rageeb Ahsan, Brett Benda, Tyler Au-dino, Samuel Lonneman, and Eric D Ragan. 2024. Multi-Modal User Modeling for Task Guidance: A Dataset for Real-Time Assistance with Stress and Interruption Dynamics. In 2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW) . IEEE, 544â€“550. [23] Xiyun Hu, Dizhi Ma, Fengming He, Zhengzhe Zhu, Shao-Kang Hsia, Chenfei Zhu, Ziyi Liu, and Karthik Ramani. 2025. GesPrompt: Leveraging Co-Speech Gestures to Augment LLM-Based Interaction in Virtual Reality. In Proceedings of the 2025 ACM Designing Interactive Systems Conference . 59â€“80. [24] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. 2024. SWE-bench: Can Language Models Resolve Real-world Github Issues?. In The Twelfth International Conference on Learning Representations . https://openreview.net/forum?id=VTF8yNQM66 [25] Frederick Phillips Brooks Jr. 1987. No Silver Bullet Essence and Accidents of Software Engineering. Computer 20, 4 (1987), 10â€“19. https://doi.org/10.1109/ MC.1987.1663532 [26] Heewoo Jun and Alex Nichol. 2023. Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463 (2023). [27] Alan C Kay. 1977. Microelectronics and the personal computer. Scientific Ameri-can 237, 3 (1977), 230â€“245. Caetano et al. 

[28] David Ledo, Steven Houben, Jo Vermeulen, Nicolai Marquardt, Lora Oehlberg, and Saul Greenberg. 2018. Evaluation Strategies for HCI Toolkit Research. In 

Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems 

(Montreal QC, Canada) (CHI â€™18) . Association for Computing Machinery, New York, NY, USA, 1â€“17. https://doi.org/10.1145/3173574.3173610 [29] Jaewook Lee, Andrew D Tjahjadi, Jiho Kim, Junpu Yu, Minji Park, Jiawen Zhang, Jon E Froehlich, Yapeng Tian, and Yuhang Zhao. 2024. CookAR: Affordance augmentations in wearable AR to support kitchen tool interactions for people with low vision. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology . 1â€“16. [30] Jaewook Lee, Jun Wang, Elizabeth Brown, Liam Chu, Sebastian S Rodriguez, and Jon E Froehlich. 2024. GazePointAR: A context-aware multimodal voice assistant for pronoun disambiguation in wearable augmented reality. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems . 1â€“20. [31] Chenyi Li, Guande Wu, Gromit Yeuk-Yin Chan, Dishita Gdi Turakhia, Sonia Castelo Quispe, Dong Li, Leslie Welch, Claudio Silva, and Jing Qian. 2025. Satori: Towards Proactive AR Assistant with Belief-Desire-Intention User Modeling. In 

Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI â€™25) . Association for Computing Machinery, New York, NY, USA, Article 1229, 24 pages. https://doi.org/10.1145/3706598.3714188 [32] Jiefeng Li, Siyuan Bian, Chao Xu, Zhicun Chen, Lixin Yang, and Cewu Lu. 2025. HybrIK-X: Hybrid Analytical-Neural Inverse Kinematics for Whole-Body Mesh Recovery. IEEE Transactions on Pattern Analysis and Machine Intelligence 47, 4 (2025), 2754â€“2769. https://doi.org/10.1109/TPAMI.2025.3528979 [33] Zhipeng Li, Christoph Gebhardt, Yves Inglin, Nicolas Steck, Paul Streli, and Christian Holz. 2024. SituationAdapt: Contextual UI Optimization in Mixed Reality with Situation Awareness via LLM Reasoning. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology (Pittsburgh, PA, USA) (UIST â€™24) . Association for Computing Machinery, New York, NY, USA, Article 43, 13 pages. https://doi.org/10.1145/3654777.3676470 [34] Chen-Chieh Liao, Zhihao Yu, and Hideki Koike. 2025. ShiftingGolf: Gross Motor Skill Correction Using Redirection in VR. IEEE Transactions on Visualization and Computer Graphics 31, 5 (2025), 3429â€“3439. https://doi.org/10.1109/TVCG.2025. 3549170 [35] Spencer Lin, Basem Rizk, Miru Jun, Andy Artze, CaitlÃ­n Sullivan, Sharon Mozgai, and Scott Fisher. 2024. Estuary: A Framework For Building Multimodal Low-Latency Real-Time Socially Interactive Agents. In Proceedings of the 24th ACM International Conference on Intelligent Virtual Agents . 1â€“3. [36] Sara Mandic, Rhys Tracy, and Misha Sra. 2023. ARFit: Pose-based Exercise Feedback with Mobile AR. In Proceedings of the 2023 ACM Symposium on Spatial User Interaction (Sydney, NSW, Australia) (SUI â€™23) . Association for Computing Machinery, New York, NY, USA, Article 45, 3 pages. https://doi.org/10.1145/ 3607822.3618008 [37] Ben Moseley and Peter Marks. 2006. Out of the tar pit. Software Practice Ad-vancement (SPA) 2006 (2006). [38] Brad Myers, Scott E. Hudson, and Randy Pausch. 2000. Past, present, and future of user interface software tools. ACM Trans. Comput.-Hum. Interact. 7, 1 (March 2000), 3â€“28. https://doi.org/10.1145/344949.344959 [39] Bonnie A Nardi. 1993. A small matter of programming: perspectives on end user computing . MIT press. [40] Nathalia Nascimento, Everton Guimaraes, Sai Sanjna Chintakunta, and San-thosh Anitha Boominathan. 2025. How Effective are LLMs for Data Science Coding? A Controlled Experiment. In 2025 IEEE/ACM 22nd International Confer-ence on Mining Software Repositories (MSR) . 211â€“222. https://doi.org/10.1109/ MSR66628.2025.00041 [41] Nels Numan, Gabriel Brostow, Suhyun Park, Simon Julier, Anthony Steed, and Jessica Van Brummelen. 2025. CoCreatAR: Enhancing authoring of outdoor augmented reality experiences through asymmetric collaboration. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems . 1â€“22. [42] Raf Ramakers, Fraser Anderson, Tovi Grossman, and George Fitzmaurice. 2016. RetroFab: A Design Tool for Retrofitting Physical Interfaces using Actuators, Sensors and 3D Printing. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (San Jose, California, USA) (CHI â€™16) . Association for Computing Machinery, New York, NY, USA, 409â€“419. https://doi.org/10. 1145/2858036.2858485 [43] Symeon Retalis, Petros Georgiakakis, and Yannis Dimitriadis. 2006. Elic-iting design patterns for e-learning systems. Computer Science Educa-tion 16, 2 (2006), 105â€“118. https://doi.org/10.1080/08993400600773323 arXiv:https://doi.org/10.1080/08993400600773323 [44] Benjamin Rheault, Shivvrat Arya, Akshay Vyas, Jikai Wang, Rohith Peddi, Brett Bendall, Vibhav Gogate, Nicholas Ruozzi, Yu Xiang, and Eric D Ragan. 2024. Predictive task guidance with artificial intelligence in augmented reality. In 2024 IEEE conference on virtual reality and 3D user interfaces abstracts and workshops (VRW) . IEEE, 973â€“974. [45] Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. 2018. AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles. In Field and Service Robotics , Marco Hutter and Roland Siegwart (Eds.). Springer Interna-tional Publishing, Cham, 621â€“635. [46] Joel Spolsky. 2004. The Law of Leaky Abstractions . Apress, Berkeley, CA, 197â€“202. https://doi.org/10.1007/978-1-4302-0753-5_26 [47] Sruti Srinidhi, Edward Lu, and Anthony Rowe. 2024. XaiR: An XR Platform that Integrates Large Language Models with the Physical World. In 2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . 759â€“767. https://doi.org/10.1109/ISMAR62088.2024.00091 [48] Viswanath Venkatesh, Michael G. Morris, Gordon B. Davis, and Fred D. Davis. 2003. User Acceptance of Information Technology: Toward a Unified View. MIS Quarterly 27, 3 (2003), 425â€“478. http://www.jstor.org/stable/30036540 [49] Michel Wermelinger. 2023. Using GitHub Copilot to Solve Simple Programming Problems. In Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1 (Toronto ON, Canada) (SIGCSE 2023) . Association for Computing Machinery, New York, NY, USA, 172â€“178. https://doi.org/10.1145/ 3545945.3569830 [50] Niall Winters and Yishay Mor. 2009. Dealing with abstraction: Case study gener-alisation as a method for eliciting design patterns. Computers in Human Behavior 

25, 5 (2009), 1079â€“1088. https://doi.org/10.1016/j.chb.2009.01.007 Including the Special Issue: Design Patterns for Augmenting E-Learning Experiences. [51] Guande Wu, Jing Qian, Sonia Castelo Quispe, Shaoyu Chen, JoÃ£o Rulff, and Claudio Silva. 2024. Artist: Automated text simplification for task guidance in augmented reality. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems . 1â€“24. [52] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. 2025. Struc-tured 3D Latents for Scalable and Versatile 3D Generation. In CVPR 2025 . https://www.microsoft.com/en-us/research/publication/structured-3d-latents-for-scalable-and-versatile-3d-generation/ [53] Chengyuan Xu, Radha Kumaran, Noah Stier, Kangyou Yu, and Tobias HÃ¶llerer. 2024. Multimodal 3D Fusion and In-Situ Learning for Spatially Aware AI. In 2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 485â€“494. [54] Xuhai Xu, Anna Yu, Tanya R. Jonker, Kashyap Todi, Feiyu Lu, Xun Qian, JoÃ£o Marcelo Evangelista Belo, Tianyi Wang, Michelle Li, Aran Mun, Te-Yen Wu, Junxiao Shen, Ting Zhang, Narine Kokhlikyan, Fulton Wang, Paul Soren-son, Sophie Kim, and Hrvoje Benko. 2023. XAIR: A Framework of Explain-able AI in Augmented Reality. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI â€™23) . Associ-ation for Computing Machinery, New York, NY, USA, Article 202, 30 pages. https://doi.org/10.1145/3544548.3581500 [55] Nur Yildirim, Changhoon Oh, Deniz Sayar, Kayla Brand, Supritha Challa, Violet Turri, Nina Crosby Walton, Anna Elise Wong, Jodi Forlizzi, James McCann, and John Zimmerman. 2023. Creating Design Resources to Scaffold the Ideation of AI Concepts. In Proceedings of the 2023 ACM Designing Interactive Systems Conference (Pittsburgh, PA, USA) (DIS â€™23) . Association for Computing Machinery, New York, NY, USA, 2326â€“2346. https://doi.org/10.1145/3563657.3596058 [56] Dong Woo Yoo, Hamid Tarashiyoun, and Mohsen Moghaddam. 2023. Modeling gaze behavior for real-time estimation of visual attention and expertise level in augmented reality. In 2023 IEEE international symposium on mixed and augmented reality adjunct (ISMAR-Adjunct) . IEEE, 487â€“492.
