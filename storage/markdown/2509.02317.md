Title: AI Agent Communication from Internet Architecture Perspective: Challenges and Opportunities

URL Source: http://arxiv.org/pdf/2509.02317v1

Published Time: Wed, 03 Sep 2025 02:51:34 GMT

Markdown Content:
# AI Agent Communication from Internet Architecture Perspective: Challenges and Opportunities 

Chenguang Du 

Zhongguancun Laboratory 

Beijing, China ducg@zgclab.edu.cn 

Chuyi Wang 

Tsinghua University 

Beijing, China wangchuy21@mails.tsinghua.edu.cn 

Yihan Chao 

Zhongguancun Laboratory 

Beijing, China chaoyh@zgclab.edu.cn 

Xiaohui Xie 

Tsinghua University 

Beijing, China xiexiaohui@tsinghua.edu.cn 

Yong Cui 

Tsinghua University 

Beijing, China cuiyong@tsinghua.edu.cn 

Abstract —The rapid development of AI agents leads to asurge in communication demands. Alongside this rise, a vari-ety of frameworks and protocols emerge. While these efforts demonstrate the vitality of the field, they also highlight in-creasing fragmentation, with redundant innovation and siloed designs hindering cross-domain interoperability. These chal-lenges underscore the need for a systematic perspective to guide the development of scalable, secure, and sustainable AI agent ecosystems. To address this need, this paper provides the first systematic analysis of AI agent communication from the standpoint of Internet architecture—the most successful global-scale distributed system in history. Specifically, we distill decades of Internet evolution into five key elements that are directly relevant to agent communication: scalability, security, real-time performance, high performance, and manageability. We then use these elements to examine both the opportunities and the bottlenecks in developing robust multi-agent ecosystems. Overall, this paper bridges Internet architecture and AI agent communication for the first time, providing a new lens for guiding the sustainable growth of AI agent communication ecosystems. 

Index Terms —ai agent communication, multi-agent systems, protocol standardization, internet architecture 

I. I NTRODUCTION 

An AI agent is an entity capable of autonomous perception, decision-making, and action. Supported by foundational com-puting, storage, and communication infrastructures, agents are proliferating across diverse forms, ranging from autonomous applications to physical robots. With the advent of large language models (LLMs), the capabilities of AI agents have been significantly enhanced, greatly expanding the scope of multi-agent collaboration. As this trend accelerates, a variety of communication tech-nologies and protocols have rapidly emerged. For example, Google proposed the Agent2Agent Protocol (A2A) [1], while open-source contributors developed the Agent Network Pro-tocol (ANP) [2], both aiming to enable cross-agent interoper-ability and coordination. This vibrant wave of innovation un-derscores the vitality of the field but also reveals growing frag-mentation. Two issues stand out in particular. First, innovation redundancy: leading companies and talented developers are frequently duplicating efforts, resulting in wasted resources. Second, value fragmentation: while the core purpose of agent communication is to enable cross-domain interoperability and collaboration, today’s siloed designs often hinder seamless interaction across systems. These challenges highlight the urgent need for systematic perspectives and established design principles. A unified analytical lens is essential to coordinate diverse efforts, reduce fragmentation, and promote the healthy and sustainable development of the agent ecosystem. Building on this perspective, this paper provides the first systematic analysis of AI agent communication from the standpoint of Internet architecture. Instead of focusing on isolated protocols or platforms, we draw on decades of Internet evolution to extract key elements for understanding emerging agent ecosystems. Using this approach, we make three main contributions: (1) we identify five key elements—scalability, security, real-time performance, high performance, and man-ageability—that capture the fundamental challenges of agent communication; (2) we examine these elements to highlight both the bottlenecks and the opportunities in developing robust multi-agent ecosystems; and (3) we extend the discussion to standardization, identifying future opportunities for agent communication standards. The remainder of this paper is organized as follows. Section II reviews the evolution of agent communication technologies. Section III analyzes the key challenges and opportunities of agent communication through the lens of Internet architecture. Section IV discusses potential standardization pathways at both the transport and application layers, and examines the strategic competition for agent communication entry points. Finally, Section V concludes the paper. 

> arXiv:2509.02317v1 [cs.NI] 2 Sep 2025

II. E XISTING AGENT COMMUNICATION TECHNOLOGIES 

This section presents a structured and historically grounded review of agent-to-agent communication technologies. The organizing principle is the progressive enhancement of in-teroperability capabilities: from early semantic formalisms, to web identity and engineered bearers, and eventually to protocolized interconnection in the large-model era. Rather than following a linear chronology, the analysis is problem-driven, reconstructing successive technical attempts to address discovery, authentication, capability description, negotiation, execution, and auditability across various domains. The review is divided into two major periods: the pre-LLM era, where technical primitives laid conceptual and infrastructural ground-work yet left interoperability gaps unresolved, and the era of LLMs, where orchestration frameworks and emerging proto-cols have reframed the boundaries and bottlenecks of cross-ecosystem interaction. Each subsection situates technologies within their problem context, tracing both achievements and enduring limitations, thereby generating evidence that informs the standardization pathways discussed in later sections. 

A. Pre-LLM Era: Foundation of Communication Framework 

This subsection revisits the foundational primitives estab-lished prior to large models, providing an analysis of their ca-pacities and limitations. Speech-act semantics translated intent into executable capability descriptions, providing expressive-ness; the Web identity and semantic stack bound these descrip-tions to resolvable entities, ensuring reference and traceability; engineered carrier models encapsulated them as messages, tokens, or API calls, securing reliable delivery; and decentral-ized identifiers reduced dependence on single trust anchors, enhancing cross-domain verifiability. Together, these form a causal chain: expression, binding, transmission, verification, but unresolved challenges in Internet-scale addressing, cross-domain trust, and governance show that no single primitive can sustain interoperability. The achievement of Internet-scale coordination requires the architectural composition of multiple primitives. 

1) Semantic Communication Foundations: KQML [3] and FIPA-ACL [4] introduced the “speech act” paradigm and session management into engineering practice, establishing the semantic framework for multi-agent interaction. These protocols treated messages as intentional behavioral units (per-formatives) that carry performative verbs such as “request”, “inform”, and “query”, while defining conversation turns, com-mitments, and semantic constraints in dialogue protocols. This paradigm elevated complex collaboration from program inter-faces to semantic contracts, providing operational primitives for capability description, negotiation, and verification. How-ever, these protocols failed to address Internet-level addressing, verifiable identity, and governance mechanisms, making them insufficient for large-scale cross-domain interconnection and remaining largely in research prototypes. 

2) Web/Identity and Semantic Infrastructure: OAuth 2.0/OIDC [5] and JSON-LD [6] solidified authoriza-tion/identity and Web-native semantic expression as de facto standards, providing a reusable foundation for agents to achieve “verifiable identity” and “linkable semantics” in cross-domain environments. OAuth 2.0 provided operational pro-cesses for delegated authorization, while OpenID Connect added identity claims and basic trust interactions. JSON-LD treated semantic context as a first-class element of data, enabling machine-parsable semantic annotations during cross-domain data flow. These technologies were not designed specifically for agents, but provided direct toolchains for agent capability description, credential exchange, and semantic alignment. 

3) Engineering Implementation: RPC and Event Models: 

The “RPC extremes” of gRPC [7] and JSON-RPC [8], along with CloudEvents’ unified event meta-model, engi-neered the bearer layer of multi-agent communication into deliverable runtime foundations. Strongly typed gRPC suited high-throughput, low-latency service mesh scenarios, while lightweight JSON-RPC attracted developers with lower in-tegration costs in heterogeneous environments. CloudEvents standardized the event stream metadata to enable consistent event envelopes for publishers and consumers. For multi-agent systems, engineered bearers provided reliable communication, observability, and performance guarantees, incorporating task scheduling, tool invocation, and log tracking into measurable runtime environments. 

4) Verifiable Identity Enhancement: The decentralized identifier (DID) [9] combines the identifier, public key, ver-ification method, and service endpoint into independently resolvable “DID documents”. This provides a practical path to establish trusted identity and discovery endpoints in multi-autonomous domain environments that lack a shared root of trust. DID shifts identity from platform accounts to self-sovereign units that support cross-domain verification. It also enables endpoint discovery bound to identity claims, laying the foundation for verifiable discovery despite governance and performance challenges in large-scale deployment. 

B. The Era of LLMs: From Platformization to Protocolized Interoperability 

This subsection analyzes platformization and nascent pro-tocolization in the large language model era, encompassing orchestration schemes, MCP-style [10] abstraction, Agent-Card [1] /ANS [11] proposals, and ANP [2] meta-protocols. These efforts extend earlier primitives while addressing new constraints: platform silos impede interoperability, auditability safeguards traceability, and credential-recognition mechanisms sustain trust in multi-actor collaboration. In doing so, they amplify expressiveness, linkability, and delivery guarantees, and advance interface-level standardization toward Internet-scale agent cooperation. 

1) Platformization of Orchestration: LLM-centric multi-agent [12] frameworks made “conversation-as-orchestration” mainstream, maturing orchestration patterns including supervisor-executor, debate-style reasoning, and tool chain invocation, significantly lowering thresholds for building collaborative agent systems. Frameworks like TABLE I COMPARISON OF AGENT COMMUNICATION PROTOCOLS /F RAMEWORKS ACROSS SIX ATTIBUTES .                                                                   

> #:NOT SUPPORTED ,G#:PARTIALLY SUPPORTED ,:WELL SUPPORTED .
> Framework / Protocol Disc. Auth. Cap. Desc. Neg. Exec. Audit. Strengths Weaknesses
> KQML / FIPA-ACL G#G#G##Rich speech-act semantics, formal negotiation No Internet-scale identity, no governance, weak runtime OAuth / OIDC G#G###G#Standardized delegated auth, identity claims Not agent-native, no execution, limited discovery DID #G##G#Self-sovereign, verifiable identity and discovery endpoints Limited semantics, runtime and governance challenges AutoGen G#G#G##G#Orchestration via role-based dialogue Platform-bound, weak discovery/auth, limited audit Dify G#G#G##G#Graphical orchestration, observability Proprietary SDKs, inconsistent semantics, poor cross-platform MCP G#G#G#Unified model-tool interface, context injection Externalized security, needs shared audit/governance A2A / ANS G#G#G#G#AgentCard metadata, ANS naming Immature ecosystem, modest semantics, scaling issues ANP G#G#Integrated discovery, description, negotiation, session security Deployment immaturity, governance overhead Agents SDK G#G#G#G#Maps protocol to runtime, tool registration, guardrails Platform-specific, weak interoperability, limited audit

AutoGen [13] decomposed complex tasks into role-based dialogue participants, allowing decision-making, search, and tool invocation through message interaction. However, most frameworks relied on platform-provided storage, logging, and routing mechanisms, with discovery, authentication, and credentials often being platform-specific or manually configured, limiting cross-organization interoperability. This gap underscores the importance of developing standardized identity, communication, and coordination protocols, an area increasingly emphasized in recent systemization-of-knowledge studies. Dify [14], LangGraph [15], and others introduced graphical orchestration and finite-state machines into production envi-ronments, significantly improving controllability and observ-ability while exposing “platform silo” and interface privati-zation barriers. Graphical orchestration explicitly represented complex tasks as node-edge structures where nodes represent agents/tools and edges represent data/control flow. Through visualization and finite-state machine constraints, platforms improved fault recovery and debugging efficiency. However, proprietary SDKs and metadata models in the interface layers led to inconsistent capability descriptions and context models between platforms, increasing cross-platform integration costs. This fragmentation highlights the need for shared orchestration ontologies and interoperability standards to enable heteroge-neous agent ecosystems to collaborate more effectively. 

2) Protocolization of Interfaces: Model Context Protocol (MCP) [10] standardized access between models and external tools/data sources, reducing platform adaptation costs and representing milestones in elevating SDK layer capabilities to cross-ecosystem protocols. MCP abstracted model-tool interactions into unified capability descriptions, handshake processes, and context injection contracts, enabling dynamic requests and secure use of external tools at runtime. As the “USB-C for AI” analogy suggests, MCP’s value lies in reducing engineering friction in tool access between different platforms or models, though success depends on common authentication, quota, and audit specifications. Recent studies also emphasize that without converging governance mecha-nisms and standardized auditing practices, such protocols risk reproducing existing fragmentation across ecosystems. A2A’s proposed AgentCard (e.g., /.well-known/agent.json) and ANS (Agent Name Service) [1], [11] aim to expose “who can do what” and “how to contact it” in Web-native and ver-ifiable forms, bringing discovery/naming problems into gov-ernable scope. AgentCard provides self-descriptive metadata (provider, endpoint, capability list, authentication/signature information), enabling discovery services to achieve trusted indexing without centralized directories. ANS introduces DNS-like naming and resolution mechanisms that support capability-oriented search and aggregation. Together, they transform traditional “finding people” problems into engineer-ing problems of finding autonomous capability units. Previous work in service discovery and naming has highlighted sim-ilar tensions between decentralization, scalability, and trust management. More recent studies in large-scale distributed systems also show that integrating semantic descriptions with resolution infrastructures is key to enabling interoperability across heterogeneous agent environments. Agent Network Protocol (ANP) [2] merges Discovery and Description (represented by JSON-LD, schema.org) with a Communication Meta-Protocol, incorporating name, capabil-ity, credential, and dialogue negotiation into unified compos-able models for cross-ecosystem end-to-end interoperability. ANP’s core idea is explaining not only “what can be done” but also “how to agree on interaction input-output semantics, authorization boundaries, and failure compensation strategies.” Therefore, ANP defines capability description vocabularies, negotiation message formats, session security and encryption strategies, as well as failure semantics and fallback schemes. If ANP forms mutually recognized contracts across platforms, it will significantly reduce cross-domain integration costs. 

3) Execution and Governance Runtime: Agents SDK [16] maps protocol-level descriptions to executable toolsets and runtime primitives (tool registration, handoff, guardrails, state management), implementing “protocolized discovery and se-mantic negotiation” as orchestrable and observable application processes. However, cross-protocol mutual recognition and auditing remain shortcomings. In production settings, the SDK converts high-level negotiations into concrete invocation flows. Reliability is ensured through error handling, compen-sation, and monitoring. Unified access to external tools, token management, and request routing is also provided. However, SDK implementations typically favor specific platforms, mak-ing mutual recognition across platforms difficult, along with challenges in maintaining auditable traceability chains and protecting sensitive contexts at runtime. 

C. Summary and Insights 

The historical trajectory of agent communication reveals a recurring tension between expressive semantics and practical interoperability. Early frameworks such as KQML and FIPA were characterized by strong formal expressiveness, but their adoption was hindered by deployment barriers. Web-native standards such as OAuth and JSON-LD were introduced to im-prove trust and portability, yet fragmentation across domains was left unresolved. Transport-oriented efforts such as gRPC and CloudEvents were designed to enhance delivery efficiency, but shared semantics and governance were largely neglected. Emerging proposals, including MCP, AgentCard/ANS, and ANP, aim to restore large-scale interoperability but are still in formative stages. Taken together, these developments show that incremental advances have addressed isolated dimensions of the interoperability problem, yet no architecture has rec-onciled semantics, trust, and runtime integration. Fragmented frameworks also create reliability gaps, including message loss, inconsistent state propagation, and failures in coordinated behavior, limiting sustainable ecosystem growth. Building on these observations, the next section develops an analytical framework to systematically evaluate the fundamental chal-lenges and potential development paths of agent communica-tion. III. C HALLENGES AND OPPORTUNITIES IN AI A GENT 

COMMUNICATION 

As AI agents scale across heterogeneous and cross-domain environments, fragmented frameworks reveal significant gaps in interoperability and reliability, limiting sustainable ecosys-tem growth. To this end, we draw on the principles of Internet architecture to extract key elements. These elements capture the fundamental challenges and point toward potential devel-opment paths for emerging agent communication development. 

A. Internet-Inspired Key Elements 

To systematically analyze the agent ecosystem, we adopt the architecture of the Internet not merely as a historical analogy, but as a source of time-tested design elements. The Internet’s success in connecting a global, heterogeneous network was contingent on mastering a set of fundamental design chal-lenges. Its solutions, refined over decades of operation and evolution, provide a time-tested lens through which to evaluate the nascent field of agent communication. Specifically, we distill the Internet’s architectural experience into five key elements that closely parallel the challenges agents face today: scalability, to manage explosive growth in entities and interactions; security, to establish trust among autonomous and often unfamiliar entities; real-time and high performance, to enable complex, time-sensitive collaboration; and manageability, to ensure operational stability and observ-ability in a dynamic environment. These five pillars represent the foundational issues that will determine the future viability of an interconnected “Internet of Agents”. By examining the emerging landscape of agent communication through these dimensions, we can move beyond ad-hoc observations to systematically identify foundational problems and uncover architecturally-sound opportunities for future development. 

B. Scalability 1) Definition of Scalability and Internet Implementation: 

Scalability refers to the ability of a system, network, or process to operate efficiently and meet performance requirements as its scale expands. In complex communication systems, when nodes and data surge and interaction relationships become increasingly complex, scalability is crucial: it ensures that the system can respond to load growth without crashing. The Internet architecture has attached great importance to scalability issues since its birth. Through carefully designed mechanisms to support large-scale expansion, it has success-fully evolved from the initial small experimental network connecting several computers to today’s vast network connect-ing billions of devices worldwide. Among them, the design ideas of packet switching, regional autonomy, and layered architecture have played important roles. 

2) Scalability Challenges: With the rise of multi-agent systems, their communication architecture faces scalability challenges similar to but more severe than those of the early Internet. These challenges can be summarized into two main categories: 

Communication Explosion . Multi-agent collaboration in-herently requires frequent information exchange. As the num-ber of agents and tool-based agents continues to grow rapidly, and as their states and capabilities change dynamically, com-munication overhead can quickly spiral out of control. Without effective organization, systems may easily degenerate into full-mesh patterns, where each agent interacts with all others, lead-ing to quadratic O(N 2) growth in communication links. This uncontrolled communication explosion undermines efficiency and threatens overall system stability. 

Discovery Dilemma . Another key bottleneck lies in agent discovery, which departs fundamentally from the Internet’s DNS. Instead of exact name-to-address resolution, discovery is similar to a search engine, matching user intent to ranked capabilities through semantic resolution. This raises several intertwined issues. First, the challenge of matching demand and capability is no longer a simple lookup problem; it requires contextual understanding, assessment of quality, and interpre-tation of user intent to rank multiple dynamic capabilities. Second, discovery must also incorporate mechanisms for con-flict resolution and misuse prevention. For instance, in weather forecasting, different providers’ agents may simultaneously return contradictory outputs, making trust management and conflict resolution indispensable. Third, the assignment and management of agent identifiers become critical as the ecosys-tem scales. Agents require unique and verifiable IDs, but whether these should be allocated by centralized platforms or dynamically generated based on content and capabilities remains an open question, with significant implications for both scalability and governance. 

3) Scalability Development Opportunities: Drawing lessons from Internet design, future agent communication systems may embrace the following development opportunities: Global Agent Identity . A Global Agent Identity System is required to issue and manage verifiable, resolvable, and unique IDs for all agents. Unlike traditional addressing, these identifiers would be designed to embed descriptive metadata about an agent’s capabilities and roles. A hierarchical parsing and addressing scheme within GAIS would then enable both reliable identification and intent-driven routing in large-scale, heterogeneous environments. 

Capability Discovery . A Capability Discovery System would establish a layered, distributed service to replace cen-tralized, omniscient registries. Functioning as a DNS for the agent era, this system would resolve queries for “what can be done” (finding capabilities) rather than “where is it” (finding addresses). Through mechanisms like caching and hierarchical aggregation, a CDS would enable agents to dynamically locate appropriate peers based on their intentions and functional descriptions, rather than relying on static identifiers. 

Autonomous Agent . Inspired by Internet Autonomous Sys-tems, agents can be grouped into cooperative clusters that handle internal communication efficiently while exposing stan-dardized boundary interfaces for cross-cluster interaction. This transforms the uncontrolled explosion of connections into a manageable engineering problem of intra-group optimization and inter-group standardization. 

C. Security 1) Definition of Security and Internet Implementation: 

Security refers to a communication system’s ability to protect against malicious attacks, unauthorized access, and misuse, while ensuring confidentiality, integrity, and availability of data and services. The Internet has accumulated a layered security architec-ture over decades, including authentication and authorization frameworks (e.g., OAuth 2.0), secure transmission protocols (e.g., TLS/SSL), and auditability mechanisms such as digital signatures and Public Key Infrastructure (PKI). These mecha-nisms enable reliable trust management across billions of het-erogeneous entities, but they also expose important limitations when extended to the more dynamic and semantically complex world of agent communication. 

2) Security Challenges: With the proliferation of au-tonomous agents, security challenges expand beyond conven-tional network threats, encompassing identity, delegation, data confidentiality, and systemic governance. These challenges can be grouped into three categories: 

Authentication and Authorization . Multi-agent systems demand not only identity authentication but also capability authentication. An agent may represent a user, an organization, or even another agent, creating multi-layered trust chains. This raises three intertwined issues. First, identity authentication is essential to prevent impersonation attacks in which mali-cious agents attempt to disguise themselves as trusted service providers. Second, capability authentication must ensure that an agent truly possesses the skills or permissions it claims, rather than merely advertising false capabilities. Third, del-egated authorization becomes critical in complex workflows, where permissions are passed from a user to an agent, and potentially onward to other agents. Such delegation increases the risk of privilege escalation or misuse, making robust au-thorization mechanisms indispensable for secure collaboration in large-scale agent ecosystems. 

Communication and Data Transmission . The dynamic and high-frequency communication of agents introduces unique risks in data transmission. One critical threat is the man-in-the-middle (MITM) attack: without strong end-to-end encryption, adversaries may intercept, manipulate, or replay the messages exchanged among agents. Another challenge concerns context leakage. Since agents rely on sharing con-textual information to collaborate, excessive disclosure can expose sensitive data, such as personal schedules or proprietary organizational knowledge. Addressing these risks requires secure communication protocols that balance information shar-ing with confidentiality and integrity guarantees. 

Governance and Accountability . Open multi-agent ecosys-tems pose significant challenges in governance and traceabil-ity, which can be broadly categorized into three aspects. First, abuse prevention is critical, as agents may be exploited for misinformation, spam, or automated attacks. A particularly concerning risk is discovery abuse, where agents manipulate ranking algorithms in ways analogous to adversarial prac-tices in Search Engine Optimization (SEO). By falsifying capabilities, padding descriptions with irrelevant metadata, or fabricating usage statistics, agents can gain disproportionate visibility. Such “agent-oriented SEO manipulation” threatens the integrity of discovery services and erodes ecosystem trust. Second, content and behavior management becomes essential, since autonomous agents can generate harmful or misleading outputs that require effective moderation strategies. Third, traceability and non-repudiation remain pressing concerns: when agents act erroneously or maliciously, assigning re-sponsibility is difficult without tamper-proof audit trails and accountability mechanisms. 

3) Security Development Opportunities: Drawing on Inter-net security experience while addressing agent-specific com-plexities, several development opportunities can be identified: 

Agent Identity Authentication . Establishing a standardized identity layer with globally unique identifiers for agents is crit-ical. These IDs should be verifiable and resolvable. Embedding metadata that describe an agent’s skills and capabilities into credential certificates enables validation of both identity and competence. 

Authorization Framework . Traditional user–service au-thorization must evolve into multi-entity frameworks that account for both user-to-agent and agent-to-agent interactions. In the first case, user-to-agent authorization requires clear mechanisms for scoping permissions and enabling timely revocation, ensuring that users retain effective control over what their agents can do. In the second case, agent-to-agent delegation necessitates cryptographically enforced constraints on the scope of delegated rights, so as to prevent privilege escalation when permissions are passed along through chained grants. Together, these mechanisms form the foundation for secure and accountable authorization in large-scale multi-agent ecosystems. 

Minimal Context Sharing . To mitigate information leak-age, communication protocols should adopt the “least context exposure” principle, with fine-grained data isolation, content redaction, and privacy-preserving techniques ensuring agents exchange only what’s strictly necessary. 

Trusted Governance and Accountability Mechanisms .Ensuring integrity in agent ecosystems requires mechanisms that combine trustworthy discovery with accountable gover-nance. Discovery should move beyond unverifiable claims by embedding Verifiable Capability Credentials into the process, allowing skills and permissions to be cryptographically proven. To counter manipulation such as “Agent SEO”, reputation systems can rank agents based on historical performance, feedback, and auditable logs—functioning as a “PageRank for Agents”. In parallel, governance can be reinforced through digital signatures and distributed ledger technologies, ensuring tamper-resistant records, auditability, and non-repudiation. To-gether, these mechanisms foster a meritocratic and accountable discovery environment where trust, not manipulation, deter-mines visibility. 

D. Real-time 1) Real-time Definition and Internet Implementation: Real-time capability refers to a system’s ability to respond within strict time constraints when receiving inputs or events. The core metric is low end-to-end latency and strong state con-sistency. In practical scenarios such as live streaming, on-line gaming, or AR/VR applications, real-time performance directly affects user-perceived responsiveness (e.g., video stut-tering or control lag). For autonomous vehicles, real-time communication between sensors and control units must occur within milliseconds to ensure safe navigation. The Internet has achieved real-time capabilities through both protocol-layer and architecture-layer optimizations. At the protocol level, UDP provides lightweight, connectionless transmission suitable for latency-sensitive applications (e.g., VoIP, live streaming). QUIC, built on UDP, integrates en-cryption, multiplexing, and 0-RTT connections, significantly reducing handshake latency (e.g., YouTube Live). WebRTC enables peer-to-peer low-latency communication in browsers, supporting multiparty conferencing via Pub/Sub mechanisms. At the architectural level, Content Delivery Networks (CDN) cache static and dynamic content at global edge nodes, reduc-ing access latency (e.g., Netflix video loading time improved by 50%). Edge computing further brings computation closer to end users (e.g., MEC for IoT or AR rendering), avoiding round trips to central clouds. In addition, intelligent routing techniques such as SDN dynamically optimize paths based on bandwidth, delay, and packet loss metrics. 

2) Real-time Challenges: In multi-agent systems, real-time demands are particularly stringent. 

State synchronization latency . In highly dynamic envi-ronments, agents must synchronize critical state information at millisecond-level precision. Delays may cause inconsistent worldviews among agents. For example, warehouse robots competing for the same shelf due to unsynchronized positions may collide, and distributed UAV swarms may lose formation if their locations are not consistently updated. In autonomous driving scenarios, vehicles must share position and velocity data within 10ms to prevent collisions—a requirement TCP cannot meet due to its retransmission overhead. 

Protocol design dilemma . Agent communication must simultaneously support ultra-low latency (e.g., control signals within <10ms) and structured data processing (e.g., semantic-rich messages containing priorities, logic, or environmental metadata). For instance, a video-analysis agent must stream frames in real-time (bandwidth-intensive) while also parsing semantic annotations (computation-intensive), creating ten-sion between transmission speed and processing overhead. In industrial automation, robotic arms coordinating assembly operations require both sub-millisecond control signals and complex task context exchange. 

3) Real-time Development Opportunities: Addressing real-time challenges requires architectural innovations combined with protocol redesign. 

Integrating edge computing with asynchronous event-driven models . For instance, multi-access edge computing (MEC) nodes can handle local synchronization and lightweight decision-making, reducing round-trip delays to under 5ms in scenarios like vehicle-to-infrastructure (V2I) coordination or industrial robotics. In smart factories, edge-deployed agents synchronize robotic arm movements with local processing, avoiding cloud latency. Asynchronous publish/subscribe mech-anisms (e.g., DDS in ROS2) can further decouple depen-dencies, avoiding blocking behaviors during high-frequency interactions. 

Reliability-tunable communication protocols . Building on QUIC/UDP, such protocols could offer 0-RTT connection establishment, forward error correction (FEC) for unreliable updates, and congestion-aware transmission rate control (e.g., BBR), while dynamically adapting reliability requirements. For example, emergency stop commands in industrial settings must be fully reliable, whereas temperature sensor updates can tolerate occasional loss. These protocols should support context-aware reliability selection, where critical operations automatically trigger higher reliability modes. 

E. High Performance 1) High Performance Definition and Internet Implementa-tion: High performance emphasizes high bandwidth utiliza-tion, massive concurrent processing, and throughput stability under extreme workloads. In systems supporting millions of concurrent users (e.g., social media platforms, online gaming), high performance ensures system stability and responsiveness during traffic spikes. For large-scale IoT deployments, high performance enables efficient data collection from millions of devices while maintaining low latency for critical operations. The Internet achieves high performance through layered optimizations. Content Delivery Networks (CDN) distribute content globally, reducing origin server load and improving access speeds. Load balancers distribute traffic across multiple servers to prevent overload. Protocol optimizations include HTTP/2 for multiplexed requests and HTTP/3 for improved congestion control. At the network infrastructure level, tech-niques like traffic engineering, bandwidth reservation, and Quality of Service (QoS) mechanisms prioritize critical traffic. Cloud computing provides elastic resources that scale dynam-ically with demand, while edge computing reduces backhaul traffic by processing data locally. 

2) High Performance Challenges: High performance in agent communication introduces distinct bottlenecks. 

Heterogeneous scheduling complexity . Agents often mix RPC-style control instructions, event-driven status updates, and continuous streaming, each with divergent latency, reli-ability, and bandwidth requirements. For example, in smart city management, traffic control agents use RPC for inter-section coordination, environmental sensors employ event-driven alerts, and surveillance cameras stream continuous video—creating conflicting demands on network resources. 

Large-scale concurrency and resource management . At the million-agent scale (e.g., IoT clusters in smart cities), net-works must allocate bandwidth fairly among urgent tasks (e.g., fire alarms) and routine tasks (e.g., environmental sensing), enforce task priorities (e.g., surgical robot commands over warehouse routing updates), and prevent congestion collapse during traffic bursts. During emergency response scenarios, thousands of agents may simultaneously request communica-tion resources, overwhelming traditional static QoS strategies that cannot adapt effectively to such dynamic workloads. 

3) High Performance Development Opportunities: For high performance, development opportunities include protocol, in-terface, and QoS standardization to ensure interoperability and guarantee resource allocation. Unified frameworks should cover diverse modes such as RPC, event-driven messaging, and streaming, with standardized formats (e.g., Protobuf se-rialization). Hierarchical QoS guarantees could classify flows into Platinum (e.g., urgent control <10ms), Gold (e.g., collab-orative formation <50ms), and Bronze (e.g., batch analytics 

<500ms), mapping task priorities to network-level enforce-ment. In smart grid management, critical control commands receive Platinum QoS while routine monitoring uses Bronze resources. 

Globally optimized intelligent scheduling algorithms .The algorithms represent another key opportunity. These al-gorithms can balance multiple dimensions including real-time bandwidth, latency, packet loss, and task roles (e.g., leader vs. follower). Techniques such as adaptive bitrate streaming, resource pre-reservation for critical agents, and edge caching of frequently used models (similar to CDN) can mitigate congestion. For instance, in large-scale drone swarms, leader drones receive reserved bandwidth while follower drones use adaptive rates based on network conditions. 

AI Agent Quality of Experience (QoE) . It extends beyond human perception, emphasizing metrics such as task com-pletion accuracy, decision timeliness, coordination efficiency, and energy/resource consumption. For example, in disaster response scenarios, QoE metrics would prioritize task com-pletion rate (e.g., 95% of areas surveyed within 2 hours) and resource efficiency (battery consumption per mission). By mapping QoE requirements to network-level QoS, future systems can optimize both communication infrastructure and collaborative task effectiveness, ensuring that large-scale MAS operate with both responsiveness and robustness. 

F. Manageability 1) Definition of Manageability and Internet Implementa-tion: Manageability refers to a system’s ability to support real-time monitoring, dynamic configuration, performance opti-mization, and fault diagnosis. Its foundation lies in observabil-ity (metrics, logs, traces) and controllability (APIs, policies). In distributed environments, manageability and collaboration are interdependent: manageability provides transparency and control, while efficient collaboration relies on standardized interfaces and reliable management tools. The Internet has long relied on layered mechanisms to ensure manageability. Monitoring is supported by protocols such as SNMP and telemetry stacks like Prometheus+Grafana; configuration is enabled through standards such as NET-CONF/YANG and automated frameworks like Ansible and Terraform; and fault diagnosis is facilitated by distributed tracing systems (e.g., Jaeger) and log aggregation tools (e.g., ELK Stack). Service interoperability further benefits from HTTP/REST and gRPC, while structured formats such as JSON/XML and semantic frameworks like RDF/OWL en-hance information sharing. Naming and routing mechanisms such as DNS and BGP provide global scalability and co-ordination. These experiences collectively demonstrate how layered design and standardization enable large-scale system manageability, and they offer valuable lessons for agent com-munication systems. 

2) Manageability Challenges: Compared with the relatively structured Internet environment, agent communication presents additional manageability challenges. 

Heterogeneous interoperability and the semantic gap .Agents often need to exchange multimodal data—ranging from text and images to sensor streams—yet the absence of unified encapsulation standards complicates integration. For instance, autonomous driving systems must simultaneously handle high-bandwidth video feeds and structured LiDAR data, but existing protocols are not optimized for synchronized multimodal transmission. Similarly, in healthcare applications, exchanging CT images alongside diagnostic text requires dif-ferentiated QoS guarantees that are beyond the capabilities of traditional FTP/HTTP. Beyond data formats, semantic align-ment is even more difficult: different agents may interpret the same concept in conflicting ways (e.g., “emergency task” may denote shutdown in industrial robots but expedited delivery in logistics robots). Syntax-based schemas such as JSON can ensure structural correctness but cannot resolve such semantic ambiguities, while ontology-based reasoning often incurs prohibitive computational overhead. Integration of large language models (LLMs) into agent communication . LLM inference is computationally expen-sive, with typical response latencies exceeding one second, which is incompatible with real-time collaboration scenarios that demand sub-100ms responsiveness. In addition, LLM hallucinations pose serious risks in mission-critical domains, where misinterpreted instructions could lead to catastrophic consequences. Beyond inference itself, distributed context management remains difficult. Agents must maintain consis-tent task histories, environmental states, and role permissions across dynamic environments, but ensuring consistency, pri-vacy, and accountability simultaneously introduces fundamen-tal tensions that current frameworks struggle to resolve. 

3) Manageability Development Opportunities: Opportuni-ties for improving manageability in agent communication systems emerge along several promising directions. 

Semantic-aware communication architectures . Drawing inspiration from information-centric paradigms such as NDN and ICN, agents can interact based on content names rather than addresses, with edge nodes caching frequently used semantic units. Lightweight inference engines deployed at the edge (e.g., TensorFlow Lite) can perform real-time se-mantic annotation—such as labeling frames as “pedestrian crossing”—thus reducing central cloud workloads and improv-ing overall observability. The standardization of multimodal data encapsulation formats, such as MPEG-V combined with JSON-LD, would further facilitate cross-agent interoperability. 

Optimizing LLM integration . Techniques such as deploy-ing quantized or distilled models at the edge can reduce inference latency to hundreds of milliseconds, while asyn-chronous pipelines can divide tasks into preprocessing, edge inference, and cloud verification to balance responsiveness with accuracy. Reliability can be strengthened through hal-lucination suppression strategies, including knowledge-base validation using vector databases and cross-agent consensus, in which critical decisions are only executed when confirmed by multiple heterogeneous models. Furthermore, defining explicit task-level QoE metrics—such as consistency, privacy, and traceability—and mapping them to QoS guarantees would enable systematic evaluation and management of agent col-laboration. 

Incentive Alignment and Economic Models. The long-term manageability of the agent ecosystem is inseparable from its economic model. Drawing parallels to search engines, agent discovery platforms hold immense potential for monetization, such as through sponsored capabilities (analogous to search ads) where agents pay for premium placement. This introduces a fundamental tension: how to balance monetization with the neutrality and integrity of discovery results? A key opportunity lies in designing transparent, auditable advertising models and incentive mechanisms that reward genuine value creation over mere visibility. A sustainable business model will be a cornerstone of a well-managed, trustworthy agent ecosystem. 

Configurable management frameworks and standardiza-tion . Emerging paradigms such as policy-as-code can allow dynamic loading and enforcement of management policies at the edge, while blockchain-based auditing ensures ac-countability for sensitive operations. On the standardization front, semantic protocols (RDF/OWL), multimodal interaction interfaces, and LLM integration APIs (with latency SLAs and hallucination thresholds) will be critical. Together, these innovations pave the way toward configurable manageabil-ity, balancing centralized control with agent autonomy, and enabling multi-agent ecosystems to remain both flexible and governable at scale. IV. S TANDARDIZATION REQUIREMENTS 

As agent ecosystems evolve, standardization has become imperative for ensuring interoperability and trust. Several leading organizations have already taken initiative: the World Wide Web Consortium (W3C) has launched the AI Agent Protocol Community Group to define interoperable discov-ery, identity, and collaboration protocols for the Agentic Web 1; International Organization for Standardization (ISO) has issued standards 15067-3-30 [17] and 15067-3-31 [18], specifying communication protocols for energy management agents; Internet Engineering Task Force (IETF) has produced multiple drafts on agent communication, such as the agent name service [19], agent authorization [20] and the agent identification [21]. These efforts underscore the urgency of establishing standards that support high performance, scalabil-ity, interoperability, and security in the agent era. This section explores these emerging opportunities in depth. 

A. The Necessity and Opportunities of Standardization 1) Performance Standards: In an era of agents exchang-ing ultra-long, multi-modal contexts and engaging in real-time, multi-agent collaboration, the demands for bandwidth, throughput, and low latency escalate dramatically. The un-derpinnings of communication—particularly transport proto-cols—must therefore evolve. Innovations such as agent-centric QUIC adaptations and quality-of-service (QoS) mechanisms can ensure efficient, secure, and context-aware routing of agent messages. 

2) Scalability Standards: Effective discovery and intercon-nection of vast numbers of agents necessitate scalable naming and routing paradigms. Emerging concepts like “Agent-era DNS” or BGP-style routing for agents can provide structured, hierarchical discovery and reachability. For example, protocols such as the “Internet of Agents Protocol (IoA Protocol)” propose a layered, decentralized architecture tailored to het-erogeneous agent collaboration. Likewise, “AgentDNS” offers a unified naming, resolution, and secure invocation system for LLM agents, inspired by Internet DNS. 

3) Compatibility Standards: The diversity of agent commu-nication protocols—ranging from ontology-based languages like FIPA-ACL and KQML to newer, generative-AI-driven protocols such as NLIP—leads to fragmentation and inef-ficiency. Standardizing message formats, performatives, and interaction protocols, and achieving semantic interoperability, 

> 1https://www.w3.org/community/agentprotocol/

is essential. A shared, extensible interface would allow het-erogeneous agents and platforms to cooperate seamlessly. 

4) Security Standards: Agent collaboration introduces se-curity challenges absent in traditional networks—such as cascading authorization, dynamic delegation, and privacy-sensitive context sharing. Developing formal security protocols and standards for identity authentication, capability attestation, delegation constraints, and confidentiality guarantees is indis-pensable for safeguarding large-scale agent ecosystems. 

B. Layers for Standardization 1) Lower-Layer (Transport Layer) Standardization: The transport layer encompasses foundational protocols responsi-ble for reliable, orderly, and efficient data exchange between endpoints. Core protocols such as TCP and UDP have served this role for decades, providing error correction, flow control, and multiplexing capabilities. In recent years, QUIC has emerged as a modern addition to this layer: a UDP-based, multiplexed protocol engineered to reduce latency, avoid head-of-line blocking, and resist protocol ossification through en-crypted metadata and flexible extension mechanisms. Standardizing at this layer yields notable advantages. A unified, efficient transport substrate ensures that diverse agent systems benefit from consistent performance characteristics and baseline security guarantees. QUIC’s design, for instance, demonstrates how transport-layer evolution can deliver im-proved latency and extensibility without requiring kernel-level modifications. Moreover, broad adoption of such standards can facilitate interoperable and high-throughput agent-to-agent exchanges in distributed settings. Nonetheless, there are inherent challenges. Creating consen-sus around new transport standards is a slow, multi-stakeholder endeavor and may lag behind rapid shifts in agent communi-cation needs. Once entrenched, these standards often exhibit “ossification,” making future extension or adaptation difficult. Historical trends—such as the limited adoption of the OSI model, versus the pragmatic dominance of TCP/IP—highlight the danger that rigid bottom-up standards may stifle innovation rather than support it. 

2) Upper-Layer (Application Layer) Standardization: 

Application-layer standardization concerns the syntax, seman-tics, and protocols that enable agents to understand, coordinate, and invoke each other’s capabilities. Emerging standards like the Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP) address cross-agent interoperability by defining structured messaging formats, discovery mecha-nisms, and peer-to-peer orchestration frameworks. Standardizing at the application layer provides agility and responsiveness to domain-specific use cases. For example, ACP enables REST-native, asynchronous, multi-modal com-munication, while A2A facilitates peer-to-peer task delega-tion using capability-based cards—allowing rapid evolution of agent workflows tailored for collaborative pipelines and enterprise-scale deployment. This flexibility empowers devel-opers to quickly adapt communication protocols to new agent collaboration forms without waiting for slow consensus cycles. However, such modular autonomy introduces risks of frag-mentation. Without coordination across industries and plat-forms, application-level standards may proliferate into isolated “protocol islands,” undermining interoperability. Agents built on different standards may struggle to discover or collaborate with one another, negating many benefits of standardization. As the landscape evolves, balancing standard flexibility with ecosystem coherence becomes a central governance challenge. 

C. Competition for Agent Communication Entry Points 

Beyond technical standardization, a key strategic issue is the competition among different players for control of the primary entry points of agent communication, reminiscent of past struggles over browsers, search engines, and super-apps. Hardware and OS vendors seek dominance by embedding default agents, while application platforms leverage user bases and network effects, though both face constraints from ecosys-tem dependencies. Cloud providers and model companies contribute advanced AI capabilities but lack direct consumer interfaces, whereas telecom operators rely on network and identity control yet struggle with weak user engagement. These competitions map onto different layers of standardization: hardware vendors and operators favor lower-layer standards 

(e.g., transport protocols, identity infrastructures) to retain performance and reliability control; application platforms and cloud providers push for upper-layer standards (e.g., com-patibility, semantic interoperability) to differentiate services; model providers emphasize performance and scalability stan-dards ; and various stakeholders promote security standards 

to strengthen trust and governance. The outcome of this race for entry points will shape the direction of standardization: dominant players may push proprietary protocols that fragment ecosystems, while open standards promise interoperability but face slower adoption. V. C ONCLUSION 

In this paper, we reviewed the evolution of agent communi-cation technologies and distilled five key elements from Inter-net architecture: scalability, security, real-time performance, high performance, and manageability. Using these elements, we analyzed the main challenges and opportunities of agent communication and outlined potential standardization path-ways. Nonetheless, this study has limitations: it does not fully cover fast-evolving technologies, practical deployment cases remain limited, and the impact of ongoing standardization efforts has yet to be validated. Future work should broaden the scope of technology review, integrate empirical case studies, and evaluate the outcomes of emerging standardization efforts. REFERENCES [1] Google, “Agent-to-agent (a2a) project pages and key concepts,” https: //github.com/google/a2a, 2025, accessed: 2025-08-27. [2] ANP Contributors, “Agent network protocol (anp) specifications and repository,” https://github.com/agent-network-protocol/ AgentNetworkProtocol, 2025, accessed: 2025-08-27. [3] T. Finin, R. Fritzson, D. P. McKay, R. McEntire et al. , “Kqml-a language and protocol for knowledge and information exchange,” in 13th Int. Distributed Artificial Intelligence Workshop , 1994, pp. 93–103. [4] A. Fipa, “Fipa acl message structure specification,” Foundation for Intel-ligent Physical Agents, http://www. fipa. org/specs/fipa00061/SC00061G. html (30.6. 2004) , 2002. [5] D. Hardt, “Rfc 6749: The oauth 2.0 authorization framework,” 2012. [6] G. Kellogg, P.-A. Champin, and D. Longley, “Json-ld 1.1–a json-based serialization for linked data,” Ph.D. dissertation, W3C, 2019. [7] X. Wang, H. Zhao, and J. Zhu, “Grpc: A communication cooperation mechanism in distributed systems,” ACM SIGOPS Operating Systems Review , vol. 27, no. 3, pp. 75–86, 1993. [8] C. Samsel, S. G¨ okay, P. Heiniz, and K.-H. Krempels, “Web service to json-rpc transformation,” in International Conference on Software Engineering and Applications , vol. 2. SCITEPRESS, 2013, pp. 214– 219. [9] D. Reed, M. Sporny, D. Longley, C. Allen, R. Grant, M. Sabadello, and J. Holt, “Decentralized identifiers (dids) v1. 0,” Draft Community Group Report , 2020. [10] Anthropic, “Model context protocol (mcp): Announcement and specifi-cation,” https://github.com/modelcontextprotocol, 2025, accessed: 2025-08-27. [11] K. Huang, V. S. Narajala, I. Habler, and A. Sheriff, “Agent name service (ans): A universal directory for secure ai agent discovery and interoperability,” arXiv preprint arXiv:2505.10609 , 2025. [12] F. Bellifemine, A. Poggi, and G. Rimassa, “Developing multi-agent systems with jade,” in International workshop on agent theories, ar-chitectures, and languages . Springer, 2000, pp. 89–103. [13] Q. Wu, G. Bansal, J. Zhang, Y. Wu, B. Li, E. Zhu, L. Jiang, X. Zhang, S. Zhang, J. Liu et al. , “Autogen: Enabling next-gen llm applications via multi-agent conversations,” in First Conference on Language Modeling ,2024. [14] Dify Team, “Dify documentation: Agent node and workflow orchestra-tion,” https://docs.dify.ai/, 2025, accessed: 2025-08-27. [15] LangChain and Langgraph, “Langgraph studio and documentation,” https://www.langchain.com/langgraph, 2025, accessed: 2025-08-27. [16] OpenAI, “Openai agents sdk documentation and repository,” https: //openai.github.io/openai-agents-python/, 2025, accessed: 2025-08-27. [17] ISO/IEC JTC 1/SC 25, “Energy management agent functional requirements and interfaces,” International Organization for Standardization / International Electrotechnical Commission, ISO/IEC Standard 15067-3-30:2024, March 2024. [Online]. Available: https://www.iso.org/standard/89221.html [18] ——, “Protocol of energy management agents for demand-response energy management and interactions among these agents,” International Organization for Standardization / International Electrotechnical Commission, ISO/IEC Standard 15067-3-31:2024, March 2024. [Online]. Available: https://www.iso.org/standard/89222.html [19] H. Ken, S. N. Vineeth, H. Idan, and S. Akram, “Agent name service (ans): A universal directory for secure ai agent discovery and interoperability,” Internet Engineering Task Force, Internet-Draft draft-narajala-ans-00, May 2025. [Online]. Available: https: //datatracker.ietf.org/doc/draft-narajala-ans/ [20] R. Jonathan and W. Pat, “Aauth - agentic authorization oauth 2.1 extension,” Internet Engineering Task Force, Internet-Draft draft-rosenberg-oauth-aauth-00, July 2025. [Online]. Available: https: //datatracker.ietf.org/doc/draft-rosenberg-oauth-aauth/ [21] Y. Kehan and L. Peng, “Digital identity management for ai agent communication protocols,” Internet Engineering Task Force, Internet-Draft draft-yl-agent-id-requirements-00, July 2025. [Online]. Available: https://datatracker.ietf.org/doc/draft-yl-agent-id-requirements/
