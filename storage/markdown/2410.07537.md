Title: Understanding the AI-powered Binary Code Similarity Detection

URL Source: http://arxiv.org/pdf/2410.07537v1

Published Time: Fri, 11 Oct 2024 00:22:30 GMT

Markdown Content:
# Understanding the AI-powered Binary Code Similarity Detection 

# Lirong Fu ∗

fulirong007@zju.edu.cn HangZhouDianZi University Hangzhou, Zhejiang, China 

# Peiyu Liu 

liupeiyu@zju.edu.cn Zhejiang University Hangzhou, Zhejiang, China 

# Wenlong Meng 

mengwl@zju.edu.cn Zhejiang University Hangzhou, Zhejiang, China 

# Kangjie Lu 

kjlu@umn.edu University of Minnesota Minneapolis, Minnesota, USA 

# Shize Zhou 

3210100630@zju.edu.cn Zhejiang University Hangzhou, Zhejiang, China 

# Xuhong Zhang 

xuhongnever@gmail.com Zhejiang University Hangzhou, Zhejiang, China 

# Wenzhi Chen 

chenwz@zju.edu.cn Zhejiang University Hangzhou, Zhejiang, China 

# Shouling Ji 

sji@zju.edu.cn Zhejiang University Hangzhou, Zhejiang, China 

## ABSTRACT 

AI-powered binary code similarity detection ( BinSD ), which trans-forms intricate binary code comparison to the distance measure of code embedding through neural networks, has been widely ap-plied to program analysis. However, due to the diversity of the adopted embedding strategies, evaluation methodologies, running environments, and/or benchmarks, it is difficult to quantitatively understand to what extent the BinSD problem has been solved, especially in real-world applications. Moreover, the lack of an in-depth investigation of the increasingly complex embedding neural networks and various evaluation methodologies has become the key factor hindering the development of AI-powered BinSD. To fill these research gaps, in this paper, we present a systematic evaluation of state-of-the-art AI-powered BinSD approaches by conducting a comprehensive comparison of BinSD systems on sim-ilar function detection and two downstream applications, namely vulnerability search and license violation detection. Building upon this evaluation, we perform the first investigation of embedding neural networks and evaluation methodologies. The experimental results yield several findings, which provide valuable insights in the BinSD domain, including (1) despite the GNN-based BinSD sys-tems currently achieving the best performance in similar function detection, there still exists considerable space for improvements; (2) the capability of AI-powered BinSD approaches exhibits significant variation when applied to different downstream applications; (3) ex-isting evaluation methodologies still need substantial adjustments. For instance, the evaluation metrics (such as the widely adopted ROC and AUC) usually fall short of accurately representing the model performance of the practical use in real-world scenarios. Based on the extensive experiments and analysis, we further pro-vide several promising future research directions. To facilitate future work in this interesting research, we will open source the entire datasets, benchmarks, and implementation details. 

> , ,
> .

## 1 INTRODUCTION 

BinSD aims to measure the similarity of two binary programs with-out the corresponding source code. Currently, BinSD has become a fundamental component supporting multiple applications, in-cluding vulnerability detection [ 9, 17 , 19 , 25 , 43 , 49 , 56 , 60 , 65 , 67 ], license violation detection [ 44 , 71 ], patch analysis [ 33 , 72 ], and mal-ware analysis [ 13 , 31 , 52 ]. Particularly, BinSD has become the de facto approach to decrease the severe consequences brought by supply-chain attacks [ 9, 17 , 19 , 25 , 43 , 49 , 60 , 65 , 67 ]. Over the past few decades, researchers have proposed many remarkable BinSD approaches that enable different capabilities and characteristics in the problem domain. Unfortunately, significant binary changes in-troduced by compilation configurations, instruction set architecture (ISA), and toolchains make BinSD non-trivial. Moreover, with the rapid development of software engineering, modern software pro-grams typically include an increasing number of lines of code and are widespread, resulting in an overwhelming number of programs with complex structures and making BinSD harder than ever. Under the immense surge in AI technologies, including natural language processing (NLP) [ 12 , 29 , 46 , 50 ] and graph representa-tion learning (GRL) [ 11 , 68 ], many research works [ 4, 17 , 18 , 23 ,25 , 36 , 38 , 40 , 42 , 45 , 48 , 49 , 54 , 57 , 65 – 67 , 70 , 74 ] are proposed to apply the widely used neural networks such as graph neural net-work (GNN) [ 20 ], convolutional neural network (CNN) [ 5 ], and long short-term memory (LSTM) [ 26 ] to solve BinSD problem. Compared to traditional BinSD approaches [ 15 , 16 , 21 , 24 , 30 , 35 , 37 , 44 , 51 , 53 ,55 , 56 , 56 , 60 ] that employ various strategies such as symbolic exe-cution, graph matching, hashing technique, etc., AI-powered BinSD approaches have achieved considerable success [ 65 ]. Nonetheless, understanding the extent to which the BinSD problem has been resolved remains challenging due to the following reasons. 

Inconsistent embedding strategies and evaluation methodologies. 

Due to the inconsistencies of the adopted embedding strategies, running environments, and/or benchmarks, it is difficult to under-stand the performance of existing AI-powered BinSD approaches in a fair and quantitative way. Specifically, as shown in Table 1, we carefully select and investigate 24 representative AI-powered BinSD systems published in recent years. We find that each BinSD  

> 1
> arXiv:2410.07537v1 [cs.SE] 10 Oct 2024 , , Lirong Fu, Peiyu Liu, Wenlong Meng, Kangjie Lu, Shize Zhou, Xuhong Zhang, Wenzhi Chen, and Shouling Ji

tool adopts a different set of binary feature extraction methods, code embedding networks, and evaluation datasets, which impedes a fair understanding of the AI-powered BinSD tools. 

Opaque neural network understanding . Currently, the neural net-work models used to embed binaries in BinSD are becoming in-creasingly complex and diverse. However, most neural networks are still treated as black-box function approximators [ 8 ]. Thus, the complex structure of neural networks and opaque data processing procedures may hinder the justification and users’ trust when there is a lack of an in-depth model understanding. 

The lack of in-depth metric understanding . In Table 1, 16 out of the 24 BinSD approaches employ receiver operating characteristics (ROC) curve or area under curve (AUC) values, two discrimination indexes used to evaluate classifiers to show the BinSD performance. However, BinSD is more similar to the recommendation process— given a query item, reporting the top-K similar search results from a predefined searching repository. It is unknown whether these ma-chine learning metrics are sufficient for evaluating the performance of existing BinSD approaches. Insufficient metrics would mislead users about the actual capabilities of BinSD approaches. 

The lack of downstream application evaluation . We note that most BinSD approaches do not actually examine their performance in real-world downstream applications, which may confuse users about their true capabilities. Indeed, part of BinSD works [ 23 , 25 , 65 ]evaluated their effectiveness by vulnerability detection. However, such case studies may not be representative enough. After all, each downstream application has its own characteristics and requires dif-ferent BinSD abilities. Currently, the research community still lacks a systematic understanding of how well the AI-powered BinSD tools support downstream applications. Without an in-depth investigation of the above problems, it is difficult to boost meaningful and effective progress in the BinSD field. To fill these research gaps, we first perform a comprehensive evaluation of the AI-powered BinSD approaches by using the same benchmarks and evaluation metrics on the tasks of similar function detection and downstream application utilities. We set up two rep-resentative applications—vulnerability search and license violation detection, as they are the primary downstream applications that existing BinSD approaches claim that they can support. Then, we perform an in-depth analysis of the adopted binary embedding neural networks and evaluation methodologies to understand their strengths and limitations. In summary, we aim to investigate the following three aspects of the BinSD research field: 

• Evaluating how state-of-the-art AI-powered BinSD meth-ods perform in similar function detection and downstream applications; 

• Understanding the widely adopted embedding neural net-works and evaluation methodologies in BinSD approaches; 

• Exploring the potential promising future directions. To achieve these research goals, we design and implement a framework that includes representative state-of-the-art AI-powered BinSD tools (function level approaches), two datasets, and evalua-tion benchmarks. With the framework, we conduct comprehensive evaluations and make insightful observations that amend or comple-ment prior knowledge. The main findings include: (1) despite GNN-based BinSD approaches performing the best in similar function detection, their intrinsic limitation is the problem of “embedding collision”—different binary functions happen to have similar func-tion embeddings; (2) the BinSD capability varies when applied to different downstream applications. For instance, GNN-based BinSD approaches achieve better performance on vulnerability search, while CNN-based BinSD approaches are more applicable in the detection of license violations; (3) the evaluation metrics used in many existing AI-powered BinSD approaches are insufficient to represent the actual capability of an AI-powered BinSD tool; (4) the problem of BinSD has not been well addressed, and most existing BinSD solutions still have considerable improvement space. Beyond the above insightful findings, we also provide several promising strategies to enhance BinSD . For instance, the proposed embedding concatenation and graph alignment can highly likely alleviate the “embedding collision” problem. In summary, The key contributions of this paper are summarized as follows. 

• We present a systematic evaluation of the state-of-the-art AI-powered BinSD approaches in a quantitative and fair way. The experimental results show that the current embedding neural networks and evaluation methodologies still need further improvements, implying that both subjectivity and bias may exist in the evaluation of prior BinSD approaches. 

• We employ two widely used downstream applications to validate the utilities of current AI-powered BinSD tools. We observe that many BinSD tools are applicable for license violation detection. By contrast, for vulnerability search, the accuracy of existing BinSD approaches is still far from expectation. 

• We summarize the findings and implications of our study, which can help improve the understanding of the existing bi-naries embedding neural networks. Our findings can further provide guidelines for future improvements to the BinSD problem. We will open-source the artifacts to support the research com-munity for further research and the enhancement of BinSD. 

## 2 BACKGROUND 

In this section, we first elaborate on the workflow of BinSD ap-proaches to understand the mainstream solutions. Then, we de-scribe the paper selection process of our measurement. 

## 2.1 Workflow of AI-powered BinSD 

Despite the AI-powered BinSD approaches are often built on dif-ferent techniques and complex neural networks, their mainstream workflow usually consists of three phases: preprocessing, code representation, and code embedding. 

Preprocessing. Given two binary code snippets in the first phase, the AI-powered BinSD tools usually perform binary fea-ture extraction. Specifically, after code disassembly, most BinSD approaches [ 18 , 23 , 49 , 74 ] perform instruction normalization to avoid the out-of-vocabulary (OOV) problem [ 69 ] (a common issue in NLP). Besides, some BinSD tools [ 9, 17 ] perform selective callee inline to decrease the CFG change brought by function inlining. Several other BinSD tools [ 25 , 65 ] perform manual feature extrac-tion or instruction embedding [ 23 , 49 , 70 ] to obtain binary features as the unique label of binaries.  

> 2Understanding the AI-powered Binary Code Similarity Detection , ,

Table 1: Comparison among state-of-the-art AI-powered BinSD approaches. MFE = manual feature extraction. AE = assembly extraction. IRE = intermediate representation extraction. ASTE = abstract syntax tree extraction. PStructure2Vec = Parameterized Structure2Vec network. NGMN = node-graph matching network. Func = function level. BB = basic block level. INST = instruction level. P@K = precision at the top-K search result. DEV = deviation.                                                                                                                                                               

> Detector Feature Extraction Embedding Networks Approach Granularities Evaluated Arches Bug Detection License Violation Detection Evaluated Metrics
> VulHawk [45] RoBERTa GCN Func x86, x64, MIPS MIPS64, ARM, AArch64 ✓✗
> ROC [73], AUC [73] P@K [6], R@K [6] Gemini [65] MFE PStructure2Vec Func x86, MIPS, ARM ✓✗ROC [73], AUC [73], P@K [6] Gemini-skip [23] skip-gram PStructure2Vec Func x86, MIPS, ARM ✗✗ROC [73], AUC [73], P@K [6] VulSeeker [25] MFE DNN Func x86, x64, MIPS MIPS64, ARM, AArch64 ✓✗
> ROC [73], AUC [73] ACC [73], P@K [6] VulSeeker-skip [23] skip-gram DNN Func x86, x64, MIPS MIPS64, ARM, AArch64 ✗✗
> ROC [73], AUC [73] ACC [73], P@K [6] Asm2Vec [17] AE PV-DM Func x86 ✓✗P@K [6], TPR[73] SAFE [49] skip-gram SAN Func x64, ARM ✓✗ROC [73], AUC [73] UFE-mean [48] i2v_mean Structure2Vec Func x86, ARM ✗✗ROC [73], AUC [73] UFE-attention [48] i2v_attention Structure2Vec Func x86, ARM ✗✗ROC [73], AUC [73] UFE-rnn [48] i2v_rnn Structure2Vec Func x86, ARM ✗✗ROC [73], AUC [73] Focus [23] MFE GTN Func x86, x64, MIPS, MIPS64, ARM, AArch64, PPC, PPC64 ✗✗
> ROC [73] AUC [73], P@K [6] Focus-skip [23] skip-gram GTN Func x86, x64, MIPS, MIPS64, ARM, AArch64, PPC, PPC64 ✓✗
> ROC [73] AUC [73], P@K [6] BinaryAI-skipt [70] skip thought CNN, MPNN Func x64, ARM ✗✗Rank-1 [2], MRR [1] BinaryAI-bert2 [70] BERT CNN, MPNN Func x64, ARM ✗✗Rank-1 [2], MRR [1] MGMN [42] MFE SGNN, NGMN Func x86, MIPS, ARM ✗✗AUC [73] Oscar [54] IRE Transformer Func x86 ✗✗R@1 [6] Codee [66] skip-gram ADMM Func x64, MIPS, ARM ✓✗
> ROC [73] F1-score [73], R@K [6] Asteria [67] ASTE Tree-LSTM Func x86, x64, ARM, PPC ✓✗ROC [73], AUC [73]
> DeepBinDiff [18] CBOW TADW BB x86 ✓✗F1-score [73], CDF [58] RLZ [57] AE CBOW BB x64, ARM ✗✗ROC [73], AUC [73] INNEREYE [74] skip-gram LSTM BB x64, ARM ✗✗ROC [73], AUC [73] XBA [36] MFE GCN BB x64, AArch64 ✓✗Hit@K [43] PALMTREE [40] AE BERT INST x64 ✗✗ACC [73], AUC [73], DEV [3] Instruction2Vec [38] AE Text-CNN INST -✗✗ACC [73]

Code representation. After preprocessing, intuitively, one needs to consider the specific binary code representation as the input of embedding neural networks. According to our empirical analysis, the AI-powered BinSD tools usually represent a binary code snippet as an abstract syntax tree (AST), control flow graph (CFG), data flow graph (DFG), basic block adjacent matrix, sequential instruction sequences, or the combinations of them. 

Code embedding. Finally, based on the widely used neural net-works such as GNN, CNN, and RNN/LSTM, the AI-powered BinSD approaches convert specific code representation to low-dimensional vector—code embedding, which will be further compared to deter-mine the similarity score of the input binary code pairs. 

## 2.2 Paper Selection 

BinSD approaches are supposed to be used in large-scale down-stream applications. In this situation, most researchers employ static analysis to ensure efficiency and avoid the code coverage problem in dynamic analysis. Thus, this paper aims to evaluate static AI-powered BinSD approaches. Moreover, depending on the specific downstream applications, the granularity of BinSD approaches varies, including instruction, basic block, function, and binary file comparison. We mainly evaluate existing BinSD approaches at the function level—providing a pair of binary functions, we examine whether they are semantically similar (compiled from the same source code) because most BinSD approaches are based on function level comparison. Besides, the comparison of similar binary seman-tics implemented by different source code snippets and the analysis of the obfuscated binaries and malware (highly likely obfuscated programs) is out of our scope since most BinSD approaches do not support such binary analysis. Finally, this paper aims to measure existing research works specifically designed for the BinSD prob-lem. The evaluation of various embedding networks [ 20 , 41 ] that can be used to solve BinSD problem after extra feature engineering is also out of our scope. We clarify that this paper focuses on the investigation of the mainstream techniques employed in BinSD approaches instead of specific BinSD methods. Thus, we do not aim for the evaluation of all the AI-powered BinSD approaches but the representative state-of-the-art BinSD approaches (as shown in Table 1) using various embedding strategies. 

## 3 HOW BINSD APPROACHES PERFORM 

This section comprehensively evaluates how state-of-the-art AI-powered BinSD approaches perform in similar function detection and two representative downstream applications. 

Similar function detection. This section uses the same dataset and various metrics to perform a fair and comprehensive accu-racy and efficiency comparison. In the accuracy evaluation, given a query function, we investigate how many semantically similar func-tions (the ones compiled from the same source code as the query function) can be identified among a large number of functions in a repository by existing BinSD approaches. Specifically, from the  

> 3, , Lirong Fu, Peiyu Liu, Wenlong Meng, Kangjie Lu, Shize Zhou, Xuhong Zhang, Wenzhi Chen, and Shouling Ji

24 BinSD approaches listed in Table 1, we evaluate 15 represen-tative BinSD approaches that perform function level comparison. Oscar [ 54 ] is excluded because it requires too many computing resources (8 V100 GPUs). Besides, we also excluded Codee [ 66 ], XBA [ 36 ], and VulHawk [ 45 ], which are partially open-sourced and cannot be accurately implemented based on the available descrip-tions in the corresponding papers. We have yet to receive a reply or obtain limited support from the authors after contacting them. 

Downstream applications. Many researchers claim the pro-posed BinSD can be widely applied to vulnerability search and license violation detection in IoT firmware images. This paper thus evaluates the selected BinSD approaches in these two real-world scenarios. Considering we have comprehensively compared each BinSD approach in similar function detection and the considerable manual effort cost in the bug and license violation confirmation process, we sample several BinSD approaches to understand the actual performance of mainstream BinSD approaches according to the following rules. (1) Support BinSD analysis under ARM (the architecture of IoT devices analyzed in this evaluation). (2) Be ap-plicable in large-scale analysis. For instance, it takes more than six months for Focus-skip to generate the function embedding for all IoT firmware. Thus, we exclude this method. (3) Present the mainstream function embedding techniques, including RNN/LSTM-based, GNN-based, and CNN-based models. (4) Perform well in the similar function detection task; For instance, for Gemini variations, Gemini-skip performs the best. Thus, we include Gemini-skip in the downstream evaluation. Finally, we select five BinSD approaches to understand how state-of-the-art BinSD approaches perform in real-world scenarios. 

## 3.1 Implementation and Setup 

Implementation: Our implementation includes the following three aspects. (1) We correct part of the open-sourced BinSD ap-proaches that cannot be reproduced due to some logic errors or program implementation flaws. Moreover, we reimplement part of closed-sourced BinSD approaches such as BinaryAI-bert2 [ 70 ]. The reimplemented models achieve similar AUC as those reported in the corresponding papers. (2) To perform a comparative evaluation, this paper uses the same datasets to understand the capabilities of BinSD systems. For AI-powered approaches, when changing the experiment datasets, the hyper-parameters of neural networks may also need to change to ensure the best performance. Thus, we adjust the hyper-parameters for the evaluated BinSD embed-ding neural networks to ensure each BinSD approach presents the best performance when compared to other BinSD tools. (3) To calculate evaluation metrics such as precision, recall, and MAP in similar function detection, we need to generate binary function embeddings. Then, we can perform embedding comparisons. How-ever, most BinSD approaches do not provide function embedding generation methods. Thus, we implement the function embedding generation for these BinSD approaches. How we improve/reim-plement existing BinSD approaches, select hyper-parameters, and generate function embedding are publicly available in artifacts 1.

Setup: During the training process, the training, validation, and testing dataset are split according to each evaluated BinSD paper. 

> 1https://anonymous.4open.science/r/AI-powered-BinSD-Analysis-9251/

Moreover, similar to existing BinSD approaches, for a function pair 

(𝑓 1, 𝑓 2), if they are compiled from the same source code, we label their similarity to be 1. Otherwise, we label their similarity to be -1. All the experiments are conducted on a server equipped with 256GB RAM, two Intel CPUs (Xeon R CPU E5-2680, 56 core), and 4 GeForce GTX 2080 GPU cards. 

## 3.2 Evaluation Datasets 

Prior works use different evaluation datasets, making them not directly comparable. To avoid this problem, we carefully construct two representative datasets 1, which are publicly available. 

Basic-dataset. Basic-dataset is used to train the AI-powered BinSD models and perform similar function detection, consisting of 25 open-source programs, 33 ELFs, and 1,351,104 functions. This dataset involves diverse utilities, data-processing, and OS-assisting programs, including OpenSSL, Busybox, Findutils, etc. We compile these 25 programs with four popular optimization levels (from O0 to O3) under three architectures—ARM, x86, and x64 (supported by over half of the BinSD approaches in Table 1). 

Application-dataset. The application-dataset consists of ten vulnerable functions, querying libraries (OpenSSL and Busybox), and ten IoT firmware images. Specifically, the ten vulnerable func-tions in the widespread third-party library OpenSSL consist of numeric errors, resource management errors, and improper input validation. The ten IoT firmware images involving various utilities, including switch, router, IP camera, and access point, are randomly chosen from four leading IoT vendors (DLink, TPLink, Tuya, and Trendnet). We claim that in this evaluation, we do not plan to per-form a large-scale bug or license violation detection to investigate the security state of a large number of IoT firmware images. By con-trast, we aim to evaluate how the AI-powered BinSD approaches perform in real-world scenarios. Thus, the ten IoT firmware images which include 1,935 ELFs and 651,048 functions are enough for this evaluation. 

## 3.3 Similar Function Detection 

This section performs accuracy and efficiency comparisons of the selected BinSD approaches. According to prior research [ 23 , 43 ], cross-version and cross-compiler analysis of binary snippets com-piled with the same architecture and optimization level is a straight-forward BinSD problem and has been well addressed (since the change of cross-version and cross-compiler binaries is relatively small as discussed in our artifacts). Thus, this paper conducts cross-optimization level and architecture evaluation. 

3.3.1 Accuracy Evaluation. We first perform cross-optimization level and architecture comparison. Then, we investigate two fac-tors that may affect the performance of a BinSD tool. Specifically, according to our empirical analysis, prior researchers usually label a repository function in the top-K search list as a true positive if the repository function has the same name as the query function. Other-wise, the repository function is a false positive. However, sometimes the repository function is just renamed and is still compiled from the same source code as the query function. In this condition, the repository function should be considered as a true positive. Thus, it is necessary to investigate how function renames influence similar function detection. Moreover, BinSD performance is also influenced 

> 4

Understanding the AI-powered Binary Code Similarity Detection , , 

Table 2: Comparison among AI-powered BinSD approaches. We rank the experimental results according to precision@5 on mono-architecture under seen programs. We highlight the top-3 experimental results with the best performance. RQ = randomly query result. GQ = the query result after the ground truth change. NQ = the query result on the new repository. 

Detector Setting AUC ACC Precision@5 [6] (%) Recall@5 [6](%) F1-score@5 [6] (%) Rank-1 [2] (%) MAP@5 [6] (%) MRR@5 [1] (%) NDCG@5 [6] (%) 

RQ/+GQ/NQ RQ/+GQ/NQ RQ/+GQ/NQ RQ/+GQ/NQ RQ/+GQ/NQ RQ/+GQ/NQ RQ/+GQ/NQ mono-seen 44.52 /+0.79/ 30.71 62.30 /+0.20/ 38.82 50.56 /+0.56/ 34.11 99.70/+0.00/ 56.68 98.04/+0.10/ 76.82 99.84/+0.00/ 79.09 99.03/+0.05/ 79.06 

mono-unseen 0.981 0.930 33.25/+0.19/11.22 58.95/+0.06/16.26 40.19/+0.14/13.14 99.98 /+0.00/0.67 97.49/-0.02/25.43 99.99 /+0.00/29.48 98.81/-0.01/30.67 cross-seen 41.85/+1.18/30.01 25.42/+0.37/17.16 30.29/+0.66/21.23 98.40/+0.08/52.40 97.13/+0.12/71.84 99.29/+0.03/74.49 98.33/+0.07/74.37 Gemini-skip [23] cross-unseen 0.953 0.886 32.83/+0.13/13.68 23.57/+0.05/7.95 25.48/+0.07/9.69 99.96 /+0.00/31.62 97.32/-0.07/37.21 99.98 /+0.00/41.63 98.72/-0.03/40.95 mono-seen 43.28 /+0.99/ 31.25 60.96 /+0.18/ 39.16 49.09 /+0.64/ 34.55 99.90/+0.00/53.64 98.08/+0.19/ 78.80 99.95/+0.00/ 81.07 99.09/+0.09/ 81.16 

mono-unseen 0.982 0.929 31.99/+0.13/7.47 57.26/+0.01/12.00 38.75/+0.09/9.08 99.95/+0.00/0.73 97.19/-0.08/22.18 99.98/+0.00/26.69 98.66/-0.04/25.68 cross-seen 39.90/+1.31/29.89 23.87/+0.40/16.81 28.63/+0.65/20.98 98.43/+0.08/49.94 97.53/+0.13/74.76 99.31/+0.03/77.73 98.53/+0.08/77.42 VulSeeker-skip [23] cross-unseen 0.952 0.877 30.50/+0.15/12.49 22.22/+0.05/6.91 23.84/+0.09/8.68 99.96 /+0.00/34.00 97.65/-0.08/39.86 99.98 /+0.00/43.72 98.88/-0.04/43.03 mono-seen 40.85/+0.81/28.54 58.22/+0.11/ 35.99 46.53 /+0.57/ 31.66 99.87/+0.03/49.53 98.15/+0.07/74.47 99.92/+0.02/76.93 99.10/+0.04/76.78 mono-unseen 0.980 0.933 32.98/+0.16/11.23 58.61/+0.03/17.47 39.88/+0.11/13.49 99.98 /+0.00/0.67 97.61/-0.09/24.92 99.98/+0.00/29.51 98.86/-0.04/29.66 cross-seen 41.62/+1.23/28.93 24.97/+0.22/16.50 29.84/+0.52/20.37 98.45/+0.08/51.22 96.93/+0.14/68.37 99.28/+0.04/71.40 98.27/+0.08/71.26 Focus-skip [23] cross-unseen 0.964 0.904 33.83/+0.11/16.56 24.26/+0.02/9.76 26.19/+0.05/11.81 99.85/+0.00/36.88 96.92/-0.04/42.77 99.93/+0.00/47.34 98.52/-0.02/47.04 mono-seen 39.11/+0.74/28.27 55.70/+0.19/35.66 44.55/+0.54/31.36 99.00/+0.00/46.96 97.93/+0.04/76.41 99.40/+0.00/78.54 98.83/+0.02/78.53 mono-unseen 0.970 0.909 33.54/+0.13/10.50 59.36/-0.01/15.83 40.51/+0.08/12.50 99.78/+0.00/0.67 97.25/-0.03/25.06 99.89/+0.00/30.05 98.66/-0.01/29.81 cross-seen 40.55/+1.65/28.43 24.95/+0.54/16.63 29.36/+0.87/20.26 97.60/+2.20/51.45 96.04/+1.15/70.36 98.70/+1.13/73.09 97.63/+0.92/73.34 Focus [23] cross-unseen 0.922 0.841 38.93/+0.14/20.04 28.94/+0.03/13.25 30.73/+0.07/15.05 99.15/+0.00/45.25 95.59/-0.03/52.39 99.57/+0.00/56.39 97.79/-0.02/56.88 mono-seen 39.01/+0.77/27.18 55.00/+0.15/34.30 44.36/+0.52/30.13 99.60/+0.00/45.73 97.85/+0.03/72.22 99.77/+0.00/74.45 98.92/+0.01/74.72 mono-unseen 0.965 0.907 31.54/+0.17/8.61 56.64/+0.04/12.55 38.22/+0.12/10.11 99.89/+0.00/0.50 97.37/-0.09/19.37 99.94/+0.00/24.32 98.73/-0.04/23.36 cross-seen 44.32/+1.09/30.83 27.10/+0.31/18.14 32.15/+0.59/22.12 98.97/+0.16/58.19 96.68/+0.15/72.96 99.44/+0.01/75.64 98.20/+0.07/75.84 Gemini [65] cross-unseen 0.927 0.853 40.16/+0.16/18.08 29.96/+0.05/12.13 31.81/+0.09/13.68 99.93/+0.00/40.88 95.03/-0.06/49.19 99.97/+0.00/52.84 97.62/-0.03/53.87 mono-seen 36.49/+1.86/23.17 52.02/+0.09/28.20 41.09/+0.95/24.92 99.57/+0.00/39.54 96.97/-0.01/61.28 99.73/+0.00/64.77 98.45/+0.00/64.85 mono-unseen 0.985 0.944 30.81/+0.16/9.46 55.83/+0.03/14.19 37.44/+0.11/11.19 99.97/+0.00/0.56 97.52/-0.09/22.48 99.99 /+0.00/26.54 98.82/-0.04/27.07 cross-seen 33.92/+1.99/23.28 18.76/+0.75/11.38 22.82/+1.12/14.83 98.33/+0.07/38.92 97.48/+0.08/66.03 99.49/+0.05/69.04 98.59/+0.06/69.00 BinaryAI-skipt [70] cross-unseen 0.969 0.915 29.67/+0.12/17.12 21.64/+0.06/9.20 23.20/+0.08/11.74 99.96 /+0.00/46.76 98.07/-0.07/52.34 99.98 /+0.00/56.01 99.08/-0.03/56.01 mono-seen 36.01/+0.61/25.29 52.49/+0.07/32.00 41.26/+0.44/28.11 99.93 /+0.00/37.78 97.67/-0.06/66.56 99.96 /+0.00/69.23 98.88/-0.03/69.72 mono-unseen 0.958 0.895 30.37/+0.11/11.05 55.18/-0.01/18.09 36.93/+0.06/13.52 99.94/+0.00/0.73 97.09/-0.03/26.03 99.97/+0.00/30.80 98.60/-0.01/31.16 cross-seen 34.09/+1.11/24.40 21.32/+0.30/14.69 24.96/+0.57/17.76 97.93/+1.64/35.36 96.94/+0.62/61.66 99.17/+0.67/64.73 98.21/+0.57/64.94 VulSeeker [25] cross-unseen 0.905 0.819 32.06/+0.09/12.92 23.60/+0.02/8.22 25.24/+0.04/9.55 99.96 /+0.00/29.68 96.50/-0.02/36.84 99.98 /+0.00/40.78 98.32/-0.01/40.64 mono-seen 34.77/+0.00/22.14 66.51 /+0.00/30.22 42.75/+0.00/25.34 89.93/+0.00/31.79 90.85/+0.00/58.15 93.64/+0.00/61.36 93.52/+0.00/62.51 mono-unseen 0.983 0.942 33.73/+0.00/15.71 56.65/+0.00/21.13 40.53/+0.00/17.96 84.43/+0.00/0.34 87.94/+0.00/29.40 91.07/+0.00/30.83 91.77/+0.00/39.75 cross-seen 45.78 /+0.00/ 32.44 35.85/+0.00/20.46 35.82/+0.00/23.27 91.60/+0.00/47.91 89.93/+0.00/59.90 94.53/+0.00/64.10 93.31/+0.00/64.88 SAFE [49] cross-unseen 0.977 0.929 36.36/+0.00/19.33 28.25/+0.00/12.21 28.67/+0.00/13.98 80.11/+0.00/28.17 83.34/+0.00/42.04 87.81/+0.00/46.68 88.27/+0.00/46.64 mono-seen 32.50/+0.55/21.60 39.62/+0.03/31.14 33.62/+0.33/24.24 78.93/+0.22/27.97 82.31/+0.10/48.13 85.56/+0.14/51.98 85.90/+0.11/52.77 mono-unseen 0.976 0.940 31.37/+0.45/20.83 41.46/+0.03/31.57 33.58/+0.27/23.96 80.45/+0.19/26.41 83.67/+0.09/48.67 86.77/+0.12/52.52 87.16/+0.09/53.47 cross-seen 29.99/+0.48/19.05 19.11/-0.01/14.06 22.19/+0.23/15.13 61.87/+0.14/24.62 67.80/+0.06/40.36 71.98/+0.15/44.57 72.49/+0.15/44.88 MGMN [42] cross-unseen 0.961 0.899 28.88/+0.44/17.94 18.73/+0.01/13.23 21.51/+0.22/14.24 61.18/+0.12/23.11 67.60/+0.05/38.97 71.68/+0.13/43.14 72.22/+0.13/43.32 mono-seen 32.21/+1.59/18.92 46.61/+0.02/23.36 36.60/+1.04/20.56 99.80/+0.00/28.61 96.65/+0.03/51.25 99.88/+0.00/54.91 98.34/+0.02/55.42 mono-unseen 0.992 0.949 29.29/+0.17/8.24 53.64/+0.06/12.38 35.67/+0.13/9.76 99.89/+0.00/0.44 97.36/-0.09/19.42 99.94/+0.00/22.17 98.72/-0.04/23.02 cross-seen 28.24/+0.91/26.85 14.68/+0.37/14.02 18.54/+0.53/17.66 89.35/+0.86/ 86.02 87.35/+0.56/ 84.87 90.80/+0.72/ 87.96 89.37/+0.59/ 86.84 

BinaryAI-bert2 [70] cross-unseen 0.983 0.931 28.57/+0.11/11.43 20.96/+0.05/6.10 22.45/+0.07/7.78 99.90/+0.00/28.06 97.41/-0.07/34.20 99.95/+0.00/38.47 98.75/-0.03/37.91 mono-seen 22.35/+0.00/8.49 48.66/+0.00/11.44 28.39/+0.00/9.67 97.81/+0.00/4.48 97.64/+0.00/25.13 98.78/+0.00/29.32 98.52/+0.00/28.50 mono-unseen 0.936 0.873 20.08/+0.00/8.57 60.46/+0.00/14.29 28.88/+0.00/10.71 98.27/+0.00/0.40 99.09/+0.00/42.86 99.11/+0.00/47.19 99.33/+0.00/42.86 cross-seen 23.13/+0.00/10.34 21.33/+0.00/6.20 19.36/+0.00/7.23 98.66/+0.00/7.45 97.88/+0.00/29.60 99.25/+0.00/33.82 98.78/+0.00/33.26 UFE-mean [48] cross-unseen 0.895 0.812 25.44/+0.00/10.20 20.03/+0.00/6.01 20.19/+0.00/7.21 97.31/+0.00/8.54 96.08/+0.00/24.13 98.63/+0.00/28.47 97.75/+0.00/28.47 mono-seen 21.76/+0.00/11.25 49.06/+0.00/15.41 27.90/+0.00/12.90 97.98/+0.00/4.59 98.23/+0.00/39.35 98.88/+0.00/42.79 98.84/+0.00/42.85 mono-unseen 0.929 0.884 20.32/+0.00/5.71 60.86/+0.00/9.52 29.18/+0.00/7.14 98.27/+0.00/0.27 98.85/+0.00/28.57 99.11/+0.00/33.04 99.22/+0.00/28.57 cross-seen 21.91/+0.00/9.64 20.61/+0.00/6.47 18.58/+0.00/7.00 98.60/+0.00/5.07 98.34/+0.00/30.80 99.17/+0.00/34.56 98.97/+0.00/34.10 UFE-attention [48] cross-unseen 0.903 0.828 24.59/+0.00/7.88 19.46/+0.00/4.57 19.60/+0.00/5.52 97.62/+0.00/7.78 96.47/+0.00/21.46 98.78/+0.00/26.51 97.98/+0.00/24.69 mono-seen 20.86/+0.00/9.39 47.64/+0.00/12.47 26.82/+0.00/10.65 98.89/+0.00/1.59 98.85 /+0.00/27.53 99.39/+0.00/30.96 99.28 /+0.00/32.13 mono-unseen 0.838 0.702 20.05/+0.00/8.57 60.42/+0.00/14.29 28.85/+0.00/10.71 98.27/+0.00/0.13 99.11 /+0.00/26.19 99.11/+0.00/30.18 99.34 /+0.00/30.44 cross-seen 21.07/+0.00/7.62 20.49/+0.00/5.00 18.18/+0.00/5.60 99.12/+0.00/2.74 99.02 /+0.00/23.27 99.52/+0.00/27.38 99.40 /+0.00/26.14 UFE-rnn [48] cross-unseen 0.691 0.548 21.07/+0.00/5.45 17.91/+0.00/3.47 17.36/+0.00/3.97 98.35/+0.00/1.41 98.32/+0.00/12.76 99.15/+0.00/17.12 98.96/+0.00/16.07 mono-seen 2.00/+0.00/2.00 36.11/+0.00/33.33 3.78/++0.00/3.72 8.33/++0.00/2.00 12.38/++0.00/23.51 12.38/++0.00/23.51 27.9/+0.00/37.26 Asm2Vec [17] mono-unseen 0.703 0.300 2.00/+0.00/2.21 4.59/+0.00/3.62 3.81/+0.00/4.13 1.46/+0.00/1.45 8.08/+0.00/6.54 8.12/+0.00/7.19 25.05/+0.00/24.67 mono-seen 0.811 0.496 - - - - - - -Asteria [67] cross-seen 0.874 0.496 - - - - - - -

by how we construct the searching repository. Specifically, suppose a repository contains functions very similar to the query function, i.e., the binary change between the query and repository functions is small (for instance, when the query and repository functions are compiled with -O2 and -O3 optimization levels, respectively). In that case, a BinSD tool can easily recognize the corresponding similar functions and achieve good performance. Thus, we also need to explore how the constructed searching repository affects BinSD performance. 

Cross-optimization level and architecture evaluation. As shown in Table 2, we compare existing AI-powered BinSD ap-proaches by using 2 common machine learning metrics and 7 widely used ranking metrics in recommendation systems on both the seen and unseen datasets (the seen dataset is used to train embedding models). Each metric value in the RQ columns is the average of 3,000 times random function queries. For each query, the reposi-tory consists of 100,000 randomly selected functions. The function comparison process of Asteria [ 67 ] is extremely slow (several hours per function search). We fail to perform 3000 times of queries for this approach. Thus, we do not report its ranking metrics. By analyzing these experimental results, we obtain the following findings. (1) No one BinSD approach always performs the best on all metrics. For instance, BinaryAI-bert2 achieves the best AUC, while Gemini-skip performs better in ranking metrics. This ex-perimental result indicates that different BinSD approaches may be applicable in different scenarios. (2) The GNN-based BinSD ap-proaches achieve top-level ranking metrics in the current litera-ture, which demonstrates GNN is a promising embedding neural 

5, , Lirong Fu, Peiyu Liu, Wenlong Meng, Kangjie Lu, Shize Zhou, Xuhong Zhang, Wenzhi Chen, and Shouling Ji 0 10 20 30 40 50 60 70 80 90 100  

> Ratio (%)
> 020 40 60 80 100
> Metric Value (%)
> (a) BinaryAI-bert2 0 10 20 30 40 50 60 70 80 90 100
> Ratio (%)
> 020 40 60 80 100
> Metric Value (%)
> MRR NDCG
> MAP Rank-1 Recall@5 F1-score Precision@5 (b) Gemini-skip

Figure 1: We change the way of building the searching repos-itory by including different ratios of query functions in the constructed repository. (a) and (b) show how metric values change under different ratios. 

network in BinSD . (3) Compared to mono-ISA, recall values de-crease significantly in cross-ISA. For instance, the recall@5 value of Gemini-skip decreases from 62.3% to 25.42% when the evaluation setting changes from mono-seen (the evaluation on mono-ISA and seen dataset) to cross-seen (the evaluation on cross-ISA and seen dataset). This experimental result demonstrates that BinSD models tend to identify similar functions compiled under the same ISA. Thus, cross-architecture BinSD still needs further improvement. (4) Most BinSD approaches perform well on ranking metrics, including Rank-1, MAP, MRR, and NDCG. Furthermore, for most BinSD meth-ods, these metric values are similar, which means sometimes, using these metrics can hardly differentiate excellent BinSD approaches. (5) A higher AUC value does not mean a better capability of similar function detection. For instance, though BinaryAI-bert2 achieves the best AUC (99.2%) and ACC (94.9%), its precision (32.21%) is 12.3% less than Gemini-skip. Besides, the AUC values of many BinSD approaches are similar. Thus, only using the AUC value can hardly present the capability of a BinSD tool. (6) The BinSD systems that use instruction embeddings (obtained by skip-gram model [ 50 ]) as the basic block features perform better than the ones using manually extracted features. For instance, both the AUC and precision of Focus-skip and VulSeeker-skip are better than Focus and VulSeeker, respectively. This experimental result demonstrates that neural networks are effective in extracting representative bi-nary features in code embedding. Next, we explore key factors affecting the performance of each BinSD approach. 

The influence of function rename. Instead of function name comparison, we identify all the real TPs in the function search list by comparing the source code of the search results with query func-tions. After we rectify the FPs (compiled by the same source code as the query function while having different function names) to TPs, we show the metric value changes in the GQ columns in Table 2. We observe that the prior ground truth labeling method decreases the precision and recall of BinSD approaches. For instance, after rectifi-cation, the precision of Gemini-skip increased by 1.18%. Fortunately, the influence of function renames is limited. 

The influence of repository construction. According to our empirical analysis, the same functions as the query function in the repository are usually identified at the top-1 ranking, which significantly influences the value of most position-sensitive metrics in recommendation systems such as MRR, MAP, and NDCG. To investigate how repository construction affects the performance of BinSD approaches, after randomly generating repository functions, we intentionally remove the functions that are the same as the query functions to construct a new repository. Then, we perform similar function detection by using the newly built repository. The metric values obtained by querying the new repository are shown in the NQ columns in Table 2. After removing the same functions as query functions from the constructed repository, ranking metrics such as Rank-1, MAP, and MRR decrease significantly. For instance, the Rank-1 value of BinaryAI-bert2 drops from 99.80% to only 28.61% on mono-architecture evaluation. Moreover, we further investi-gate how repository construction influences evaluation metrics by involving different ratios of query functions in the constructed repository. The experimental result shown in Figure 1 indicates that how we build the searching repository significantly influences the ranking metrics of BinSD —the more the repository contains the query functions, the better the evaluation metrics we obtain. 

Summary of findings. We summarize how most metrics change under different conditions in Table 3 to clearly present the findings in accuracy evaluation. For example, when the evaluated dataset changes from the seen dataset to the unseen dataset, the AUC and precision decrease a little, the recall decreases significantly, and other ranking metrics do not change much. Particularly, the accuracy of similar function detection is significantly influenced by the value of K in ranking metrics [ 62 ]. Thus, we also measure the impact of K. More comparisons can be found in our artifacts. 

3.3.2 Efficiency Comparision. This section presents the efficiency comparison of state-of-the-art AI-powered BinSD tools. Consider-ing that (1) the preprocessing and code representation phases of many BinSD tools are similar, and (2) the efficiency of these two phases has been well described in prior research. For instance, Fo-cus [ 23 ] demonstrates the extraction time of CFG is within 0.05s per function. Thus, this section evaluates the training and func-tion embedding time of BinSD approaches, which are influenced by the structures of neural networks. We perform this evaluation under single and cross-platforms using the training data. As shown in Table 4, the training and function embedding time of each BinSD approach vary a lot because different BinSD approaches are com-posed of different neural network structures. For instance, it takes 36 minutes for MGMN to train a mono-architecture model. By con-trast, Asteria requires 23 hours and 33 minutes to finish the training process. Moreover, the difference in the function embedding time is even larger. It takes 336 hours for Focus-skip to generate function embeddings on cross-platforms. By contrast, Gemini only needs 1 hour and 31 minutes to finish the embedding generation pro-cess. Considering that model training is a one-time effort work, researchers should focus on the embedding time, which directly influences the practicability of a BinSD tool in real-world scenarios. 

## 3.4 Downstream Application 

3.4.1 Vulnerability Search. This section evaluates whether the se-lected BinSD approaches can identify the ten vulnerabilities existing in IoT firmware images. To facilitate the vulnerability detection process, we compile the ten vulnerabilities under ARM, the same platform as the chosen IoT firmware images. Moreover, most IoT firmware images are compiled with -O2/-O3 optimization levels. To increase the detection accuracy, we compile the ten vulnerabilities  

> 6Understanding the AI-powered Binary Code Similarity Detection , ,

Table 3: Summary of findings in this section. ↓ = decrease. ⇓ =decrease significantly. ↑ = increase. ⇑ = increase significantly. 

⇝ = does not change much. - = unrelated to the condition.                                               

> ID Condition AUC Precision Recall Rank-1 MAP MRR NDCG
> 1Mono -> Cross ↓↓⇓⇝⇝⇝⇝
> 2Seen -> Unseen -⇓↓⇝⇝⇝⇝
> 3MFE -> NLP ↑↑↑⇝⇝⇝⇝
> 4K_Value
> ↩→ -⇓↑⇝↓↓↓
> 5Query_Function_Ratio
> ↩→ -↑↑↑↑↑↑

Table 4: Efficiency comparison of AI-powered BinSD ap-proaches. Asm2Vec does not support cross platform BinSD .MGMN and Asteria are end-to-end methods. Thus, we cannot calculate their function embedding time.                                                                  

> Detector Training Time Embedding Time x86 cross x86 cross
> MGMN [42] 36m 1h42m --SAFE [49] 1h12m 3h1m 35m 4h11m Asm2Vec [17] 1h27m -3h27m -Gemini [65] 2h05m 2h06m 32m 1h31m Gemini-skip [23] 2h56m 2h49m 43m 1h29m VulSeeker [25] 5h34m 5h44m 2h12m 6h7m BinaryAI-bert2 [70] 5h44m 5h29m 4h56m 10h1m BinaryAI-skipt [70] 6h14m 9h38m 5h9m 8h21m VulSeeker-skip [23] 6h27m 5h33m 3h25m 5h59m UFE-mean [48] 9h46m 25h30m 3h6m 16h51m UFE-attention [48] 10h7m 24h50m 3h7n 16h15m Focus-skip [23] 11h32m 18h56m 23h48m 336h38m UFE-rnn [48] 12h40m 16h43m 3h14m 16h43m Focus [23] 17h44m 17h51m 23h49m 263h42m Asteria [67] 22h33m 94h29m --

with -O2 and -O3 optimization levels. Finally, we obtain 20 vulner-able functions. Next, conduct the vulnerability search and obtain a list of search results that BinSD systems report being similar vul-nerabilities in IoT firmware images. To check whether the top-K (we choose K to be 10 to identify as many vulnerable functions as possible) search results are real vulnerabilities, we first examine whether the executable file name of the search result is identical to the one of the query function. If so, we further check the stripped binary functions to determine whether it is a bug by manually com-paring the pseudocode snippets (an ISA agnostic language similar to source code) of the search result and query function obtained by IDA pro [ 28 ]. For instance, we manually check and compare similar function bodies such as local variables, program loops, call functions, and constant strings. 

Findings. From the vulnerability search result shown in Ta-ble 5, we find that: (1) Gemini-skip achieves the best performance (detecting 24 vulnerabilities in the ten IoT firmware images). This experimental result is consistent with the one shown in similar function detection—GNN-based BinSD approaches are more appli-cable in the problem of BinSD ; (2) The false positives need further improvement for the evaluated BinSD approaches. For instance, for all the search results reported by UFE-attention, only four of them are vulnerable functions. In this condition, it requires great manual effort to identify the real bugs among the reported search results. (3) It is infeasible to use a certain threshold to determine whether a search result is a real vulnerability or not. For instance, when querying the CVE-2014-0195, Gemini-skip reports seven bugs with maximum and minimum similarity scores of 0.93 and 0.8, respec-tively. In this condition, the threshold should be lower than 0.8 to ensure all the vulnerabilities are reported. However, when querying CVE-2014-3513, the similarity scores of all the top 10 results are higher than 0.9. In this condition, Gemini-skip is supposed to report all ten search results to be vulnerable. Unfortunately, none of them are true vulnerabilities, meaning that using a threshold to deter-mine semantically similar functions is impractical. In summary, the current BinSD approaches still need more improvement to perform vulnerability detection. 

3.4.2 License Violation Detection. This section checks whether the evaluated BinSD approaches can recognize the GPL-licensed third-party libraries such as OpenSSL in the closed-source IoT firmware images. We choose 4 ELFs in OpenSSL-1.0.1f and Busybox-1.27.0 as the query libraries since they are widely used in IoT firmware images. Considering a library consists of many functions, and the similarity of two libraries is closely related to their function similar-ity, we average the similarity score ( 𝑠 𝑓 ) of all the query functions in a query library 𝑄 as the similarity score 𝑆 𝑄𝑇 between the query library 𝑄 and the target library 𝑇 . Specifically, for each query func-tion 𝑓 in the query library, we search for it in each target library 𝑇 

and record its top-1 similarity score 𝑠 𝑓 . Then, we average all the 

𝑠 𝑓 to obtain 𝑆 𝑄𝑇 . In this way, we compare the query library with all the target libraries in the IoT firmware images, sort all target libraries according to 𝑆 𝑄𝑇 , and record the rank of the target library that is similar to the query library. 

Findings. From the experimental result shown in Table 6, we find that (1) AI-powered BinSD approaches achieve relatively good performance in license violation detection. Specifically, BinaryAI-bert2, Gemini-skip, VulSeeker, SAFE, and UFE-attention recognize 15, 11, 6, 2, and 0 query ELFs at the top-1 position. (2) Among all the evaluated BinSD systems, BinaryAI-bert2 can identify the most GPL libraries in IoT firmware images. We conclude the root reason is that in license violation detection, the search repository for each function is relatively small (compared to the task of vulnerability search, the repository functions in this task consist of the functions in the target library and are usually no more than several thousand). In this condition, the adjacency matrix of CFG is a good feature to characterize a binary function since there exist fewer different functions that happen to have similar adjacency matrices as the query function in a relatively small repository. (3) Compared to vulnerability detection, the evaluated five BinSD approaches can detect program similarity with acceptable accuracy. For instance, BinaryAI-bert2 can report most target libraries (the one similar to the query library) within top-3 search results. The reason is that vulnerability detection requires accurate BinSD detection for each function pair, which is still challenging for existing BinSD approaches. By contrast, license violation detection is based on the statistical BinSD detection results of all the functions in the query program. Thus, existing BinSD approaches can achieve good performance in license violation detection. 

## 4 UNDERSTANDING OF EMBEDDING NETWORKS AND EVALUATION METRICS 

As shown in §2.1, BinSD approaches first perform code disassembly and feature extraction and then use embedding neural networks to obtain low-dimensional vector representation. This evaluation does not evaluate the impact of disassembly tools since Zhou. et 

> 7

, , Lirong Fu, Peiyu Liu, Wenlong Meng, Kangjie Lu, Shize Zhou, Xuhong Zhang, Wenzhi Chen, and Shouling Ji 

Table 5: Vulnerability detection.                                                                                                                                                            

> CVE Gemini-skip [23] BinaryAI-bert2 [70] VulSeeker-skip [25] SAFE [49] UFE-attention [48] Max Similarity Min Similarity Bugs Max Similarity Min Similarity Bugs Max Similarity Min Similarity Bugs Max Similarity Min Similarity Bugs Max Similarity Min Similarity Bugs
> CVE-2014-3571 0.97 0.9 00.97 0.85 40.99 0.96 00.31 0.21 40.97 0.88 2CVE-2015-0289 0.95 0.83 00.99 0.77 00.98 0.92 00.32 0.2 00.95 0.82 1CVE-2016-2181 0.96 0.88 00.98 0.88 00.98 0.9 00.37 0.31 00.98 0.91 0CVE-2015-1791 0.95 0.84 00.99 0.82 00.98 0.9 00.48 0.33 00.93 0.79 0CVE-2015-1792 0.95 0.87 10.99 0.91 01.0 0.96 20.41 0.32 00.96 0.82 0CVE-2014-3513 0.98 0.9 00.98 0.88 00.98 0.91 00.35 0.27 00.98 0.88 0CVE-2014-0160 0.92 0.77 00.97 0.82 00.98 0.91 00.37 0.27 00.96 0.84 0CVE-2016-0797 0.97 0.88 80.94 0.7 60.98 0.93 60.3 0.24 00.97 0.88 0CVE-2016-2176 0.98 0.9 80.95 0.74 00.98 0.91 00.41 0.34 00.93 0.78 1CVE-2014-0195 0.93 0.8 70.93 0.76 40.97 0.9 20.41 0.27 00.97 0.81 0

Table 6: License violation detection (the rank of the target program that is similar to the query program). “-1" = the query ELF does not exist in the firmware; “bb" = Busybox; “lc" = libcrypto; “ls" = libssl; “op" = openssl.                                                                                                                                                                                                                                            

> Firmware #ELF Gemini-skip [23] BinaryAI-bert2 [70] SAFE [49] UFE-attention [48] VulSeeker-skip [25]
> bb lc ls sl bb lc ls op bb lc ls op bb lc ls op bb lc ls op CAP1200v1_1.0.0_20170801-rel61314_up 176 515-1 112-1 11 314 -1 3147 67 -1 618-1 COM_T01F001_LM.1.6.18P12_sign2_TPL.TL-SC4171G 97 215-1 112-1 314-1 106 10 36 -1 214-1 COM_T01F001_LM.1.6.18P7_TPL.TL-SC4171G 96 215-1 312-1 314-1 104 782 -1 214-1 COVR-2600R_FW101b05_0911_txbfdisable0911190427 281 5141112317 415 166 2233 230 16 71711 COVR-2600R_FW101b05_beta01_hcr2 284 5141112316 414 167 267 282 284 225 71711 COVR-3902_ROUTER_v101b05 284 5141112316 414 167 202 256 180 77 71711 DAP2610-firmware-v101-rc017 128 6143123416 310 76 8126 86 54 82511 DLINK_DNR-322.2.10b022.10.0612.2014 236 17 420 33 81223 26 17 40 96 9171 47 35 7419 31 DLINK_DNR-322L.1.40b011.16.1219.2012 225 10 713 24 71222 19 10 30 84 174 47 160 28 7417 28 Dap2610-firmware-v101-beta28-rc0480306165616 128 6143123416 210 75 793 29 15 82510

al. [ 32 ] have performed a comprehensive study on disassembly tools, demonstrating that IDA Pro [ 28 ] has the highest precision on many disassembly tasks. Moreover, Marcelli et al. [ 47 ] have compared feature extraction methods and provided insightful findings. Thus, this section next investigates embedding neural networks and the widely adopted evaluation metrics in BinSD approaches. 

## 4.1 Embedding Neural Networks 

We perform a structure analysis together with the trace of neuron output to understand neural networks. It is well known that model understanding is a rather difficult task. Thus, we do not investigate all the embedding networks listed in Table 1. By contrast, we explore two representative embedding neural networks (GNN and CNN) that have been widely used in BinSD . We hope our investigation can inspire future work in this field. 

4.1.1 GNN. Many BinSD approaches [ 23 , 25 , 42 , 48 , 65 ] propose to use GNN to embed binary functions. We observe that all GNNs, including the Structure2Vec [ 14 ], DNN [ 25 ], GAT [ 63 ] used in exist-ing BinSD systems, follow a neighborhood aggregation strategy— iteratively updating the representation of a vertex by aggregating the representations of its neighbors. In this way, after 𝑇 iterations of aggregation, the representation of a vertex can contain the struc-tural information within its H-hop neighborhood. Specifically, for a vertex 𝑖 , GNN first aggregates the representations of its neigh-bors in iteration 𝑡 to be ®ℎ𝑡 +1

𝑖 (as shown in Equation 1). Then, GNN represents the vertex embedding in iteration 𝑡 as ®𝜇 𝑡 +1

𝑖 by a 𝑈 𝑛𝑖𝑜𝑛 

function (as shown in Equation 2). Finally, in Equation 3, by using a 𝑅𝐸𝐴𝐷𝑂𝑈𝑇 function, GNN aggregates all the vertex embeddings to form the entire graph’s representation ®ℎ𝐺 . Such a neighbor-hood aggregation strategy means that a maximally powerful GNN would never map two different neighborhoods, i.e., multi-sets of fea-ture vectors, to the same representation. Therefore, GNN is suitable for identifying the graph isomorphism, where the graph topolo-gies and the vertex features of the compared graphs are similar. In some cases, the graph topologies and vertex features of a pair of binary code do not change much. For instance, the binary code change between binaries compiled under x86 and x64 is small. In this condition, GNN-based BinSD approaches are effective. 

®ℎ𝑡 +1

𝑖 = 𝐴𝐺𝐺𝑅𝐸𝐺𝐴𝑇 𝐸 ©«∑︁ 𝑟𝜖 N𝑖 

®ℎ𝑡 𝑟 ª®¬

(1) 

®𝜇 𝑡 +1

𝑖 = 𝑈 𝑛𝑖𝑜𝑛 

 ®ℎ𝑡 +1

𝑖 , ®𝜇 𝑡 𝑖 )



(2) 

®ℎ𝐺 = 𝑅𝐸𝐴𝐷𝑂𝑈𝑇  ∑︁ 

𝑖𝜖𝐺 

®𝜇 𝑡 +1

𝑖 

!

(3) Unfortunately, due to the varieties of architectures and com-piler options, some logically similar binary code pieces appear to be dramatically different in the graph topology . In this condition, the direct usage of GNN needs to overcome the challenges brought by significant binary change. Moreover, the 𝑅𝐸𝐴𝐷𝑂𝑈𝑇 functions in GNNs are summation functions, which inevitability leads to an “embedding collision” problem—the summation of different node embeddings happens to be similar. For instance, when using Gemini to generate function embedding and performing function search, the embedding of query function 𝑜𝑝𝑒𝑛𝑠𝑠𝑙 _𝑠𝑒𝑡 _𝑘𝑒𝑦𝑔𝑒𝑛 _𝑐𝑡𝑥 , which contains 48 basic blocks, is very similar to the repository func-tion 𝑜𝑝𝑒𝑛𝑠𝑠𝑙 _𝑐𝑟𝑙 2𝑝𝑘𝑐𝑠 7_𝑚𝑎𝑖𝑛 which includes 92 basic blocks. We checked and compared the neuron output in Gemini and found that these two functions’ vertex embeddings are different. Their function embeddings are similar just because of the summation 

8Understanding the AI-powered Binary Code Similarity Detection , , 

in 𝑅𝐸𝐴𝐷𝑂𝑈𝑇 . To further investigate the influence of “embedding collision”, we manually analyze the FPs in the top 5 search results of 100 random function queries. We find that 95% FPs are caused by the “embedding collision”. The remaining 5% FPs are TPs caused by function rename discussed in §3.3.1. Thus, we believe that “embed-ding collision” is the primary limitation of the GNN-based BinSD methods. Notably, “embedding collision” uncovered in this paper is different from the GNN over-smoothing issue [ 7, 10 , 59 ], which causes two node embeddings with different labels presenting similar node embeddings (More comparison can be found in the artifacts). 

4.1.2 CNN. Recent research [ 43 , 70 ] proposes to use CNNs on the adjacency matrices of the graph to capture the node ordering infor-mation of binary code. The intuition is that the node order changes of many cross-platform binary function pairs compiled from the same source code are small. Thus, the adjacency matrices of such function pairs should be similar. The features (node ordering) in the adjacency matrices can be recognized through convolution layers in a CNN model, which has an excellent performance when deal-ing with structured data such as image pixels by using different kernels, i.e., feature extractors [ 5]. Unfortunately, though the CFG of a binary function can be transformed as an adjacency matrix just like the image pixels, there exist two primary characteristics when using CNN models to extract the structural features of bi-nary functions. (1) The number of basic blocks in binary functions is relatively small (90% of the functions have less than 100 basic blocks). (2) The connected relations among basic blocks are sim-ple. Consequently, when searching repositories with a large number of functions, CNN-based approaches tend to report different binary functions with similar CFGs/adjacency matrices in the top search list. 

## 4.2 Evaluation Metrics 

In Table 2, many BinSD approaches obtain high AUC values (larger than 0.98), indicating the performance of the corresponding embed-ding neural networks is good. However, when performing similar function detection, the precision and recall are not as good as the AUC values. In this section, we explore the calculation processes of AUC, precision, and recall to understand the performance gap between machine learning metrics and ranking metrics. It is well known that the higher the AUC (near 1) is, the better the performance of the classification model is, which means that the model can choose a proper threshold to differentiate the TPs and TNs and minimize the FPs and FNs. Generally, AUC is obtained by evaluating a test dataset (randomly generated function pairs whose ground truth is labeled to be 1 if they are compiled from the same source code. Otherwise, the ground truth is -1). In the test dataset, the similarity score of two unrelated binary functions is usually low. The root reason is that BinSD approaches are unlikely to determine two randomly selected binary functions compiled from different source codes to be similar since the occasionally selected functions are usually dissimilar in graph topology and function features. By contrast, the similarity score of two binary functions compiled from the same source code is usually higher than a certain threshold. Thus, BinSD systems can easily find a threshold to classify similar function pairs and obtain a good AUC value. For instance, as shown in Figure 2a, Figure 2b, and Figure 2c, we record the similarity scores of function pairs in the test dataset of each BinSD approach. We find that Gemini, BinaryAI-skipt, and Focus-skip, can achieve high AUC values because models can obtain suitable thresholds (0.478, 0.562, and 0.801, respectively ) to maximize TPs and TNs. However, in similar function detection, BinSD usage is more similar to the recommendation process. A query function is com-pared with many other functions (100,000 in accuracy evaluation). In this condition, BinSD approaches need to determine whether the top-K functions (which have high similarity scores) are com-piled from the same source code as the query function. This task is more difficult than determining whether two randomly selected functions are compiled from the same source code. Consequently, for a BinSD approach, a high AUC value can hardly demonstrate good search performance in function search. For instance, as shown in Figure 2d, Figure 2e, and Figure 2f, when performing function searches, it is difficult to differentiate TPs and TNs in the reposito-ries because different binary code snippets also have high similarity scores when compared to the query function. 

## 5 FUTURE DIRECTIONS 

In the preprocessing phase, the extracted features can greatly in-fluence what is to be learned by a neural network. High-quality binary features can absolutely benefit BinSD neural networks to generate representative code embedding. Thus, we first discuss the promising direction of feature selection. In the code embedding process, we conclude that the widely used GNN is more suitable for the BinSD problem in §4.1. However, the embedding collision prob-lem significantly impedes the performance of GNN-based BinSD approaches. Thus, we next discuss potential mitigation strategies to alleviate the embedding collision problem. 

Feature extraction by robust NLP models. Compared to man-ually extracted features obtained by experts, the features automat-ically extracted by robust NLP approaches contain more binary semantics. Particularly with the rapid development of NLP, many generative pre-trained transformers (GPT) [ 22 ] that can follow com-plex instructions have been proposed to solve difficult problems with high accuracy. Similar to NLP, BinSD also needs precise seman-tic understanding and extraction. Transferring the state-of-the-art large language models (LLM) [ 34 , 39 , 61 ] to the BinSD problem is a promising future direction. 

Embedding concatenation . Embedding concatenation, which splices different types of embeddings, has been shown to be effec-tive in structured prediction tasks [ 64 ]. This technique may help alleviate the embedding collision problem because the concate-nated embedding contains more binary semantics benefiting the differentiation of query functions and FPs (different binary func-tions that happen to be represented similarly to the query function due to the summation of basic block embeddings). For instance, we can concatenate the embedding of neural networks and con-stant strings embedding (likely unchanged features across various compiler options) as function embedding. 

Partial graph alignment. Despite optimization levels and ar-chitectures altering the structure and basic blocks of the graph representation of binaries, the graph representation of similar bi-nary functions (such as CFG) is expected to be partially aligned since they are compiled from the same source code. For FPs caused by embedding collision, their graphs can not be aligned with the       

> 9, , Lirong Fu, Peiyu Liu, Wenlong Meng, Kangjie Lu, Shize Zhou, Xuhong Zhang, Wenzhi Chen, and Shouling Ji 020 40 60 80 100
> Function Size
> 1.0
> 0.5
> 0.0 0.5 1.0
> Similarity
> Ground Truth = 1
> Ground Truth = -1

(a) Gemini threshold = 0.478 0 50 100 150 200  

> Function Size
> 1.0
> 0.5
> 0.0 0.5 1.0
> Similarity
> Ground Truth = 1
> Ground Truth = -1

(b) BinaryAI-skipt threshold =0.562 0 20 40 60 80 100  

> Function Size
> 1.0
> 0.5
> 0.0 0.5 1.0
> Similarity
> Ground Truth = 1
> Ground Truth = -1

(c) Focus-skip threshold = 0.801 0 20 40 60 80 100 

> Function Size
> 0.70 0.75 0.80 0.85 0.90 0.95 1.00
> Similarity
> Ground Truth = 1
> Ground Truth = -1

(d) Gemini function search 0 50 100 150 200  

> Function Size
> 0.70 0.75 0.80 0.85 0.90 0.95 1.00
> Similarity
> Ground Truth = 1
> Ground Truth = -1

(e) BinaryAI-skipt function search 0 20 40 60 80 100  

> Function Size
> 0.70 0.75 0.80 0.85 0.90 0.95 1.00
> Similarity
> Ground Truth = 1
> Ground Truth = -1

(f) Focus-skip function search 

Figure 2: We perform AUC calculation on the test dataset. Each point in (a), (b), and (c) presents the similarity score of each function pair in the test dataset. (d), (e) and (f) show the top-10 function search results in which the query functions are the first functions in the function pairs stored in the test dataset. Each point in (d), (e), and (f) present the similarity score between the query function and one of the search functions in the top-10 search result. Table 7: The change of precision and recall after performing graph alignment for GNN-based BinSD approaches. NPreci-sion and NRecall present the new precision and recall after graph alignment.                              

> Detector Precision@5 (%) Recall@5 (%) NPrecision@5 (%) NRecall@5 (%)
> Gemini-skip [23] 41.85 25.42 54.39 25.38 VulSeeker-skip [23] 39.90 23.87 49.82 23.50 Focus-skip [23] 40.55 24.95 59.89 25.55 Gemini [65] 44.32 27.10 56.99 26.90 VulSeeker [25] 34.09 21.32 50.16 22.34 Focus-mfe [23] 41.62 24.97 57.66 25.20 MGMN [42] 29.99 19.11 62.78 25.50

query function. Thus, intuitively, graph alignment is a good way to identify FPs. In this paper, we perform a simple graph alignment— detecting identical basic block pairs (by comparing the basic block attributes obtained by each BinSD method) in the search functions and query function to filter out FPs. As shown in Table 7, this simple graph alignment significantly improves the precision of GNN-based BinSD approaches, indicating that graph alignment is promising to mitigate the embedding collision problems. 

## 6 RELATED WORK 

To investigate the BinSD problem, recent work [ 27 ] summarizes the characteristics and implementation details of BinSD approaches over the past 20 years. However, this measurement work does not involve a quantitative comparison of BinSD approaches. Thus, this work fails to provide a straightforward and fair evaluation since most BinSD approaches adopt different embedding strategies, eval-uation methodologies, running environments, and/or benchmarks, making BinSD approaches not directly comparable. As the first quantitative measurement in the BinSD field, Marcelli et al. [ 47 ]performed a systematic study to compare the effectiveness and effi-ciency of existing BinSD approaches. However, this measurement misses or lacks the exploration of multiple other important aspects, such as the in-depth understanding of the embedding neural net-works and evaluation methodologies. In comparison (as shown in Table 8), this work is novel in the following aspects. (1) New evaluation goals and challenges: Marcelli et al. over-come reproducibility issues and perform a fair comparison to re-veal how different BinSD approaches perform under different tasks (XO, XC, XA, etc.). Our work overcomes new challenges, including understanding complex embedding processes and diverse evalua-tion methodologies (both are important for BinSD development) to 

(a) shed light on the characteristics of mainstream embedding neu-ral networks and the adopted evaluation metrics and (b) investigate the root causes . (2) New downstream applications: except for vul-nerability search, we conduct the first license violation detection evaluation and identify the characteristics of different applications. For instance, compared to vulnerability detection, existing BinSD approaches can achieve better performance in license violation de-tection. Moreover, we also summarize which BinSD approaches are applicable to which downstream scenarios. (3) Novel findings: we present several findings to reveal the advantages and limitations of current embedding neural networks and evaluation methodologies and the corresponding root causes. For instance, despite GNN-based BinSD approaches performing the best, their intrinsic limitation is 

> 10

Understanding the AI-powered Binary Code Similarity Detection , , 

Table 8: Comparison to the related work.                            

> Type Task [47] This paper
> The analysis of fuzzy hashing methods ✓✗
> Approach selection The analysis of AI-powered BinSD methods ✓✓
> Feature extraction understanding ✓✗
> Skill understanding In-depth embedding network understanding ✗✓
> Metric understanding In-depth metric understanding ✗✓
> The analysis of repository construction ✗✓
> The analysis of function rename ✗✓
> The analysis of unseen dataset ✗✓Evaluation Vulnerability detection ✓✓
> License violation detection ✗✓
> Future work The validation of promising directions ✗✓

the embedding collision problem. Moreover, evaluation methodolo-gies such as repository construction can significantly influence the performance of BinSD approaches. 

## 7 CONCLUSION 

In this paper, we perform a systematic measurement of the state-of-the-art AI-powered BinSD approaches. First, we conduct an extensive evaluation to fairly and quantitatively understand the true capability of BinSD tools in similar function detection and two main downstream applications. Moreover, we perform an in-depth analysis of binary embedding neural networks and evaluation methodologies to shed light on their pros and cons. The findings and implications of our study are expected to boost a better un-derstanding of the status-quo of AI-powered BinSD . Finally, we provide and validate several promising future directions for advanc-ing BinSD . We hope the open-source of our datasets, benchmarks, and implementation can facilitate the development of BinSD. 

11 , , Lirong Fu, Peiyu Liu, Wenlong Meng, Kangjie Lu, Shize Zhou, Xuhong Zhang, Wenzhi Chen, and Shouling Ji 

## REFERENCES 

[1] 2023. Mean reciprocal rank. https://en.wikipedia.org/wiki/Mean_reciprocal_rank. Accessed 2023-06-20. [2] 2023. Rank-1. https://en.wikipedia.org/wiki/Rank_1. Accessed 2023-06-20. [3] 2023. Standard deviation. https://en.wikipedia.org/wiki/Standard_deviation. Accessed 2023-06-20. [4] Sunwoo Ahn, Seonggwan Ahn, Hyungjoon Koo, and Yunheung Paek. 2022. Prac-tical Binary Code Similarity Detection with BERT-based Transferable Similarity Learning. In Proceedings of the 38th Annual Computer Security Applications Conference. 361–374. [5] Saad Albawi, Tareq Abed Mohammed, and Saad Al-Zawi. 2017. Understand-ing of a convolutional neural network. In 2017 International Conference on Engineering and Technology (ICET). Ieee, 1–6. [6] Felipe Almeida. 2022. Evaluation Metrics for Ranking problems: Introduc-tion and Examples. https://queirozf.com/entries/evaluation-metrics-for-ranking-problems-introduction-and-examples. Accessed 2023-06-20. [7] Chen Cai and Yusu Wang. 2020. A note on over-smoothing for graph neural networks. arXiv preprint arXiv:2006.13318 (2020). [8] Supriyo Chakraborty, Richard Tomsett, Ramya Raghavendra, Daniel Har-borne, Moustafa Alzantot, Federico Cerutti, Mani Srivastava, Alun Preece, Simon Julier, Raghuveer M Rao, et al . 2017. Interpretability of deep learning models: A survey of results. In 2017 IEEE smartworld, ubiquitous intelligence & computing, advanced & trusted computed, scalable computing & communications, cloud & big data computing, Internet of people and smart city innovation (smartworld/SCALCOM/UIC/ATC/CBDcom/IOP/SCI). IEEE, 1–6. [9] Mahinthan Chandramohan, Yinxing Xue, Zhengzi Xu, Yang Liu, Chia Yuan Cho, and Hee Beng Kuan Tan. 2016. Bingo: Cross-architecture cross-os binary search. In Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering. ACM, 678–689. [10] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. 2020. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34. 3438–3445. [11] Fenxiao Chen, Yun-Cheng Wang, Bin Wang, and C-C Jay Kuo. 2020. Graph rep-resentation learning: a survey. APSIPA Transactions on Signal and Information Processing 9 (2020). [12] KR1442 Chowdhary. 2020. Natural language processing. Fundamentals of artificial intelligence (2020), 603–649. [13] Paolo Milani Comparetti, Guido Salvaneschi, Engin Kirda, Clemens Kolbitsch, Christopher Kruegel, and Stefano Zanero. 2010. Identifying dormant functionality in malware programs. In 2010 IEEE Symposium on Security and Privacy. IEEE, 61–76. [14] Hanjun Dai, Bo Dai, and Le Song. 2016. Discriminative embeddings of latent variable models for structured data. In International Conference on Machine Learning. 2702–2711. [15] Yaniv David, Nimrod Partush, and Eran Yahav. 2016. Statistical similarity of binaries. ACM SIGPLAN Notices 51, 6 (2016), 266–280. [16] Yaniv David, Nimrod Partush, and Eran Yahav. 2017. Similarity of binaries through re-optimization. In ACM SIGPLAN Notices, Vol. 52. ACM, 79–94. [17] Steven H. H. Ding, Benjamin C. M. Fung, and Philippe Charland. 2019. Asm2Vec: Boosting Static Representation Robustness for Binary Clone Search against Code Obfuscation and Compiler Optimization. In 2019 IEEE Symposium on Security and Privacy (SP). 472–489. https://doi.org/10.1109/SP.2019.00003 [18] Yue Duan, Xuezixiang Li, Jinghan Wang, and Heng Yin. 2020. Deepbindiff: Learning program-wide code representations for binary diffing. In Proceedings of the 2020 Network and Distributed Systems Security Symposium (NDSS). [19] Sebastian Eschweiler, Khaled Yakdan, and Elmar Gerhards-Padilla. 2016. dis-covRE: Efficient Cross-Architecture Identification of Bugs in Binary Code.. In Proceedings of the 2016 Network and Distributed Systems Security Symposium (NDSS). [20] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. 2019. Graph Neural Networks for Social Recommendation. arXiv preprint arXiv:1902.07243 (2019). [21] Qian Feng, Minghua Wang, Mu Zhang, Rundong Zhou, Andrew Henderson, and Heng Yin. 2017. Extracting conditional formulas for cross-platform bug search. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. 346–359. [22] Luciano Floridi and Massimo Chiriatti. 2020. GPT-3: Its nature, scope, limits, and consequences. Minds and Machines 30 (2020), 681–694. [23] Lirong Fu, Shouling Ji, Changchang Liu, Peiyu Liu, Fuzheng Duan, Zonghui Wang, Whenzhi Chen, and Ting Wang. 2021. Focus: Function clone identification on cross-platform. International Journal of Intelligent Systems (2021). [24] Debin Gao, Michael K Reiter, and Dawn Song. 2008. Binhunt: Automatically finding semantic differences in binary programs. In International Conference on Information and Communications Security. Springer, 238–255. [25] Jian Gao, Xin Yang, Ying Fu, Yu Jiang, and Jiaguang Sun. 2018. VulSeeker: a semantic learning based vulnerability seeker for cross-platform binary. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. ACM, 896–899. [26] K Greff, R. K. Srivastava, J Koutnik, B. R. Steunebrink, and J Schmidhuber. 2017. LSTM: A Search Space Odyssey. IEEE Transactions on Neural Networks & Learning Systems 28, 10 (2017), 2222–2232. [27] Irfan Ul Haq and Juan Caballero. 2021. A survey of binary code similarity. ACM Computing Surveys (CSUR) 54, 3 (2021), 1–38. [28] Hex-Rays. 2023. IDA: About. https://www.hex-rays.com/products/ida/. Accessed 2023-06-20. [29] Julia Hirschberg and Christopher D Manning. 2015. Advances in natural language processing. Science 349, 6245 (2015), 261–266. [30] He Huang, Amr M Youssef, and Mourad Debbabi. 2017. Binsequence: Fast, accurate and scalable binary code reuse detection. In Proceedings of the 2017 ACM on Asia conference on computer and communications security. 155–166. [31] Jiyong Jang, David Brumley, and Shobha Venkataraman. 2011. Bitshred: feature hashing malware for scalable triage and semantic analysis. In Proceedings of the 18th ACM conference on Computer and communications security. 309–320. [32] Muhui Jiang, Qinming Dai, Wenlong Zhang, Rui Chang, Yajin Zhou, Xiapu Luo, Ruoyu Wang, Yang Liu, and Kui Ren. 2022. A Comprehensive Study on ARM Disassembly Tools. IEEE Transactions on Software Engineering (2022). [33] Zheyue Jiang, Yuan Zhang, Jun Xu, Qi Wen, Zhenghe Wang, Xiaohan Zhang, Xinyu Xing, Min Yang, and Zhemin Yang. 2020. Pdiff: Semantic-based patch pres-ence testing for downstream kernels. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security. 1149–1163. [34] Wenxiang Jiao, Wenxuan Wang, JT Huang, Xing Wang, and ZP Tu. 2023. Is ChatGPT a good translator? Yes with GPT-4 as the engine. arXiv preprint arXiv:2301.08745 (2023). [35] Wesley Jin, Sagar Chaki, Cory Cohen, Arie Gurfinkel, Jeffrey Havrilla, Charles Hines, and Priya Narasimhan. 2012. Binary function clustering using seman-tic hashes. In 2012 11th International Conference on Machine Learning and Applications, Vol. 1. IEEE, 386–391. [36] Geunwoo Kim, Sanghyun Hong, Michael Franz, and Dokyung Song. 2022. Im-proving cross-platform binary analysis using representation learning via graph alignment. In Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis. 151–163. [37] Arun Lakhotia, Mila Dalla Preda, and Roberto Giacobazzi. 2013. Fast location of similar code fragments using semantic’juice’. In Proceedings of the 2nd ACM SIGPLAN Program Protection and Reverse Engineering Workshop. 1–6. [38] Young Jun Lee, Sang-Hoon Choi, Chulwoo Kim, Seung-Ho Lim, and Ki-Woong Park. 2017. Learning binary code with deep learning to detect software weakness. In KSII the 9th international conference on internet (ICONI) 2017 symposium. [39] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al . 2023. StarCoder: may the source be with you! arXiv preprint arXiv:2305.06161 (2023). [40] Xuezixiang Li, Yu Qu, and Heng Yin. 2021. Palmtree: learning an assembly language model for instruction embedding. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security. 3236–3251. [41] Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. 2019. Graph matching networks for learning the similarity of graph structured objects. In International conference on machine learning. PMLR, 3835–3845. [42] Xiang Ling, Lingfei Wu, Saizhuo Wang, Tengfei Ma, Fangli Xu, Alex X Liu, Chunming Wu, and Shouling Ji. 2021. Multilevel Graph Matching Networks for Deep Graph Similarity Learning. IEEE Transactions on Neural Networks and Learning Systems (2021). [43] Bingchang Liu, Wei Huo, Chao Zhang, Wenchao Li, Feng Li, Aihua Piao, and Wei Zou. 2018. 𝛼 Diff: cross-version binary code similarity detection with DNN. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. ACM, 667–678. [44] Lannan Luo, Jiang Ming, Dinghao Wu, Peng Liu, and Sencun Zhu. 2014. Semantics-based obfuscation-resilient binary code similarity comparison with applications to software plagiarism detection. In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering .ACM, 389–400. [45] Zhenhao Luo, Pengfei Wang, Baosheng Wang, Yong Tang, Wei Xie, Xu Zhou, Dan-jun Liu, and Kai Lu. 2023. VulHawk: Cross-architecture Vulnerability Detection with Entropy-based Binary Code Search. (2023). [46] Christopher Manning and Hinrich Schutze. 1999. Foundations of statistical natural language processing. MIT press. [47] Andrea Marcelli, Mariano Graziano, Xabier Ugarte-Pedrero, Yanick Fratanto-nio, Mohamad Mansouri, and Davide Balzarotti. 2022. How Machine Learning Is Solving the Binary Function Similarity Problem. In 31st USENIX Security Symposium (USENIX Security 22) . USENIX Association, Boston, MA, 2099–2116. https://www.usenix.org/conference/usenixsecurity22/presentation/marcelli [48] Luca Massarelli, Giuseppe A Di Luna, Fabio Petroni, Leonardo Querzoni, Roberto Baldoni, et al . 2019. Investigating graph embedding neural networks with un-supervised features extraction for binary analysis. In Proceedings of the 2nd Workshop on Binary Analysis Research (BAR). 1–11. 12 Understanding the AI-powered Binary Code Similarity Detection , , 

[49] Luca Massarelli, Giuseppe Antonio Di Luna, Fabio Petroni, Roberto Baldoni, and Leonardo Querzoni. 2019. Safe: Self-attentive function embeddings for binary similarity. In International Conference on Detection of Intrusions and Malware, and Vulnerability Assessment. Springer, 309–329. [50] Tomas Mikolov, Ilya Sutskever, Chen Kai, Greg Corrado, and Jeffrey Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. Advances in Neural Information Processing Systems 26 (2013), 3111–3119. [51] Jiang Ming, Meng Pan, and Debin Gao. 2012. iBinHunt: Binary hunting with inter-procedural control flow. In International Conference on Information Security and Cryptology. Springer, 92–109. [52] Reza Mirzazadeh, Mohammad Hossein Moattar, and Majid Vafaei Jahan. 2015. Metamorphic malware detection using linear discriminant analysis and graph similarity. In 2015 5th International Conference on Computer and Knowledge Engineering (ICCKE). IEEE, 61–66. [53] Beng Heng Ng and Atul Prakash. 2013. Expose: Discovering potential binary code re-use. In 2013 IEEE 37th Annual Computer Software and Applications Conference. IEEE, 492–501. [54] Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, and Tie-Yan Liu. 2021. How could Neural Networks understand Programs?. In International Conference on Machine Learning. PMLR, 8476–8486. [55] Jannik Pewny, Behrad Garmany, Robert Gawlik, Christian Rossow, and Thorsten Holz. 2015. Cross-architecture bug search in binary executables. In 2015 IEEE Symposium on Security and Privacy. IEEE, 709–724. [56] Jannik Pewny, Felix Schuster, Lukas Bernhard, Thorsten Holz, and Christian Rossow. 2014. Leveraging semantic signatures for bug search in binary programs. In Proceedings of the 30th Annual Computer Security Applications Conference. 406–415. [57] Kimberly Redmond, Lannan Luo, and Qiang Zeng. 2018. A cross-architecture instruction embedding model for natural language processing-inspired binary code analysis. arXiv preprint arXiv:1812.09652 (2018). [58] George J Resnikoff, Gerald J Lieberman, and George J Resnikoff. 1957. Tables of the non-central t-distribution: density function, cumulative distribution function and percentage points. Stanford University Press. [59] T Konstantin Rusch, Michael M Bronstein, and Siddhartha Mishra. 2023. A survey on oversmoothing in graph neural networks. arXiv preprint arXiv:2303.10993 (2023). [60] Paria Shirani, Leo Collard, Basile L Agba, Bernard Lebel, Mourad Debbabi, Lingyu Wang, and Aiman Hanna. 2018. Binarm: Scalable and efficient de-tection of vulnerabilities in firmware images of intelligent electronic devices. In International Conference on Detection of Intrusions and Malware, and Vulnerability Assessment. Springer, 114–138. [61] Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi. 2023. Evaluation of ChatGPT as a question answering system for answering complex questions. arXiv preprint arXiv:2303.07992 (2023). [62] Daniel Valcarce, Alejandro Bellogín, Javier Parapar, and Pablo Castells. 2020. Assessing ranking metrics in top-N recommendation. Information Retrieval Journal 23 (2020), 411–448. [63] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, et al . 2017. Graph attention networks. stat 1050, 20 (2017), 10–48550. [64] Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, and Kewei Tu. 2020. Automated concatenation of embeddings for struc-tured prediction. arXiv preprint arXiv:2010.05006 (2020). [65] Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, and Dawn Song. 2017. Neural network-based graph embedding for cross-platform binary code similarity detection. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. ACM, 363–376. [66] Jia Yang, Cai Fu, Xiao-Yang Liu, Heng Yin, and Pan Zhou. 2021. Codee: A Tensor Embedding Scheme for Binary Code Search. IEEE Transactions on Software Engineering (2021). [67] Shouguo Yang, Long Cheng, Yicheng Zeng, Zhe Lang, Hongsong Zhu, and Zhiqiang Shi. 2021. Asteria: Deep Learning-based AST-Encoding for Cross-platform Binary Code Similarity Detection. In 2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN) . IEEE, 224–236. [68] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. 2018. Hierarchical graph representation learning with differentiable pooling. Advances in neural information processing systems 31 (2018). [69] Sheryl R Young. 1994. Detecting misrecognitions and out-of-vocabulary words. In Proceedings of ICASSP’94. IEEE International Conference on Acoustics, Speech and Signal Processing, Vol. 2. IEEE, II–21. [70] Zeping Yu, Rui Cao, Qiyi Tang, Sen Nie, Junzhou Huang, and Shi Wu. 2020. Order matters: Semantic-aware neural networks for binary code similarity detection. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 1145– 1152. [71] Fangfang Zhang, Dinghao Wu, Peng Liu, and Sencun Zhu. 2014. Program logic based software plagiarism detection. In 2014 IEEE 25th international symposium on software reliability engineering. IEEE, 66–77. [72] Hang Zhang and Zhiyun Qian. 2018. Precise and accurate patch presence test for binaries. In 27th USENIX Security Symposium (USENIX Security 18). 887–902. [73] Kelly H Zou, A James O’Malley, and Laura Mauri. 2007. Receiver-operating characteristic analysis for evaluating diagnostic tests and predictive models. Circulation 115, 5 (2007), 654–657. [74] Fei Zuo, Xiaopeng Li, Patrick Young, Lannan Luo, Qiang Zeng, and Zhexin Zhang. 2019. Neural Machine Translation Inspired Binary Code Similarity Comparison beyond Function Pairs. In Proceedings of the 2019 Network and Distributed Systems Security Symposium (NDSS). 13
