Title: 

URL Source: http://arxiv.org/pdf/2408.14817v1

Published Time: Wed, 28 Aug 2024 01:04:43 GMT

Markdown Content:
# A Comprehensive Benchmark of Machine and Deep Learning Across Diverse Tabular Datasets 

# Assaf Shmuel 1∗, Oren Glickman 1, Teddy Lazebnik 2,31 Department of Computer Science, Bar Ilan University, Israel  

> 2

# Department of Mathematics, Ariel University, Israel  

> 3

# Department of Cancer Biology, Cancer Institute, UCL, UK  

> ∗

# Corresponding author: assafshmuel91@gmail.com 

Abstract 

The analysis of tabular datasets is highly prevalent both in scientific research and real-world applications of Machine Learning (ML). Unlike many other ML tasks, Deep Learning (DL) models often do not outperform traditional methods in this area. Previous comparative benchmarks have shown that DL performance is frequently equivalent or even inferior to models such as Gradient Boosting Machines (GBMs). In this study, we introduce a comprehensive benchmark aimed at better characterizing the types of datasets where DL models excel. Although several important benchmarks for tabular datasets already exist, our contribution lies in the variety and depth of our comparison: we evaluate 111 datasets with 20 different models, including both regression and classification tasks. These datasets vary in scale and include both those with and without categorical variables. Importantly, our benchmark contains a sufficient number of datasets where DL models perform best, allowing for a thorough analysis of the conditions under which DL models excel. Building on the results of this benchmark, we train a model that predicts scenarios where DL models outperform alternative methods with 86.1% accuracy (AUC 0.78). We present insights derived from this characterization and compare these findings to previous benchmarks. 

# 1 Introduction 

Machine learning (ML) has long been considered superior to deep learning (DL) when it comes to handling tabular data [1, 2, 3], a common data format in many real-world applications and fields like medicine [4, 5, 6], economy [7, 8], and operations [9, 10], to name a few. Nonetheless, this generalization does not hold universally [11]. While traditional ML algorithms, such as Random Forest [12] and XGboost [13], often excel in this domain, there are scenarios where DL models outperform ML methods, challenging the prevailing notion [11]. Understanding the conditions under which DL models can surpass ML methods on tabular data is crucial for practitioners seeking to leverage the full potential of these advanced techniques. Several benchmarking studies have attempted to compare the performance of ML and DL models across various types of data, in general [14, 15, 16], and for tabular data, in particular [17]. For instance, [11] proposed the TabNet model, a DL model specifically designed to handle tabular data, and showed competitive performance against traditional ML approaches. [1] used 45 tabular datasets from various domains (mainly from OpenML [18]) with heterogeneous columns, below 500 columns but over 3000 rows with at least ten times more rows than columns, no time-series, and without deterministic target column (like poker games’ data). The authors removed rows with missing data, used one hot encoding [19] for categorical columns, and for regression cases used a log transformation to the target variable. Based on these settings, the authors compared MLP [20], ResNet [21], FT transformer [22], and SAINT [23] for the DL models and the RF [12], XGboost [13], histogram-gradient boosting tree [24], and gradient boosting tree [25] for the ML models. The authors show that XGboost outperformed all DL models for both classification and regression tasks while showing that tuning hyperparameters does not make DL models outperform the ML models. They also suggest that DL models are challenged by uninformative features. 1

> arXiv:2408.14817v1 [cs.LG] 27 Aug 2024

Similarly, [2] used 11 tabular datasets with both classification and regression problems with 10 to 2000 columns and between 7000 and a million rows. The authors performed standardization (aka z-score normalization) of each feature in each dataset to have a mean of zero and a standard deviation of one. The authors take into consideration the XGboost model as a representer of the ML models, four DL models including TabNet, and three ensemble models of the DL model (with and without XGboost). The authors concluded that the DL models underperform compared to the ML models while an ensemble combining both model types produces better results, on average. [26] used 176 classification datasets from OpenML and CC-18 and 19 algorithms including 11 DL models and 8 ML models. The authors found that, on average, CatBoost [27] and XGboost outperformed the other models while also being an order of magnitude more computationally effective. The authors used the PyMFE [28] to compute a feature vector for each dataset and used it to analyze the properties in which DL models outperform ML models, on average, finding that irregular, with a large number of rows, and a high ratio of size to number of features actually decrease the DL models performance. [29] compared 300 datasets including both classification and regression tasks from multiple domains further supporting the conclusion that, on average, tree-based ML models outperform DL models. Despite these efforts, a comprehensive understanding of the nuanced conditions under which DL models excel over ML models, particularly on tabular data, remains underexplored. In particular, measurable (statistical) fea-tures of the dataset for the binary prediction of either DL or ML model will provide a better performance, are still unknown. In this paper, we aim to fill this gap by conducting an extensive benchmark study involving 111 datasets encompassing both regression and classification tasks. We evaluate 20 different model configurations, including 7 DL-based models, 7 Tree-based Ensemble models (TE), and 6 classical ML-based models, to ascertain their performance on tabular data. Based on these results, we adopted a meta-learning approach, profiling the properties of datasets where DL outperforms ML models. Our results reveal complex patterns while generic behavior like a small number of rows and a large number of columns as well as large kurtosis results in DL models outperforming ML models. We also find that the gap between the two groups is smaller for classification tasks compared to regression tasks. Our key contributions are: • Conducted an extensive and diverse benchmark involving 111 datasets encompassing both regression and classification tasks. • Evaluated the performance of 20 different model configurations, including 7 DL-based models, 7 Tree-based Ensemble models (TE), and 6 classical ML-based models. • Identified dataset characteristics, such as small number of rows and high kurtosis, that favor DL models over other ML models. • Trained a meta-learning model to predict whether DL models will outperform ML models, achieving 86.1% accuracy (AUC 0.78). • Presented explainable models, namely logistic regression and symbolic regression, which predict where DL models may perform better than alternative models. • Provided detailed insights into the comparative performance of DL and ML models on a diverse set of tabular datasets. The remainder of this paper is structured as follows: Section 2 describes the datasets and methodologies used in our benchmarking experiments as well as the evaluation strategy and profiling method. In Section 3, we present our experimental results. Finally, Section 4 discusses the applicative outcomes of these findings and suggests promising future research directions. 

# 2 Experimental setup 

In this section, we formally outline the experimental setup used to explore when DL models outperform ML models for tabular data. Initially, we present the datasets included in the analysis. Afterwards, the ML and DL models considered for the analysis are introduced, followed by the evaluation strategy for both the classification and regression tasks. Finally, the proposed profiling and meta-analysis analysis of which datasets DL models produce comparably better results than ML models. 

# 2.1 Datasets 

In order to create a diverse and comprehensive benchmark for tabular datasets, aiming to provide insights into the scenarios where DL models outperform ML models, we incorporated a wide array of datasets exhibiting considerable variability and diversity of real-world tasks and characteristics. The benchmark includes datasets for both regression and classification tasks sourced from various domains such as economics and medicine to ensure relevance across different application areas. Additionally, we selected datasets of varying sizes in terms of both the number of rows (43-245,057) and columns (4-267), which are known to be crucial to the performance of ML and DL models, from previous studies. In our selection process, we also prioritized datasets containing categorical features. Categorical features are prevalent in real-world datasets and pose unique challenges, often necessitating specific preprocessing and modeling approaches. Ensuring their inclusion allows our benchmark to accurately reflect real-world complexities and facilitates an assessment of how well different models handle such features. Moreover, we ensured that our dataset selection varies in terms of difficulty, with some datasets presenting straightforward predictive tasks while others pose more complex challenges with respect to model prediction accuracy. This varying degree of difficulty ensures that our benchmark can comprehensively evaluate and differentiate model performance across simpler as well as more challenging predictive tasks. None of the datasets in this benchmark allowed a perfect prediction with all models. In Table 1 we compare the characteristics of our datasets with previous works. Table 1: Comparison with Previous Studies                                                       

> Study # Models #Classification #Regression Median Dataset Median # Median # of Meta-Datasets Datasets Size Features Categorical Analysis
> [1] 722 33 17k 13 1No [26] 12-19* 176 17 2k 21 0Partial** [2] 69211k 32 1No [3] 19 4120k 21 2No [29] 5181 119 12K 21 1No Ours 20 54 57 5k 13 4Yes *12 models for regression datasets, 19 models for classification datasets. **[26] examine the effect of several meta-features, but do not present a predictive model for DL advantage over ML.

Overall, we included 111 datasets in this study: 57 regression datasets and 54 classification datasets. Table 2 summarizes the main parameters of the datasets. The full table of datasets is presented in the appendix. All datasets can be freely accessed online: 84 (76%) of the datasets are obtained from OpenML; an additional 20 (18%) regression datasets were obtained from a Materials Science regression benchmark [30], and 7 (6%) additional datasets were obtained from Kaggle. 

# 2.2 Machine learning and deep learning models 

We use 20 ML and DL models to capture a representative set of algorithms from both groups for our analysis. These models are mainly adopted from previous benchmarking studies and due to their overall popularity [26, 1]. Formally, for tree-based ensemble models, we use XGBoost [13], Random Forest [12], AdaBoost [31], LightGBM [32], CatBoost [27], and the H2O-GBM AutoML library [33]. For DL models we use two AutoDL libraries (AutoGluon [34] and H2O [33], both restricted to DL models), a ResNet-like model [21], MLP [20]. Additional models include AutoML libraries (TPOT [35], AutoGluon without the DL restriction [34]), and also SVM [36], KNN [37], Decision Tree [38], a symbolic regression model (GPLearn) [39], and Linear Regression or Logistic Table 2: Descriptive Statistics of the Dataset Variables 

Variable Mean STD Min 25% Median 75% Max 

Number of Rows 18576 39874 43 673.75 4720 14057.5 245057 Number of Columns 24.16 40.53 4 8.75 12.5 22.25 267 Number of Numerical Columns 14.25 30.21 0 3 7 17 247 Number of Categorical Columns 9.91 24.76 0 1 4 9 231 Kurtosis 348.01 1129.66 -2711.83 -0.31 6.44 684.99 8901.75 Average Correlation Between Features 0.08 0.12 -0.16 0.01 0.06 0.14 0.62 Average Entropy 7.70 2.29 2.45 6.07 7.96 9.33 14.17 Average Pearson to Target Feature 0.11 0.10 -0.19 0.04 0.09 0.18 0.44 Regression [40] for regressions and classifications, respectively. All details of the models’ runs are detailed in the appendix. 

# 2.3 Evaluation strategy 

Inspired by [26], we present the mean results of a 10-fold cross-validation evaluation. For the regression tasks we calcualte root mean squared error (RMSE), mean absolute error (MAE), and coefficient of determination ( R2). For the classification tasks, we present accuracy, Area Under the receiver operating characteristic (ROC) Curve (AUC), and F1 score. For each dataset, we evaluate the performance of each one of the models. We then present the performance summaries and ranking of each model. We also calculate the ranking by model groups (GBM/ML, DL, and others). 

# 2.4 Meta-analysis profiling 

In order to analyze for which datasets DL models outperform ML models, we adopted a meta-learning approach, following the same analysis proposed by [41]. Namely, we aim to find a meta-learning ML algorithm (A∗) that receives as input a set of datasets (D), a set of ML models (M L ), and a set of DL models (DL ). It outputs a model (e.g., function) (M ) such that given a new dataset and the same sets of ML and DL models, the model (M )

returns whether ML or DL is best performing on the given dataset, according to some loss function (L). Formally, the algorithm A∗ satisfies: 

A∗ := min  

> A∈A

Σd∈D L A(d, M L, DL ), (1) where A is the set of all possible meta-learning models and A ∈ A is a meta-learning model. We solve this optimization problem using a meta-learning approach. First, we construct a meta-dataset which operates as the data for the learning model. Second, we find a learning model that optimizes Eq. (1) using a search algorithm. In order to obtain A∗, we propose a meta-learning approach that requires a dataset to learn from. Thus, we constructed a meta-dataset as follows. First, each dataset is converted into a meta-feature vector with 20 parameters (full description provided in Table 8), marked as ¯X. This feature space is constructed from a simple feature [42] such as the number of records and features, statistical properties of the dataset itself [43] such as the fourth standardized moment, and statistical features measuring the connections between the independent features and the target feature [44] such as the average Pearson correlation between the independent features and the target feature. These features have been used to obtain good results in previous meta-learning tasks [42, 43, 44]. In addition, we compute the performance of each of the 20 algorithms (see Section 2.2) on the dataset (RMSE for regression, and AUC for classification), taking the model with the best performance. If this model belongs to the ML algorithms group, the target column ( ¯Y ) of the meta-learning is set to 1 and 0 otherwise. Based on these two sets ( ¯X, ¯Y ), we define a meta-dataset such that ( ¯X) are the source features and ( ¯Y ) is the target feature of the dataset. Classifying whether ML or DL will best perform for a given dataset is a binary classification problem. We formalize this task as a search problem in which one needs to find the optimal configuration, as defined in the ML pipeline in Eq. (1). One way to solve this classification problem is by using machine learning models. To this end, we used both a symbolic regression model and a ML model and to obtain both explainability and investigate the best possible theoretical prediction, respectively. We used aggressive grid search optimization for the hyperparameters of both models, as well as the 10-fold cross-validation to ensure robustness. Specifically, due to the characteristics of this meta-dataset, we chose H2O-DL as the main model for this task. 

# 3 Results 

In this section, we present the results of the benchmarking analysis. We begin by outlining the performance of each of the models on the datasets, summarizing 4,000 computation hours. Afterward, we explore the influence of several central properties of datasets and their influence on ML and DL performance. Finally, we provide a measurable profiling of when DL outperforms ML models on tabular tasks. 

# 3.1 Model Ranking 

Table 3 outlines the performance of the Tree-based Ensemble models (TE) and DL models on the 111 datasets. The models are ordered from top to bottom according to the number of datasets where they outperformed the other datasets. One can notice that ML models occupy the first four lines, led by CatBoost with 19/111 (17.1%). The first DL model appears on the fifth row with 11/111 (9.9%) datasets where it is best-performing. This ranking is preserved by other metrics such as average ranking and median ranking. Table 3: Performance ranking of TE and DL models. Model Group # Best Average Rank Median Rank # in Top 3 Models CatBoost TE 19 4.9 4 50 LightGBM TE 15 5 4 47 H2O-GBM TE 13 7 6 28 Random Forest TE 11 6.7 6.5 28 AutoGluon-DL DL 11 6.6 7 32 ResNet DL 10 7.5 8 23 TPOT TE 7 5.9 5 39 H2O-DL DL 6 8.8 9 13 MLP DL 6 7.9 8 17 XGBoost TE 5 6.4 6 34 AdaBoost TE 4 10.1 11 9DCNV2 DL 4 9 10 12 FT-Transformer DL 0 10.7 11 1TabNet DL 0 13.1 14 0Table 4 extends Table 3 as it presents the performance of all 20 models on the 111 datasets. Notably, Auto-Gluon, an ensemble of DL and ML models method best performed for 39 out of the 111 datasets (35%), almost four times more than SVM, the second-best model, which best-performed in only 10 datasets (9%). To this end, AutoGluon is also the best-performing model, on average, while SVM’s is actually poorly performing on average and excels occupancy. In a more general sense, ML models occupy the top three most performing models of each dataset most of the time (see last column). TabNet is reaching last place with no datasets where it is the best-performing one with also the worst average position from all the models. FT-Transformer shows similar behavior, only slightly outperforming TabNet. The results are also summarized using critical difference diagrams based on RMSE for regressions tasks (Fig. 1) and on accuracy for classifications tasks (Fig. 2). The performance ranking based on alternative metrics (MAE, R2, AUC, and F1-score) are presented in the appendix. Table 5 follows the same line but includes only datasets with less than 1000 rows (36). In this scenario, H2O led the chart with 6/36 (16.6%), followed closely by ResNet with 5/36 (13.9%) datasets in which they are best Table 4: Performance ranking of all models. Model Group # Best Average Rank Median Rank # in Top 3 Models AutoGluon Other 39 4.8 4 58 SVM Other 10 12.4 14 15 ResNet DL 7 9.7 10 13 CatBoost TE 7 6.6 5 35 LightGBM TE 6 6.9 6 33 H2O-GBM TE 6 8.6 8 18 TPOT TE 5 7.7 7 23 AutoGluon-DL DL 5 8.7 8 21 H2O-DL DL 4 11.5 11 11 gplearn Other 3 15 17 7MLP DL 3 9.6 10 13 LR Other 3 11.6 13 16 XGBoost TE 3 8.4 8 19 Random Forest TE 3 8.5 8 20 DCNV2 DL 3 11.6 12 10 KNN Other 2 12.1 13 12 Decision Tree Other 1 13.3 14 3AdaBoost TE 1 12.3 13 5FT-Transformer DL 0 13.9 14 1TabNet DL 0 17.2 18 012345678910 11 12 

> 10.3509

AutoGluon 9.1930 

CatBoost 8.3509 

TPOT 7.9298 

Random Forest 7.7018 

XGBoost 7.1579 

H2O-GBM 6.3684 AutoGluon-DL  

> 4.5789

LR  

> 4.3158

H2O-DL  

> 4.1228

SVM  

> 4.0175

AdaBoost  

> 3.9123

Decision Tree 

Figure 1: Critical difference diagram for regression tasks based on RMSE. The best performing model is Auto-Gluon as lower RMSE scores indicate better performance. 12345678910 11 12 13 14 

> 11.3981

gplearn 10.3704 

Decision Tree 9.3241 

KNN 8.7593 

LR 8.5463 

SVM 8.1019 

AdaBoost 7.8704 

H2O-DL 6.4074 XGBoost  

> 6.2037

AutoGluon-DL  

> 6.0463

TPOT  

> 5.9630

H2O-GBM  

> 5.8333

LightGBM  

> 5.8241

CatBoost  

> 4.3519

AutoGluon 

Figure 2: Critical difference diagram for classification tasks based on accuracy. The best performing model is AutoGluon as higher accuracy scores indicate better performance. performing. However, ResNet lags behind other ML models in the other metrics (average rank, median rank, and top 3 models) compared to CatBoost in the third row. Interestingly, ResNet shows similar performance to the much simpler MLP model as well as the more complex AutoGluon-DL model. Table 5: Performance metrics of TE and DL models for small datasets ( < 1000 ) rows. Model Group # Best Average Rank Median Rank # in Top 3 Models H2O-GBM TE 6 5.6 5 12 ResNet DL 5 6.7 7 8CatBoost TE 4 5.1 5 14 AutoGluon-DL DL 4 6.8 7.5 10 MLP DL 4 6.3 6 9TPOT TE 3 5.1 4 15 Random Forest TE 3 6.1 5.5 7LightGBM TE 3 4.9 4 13 XGBoost TE 2 6.6 7.5 13 H2O-DL DL 1 9.6 11 2DCNV2 DL 1 9.1 10 2AdaBoost TE 0 8.9 8.5 2FT-Transformer DL 0 10.1 11 1TabNet DL 0 13.9 14 0

# 3.2 Meta-Analysis Profiling 

We now present the results of the prediction of whether ML or DL will perform better in each dataset. Table 6 presents the coefficients of a logistic regression for this classification task. The model reveals several important findings. First, it demonstrates with statistical significance that the relative performance of DL models (compared to TE models) is better in classification tasks than in regression tasks. Second, we find that the Kurtosis variable is statistically significant. Finally, we find that the PCA components are also positive and almost statistically significant. We discuss these findings in Section 4. Next, we repeat this analysis after limiting the datasets to cases where the performance of ML/DL models was significantly different (with p < 0.05 ). This restriction leaves 36 datasets, 11 of which demonstrate higher performance for DL and 25 for TE. As we show later on, and as previously noted by [26], DL models have an advantage in small datasets; we therefore use the H2O-DL model to predict whether TE or DL performs better based on the properties of each dataset. Remarkably, H2O-DL provides a high performance in this classification task, with an AUC of 0.78, accuracy of 86.1%, and F1 score of 0.61. As a baseline, we also train a logistic regression, an explainable model, which obtains lower but still impressive performance (AUC of 0.68, accuracy of 80.6%, and F1 score of 0.44). Based on the predictions of the logistic regression model, we provide further insights into the most influential factors for TE versus DL performance. Fig. 3 presents heatmaps of four dataset’s configurations and their influence on the probability that a DL model would outperform the ML model for a given dataset, including (a) the impact of the number of columns and rows; (b) the influence of numerical and categorical feature counts; (c) the effect of X-kurtosis and row count; and (d) the role of PCA components necessary to maintain 99% of the variance. As one can see from sub-figure (a), for a small number of rows, increasing the number of columns results in a higher probability that the DL model would outperform an ML model. However, this effect decreases relatively quickly as the number of rows increases. Notably, for all the explored configurations, the probability does not increase over 0.5 which indicates no configuration found where DL models would outperform ML models, on average. For sub-figure (b), a more clear gradient is revealed where a smaller number of rows and a higher number of columns increase the probability that DL models outperform ML models. Interestingly, this heatmap reveals configurations where the probability is higher than 0.5. For sub-figure (c), one can notice that the number of rows does have Table 6: Coefficients of a logistic regression for predicting the probability that DL outperforms ML. 

Variable Coefficient Std Error z-value P¿—z— 

Intercept -0.8751 0.2345 -3.7315 0.0002 Row count -0.0195 0.5688 -0.0342 0.9727 Row over Column -1.5991 1.6984 -0.9415 0.3464 Classification/Regression 0.5563 0.2590 2.1483 0.0317 Cancor -0.2067 0.2584 -0.8000 0.4237 Kurtosis 0.8975 0.3987 2.2514 0.0244 Average Asymmetry Of Features -0.0316 0.2480 -0.1274 0.8986 Average Pearson to Target Feature 0.5247 0.2984 1.7581 0.0787 Standard Deviation Pearson To Target Feature -0.2326 0.2772 -0.8390 0.4014 Average Correlation Between Features -0.1639 0.2501 -0.6552 0.5123 Average Coefficient of Variation -0.1087 0.4039 -0.2692 0.7878 Standard Deviation Coefficient of Variation -0.0320 0.4196 -0.0764 0.9391 Average Coefficient of Anomaly 0.0588 0.2746 0.2140 0.8305 Standard Deviation Coefficient of Anomaly -0.2730 0.2859 -0.9550 0.3396 Average Entropy -0.2086 0.2708 -0.7704 0.4410 Standard Deviation Entropy 0.1769 0.2572 0.6877 0.4916 Columns after One Hot Encoding -0.5745 0.3948 -1.4550 0.1457 Rows over Columns after One Hot Encoding 1.7303 1.4779 1.1708 0.2417 PCA 0.4624 0.2648 1.7462 0.0808 much influence on the probability while a large X-kurtosis signals that DL models are probably preferred over ML models. Finally, sub-figure (d), shows a similar trend to the previous sub-figures where a small number of rows and columns, especially if these are more “computationally-attractive” like PCA-based features, results in a higher probability for DL models to outperform ML models. Finally, we trained a symbolic regression (SR) model to search for more complex equations of this prediction task. The performance of the SR model was relatively close to that of the logistic regression. The formula output, which we present in Eq. 2, also predicts higher relative DL success rate in small datasets and with high Kurtosis values. logreg  0.005 · xkurtosis − 4.3 × 10 −5 · xrow count − 0.053 · xstd coefficient of anomaly − 23 .0 · xstd linearly to target + 0 .89 

(2) where, logreg (z) = 11 + e−z

xkurtosis is the kurtosis 

xrow count is the number of rows 

xstd coefficient of anomaly is the standard deviation of the coefficient of anomaly 

xstd linearly to target is the standard deviation linearly to the target Next, to explore the effect of data size, we repeated the training of 10 large datasets after sampling them to 1000 training samples. The original and revised rankings for these 10 datasets are summarized in Table 7. While the ranking of some DL models (AutoGluon-DL and ResNet) improves in the smaller datasets, TE models still dominate this set. Although this is a relatively small sample of 10 datasets, it provides further evidence that sample size is an important factor but does not determine which model will have the best performance by itself. Figure 3: The effect of various factors on the probability that DL outperforms ML. The heatmaps are generated using the prediction of the logistic regression models. The scatter plot represents the actual observations of the datasets. (a) the impact of the number of columns and rows; (b) the influence of numerical and categorical feature counts; (c) the effect of X-kurtosis and row count; and (d) the role of PCA components necessary to maintain 99% of the variance and number of rows. 

# 4 Discussion 

In this study, we benchmark the performance of 20 data-driven models, divided between ML and DL models, for the tasks of regression and classification of tabular data from 111 datasets. While DL models are currently state-of-the-art in multiple computational tasks such as computer vision, natural language processing, and signal processing, to name a few, they are outperformed by ML models for tabular data. Following several recent bench-mark studies, we computed one of the most comprehensive benchmarking of ML and DL models’ performance on tabular data and their profiling. In this study, we focused on providing measurable (statistical) features of datasets, available before running any model, which can indicate when DL models would outperform ML models in both regression and classification tasks. The trained model obtained a remarkable accuracy of 86.1% and AUC of 0.76. We also presented explainable models, logistic regression and symbolic regression, with slightly lower performance. Similar to previous benchmarking studies [45, 2], our analysis shows that ML models, on average, outperform DL models on tabular data. Specifically, Tree-based Ensemble models consistently exhibit the highest performance in this field. This behavior is consistent for the best-performing model, as well as the mean rank, and 3-top model, indicating that on a random tabular dataset, ML models would be the safe “bate”, as indicated by Table 4. In Table 7: Comparison of model performance on original and sampled datasets                                                                                                                                                       

> Original Model Group # Best Average Rank Median Rank # in Top 3 Models
> LightGBM TE 34.1 35CatBoost TE 23.3 3.5 5XGBoost TE 14.5 43Random Forest TE 14.3 36H2O-GBM TE 1662H2O-DL DL 1892MLP DL 18.1 81TPOT TE 05.7 44AdaBoost TE 013.2 13 0AutoGluon-DL DL 06.9 71ResNet DL 07.3 91FT-Transformer DL 010.8 11 0TabNet DL 013.8 14 0DCNV2 DL 014.5 14.5 0
> Sampled to 1,000 Rows Model Group # Best Average Rank Median Rank # in Top 3 Models
> LightGBM TE 42.4 27XGBoost TE 24.2 36TPOT TE 14.2 44CatBoost TE 13.6 3.5 5AutoGluon-DL DL 17.6 7.5 2ResNet DL 16.9 8.5 2Random Forest TE 06.5 61AdaBoost TE 011.7 12 0H2O-GBM TE 06.1 5.5 2H2O-DL DL 09.5 90MLP DL 07.6 81FT-Transformer DL 010.9 11 0TabNet DL 013.7 14 0DCNV2 DL 012.4 13.5 0

particular, we found that AutoGluon, an automatic ML model [46] that uses ensembles of both ML and DL models, outperforms the other models by a large margin. This outcome aligns with the findings presented by [26]. In addition, we found that TabNet actually performs worse than DL models that are not specifically designed for tabular data such as MLP and H2O, which agrees with previous attempts of using TabNet as part of benchmarking attempts [47]. Moreover, previous studies do not show clear agreement on the influence of the number of rows and cols on the probability that DL models would outperform ML models [1, 2, 26], we tackle this challenge as shown in Fig. 3. Our results clearly indicate a somewhat linear trend where a smaller number of rows and a larger number of columns, on average, results in a higher probability that DL models would outperform ML models. However, the analysis in Table 7 which tries to isolate the size factor does not show clear results; our conclusion is that while DL models may outperform other ML models in small datasets in many cases, this is just one of many other factors which affect the relative performance of the two model groups. Regarding the profiling of the dataset characteristics to predict whether either DL model or ML model would provide the best-performing results, a logistic regression analysis (see Table 6) reveals only two statistically signif-icant dataset’s features - is the task a regression or classification and the kurtosis metric. In particular, the relative performance of DL models compared to other ML models is better in classification tasks compared to regression tasks. This is also highlighted by the separate regression and classification ranking in the appendix (Tables 14, 20). A possible explanation for this phenomenon is that in classification tasks, all errors contribute equally to the performance metric while in regression tasks, large errors have more weight (RMSE is unbounded). Due to the large parameter space of DL models, it is possible that they could sometimes exhibit large errors which are heavily penalized by metrics such as RMSE. Indeed, when ranking the models by MAE for regression tasks, the relative ranking of DL models improves significantly with AutoGluon-DL positioned second after CatBoost (Table 16). Finally, focusing on the kurtosis metric, a large value indicates a long “tail” distribution where DL is known to excel compared to ML models [48]. 

Limitations and future work. While this study presented an exhaustive evaluation of the different models over 111 datasets, it still has several limitations. First, the choice to include diverse datasets, in contrast to several previous benchmarks, has many advantages but also some disadvantages. For example, including small datasets could introduce more noise into the results. In addition, including many types of datasets inevitably means each type will have fewer instances. We tackled this problem by including a large number of datasets, but including one type of homogeneous datasets would obviously result in more instances for this type. Second, an analysis of feature selection or feature engineering, which is known to have a significant impact on the down-the-line model’s performance, has not been included in this work. Finally, while this study included diverse regression and classification datasets, there are additional tasks that have not been included, such as time-series or multilabel classifications. These extensions can reveal additional insights regarding the performance of ML and DL on tabular data and are promising research future venues. 

# References 

[1] L. Grinsztajn, E. Oyallon, and G. Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? In Advances in Neural Information Processing Systems , volume 35, pages 507–520, 2022. [2] R. Shwartz-Ziv and A. Armon. Tabular data: Deep learning is not all you need. Information Fusion , 81:84– 90, 2022. [3] V. Borisov, T. Leemann, K. Sebler, J. Haug, M. Pawelczyk, and G. Kasneci. Deep neural networks and tabular data: A survey. IEEE Transactions on Neural Networks and Learning Systems , pages 1–21, 2022. [4] W. W. B. Goh and L. Wong. Evaluating feature-selection stability in next-generation proteomics. Journal of Bioinformatics and Computational Biology , 14(5):1650029, 2016. [5] N. Yu, Z. Li, and Z. Yu. Survey on encoding schemes for genomic data representation and feature learn-ing—from signal processing to machine learning. Big Data Mining and Analytics , 1(3):191–210, 2018. [6] Y. A. Veturi, W. Woof, T. Lazebnik, I. Moghul, P. Woodward-Court, S. K. Wagner, T. A. Cabral de Guimaraes, M. D. Varela, B. Liefers, P. J. Patel, S. Beck, A. R. Webster, O. Mahroo, P. A. Keane, M. Michaelides, K. Balaskas, and N. Pontikos. Syntheye: Investigating the impact of synthetic data on ai-assisted gene diagnosis of inherited retinal disease. Ophthalmology Science , page 100258, 2022. [7] J. Felix, M. Alexandre, and G. T. Lima. Applying machine learning algorithms to predict the size of the informal economy. Computational Economics , 2024. [8] L. Shami and T. Lazebnik. Implementing machine learning methods in estimating the size of the non-observed economy. Computational Economy , 53:1459–1476, 2024. [9] G. Zheng, L. Kong, and A. Brintrup. Federated machine learning for privacy preserving, collective supply chain risk prediction. International Journal of Production Research , 61(23):8115–8132, 2023. [10] R. Volpe. Evaluating the performance of u.s. supermarkets: Pricing strategies, competition from hypermar-kets, and private labels. Journal of Agricultural and Resource Economics , 36(3):488–503, 2011. [11] S. O. Arik and T. Pfister. Tabnet: Attentive interpretable tabular learning. arXiv , 2020. [12] L. Rokach. Decision forest: twenty years of research. Information Fusion , 27:111–125, 2016. [13] T. Chen and C. Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 785–794, 2016. [14] A. Theofilatos, C. Chen, and C. Antoniou. Comparing machine learning and deep learning methods for real-time crash prediction. Transportation Research Record , 2673(8):169–178, 2019. [15] F. Matrone, E. Grilli, M. Martini, M. Paolanti, R. Pierdicca, and F. Remondino. Comparing machine and deep learning methods for large 3d heritage semantic segmentation. ISPRS International Journal of Geo-Information , 9(9):535, 2020. [16] N. Thapa, Z. Liu, D. B. KC, B. Gokaraju, and K. Roy. Comparison of machine learning and deep learning models for network intrusion detection systems. Future Internet , 12(10):167, 2020. [17] S. A. Fayaz, S. Kaul, M. Zaman, and M. A. Butt. Is deep learning on tabular data enough? an assessment. 

International Journal of Advanced Computer Science and Applications , 13(4), 2022. [18] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. Openml: Networked science in machine learning. SIGKDD Explorations , 15(2):49–60, 2013. [19] P. Rodr´ ıguez, M. A. Bautista, J. Gonzalez, and S. Escalera. Beyond one-hot encoding: Lower dimensional target embedding. Image and Vision Computing , 75:21–31, 2018. [20] H. Taud and J. F. Mas. Multilayer Perceptron (MLP) , pages 451–455. Springer Cham, 2018. [21] I. Z. Mukti and D. Biswas. Transfer learning based plant diseases detection using resnet50. In 2019 4th In-ternational Conference on Electrical Information and Communication Technology (EICT) , pages 1–6, 2019. [22] Y. Gorishniy, I. Rubachev, V. Khrulkov, and A. Babenko. Revisiting deep learning models for tabular data. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages 18932–18943. Curran Associates, Inc., 2021. [23] G. Somepalli, M. Goldblum, A. Schwarzschild, C. B. Bruss, and T. Goldstein. Saint: Improved neural networks for tabular data via row attention and contrastive pre-training. arXiv , 2021. [24] G. Marvin, L. Grbcic, S. Druzeta, and L. Kranjcevic. Water distribution network leak localization with histogram-based gradient boosting. Journal of Hydroinformatics , 25(3):663–684, 2023. [25] C. Bentejac, A. Csorgo, and G. Mart´ ınez-Munoz. A comparative analysis of gradient boosting algorithms. 

Artificial Intelligence Review , 54:1937–1967, 2020. [26] D. McElfresh, S. Khandagale, J. Valverde, V. Prasad, G. Ramakrishnan, M. Goldblum, and C. White. When do neural nets outperform boosted trees on tabular data? In Advances in Neural Information Processing Systems , volume 36, 2024. [27] L. Prokhorenkova, G. Gusev, A. Vorobev, A. V. Dorogush, and A. Gulin. Catboost: unbiased boosting with categorical features. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 31, 2018. [28] E. Alcobac ¸a, F. Siqueira, A. Rivolli, L. P. F. Garcia, J. T. Oliva, and A. C. P. L. F. de Carvalho. Mfe: Towards reproducible meta-feature extraction. Journal of Machine Learning Research , 21(111):1–5, 2020. [29] H-J. Ye, S-Y. Liu, H-R. Cai, Q-L. Zhou, and D-C. Zhan. A closer look at deep learning on tabular data. 

arXiv , 2024. [30] F. Conrad, M. M¨ alzer, M. Schwarzenberger, H. Wiemer, and S. Ihlenfeldt. Benchmarking AutoML for regression tasks on small tabular data in materials design. Scientific Reports , 12(1):19350, 2022. [31] R. E. Schapire. Explaining adaboost. In Empirical inference , pages 37–52. Springer, 2013. [32] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems , 30:3146–3154, 2017. [33] E. LeDell and S. Poirier. H2o automl: Scalable automatic machine learning. In Proceedings of the AutoML Workshop at ICML , volume 2020, 2020. [34] N. Erickson, J. Mueller, A. Shirkov, H. Zhang, P. Larroy, M. Li, and A. Smola. Autogluon-tabular: Robust and accurate automl for structured data. arXiv , 2020. [35] R. S. Olson and J. H. Moore. Tpot: A tree-based pipeline optimization tool for automating machine learning. In JMLR: Workshop and Conference Proceedings , volume 64, pages 66–74, 2016. [36] J. Neumann, C. Schnorr, and G. Steidl. Combined svm-based feature selection and classification. Machine Learning , 61:129–150, 2005. [37] B. Zang, R. Huang, L. Wang, J. Chen, F. Tian, and X. Wei. An improved knn algorithm based on minority class distribution for imbalanced dataset. In 2016 International Computer Symposium (ICS) , pages 696–700, 2016. [38] P. H. Swain and H. Hauska. The decision tree classifier: Design and potential. IEEE Transactions on Geoscience Electronics , 15(3):142–147, 1977. [39] K. Tanemura, Y. Tachibana, Y. Tokuni, H. Manabe, and R. Miyadera. Application of generic programming to unsolved mathematical problems. In 2022 IEEE 11th Global Conference on Consumer Electronics (GCCE) ,pages 845–849. IEEE, October 2022. [40] M. P. LaValley. Logistic regression. Circulation , 117(18), 2008. [41] T. Lazebnik and A. Rosenfeld. Fspl: A meta-learning approach for a filter and embedded feature selection pipeline. International Journal of Applied Mathematics and Computer Science , 33, 2023. [42] R. Engels and C. Theusinger. Using a data metric for preprocessing advice for data mining applications. In 

ECAI , 1998. [43] M. Reif, F. Shafait, and A. Dengel. Meta-learning for evolutionary parameter optimization of classifiers. 

Machine Learning , 87:357–380, 2012. [44] Z. Shen, X. Chen, and J. M. Garibaldi. A novel meta learning framework for feature selection using data synthesis and fuzzy similarity. In y IEEE World Congress on Computational Intelligence , 2020. [45] Evgenii A. Neverov, Ilia I. Viksnin, and Sergei S. Chuprov. The research of automl methods in the task of wave data classification. In 2023 XXVI International Conference on Soft Computing and Measurements (SCM) , pages 156–158, 2023. [46] T. Lazebnik, A. Somech, and A. I. Weinberg. Substrat: A subset-based optimization strategy for faster automl. Proc. VLDB Endow. , 16(4):772–780, 2022. [47] A. Kadra, M. Lindauer, F. Hutter, and J. Grabocka. Well-tuned simple nets excel on tabular datasets. In 

Advances in Neural Information Processing Systems , volume 34, pages 23928–23941, 2021. [48] G. Valle-Perez and A. A. Louis. Generalization bounds for deep learning. arXiv , 2020. [49] A. Rosenfeld and M. Freiman. Explainable feature ensembles through homogeneous and heterogeneous intersections. In JCAI-PRICAI 2020 Workshop on Explainable Artificial Intelligence , 2021. [50] K. K. Vasan and B. Surendiran. Dimensionality reduction using principal component analysis for network intrusion detection. Perspectives in Science , 8:510–512, 2016. 

# 5 Supplementary Material 

# 5.1 Description of models 

TPOT is an open-source library that automates the process of designing and optimizing ML pipelines. It uses genetic programming to explore a wide range of models and preprocessing steps, aiming to find the best pipeline for a given dataset. TPOT also performs hyperparameter optimization. We ran the AutoML library with mostly default settings, limiting each model’s run to one hour. 

H2O is an open-source software for data analysis that facilitates the development and deployment of machine learning models. It provides a scalable and fast platform for building models, with support for a variety of al-gorithms, including generalized linear models, gradient boosting, and deep learning. H2O’s automated machine learning functionality assists in discovering the best models by automatically training and tuning multiple models within a user-specified time frame. We ran two H2O models, both the H2O Gradient Boosting Machine (labeled H2O-GBM) and the H2O Deep Learning (labeled H2O-DL). Both were limited to one hour for each model run. 

XGBoost is an open-source ML algorithm developed for supervised learning tasks. It is an implementation of gradient boosted decision trees. Technically, XGBoost enhances performance by optimizing for speed and scalability, using techniques like parallel processing, tree pruning, and regularization to prevent overfitting. It also supports missing value handling and a range of objective functions. Widely recognized for its superior predictive power and fast execution, XGBoost has been a top choice in ML tabular tasks on a wide range of domains. We performed hyperparameter optimization using TPOT, with a one hour time limit. 

Random Forest extends the bagging method by incorporating both bagging and feature randomness to gen-erate an uncorrelated ensemble of decision tree models. Feature randomness creates a random subset of features, ensuring low correlation among the decision trees which improves the generalization of Random Forest compared to other bagging decision tree models. We performed hyperparameter optimization using TPOT, with a one hour time limit. 

AdaBoost classifier is a meta-estimator that starts by fitting a classifier, usually a decision tree, to the original dataset. It then fits additional copies of the classifier to the same dataset, adjusting the weights of incorrectly classified instances so that subsequent classifiers focus more on the challenging cases - a method usually called boosting. We performed hyperparameter optimization using TPOT, with a one hour time limit. 

CatBoost short for Categorical Boosting, is an open-source boosting library. It is tailored for regression and classification problems with a large number of independent features. Unlike traditional gradient boosting methods, CatBoost can directly handle both categorical and numerical features without needing feature encoding techniques (like One-Hot Encoding or Label Encoding). Moreover, it employs an algorithm called symmetric weighted quantile sketch to automatically handle missing values, thereby reducing overfitting and enhancing the overall performance of the model. We performed hyperparameter optimization using TPOT, with a one hour time limit. 

Decision Tree is a family of models that defines the connections between features and their possible conse-quences as a tree-like structure of nodes, where each internal node represents a feature (or attribute) test, each branch represents the outcome of the test, and each leaf node represents a class label (for classification) or a con-tinuous value (for regression). The algorithm splits the dataset into subsets based on the feature that results in the highest information gain or the lowest impurity, according to the popular Scikit-learn library, but not limited to these options. This process occurs recursively until no further division is possible. We performed hyperparameter optimization using TPOT, with a one hour time limit. 

Linear Regression and Logistic Regression. model the relationship between a set of features by fitting a linear equation to the observed data. Linear regression aims to minimize the sum of squared differences between the observed and predicted values. It is widely used due to its simplicity, interpretability, and efficiency, making it suitable for a variety of applications such as trend analysis, forecasting, and inferential statistics. 

GPLearn , or Genetic Programming for scikit-learn, is an open-source Python library that applies genetic programming to ML tasks. It allows for the automatic generation of analytical (formula-based) models by evolving programs to fit data, using principles inspired by biological evolution such as selection, mutation, and crossover (also known as symbolic regression). GPlearn can be used for regression and classification problems, where it evolves mathematical expressions to optimize predictive performance. We configured the genetic programming algorithm with 50 generations and a parsimony coefficient of 0.001 to control model complexity. 

SVM is a supervised ML algorithm used for classification and regression tasks. SVM works by finding the optimal hyperplane that best separates the data points of different classes in a high-dimensional space. This hyperplane maximizes the margin, which is the distance between the closest data points (support vectors) of each class. SVM can handle both linear and non-linear classification by using kernel functions to transform the input data into a higher-dimensional space where a linear separator can be found. Popular kernels include linear, polynomial, and radial basis functions. We performed hyperparameter optimization using TPOT, with a one hour time limit. 

KNN is a simple yet powerful supervised ML algorithm used for classification and regression tasks. It operates on the principle of similarity, where new instances are classified based on the majority class or average of the k nearest neighbors in the feature space. The algorithm does not involve explicit training; instead, it stores the entire training dataset and performs computations at prediction time. We performed hyperparameter optimization using TPOT, with a one hour time limit. 

FT-Transformer The FT-Transformer (FTT) model is a novel deep learning architecture specifically designed to handle tabular data effectively. It leverages the principles of the Transformer model, which is widely used in natural language processing, to capture the intricate relationships within tabular datasets. The model employs feature tokenization to handle numerical and categorical features and uses self-attention mechanisms to learn complex interactions between features. In our implementation, we used Optuna to optimize the hyperparameters of the FT-Transformer. As in [22], we set the number of heads to 8. We varied the number of Transformer blocks (n blocks) between 1 and 6. The dimension of the token embeddings (d token) was set to be a multiple of 8, ranging from 8 to 32 times 8. The dropout rate for the attention mechanism (attention dropout) and the feed-forward network (ffn dropout) was varied between 0.1 and 0.5. The hidden dimension in the feed-forward network (ffn d hidden) was set between 64 and 256, and the residual dropout (residual dropout) was also varied between 0.1 and 0.5. The learning rate (learning rate) was set between 10ˆ -4 and 10ˆ -2, and the batch size (batch size) was chosen as one of 32, 64, or 128. 

ResNet is a DL architecture designed to address the vanishing gradient problem in very deep neural networks. It introduces skip connections, also known as residual connections, that allow gradients to flow more effectively during training. ResNet architectures typically stack multiple residual blocks to form a deep network. The skip connections in each block enable the gradient to propagate more efficiently through the network, alleviating the vanishing gradient problem and enabling the training of very deep models. we utilized Optuna to optimize the hyperparameters by suggesting a range of values for each parameter. Specifically, we set the number of blocks (n blocks) between 1 and 5. For the main dimension of each block (d main), the value was set between 16 and 64, and for the hidden dimension (d hidden) within each block, the value was set between 16 and 64. The dropout rates for the first dropout layer (dropout first) and the second dropout layer (dropout second) were varied between 0.1 and 0.5. The learning rate (learning rate) was set between 10ˆ -4 and 10ˆ -2, and the batch size (batch size) was chosen as one of 32, 64, or 128. 

MLP , a Multilayer Perceptron is a type of artificial neural network that consists of multiple layers of intercon-nected nodes (neurons). MLPs are widely used for supervised learning tasks such as classification and regression. The network typically consists of an input layer, one or more hidden layers, and an output layer. Each neuron in an MLP performs a weighted sum of its inputs, followed by the application of an activation function to produce an output. The weighted sum is calculated by multiplying each input by a corresponding weight and summing them up, usually with the addition of a bias term. The activation function introduces non-linearity into the network, enabling it to learn complex relationships in the data. To train the MLP model in this code, we implemented a MLP model using PyTorch, and optimized its hyperparameters with Optuna to minimize the mean squared error on the validation set. We set the number of layers (n layers) between 1 and 5. For each layer, the number of units (n units li) was set between 4 and 128, and dropout rates (dropout li) for each layer were set between 0.1 and 0.5. The learning rate (learning rate) was varied between 10ˆ -4 and 10ˆ -2, and the batch size (batch size) was chosen as one of 32, 64, or 128. 

AutoGluon is an open-source library for automated machine learning. AutoGluon supports a wide range of ML tasks, including classification, regression, and even object detection for computer vision applications. It is built on top of the deep learning framework Apache MXNet, providing scalability and efficiency for training models on large datasets. It automatically handles tasks such as feature selection, algorithm selection, hyperparameter tuning, and model ensembling. We ran either the “full” library (labeled as the AutoGluon model) or by restricting it to DL architectures (labeled as AutoGluon-DL) with a limit of 200 epochs. 

# 5.2 Meta-learning features 

Table 8 mostly adopted from [41] and contains 20 features computationally useful for meta-learning tasks. Table 8: The meta-feature vector representing a dataset. 

Name Description Source 

Row count The number of records (rows) in the dataset. [42] Column count The number of features (columns) in the dataset. [42] Columns after One Hot Encoding The number of columns after one-hot encoding categorical features. [42] Numerical Features The number of numerical features in the dataset. [42] Categorical Features The number of categorical features in the dataset. [42] Classification/Regression The type of task, whether classification or regression. [43] Cancor Canonical correlation for the best single combination of features. [43] Kurtosis The fourth standardized moment. [43] Average Entropy The average entropy of the features in the dataset. [44] Standard Deviation Entropy The standard deviation entropy of the features in the dataset. [44] Row Over Column The number of records divided by the number of features in the dataset. [49] Average Asymmetry Of Features The average value of Pearson’s asymmetry coefficient. [44] Average Pearson to Target Feature The average Pearson correlation score of all the features in the dataset and the target feature. [44] Standard Deviation Pearson To Target Feature The standard deviation of the Pearson correlation scores between all the features in the dataset and the target feature. [44] Average Correlation Between Features The average Pearson correlation score between all the features. [44] Average Coefficient of Variation The average value of the standard deviation divided by the mean of each feature for all the features in the dataset. [44] Standard Deviation Coefficient of Variation The standard deviation value of the standard deviation divided by the mean of each feature for all the features in the dataset. [44] Average Coefficient of Anomaly The average value of the mean divided by the standard deviation of each feature for all the features in the dataset. [44] Standard Deviation Coefficient of Anomaly The standard deviation value of the mean divided by the standard deviation of each feature for all the features in the dataset. [44] PCA The number of PCA components required to explain 99% of the vari-ance in the data. [50] 

# 5.3 Additional results 

In this section, we provide several additional results to further support the claims presented in this study. In particular, we provide the model’s performance in several contexts such as larger datasets, few dimensions, and others. Table 9 is equivalent to Table 5 but includes all 20 models (including those which are not TE or DL based). Tables 10 and 11 present the results for datasets with low dimensions (smaller than 10); Tables 12 and 13 present the results of medium-large datasets (over 10,000 samples); Tables 14 and 15 present the rankings for the regres-sion datasets based on RMSE; Tables 16 and 17 and Tables 18 and 19 present the rankings based on MAE and 

R2, respectively. Finally, the results for the classification datasets based on AUC, accuracy, and F1 scores are summarized in Tables 20, 21, 22, 23, 24, and 25, respectively. Figs. 4, 5, 6, and 7 present the critical difference diagrams based on additional metrics. Table 9: Performance ranking of all models for small datasets ( < 1000 ). Model Group # Best Average Rank Median Rank # in Top 3 Models H2O-GBM TE 6 5.6 5 12 ResNet DL 5 6.7 7 8CatBoost TE 4 5.1 5 14 AutoGluon-DL DL 4 6.8 7.5 10 MLP DL 4 6.3 6 9TPOT TE 3 5.1 4 15 Random Forest TE 3 6.1 5.5 7LightGBM TE 3 4.9 4 13 XGBoost TE 2 6.6 7.5 13 H2O-DL DL 1 9.6 11 2DCNV2 DL 1 9.1 10 2AdaBoost TE 0 8.9 8.5 2FT-Transformer DL 0 10.1 11 1TabNet DL 0 13.9 14 0Table 10: Performance ranking of TE and DL models for datasets with few dimensions ( < 10 ). Model Group # Best Average Rank Median Rank # in Top 3 Models Random Forest TE 5 6.7 6 7H2O-GBM TE 4 7.1 6 8MLP DL 4 6.3 6 9TPOT TE 3 5.2 5 13 LightGBM TE 3 4.9 4 12 CatBoost TE 3 5 5 11 ResNet DL 3 7.3 8 7AutoGluon DL 2 6.8 7 9XGBoost TE 1 6.6 7 10 AdaBoost TE 1 10.9 11 2H2O-DL DL 1 10.6 11 2DCNV2 DL 1 8.2 8 2FT-Transformer DL 0 10.7 11 1TabNet DL 0 12.4 13 0Table 11: Performance ranking of all models for datasets with few dimensions ( < 10 ). Model Group # Best Average Rank Median Rank # in Top 3 Models AutoGluon Other 11 5.3 5 13 ResNet DL 3 9 10 4H2O-GBM TE 3 9.3 8 5MLP DL 2 7.7 7.5 8LightGBM TE 2 6.4 5 9TPOT TE 1 6.9 7 10 XGBoost TE 1 8.1 8 6AutoGluon-DL DL 1 9.1 10 5Decision Tree Other 1 12.9 13.5 1gplearn Other 1 13.5 16 2DCNV2 DL 1 10.4 10 2KNN Other 1 11.8 13 5SVM Other 1 13.9 16 1CatBoost TE 1 6.4 6 10 Random Forest TE 1 8.4 7 6H2O-DL DL 0 13.5 15 1AdaBoost TE 0 13.2 15 1FT-Transformer DL 0 13.9 16 1TabNet DL 0 15.2 15 0LR Other 0 14.1 17 3Table 12: Performance ranking of TE and DL models for medium-large datasets ( > 10 , 000 ). Model Group # Best Average Rank Median Rank # in Top 3 Models LightGBM TE 8 4.3 3.5 19 CatBoost TE 8 4.3 3 20 AutoGluon-DL DL 5 6 6 10 XGBoost TE 3 5.5 5 13 Random Forest TE 3 6.5 7 13 H2O-GBM TE 3 7.4 7 8H2O-DL DL 2 8.1 8 3ResNet DL 2 7.5 8 6TPOT TE 1 6.4 6 11 MLP DL 1 8.9 9 3AdaBoost TE 0 11.6 12 1FT-Transformer DL 0 11.2 11 0TabNet DL 0 13.3 14 0DCNV2 DL 0 10.5 12 1Table 13: Performance ranking of all models for medium-large datasets ( > 10 , 000 ). Model Group # Best Average Rank Median Rank # in Top 3 Models AutoGluon Other 21 3.1 1 26 AutoGluon-DL DL 3 7.6 7 10 ResNet DL 2 9.6 10 3LightGBM TE 2 6.2 4 15 SVM Other 2 14.5 16 2TPOT TE 1 8 7 5XGBoost TE 1 6.9 6 7H2O-DL DL 1 10.7 10 3gplearn Other 1 16.3 18 2LR Other 1 13.2 14.5 3H2O TE 1 8.5 8 3KNN Other 0 11.7 12 3Decision Tree Other 0 12.9 14 0CatBoost TE 0 5.5 5 12 AdaBoost TE 0 14.5 16 0Random Forest TE 0 7.5 7 10 MLP DL 0 10 10.5 3FT-Transformer DL 0 13.5 13 0TabNet DL 0 17.4 17.5 0DCNV2 DL 0 12.8 14 1Table 14: Performance ranking of TE and DL models for regression datasets, ranked by RMSE score. Model Group # Best Average Rank Median Rank # in Top 3 Models CatBoost TE 15 3.7 3 31 Random Forest TE 9 5.5 5 19 LightGBM TE 7 4.1 3 29 AutoGluon-DL DL 6 7.3 7 12 TPOT TE 4 5.3 4 23 XGBoost TE 4 5.6 5 22 H2O-DL TE 4 6.4 6 10 ResNet DL 4 6.9 8 11 MLP DL 3 7 7 10 H2O-DL DL 1 9.8 10.5 2TPOT AdaBoost TE 0 11 12 2FT-Transformer DL 0 10.5 11 0TabNet DL 0 13.8 14 0DCNV2 DL 0 10.8 11.5 0Table 15: Performance ranking of all models for regression datasets, ranked by RMSE score. Model Group # Best Average Rank Median Rank # in Top 3 Models AutoGluon Other 30 3.5 1 38 SVM Other 4 12.9 14 6TPOT TE 3 7 5.5 11 CatBoost TE 3 5.2 5 26 H2O-GBM TE 3 8.3 8 7XGBoost TE 3 7.5 7 13 gplearn Other 2 15.5 18 3Random Forest TE 2 7.2 7 14 ResNet DL 2 8.9 10 3AutoGluon-DL DL 1 9.7 9 8H2O-DL DL 1 13 13.5 2LR Other 1 12.9 15 7KNN Other 1 11.1 12 8LightGBM TE 1 5.8 4 18 Decision Tree Other 0 13.4 14 0AdaBoost TE 0 13.8 14.5 0MLP DL 0 8.8 9 7FT-Transformer DL 0 13.7 14 0TabNet DL 0 18.2 19 0DCNV2 DL 0 13.7 14 0Table 16: Performance ranking of TE and DL models for regression datasets, ranked by MAE score. Model Group # Best Average Rank Median Rank # in Top 3 Models CatBoost TE 15 3.8 3 31 AutoGluon-DL DL 15 6.4 7 18 LightGBM TE 6 4.2 4 28 H2O-GBM TE 6 6.4 6 11 Random Forest TE 5 5.2 5 20 TPOT TE 4 5.4 5 22 XGBoost TE 3 5.8 6 16 H2O-DL DL 1 10.3 11 2ResNet DL 1 7.2 8 12 MLP DL 1 7.1 7 9AdaBoost TE 0 11.2 12 1FT-Transformer DL 0 10.4 11 1TabNet DL 0 13.8 14 0DCNV2 DL 0 10.6 11 0Table 17: Performance ranking of all models for regression datasets, ranked by MAE score. Model Group # Best Average Rank Median Rank # in Top 3 Models AutoGluon Other 25 3.6 2 39 CatBoost TE 7 5.3 4.5 25 AutoGluon-DL DL 6 8.1 8.5 16 TPOT TE 3 7.2 6 13 H2O-GBM TE 3 8.5 8 6SVM Other 3 12.6 15 5KNN Other 3 10.4 11 8LightGBM TE 2 6 5 17 gplearn Other 2 15 17.5 4ResNet DL 1 9.1 10 4XGBoost TE 1 7.9 8 8H2O-DL DL 1 13.5 14 1TabNet DL 0 17.9 19 0FT-Transformer DL 0 13.7 14 0MLP DL 0 9.4 9 5LR Other 0 13 15 6Decision Tree Other 0 13.2 14 1AdaBoost TE 0 14.6 15 0Random Forest TE 0 7 6 13 DCNV2 DL 0 14 14 0Table 18: Performance ranking of TE and DL models for regression datasets, ranked by R2 score. Model Group # Best Average Rank Median Rank # in Top 3 Models CatBoost TE 15 3.8 3 32 Random Forest TE 8 5.3 5 19 LightGBM TE 8 4 3 30 TPOT TE 6 5.4 4.5 19 H2O-GBM TE 6 6.2 6 11 AutoGluon-DL DL 5 7.1 7 12 MLP DL 4 6.8 7 14 XGBoost TE 3 5.7 5.5 22 H2O-DL DL 1 10.2 11 2ResNet DL 1 7 8 9AdaBoost TE 0 10.9 11 1FT-Transformer DL 0 10.6 11 0TabNet DL 0 13.9 14 0DCNV2 DL 0 10.9 12 0Table 19: Performance ranking of all models for regression datasets, ranked by R2 score. Model Group # Best Average Rank Median Rank # in Top 3 Models AutoGluon Other 31 3.3 1 40 TPOT TE 5 7.1 6 10 LightGBM TE 3 5.5 4 23 CatBoost TE 3 5.4 4 26 H2O-GBM TE 3 8.2 8 6SVM Other 3 12.6 14 6Random Forest TE 2 7.1 6 13 AutoGluon-DL DL 2 9.2 9 8XGBoost TE 2 7.6 7 12 H2O-DL DL 1 13.3 14 2MLP DL 1 8.6 8 8LR Other 1 13 15 6Decision Tree Other 0 13.1 14 0gplearn Other 0 16.2 18 2KNN Other 0 11.1 12 7ResNet DL 0 9.2 10 2AdaBoost TE 0 13.7 15 0FT-Transformer DL 0 13.9 14 0TabNet DL 0 18.4 19 0DCNV2 DL 0 13.8 14 0Table 20: Performance ranking of TE and DL models for classification datasets, ranked by AUC score. Model Group # Best Average Rank Median Rank # in Top 3 Models H2O-GBM TE 9 7.6 7 18 LightGBM TE 8 5.8 5.5 18 ResNet DL 6 8.1 8 12 H2O-DL DL 5 7.8 7 11 AutoGluon-DL DL 5 5.9 5 20 AdaBoost TE 4 9.2 10 7CatBoost TE 4 6.1 5 19 DCNV2 DL 4 7.2 7 12 TPOT TE 3 6.6 6 16 MLP DL 3 9 9 7Random Forest TE 2 8 8 9XGBoost TE 1 7.3 7 12 FT-Transformer DL 0 11.1 12 1TabNet DL 0 12.1 13 0Table 21: Performance ranking of all models for classification datasets, ranked by AUC score. Model Group # Best Average Rank Median Rank # in Top 3 Models AutoGluon Other 9 6.1 5 20 SVM Other 6 12 14 9ResNet DL 5 10.5 10.5 10 LightGBM TE 5 8 7.5 15 CatBoost TE 4 7.9 6.5 9AutoGluon-DL DL 4 7.8 7 13 DCNV2 DL 3 9.8 10 10 H2O-DL DL 3 9.9 10 9MLP DL 3 10.7 11 6H2O-GBM TE 3 8.9 8.5 11 TPOT TE 2 8.4 8 12 LR Other 2 10.7 10 9gplearn Other 1 14.4 16 4Decision Tree Other 1 13.3 15 3KNN Other 1 13.2 14 4AdaBoost TE 1 10.7 12 5Random Forest TE 1 9.9 10 6XGBoost TE 0 9.3 9 6FT-Transformer DL 0 14.2 15 1TabNet DL 0 15.9 18 0Table 22: Performance ranking of TE and DL models for classification datasets, ranked by accuracy score. Model Group # Best Average Rank Median Rank # in Top 3 Models LightGBM TE 11 5.9 5.5 19 H2O-GBM TE 10 7.6 7 21 AutoGluon-DL DL 6 6.4 6 15 DCNV2 DL 5 7.4 8 9Random Forest TE 4 7.4 6 15 TPOT TE 3 6.4 6 15 CatBoost TE 3 6 5 22 H2O-DL DL 3 8.6 8 7ResNet DL 3 7.6 8 12 XGBoost TE 2 7.4 7 9AdaBoost TE 2 8.6 10 9MLP DL 2 8.9 10 8FT-Transformer DL 0 11 11 1TabNet DL 0 12.5 13 0Table 23: Performance ranking of all models for classification datasets, ranked by accuracy score Model Group # Best Average Rank Median Rank # in Top 3 Models AutoGluon Other 13 5.7 5 27 LightGBM TE 6 7.9 7 15 SVM Other 5 11.6 12 6H2O-GBM TE 5 8.2 8 17 DCNV2 DL 4 10.1 10 8AutoGluon-DL DL 4 8.7 7 11 ResNet DL 2 9.3 9.5 7H2O-DL DL 2 10.7 11 4TPOT TE 2 8.4 7 11 CatBoost TE 2 7.7 6 9Random Forest TE 2 9 7 9LR Other 2 11.2 11.5 10 KNN Other 1 12.5 13 3XGBoost TE 1 9.7 9 6Decision Tree Other 1 13.6 15 3AdaBoost TE 1 10.9 13 7MLP DL 1 11.3 11.5 7gplearn Other 0 15 16 1FT-Transformer DL 0 13.9 15 1TabNet DL 0 16.3 18 0Table 24: Performance ranking of TE and DL models for classification datasets, ranked by F1 score. Model Group # Best Average Rank Median Rank # in Top 3 Models H2O-GBM TE 8 7.6 7 17 DCNV2 DL 8 6.9 7 13 LightGBM TE 7 5.7 6 17 AutoGluon-DL DL 7 6 4 21 H2O-DL DL 6 8 7 10 AdaBoost TE 4 9.1 10 10 CatBoost TE 4 6.2 5 22 TPOT TE 3 6.5 6 13 XGBoost TE 2 6.9 7 11 Random Forest TE 2 7.1 7 13 ResNet DL 2 9.3 9 7MLP DL 1 9.5 10 5FT-Transformer DL 0 11.1 12 1TabNet DL 0 11.7 13 2Table 25: Performance ranking of all models for classification datasets, ranked by F1 score. Model Group # Best Average Rank Median Rank # in Top 3 Models AutoGluon Other 11 5.6 5 23 DCNV2 DL 7 9.4 9 12 SVM Other 5 12 13.5 9AutoGluon-DL DL 4 7.8 6 16 AdaBoost TE 4 11.3 13 7H2O-DL DL 4 9.9 10 9LightGBM TE 3 8.1 7 12 H2O-GBM TE 3 8.8 9 11 gplearn Other 2 12.9 14 8TPOT TE 2 8.4 8 10 CatBoost TE 2 7.6 6 10 LR Other 2 10.7 10 9XGBoost TE 1 9.2 9 5KNN Other 1 12.8 13 4Random Forest TE 1 8.9 9 7ResNet DL 1 11.7 12 5MLP DL 1 12.3 13 4Decision Tree Other 0 14.3 15 0FT-Transformer DL 0 14.3 15 1TabNet DL 0 15.3 16.5 012345678910 11 12 

> 10.3333

AutoGluon 9.1053 

CatBoost 8.2632 

TPOT 8.0702 

Random Forest 7.5439 

XGBoost 7.0877 

AutoGluon-DL 7.0175 H2O-GBM  

> 4.5789

LR  

> 4.2807

SVM  

> 4.0351

Decision Tree  

> 4.0175

H2O-DL  

> 3.6667

AdaBoost Figure 4: Critical difference diagram for regression tasks based on MAE. The best performing model is AutoGluon as lower MAE scores indicate better performance. 12345678910 11 12 

> 9.0702

Decision Tree 9.0000 

H2O-DL 8.8947 

AdaBoost 8.7895 

SVM 8.5439 

LR 6.5789 

AutoGluon-DL 5.7895 H2O-GBM  

> 5.3509

XGBoost  

> 4.8947

Random Forest  

> 4.6667

TPOT  

> 3.9298

CatBoost  

> 2.4912

AutoGluon 

Figure 5: Critical difference diagram for regression tasks based on R2. The best performing model is AutoGluon as higher R2 scores indicate better performance. 12345678910 11 12 13 14 

> 11.0278

gplearn 9.9907 

Decision Tree 9.9537 

KNN 8.8889 

SVM 8.3704 

LR 8.0833 

AdaBoost 7.5370 

H2O-DL 6.6019 XGBoost  

> 6.4815

H2O-GBM  

> 6.1759

TPOT  

> 6.0463

CatBoost  

> 5.8981

LightGBM  

> 5.5185

AutoGluon-DL  

> 4.4259

AutoGluon 

Figure 6: Critical difference diagram for regression tasks based on AUC scores. The best performing model is AutoGluon as higher AUC scores indicate better performance. 

# 5.4 Computer resources 

We ran the experiments with 15 Google Colab sessions, using the high-RAM (51 GB) configuration with CPU. The total computing time including initial attempts and robustness tests is estimated at 4,000 hours. 12345678910 11 12 13 14 

> 10.8241

Decision Tree 10.0278 

gplearn 9.7130 

KNN 9.1481 

SVM 8.2593 

LR 8.2500 

AdaBoost 7.6111 

H2O-DL 6.3796 TPOT  

> 6.3704

H2O-GBM  

> 6.2870

XGBoost  

> 6.1389

LightGBM  

> 6.1019

CatBoost  

> 5.5000

AutoGluon-DL  

> 4.3889

AutoGluon Figure 7: Critical difference diagram for regression tasks based on F1 scores. The best performing model is AutoGluon as higher F1 scores indicate better performance.
