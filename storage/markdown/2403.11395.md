Title: 

URL Source: http://arxiv.org/pdf/2403.11395v2

Published Time: Fri, 22 Mar 2024 23:35:57 GMT

Markdown Content:
1

# Automated data processing and feature 

# engineering for deep learning and big data 

# applications: a survey 

## Alhassan Mumuni 1∗ and Fuseini Mumuni 2

Abstract —Modern approach to artificial intelligence (AI) aims to design algorithms that learn directly from data. This approach has 

achieved impressive results and has contributed significantly to the progress of AI, particularly in the sphere of supervised deep 

learning. It has also simplified the design of machine learning systems as the learning process is highly automated. However, not all 

data processing tasks in conventional deep learning pipelines have been automated. In most cases data has to be manually coll ected, 

preprocessed and further extended through data augmentation before they can be effective for training. Recently, special tech niques 

for automating these tasks have emerged. The automation of data processing tasks is driven by the need to utilize large volum es of 

complex, heterogeneous data for machine learning and big data applications. Today, end -to -end automated data processing systems 

based on automated machine learning (AutoML) techniques are capable of taking raw data and transforming them into useful feat ures 

for Big Data tasks by automating all intermediate processing stages. In this work, we present a thorough review of approaches for 

automating data processing tasks in deep learning pipelines, including automated data preprocessing –e.g., data cleaning, labeling, 

missing data imputation, and categorical data encoding –as well as data augmentation (including synthetic data generation using 

generative AI methods) and feature engineering –specifically, automated feature extraction, feature construction and feature selection. 

In addition to automating specific data processing tasks, we discuss the use of AutoML methods and tools to simultaneously op timize 

all stages of the machine learning pipeline. 

Index Terms —AutoML, automated data pre -processing, automated data processing, generative AI, automated feature engineering, 

automated machine learning. 

## ✦

## 1 INTRODUCTION 

1.1 Background 

Today, many problems are solved using machine learning 

(ML) methods. Deep learning approaches are generally 

preferred to traditional machine learning techniques for 

data -intensive tasks because of their ability to automati -

cally extract useful features from data and perform low -

level data processing. While deep learning methods have 

performed incredibly well, in the era of Big Data they have 

been shown to lack the power necessary to handle large 

volumes of complex heterogeneous data [1], [2]. Moreover, 

there are many pertinent issues (e.g., bias [3], the presence 

of anomalies [4], [5] and missing data points [6]) that 

require specific workarounds in deep learning pipelines. 

Consequently, additional steps are often needed to handle 

these complex data processing problems and to improve the 

predictive performance and reliability of machine learning 

models in big data applications. In big data analytics, data 

processing is primarily aimed at simplifying the representa -

tion of data to reveal meaningful patterns, interrelationships 

and important trends. This is important in decision support 

systems, where human decision making is enhanced by 

insights gained as a result of processing large quantities of 

> 1

Alhassan Mumuni: Department of Electrical and Electronics Engineer -

ing, Cape Coast Technical University, Cape Coast, Ghana. 

> ∗

Corresponding author, E -mail:  alhassan.mumuni@cctu.edu.gh 

> 2

Fuseini Mumuni: University of Mines and Technology, UMaT, Tarkwa, 

Ghana. E -mail:  fmumuni@umat.edu.gh 

relevant data. Figure 1 shows a simplified workflow of a 

typical big data processing system. 

The machine learning model design process itself is a

time -consuming process requiring extensive domain knowl -

edge. To train a model for image classification, for example, 

the developer would usually accomplish the task through 

the following steps: (1) Collect and fine -tune image data, (2) 

choose a suitable ML algorithm, (3) define acceptable pa -

rameter and hyperparameter settings, (4) train the selected 

model on the training data, (5) assess the performance of the 

resulting model on test data, and (6) repeat the procedure 

from steps 3 -6 until satisfactory performance is achieved. 

With this process, domain experts are needed to collect 

relevant data, carry out initial data preparation and perform 

additional processing and feature engineering to ensure 

that the resulting data is suitable for the specific machine 

learning task. 

The basic workflow of these processing steps is shown 

in Figure 2. In this paper, we explore various methods for 

automating data processing tasks for deep learning and big 

data applications. In the context of this survey, we broadly 

divide data processing tasks into three main subtasks: pre -

processing, data augmentation and feature engineering (i.e., 

feature processing) functions. 

1.2 Related works 

Approaches for automating data acquisition and processing 

functions have received significant attention recently. Inter -

est in these methods is largely driven by the need to leverage 

•

•2

IoT devices 

Geospatial data  Noisy raw 

data 

Stored data +

Business & 

industrial processes 

Social media 

Different sources of data 

Data 

acquisition 

Figure 1. Simplified representation of the basic components and workflow of a typical Big Data application. Typical Big Data applications rely on 

aggregating multimodal data from several different sources, and then applying suitable techniques to process these raw data to train deep learning 

models for downstream tasks. 

in application settings such as business, healthcare, security 

and law enforcement. Automated data processing systems 

provide a way for machine learning systems to process data 

in situations where privacy issues may restrict access of 

the raw data to human actors. Indeed, healthcare [7], [8] 

and business applications are already benefiting immensely 

from these techniques [9], [10], [11]. 

Figure 2. Basic workflow of data preparation and feature engineering in 

machine learning and Big Data application development. There are are 

a range of data preparation tasks (oval nodes) that may be performed 

before the data can be used for training. The arrows show the direction 

of of data processing steps. Note that, depending on the application 

requirements and the quality and quantity of the input data, some steps 

can be bypassed. 

the enormous amount of data available in various forms 

from diverse sources for machine learning and big data 

applications. At the same time, the complexity of machine 

learning problems has increased drastically while require -

ments have also become more stringent. Additional factors 

such as data privacy and ethical issues limit access to data 

Despite the importance and resurgence of automated 

data processing methods, only a small number of survey 

works have been authored on these methods. With the 

exception of a few works such as He et al. [12] and Zöller 

and Huber [13], who dedicate a small part of their sur -

vey to automated data processing methods, most of the 

published works [13], [14], [15], [16] are focused on ready -

to -use, end -to -end automated machine learning (AutoML) 

tools designed for general purpose applications or deal with 

the entire pipelines of custom AutoML implementations 

[7], [16] specific use -cases without particular focus on data 

processing. 

Strikingly, important aspects of low -level data process -

ing tasks, including preprocessing and feature engineer -

ing methods have not been discussed sufficiently in the 

literature. While there exists a large number of extensive 

surveys on data augmentation methods [17], [18], [19], till 

date, relatively few surveys [6], [20], [21], [22], [23] have 

been written on data preprocessing and feature engineering 

methods. However, the techniques described are mostly 

based on traditional methods. Our work is motivated by 

the scarcity of literature specifically focused on current 

approaches to automating data processing functions in deep 

learning models, especially data preprocessing and feature 

engineering tasks. 

Big data application                   

> oComputer vision
> oMachine translation
> oStock prediction
> oIntelligent diagnostics
> oDrug discovery
> oCredit default prediction
> oSpam detection
> oData visualization
> oData analytics

End application Processed data         

> Data preprocessing:
> oCleaning
> oLabeling
> oImputation
> oCategorical
> encoding
> Data Augmentation
> Feature Engineering:
> o Feature extraction
> oFeature construction
> oFeature selection

Data processing 

> Raw data
> Data
> augmentation
> Data pre -
> processing
> Feature
> engineering
> Model construction
> Algorithm selection
> Model training

Semantic 

information 

> Clean and rich data
> Machine Learning (ML) 3

Data preprocessing Data augmentation Feature engineering 

Initial data preparation aimed 

at enhancing the quality and 

transforming input data into 

suitable form for the target 

application 

Postprocessing procedures 

aimed at increasing the 

quantity and diversity of data 

to ensure representativeness 

Low -level processing tasks 

for creating and selecting 

low level features that 

effectively represent task -

relevant attributes 

Important data processing tasks in machine learning 

> Figure 3. Operational definition of the main data processing tasks discussed in this paper.
> Figure 4. Broad outline and structure of this paper.

1.3 Scope and outline of survey 

This work focuses on approaches for automating data pre -

processing and feature engineering tasks in deep learning 

pipelines. While several recent surveys [7], [12], [24] have 

discussed issues on the entire AutoML pipeline –data prepa -

ration, feature engineering model generation, algorithm se -

lection, performance evaluation and validation –our work is 

specifically focused on data preprocessing and feature engi -

neering functions within these pipelines. For completeness, 

we also discuss data augmentation methods. In this survey, 

these functions are collectively referred to as data processing 

functions. The main data processing tasks discussed in this 

survey are presented in Figure 3. 

The rest of the survey is organized as follows. Section 

2 provides an overview of the rationale, concepts and 

methods for automating data processing tasks. In Section 3, 

automated data preprocessing techniques are covered. Data 

augmentation is treated in Section 4. Section 5 covers feature 

engineering methods. In Section 6, we discuss end -to -end 

workflows for data processing in deep learning pipelines. 

The focus is on approaches to performing all the different 

data processing tasks, including data preprocessing, aug -4

mentation and feature engineering, simultaneously in an 

end -to -end manner using a single machine learning frame -

work. Section 7 summarizes the main features, applica -

tion domains and categories of generic end -to -end AutoML 

tools used for data processing. In Section 8, we present an 

overview of the implications of automated data processing 

techniques for industry and commerce, and provide an 

extensive discussion of pertinent issues and future prospects 

of automated data processing approaches in Section 9. We 

conclude in Section 10. Figure 4 (a) presents a broad outline 

of the this paper. 

## 2 AUTOMATED DATA PROCESSING IN MACHINE  

> LEARNING PIPELINES

2.1 Data processing in traditional machine learning 

pipelines 

The machine learning problem typically involves the fol -

lowing sequence of steps: data collection, data preparation 

and preprocessing, data augmentation, extraction of useful 

features from the data, construction of new features and 

pruning of the generated feature set (i.e., feature selection), 

selection of a suitable machine learning model, and opti -

mization of model hyperparameters. Traditional machine 

learning approaches [25], [26] require all of these stages of 

the machine learning process to be performed manually and 

as standalone processes (Figure 5). 

This approach presents a number of difficulties that 

make it extremely challenging to scale up machine learning 

models. For instance, traditional machine learning pipelines 

typically incorporate hand -crafted feature extractors to mine 

useful low -level features from the training data for subse -

quent processing by the machine learning model. These fea -

ture extractors are basically case -specific algorithms aimed 

at finding the best set of features for the target task. Feature 

extractors, once designed, are typically fixed – i.e., in the 

process of learning, they cannot be modified automatically 

to work differently in response to new conditions. To adjust 

their operation, the algorithm may have to be redesigned 

and deployed anew. 

2.2 Data processing in deep learning pipelines 

With modern deep learning models based on neural net -

work frameworks [27], [28], data processing tasks are 

usually implemented within the machine learning frame -

work and trained end -to -end (Figure 6). Deep learning ap -

proaches employ neural networks that are generally able to 

extract useful low -level information from raw data without 

the need for prior knowledge or additional processing. 

Information processing in deep learning pipelines roughly 

follows the information processing properties of the brains 

of biological systems, where different semantic features are 

extracted using a large and functionally diverse network 

of complex neural structures. Several hierarchical layers of 

artificial neural networks extract different types of features 

at different semantic levels. In the case of computer vision, 

for example, shallower layers extract useful information 

relating to more basic, low -level visual concepts such as 

lines, object contours and shapes. Deeper layers sequentially 

increase the abstraction level of extracted information, min -

ing higher and semantically meaningful information such as 

composite shapes and whole objects. 

Unlike traditional machine learning pipelines where 

fixed feature extractors are responsible for information pro -

cessing, in deep learning pipelines, during training, the 

synaptic elements that extract useful features use tunable 

parameters to adjust their operation on the fly based on 

feedback on the performance of the network on the target 

task. Thus, in the learning process the model automatically 

adjusts its way of feature extraction and low level data 

processing in response to performance results. 

2.3 The need for additional data processing in deep 

learning models 

2.3.1  The need for data preprocessing 

Despite the high degree of automation of data processing 

in deep learning models, their performance has been shown 

to be highly dependent on the quality of training data [29], 

[30], [31]. However, in many practical scenarios, the quali -

tative properties of raw data are not often consistent with 

the requirements of the target application or model [2], [32]. 

Consequently, data preprocessing has become an essential 

task in the deep learning application development process. 

Preprocessing includes initial data preparation tasks such 

as data normalization, cleaning (removal of outliers and 

inconsistent data points), encoding of categorical values, 

imputation of missing values and labeling. 

2.3.2  The need for data augmentation 

In many Big Data applications, the collected data may 

not be sufficient or representative enough for the target 

task. In such situations, after initial preprocessing, data 

augmentation is used to increase the quantity and variability 

of the training data by generating new samples through 

manippulating the existing data. This is the most popular 

approach to enhancing the generalization power of deep 

learning models. Data augmentation has also been used to 

addressed problems such as bias [33], imbalanced data [34] 

and domain shift [35]. 

2.3.3  The need for feature engineering 

While machine learning models constructed on the basis 

of deep neural architectures can automatically learn useful 

features [36], some application settings still required explicit 

feature processing to guarantee satisfactory performance. 

This is particularly so for complex and data -scarce appli -

cations such as fault diagnosis in machinery (e.g., in [37]), 

online fraud detection (e.g., [38]), credit risk evaluation 

[39] and industrial applications [40]. Indeed, it has been 

shown in [41] that employing dedicated feature extractors 

in deep learning networks provides more useful feature 

representations of the input data that results in significant 

performance improvements. 

Also, for big data applications, the data is generally very 

large, heterogeneous, complex and noisy, leading to the 

need to handle very large volumes of features, often with 

redundant and duplicated samples. This can be harmful to 

performance [3], [42]. This problem can be mitigated by 

feature engineering methods, which aim to find compact 5

Data processing module within deep learning pipeline 

Output 

Data sources  Hyperparameter 

tuning 

Feature 

Engineering 

Model 

training 

Data 

augmentation 

(postprocessing) 

Data 

preprocessing     

> Raw data
> Data processing: preprocessing, data
> augmentation & feature engineering
> Processed data Training traditional machine learning model
> Output
> Unusable raw data Standalone, offline processes Usable training data Model training
> Figure 5. Data processing in traditional machine learning. All processing steps are implemented manually as standalone tasks.

Input data Deep Learining model 

Raw training data Intgrated preprocessing, data augmentation, feature engineering and model training 

> Figure 6. Data processing in deep learning: All processing tasks –preprocessing, data augmentation and feature engineering functions –as well as
> model generation and training are realized as a unified process.

feature representations for the data. Another common short -

coming of features naturally learned in the natural process 

of training deep neural networks is that they are usually 

not interpretable. This can be a major limitation in mission -

critical application settings such as medical domains [43]. 

Feature engineering is also useful for tasks where training 

data is insufficient to adequately encode the desired infor -

mation [44]. 

2.4 Implementation of data processing functions in 

deep learning pipelines 

Unlike in shallow learning where data processing is per -

formed offline as a standalone process, modern deep learn -

ing favors an end -to -end learning paradigm that aims at 

realizing all functions in an integrated manner using a

single model. This means in deep learning, additional data 

processing, when needed, is usually integrated into the 

machine learning process and implemented as a unified 

process (Figure 6). The fact that this process is carried out 

online means that no additional samples are stored. This 

has the advantage of increasing the volume and diversity 

of training data without a corresponding increase in the 

amount of storage requirements. Instead of applying explic -

itly designed transformation operations to create additional 

data, many new works (e.g., [45], [46], [47], [48], [49], [50]) 

have devised special neural network architectures to learn 

the relevant transformations directly from the training data. 

In this case, the data augmentation tasks become an integral 

part of the overall learning process. 

2.5 General approaches to automating data processing 

and feature engineering tasks 

A wide range of techniques are used to automate data pro -

cessing tasks. These techniques have different strengths and 

limitations related to complexity, flexibility and predictive 

performance. While the crudest approaches to automating 

data processing tasks involve using hand -crafted routines, 

more advanced methods [51], [52], [53] utilize learned mech -

anisms to perform processing. The most advanced methods, 

however, employ automated machine learning (AutoML) 

techniques [12], [54], [55], [56] to automate data processing 

functions. These methods can perform all the underlying 

data preprocessing tasks end -to -end without delegating in -

termediate tasks to human developers. 

> Model
> Parameter tuning 6

∈ 

> Figure 7. General representation of AutoML pipeline. It incorporates deep learning and traditional machine learning models in its design. In the
> training process, optimization algorithms select the best model structure and hyperparameter setting for the specific task. T he original illustration,
> from [12], has been modified slightly to reflect the context of the current paper.

A typical AutoML pipeline for end -to -end data process -

ing is shown in Figure 7. The first part of the pipeline is 

concerned with data processing tasks. These include pre -

processing, data augmentation and feature engineering. The 

second part of the AutoML pipeline performs functions such 

as model construction, hyperparameter setting, algorithm 

selection and result evaluation and validation. Additional 

functions such as result visualization (e.g., in [57], [58]) may 

also be automated. In some cases (e.g., [59]), the automated 

machine learning problem involves composing an optimum 

model architecture to solve the machine learning task. 

Approaches for automating model structure construction 

belong to a class of automated machine learning methods 

called neural architecture search or NAS [60]. We describe 

the main features and functions of generic automated ma -

chine learning systems in Section 7. 

## 3 AUTOMATED DATA PREPROCESSING 

The concept of data preprocessing 

Data preprocessing functions consist of a set of basic 

operations that transform raw data into a form that is useful 

for the machine learning model. Important preprocessing 

subtasks include data cleaning [61], [62], labeling or re -

labeling [53], [63], [63], [64], categorical encoding [65], [66], 

[67] and imputation of missing data [68], [69], [70]. Figure 8 

depicts the main categories of data preprocessing tasks and 

the set of common problems they commonly tackle. 

Automated preprocessing 

Given a dataset D, the automated preprocessing prob -

lem can be defined as a task involving the automatic selec -

tion and application of a set of basic preprocessing opera -

tions Pi S : i = 1 , 2, 3, ..., n on D such that the predictive 

performance on a target task is maximized. This is a chal -

lenging task because, besides determining which transfor -

mations are appropriate and to what extent to transform the 

data, the ML algorithm needs to also apply the operations 

in the correct order [71]. The most advanced automated 

preprocessing methods [72] typically take as input a target 

dataset and a set of primitive transformation operations and 

their corresponding hypeparameters –parameters that define 

certain details about the model, including the strength of 

transformations and the order of their application. Opti -

mization algorithms such as reinforcement learning (RL), 

gradien descent (GD), Bayesian optimization (BO) or evolu -

tionary computational algorithms (ECA) are then employed 

to searche for the best combination of basic preprocessing 

operations. 

Aproaches to automated preprocessing 

Data preprocessing can be automated to varying de -

grees. The degree of automation depends on how pre -

processing operations and the underlying models are im -

plemented. The most basic automation still requires the 

use of traditional, hard -coded techniques while the most 

advanced methods utilize AutoML techniques. The general 

approaches for automating data processing tasks are sum -

marized in Table 2. 

Generally, all automated preprocessing approaches use 

some form of machine learning to define and/or select an 

ensemble of data preprocessing operators an/or deep learn -

ing pipelines from a set of possible options that maximize 

performance. We discuss in detail specific approaches for 

automating some of the most important preprocessing tasks 

in the next subsections. 

One major difficulty with data preprocessing is that 

not only are the processing methods dependent on data 

type, they are also highly sensitive to the machine learning 

model [73]. This means for different models and tasks, the 

application of the same preprocessing operations on the 

same type of input data can often lead to vastly different 

outcomes. Applying all possible preprocessing operations 

irrespective of model or data type is a highly combinato -

rial problem and impractical to carry out in applications 

dealing with very large data. Consequently, many fully -

automated preprocessing methods rely on first identifying 

the input data and model types and applying context -

specific preprocessing operations. Unfortunately, in some 

situations it is not often possible to determine beforehand           

> Feature Engineering Model Generation Model Estimation
> Feature
> Selection
> Search Space Optimization Methods
> Low -fidelity
> Traditional
> Models
> (SVM, KNN)
> Hyperparameter
> Optimization Early -stopping
> Feature
> Extraction Features
> Surrogate Model
> Feature
> Construction
> Deep Neural
> Networks
> (CNN, RNN)
> Architecture
> Optimization
> Weight -sharing
> Neural Architecture Search (NAS)
> Data Preparation
> Data
> Collection
> Data
> Peprocessing
> Data
> Augmentation 7

what operations could be effective for particular models or 

data types. To solve this problem, approaches (e.g., [74]) 

have been proposed to jointly estimate the performance of 

primitive preprocessing operations so as to apply them in a 

data - and model -dependent way. 

Large -scale AutoML frameworks such as AutoGluon -

Tabular [75] incorporate a two -level preprocessing scheme, 

with the first level implementing model -agnostic prepro -

cessing operations while the second one generally focuses 

on model -specific preprocessing. These AutoML models 

typically combine many different optimization algorithms 

and pipelines with different structures that define data 

preprocessing blocks and machine learning model blocks 

within a single framework. The composite nature of this 

class of models makes generic AutoML tools flexible and 

multipurpose, and allows them to solve a wide range of 

problems. 

3.1 Automated data imputation 

Handling missing data is a common task that is encountered 

when developing machine learning models [76]. The prob -

lem of missing data arises from various causes associated 

with data acquisition, including lost data entries, inability 

to collect certain data, or from situations where some data 

is simply not accessible for various reasons such as pri -

vacy concerns. Missing data can also result from incorrect 

measurements and human errors, especially in data sys -

tems relying on manual entry. Traditional data imputation 

methods (e.g., [77], [78]) rely on analytically formulated 

statistical techniques. The methods first identify missing 

data using knowledge about the statistical properties of 

the observed data. Missing values are then computed and 

incorporated into the training set. In contrast, automated 

imputation techniques [79], [80] discover and correct issues 

with missing data without encoding explicit mechanisms for 

doing so. 

Generative modeling approaches have emerged as an 

important class of methods for automated missing data 

imputation (e.g., in [51], [81], [82], [83], [84], [85], [86]). These 

methods use learned mechanisms to encode the correct dis -

tribution of "normal" data. The learned knowledge about the 

underlying data distribution can then be used to help detect 

instances of missing data and perform imputation in an 

end -to -end manner. Yoon et al., for instance, propose a so -

called generative adversarial imputation network (GAIN) 

[81] that utilizes a generative adversarial network (GAN) to 

model data distribution for identification and imputation 

of missing data. Gondara and Wang [82] perform data 

imputation with the the help of denoising autoencoders. 

Generative techniques that utilize variational autoncoders 

are also common (e.g., [52], [87], [88]). 

More recently, methods based on AutoML techniques 

[62], [68], [79], [89] have emerged as the most important 

class of methods for automating data imputation. Teague 

[68], for example, develops a tool based on AutoML tech -

nique to automatically impute missing tabular data. Hy -

perimpute [89], implements missing data imputation by 

utilizing an AutoML framework that optimizes multiple 

candidate machine learning models using different search 

algorithms. Its basic structure is shown in Figure 9. The 

approach can realize a more generalized and adaptive im -

putation process than methods that rely on optimizing a

single machine learning model using one search technique. 

In addition to these dedicated data imputation models, 

most popular automated machine learning tools such as 

AutoSklearn [90], Scikit -learn [91] and Azure Databricks 

AutoML [92] aim to, among other things, perform automatic 

imputation in an end -to -end manner. 

It has been recognized that fully automated methods of 

data imputation can be detrimental to performance in some 

situations [93]. While machine learning techniques can gen -

erally detect situations of missing data points, determining 

the correct data input can sometimes require some form of 

high -level understanding of the underlying context which 

machine learning models often lack. To address this limi -

tation, some authors adopt human -in -the -loop approaches 

(e.g., [80], [94], [95]). Bilal et al. in [80], for example, imple -

ment a mechanism to automatically detect missing data and 

prompts the user, through a data visualization interface, to 

accept or reject suggested candidate data modifications. 

3.2 Categorical data encoding 

Categorical data encoding is aimed at converting categorical 

data into numerical information. This pre -processing step 

is often necessary because deep learning models typically 

work well with numerical data but are unable to handle 

categorical data. However, many real -world datasets often 

include categorical variables. Traditional approaches to con -

verting categorical variables into numerical representations 

rely on techniques such as binary encoding, ordinal encod -

ing, label encoding, one -hot encoding and target encoding 

[65]. 

Currently, it is challenging to completely automate entire 

process of categorical data encoding. Unlike pre -processing 

functions such as data cleaning and imputation, it is dif -

ficult to define primitive operations that can be applied 

by machine learning models to convert between categor -

ical and numerical data. Because of this limitation, many 

approaches, including state -of -the -art AutoML models such 

as TPOT [96] and Autokeras [97] currently implement en -

coding manually. Some AutoML -based categorical encod -

ing methods [54], [98], [99] employ traditional encoding 

schemes within deep learning networks. The approach in 

[54] incorporates custom implementations of traditional cat -

egorical data encoding and other pre -processing functions 

within a generic AutoML framework. It employs a tree -

based ensemble technique to evaluate and select the en -

coded elements. Similarly, H2O AutoML framework [100], 

implements categorical data encoding using OneHotEncod -

ing library –an implementation of the traditional one -hot 

encoding method. 

Modern AutoML tools that attempt to automate this 

pre -processing step (e.g., LightAutoML [101], AutoGluon -

Tabular [75], Auto -sklearn [90] and Pharm -AutoML) mostly 

require some form of human supervision. For example, 

Auto -sklearn includes a so -called label encoder - a categori -

cal data to integer conversion algorithm, but it requires user 

input to define the underlying categories before performing 

the necessary operations. 

While many approaches rely on using traditional tech -

niques within automated learning frameworks to perform 8

DATA IMPUTATION  CATEGORICAL DATA 

ENCODING 

• Need to convert ordinal 

data to numerical 

• Need to convert nominal 

data to numerical 

• Need to weight data or 

attributes 

• Need to ensure 

compatibility of data 

across different systems 

Invalid data 

• Ordering errors 

• Noisy or corrupted data 

• Redundant data 

• Irrelevant data 

• Wrong data types or 

categories 

• Inconsistent formats 

• Overall low quality data 

• Unlabeled data 

• Tagging errors (for crowd -

sourced data) 

• Incorrect labels 

• Incorrect category 

nomenclature 

• Typographical errors 

in labels 

• Domain shift 

• Missing data points 

• Missing attributes 

• Incomplete data 

• Missing values 

• Missing keys 

• Missing meta data 

• Incomplete metadata 

• Insufficient data 

• Balance imbalanced data  

> Figure 8. Data preprocessing tasks and the set of common problems they typically tackle.
> Figure 9. Basic architecture and workflow of HyperImpute, an automated data imputation method proposed in [89]. The approach incorporated
> different ML models and optimization techniques and uses to find an optimal imputation pipeline.

categorical to numerical data conversion, some recent works 

suggest encoding categorical data with the help of learned 

word embeddings. The authors in [102], for instance, pro -

pose a word embedding method based on deep neural 

network model to learn useful features for encoding cate -

gorical data. BERT -Sort [99] employs a learned embedding 

approach that exploits semantic relationships among ordi -

nal data elements to perform categorical encoding. The basic 

idea is to learn the context of textual information with the 

help of a pre -trained language model. This is then used 

to encode input data according to the high -level semantic 

context of the underlying text. 

3.3 Automated data cleaning 

Data cleaning [103] is aimed at correcting errors in a dataset 

or eliminating nosy data. The main sources of these errors 

include ordering and indexing mistakes, incorrect class as -

signments and inconsistent naming. In machine learning do -

mains such as natural language processing, common errors 

include typographical, grammatical and syntax errors. Ran -

dom errors and anomalies can also have significant impact 

on the performance of time -series prediction. Duplicate or 

redundant and irrelevant data may also be present in the 

training dataset and may need to be removed to preserve the 

fidelity of the data. Data cleaning may also involve remov -

ing incomplete –i.e., absence of some necessary attribute(s) 

required for the target task –incorrect or invalid (e.g., wrong 

data types) samples. Operations such as normalization, zero 

centering and scaling are additionally used to ensure that 

the statistical characteristics of the data are suitable for use 

in the deep learning model. 

Basic data cleaning automation can be realized by im -

plementing high -level wrapper functions around low -level 

data cleaning routines. CleanTS [62], for instance, develops 

a set of high -level abstractions and functions at a semantic 

level on top of a wide -ranging suit of low -level library 

functions to automate data cleaning tasks in time -series 

applications. 

A recent trend in data cleaning is to use machine learning 

approach as a way to identify discrepancies and errors and 

apply appropriate corrections. Learning -based data cleaning 

methods (e.g., [95], [104], [105], [106]) optimize parameters 

associated with data cleaning operations based on the per -

formance the underlying network on downstream tasks. 

Learn2Clean [104], for example, employs a Q -learning algo -

rithm to learn the best data cleaning operations. BoostClean 

[105] relies on a learning approach that utilizes gradient 

boosting to automate the data cleaning process. 

Meta -learning techniques (e.g., [107], [108]) have been 

employed to extend the scope and enhance the adaptability 

of data cleaning systems. This allows them to be applied as 

generic data cleaning models to process a wide variety of 

datasets for many different machine learning tasks. End -to -

DATA CLEANING  DATA LABELING &

RE -LABELING 9

end AutomML -based data cleaning pipelines [62], [98] have 

been very proposed as stand -alone tools for generic cleaning 

purposes. For instance, Shende et al. in [62] develop a

generic time series data cleaning method and accompanying 

toolset that tackles problems with outliers, duplicates, as 

well as inconsistent data types and formats. 

3.4 Automated data labeling 

Labeling, or annotation, is an important task in data prepa -

ration and in supervised machine learning in general. It is 

arguably the most tedious job in many machine learning 

tasks. Unfortunately, in many domains, it is also one of the 

most difficult processes to automate. At present, most anno -

tations are generated either manually or semi -automatically, 

that is, using algorithms to generate possible label proposals 

which are subsequently refined and validated by human 

agents (e.g., in [109], [110], [111], [112], [113]). Only a few 

works in computer vision [114], [115] and medical image 

analysis [110], [116] domains have attempted carrying out 

the labeling process in a fully automatic manner. 

Automated labeling techniques in computer vision and 

medical image analysis applications [110], [112], [117] typ -

ically employ classification, object object or semantic seg -

mentation detection models to first identify instances before 

generating labels for them. Ince et al. [112] employ a stan -

dard object detector model as the basis for annotations but 

requires user validation of the generated labels. Zhang et 

al. [117] use metric learning approach to match unlabeled 

images with existing category information and then asso -

ciate the unlabeled images with labels of relevant image 

categories. The basic principle of the approach is illustrated 

in Figure 10. 

Another common method is to exploit the rich textual 

descriptions naturally associated with certain types of data 

to obtain annotations for unlabeled samples. This is par -

ticularly common in medical imaging domains [110], [118], 

where images are often associated with pathology reports 

that provide relevant descriptions of the corresponding im -

age content that can be leveraged though training. Zhang 

et al. [110], for instance, proposed an automatic annotation 

mechanism for thyroid nodules in ultrasound scans using 

a natural language processing (NLP) model in combination 

with fixed rules (ontologies) to extract nodule information 

from medical pathology reports. 

Even though a variety of machine -guided automated 

labeling [12], [53], [119], [120], [121] techniques have been 

proposed, they are generally limited to simple labeling tasks 

targeting very narrow application settings. It is currently 

challenging to extend them to more complex or general 

tasks. The main difficulties include the need for context 

awareness [115], [122], lexical complexity [118], semantic 

ambiguity [123] and label subjectivity [124]. 

3.5 End -to -end automated data pre -processing 

While a number of pre -processing methods are designed to 

specifically perform a single preprocessing task (e.g., in [84], 

[89]), many works (e.g., [61], [61], [80], [126]) tend to simul -

taneously implement multiple pre -processing functions. For 

example, Auto -Prep [80]) performs automatic missing data 

imputation, data type detection and deletion of duplicates, 

categorical data encoding as well as feature scaling. AutoDC 

[61] performs outlier detection and elimination, label correc -

tion, edge case selection as well as data augmentation. 

As we have already mentioned, most of the pre -

processing methods based on AutoML frameworks gener -

ally aim to automate the entire machine learning pipeline, 

including data processing as well as model structure 

construction algorithm selection functions. Because of 

their generic focus, they are able to handle only ba -

sic pre -processing tasks automatically. More complex pre -

processing steps often require human guidance to accom -

plish. Models such TPOT and Autokeras, for example, 

perform only missing data detection. While human -in -the -

machine -learning -loop has been suggested [80], [94] as a

viable solution, other researchers [129] suggest performing 

human and machine learning pre -processing independently 

and combining the results for better and reliable perfor -

mance. 

Some new approaches such as AutoData [53], DataAssist 

[125], BioAutoMATED [127], Atlantic [54] and DiffPrep [126] 

are designed as dedicated pre -processing plug -ins that can 

be integrated into standard AutoML frameworks. In this 

way, they can leverage the hyperparameter and pipeline 

optimization capabilities of large -scale AutoML models to 

optimize pre -processing functions. These models are gen -

erally aimed at addressing the limitations of generic meth -

ods and tools by developing specialized modules that can 

be used to perform specific data preparation tasks within 

general -purpose AutoML pipelines. AutoData [53] (Figure 

11), for instance, implements a dedicated data processing 

module that can be used within standard AutoML frame -

works as an end -to -end mechanism for performing many 

data pre -processing and and augmentation tasks, including 

data acquisition, labeling, cleaning and augmentation. It 

uses reinforcement learning method to search for suitable 

data from diverse external sources. It first takes a user -

defined input specifying the required data type and task, 

as well as the overall performance objective. Given this 

input, the reinforcement learning algorithm searches for 

the appropriate data from external repositories and feeds 

the results to an AutoML framework (e.g., Auto -WEKA or 

Auto -Keras) which then constructs a model and evaluates 

the performance on the given task and provide feedback 

for refining the search results. Table 2 the mail approaches 

for automating various pre -processing tasks. These reflect 

different degrees of automation –from the most basic (i.e., 

manual) to fully automated pre -processing (based on Au -

toML). 

3.6 Performance of automated data preprocessing 

methods 

Despite the proven effectiveness and the proliferation of 

data preprocessing methods, there is a general lack of 

quantitative performance results for different data prepro -

cessing methods. The scarcity of comparative performance 

data in the literature can be attributed to several factors. 

Firstly, data preprocessing methods often vary widely in 

their implementation and application; there are hardly any 

standard techniques for performing specific preprocessing 

tasks. In particular, there are no standard algorithms for 10 

modeling 

CO2 Emission 

(Kaggle) 

(regression) 

data (genomics) 

learned (RL) 

(greedy search) 

Figure 10. Automatic image annotation by the Zhang et al. [117]: The model first extracts and combines both hand -crafted and deeply learned visual 

features for both sets of images (i.e., labeled categories and unlabled images). Metric learning is then used to estimate the similarity of unlabeled 

images and the known (i.e., labeled) categories. Next, the labels of relevant images from the labeled categories are transfer red to the unlabeled 

instances based on their similarity scores. Information about the labeled categories is also used to filter noisy input samples to provide clean images 

for labeling. 

Table 1 

Summary of important data pre -processing methods. We capture the main pre -processing functions (imputation, cleaning, labeling, and 

categorical encoding) each of the techniques performs and supported the data types. We also indicate the main machine learnin g tasks, the 

classsification of the approaches and the datasets used. 

Work Data type/  Main pre -processing functions Approach  Main datasets 

Task  Data 

Imput. 

Data 

Clean. 

Data 

Label. 

Categ. 

encod. 

to automation 

scIGANs [51] Tabular (genomics) √ Generative  PMBC 10k, 

Synthetic 

Tabular 

(classification) 

Tabular, insurance 

Generative 

modeling 

√ √ √ √ Mostly 

UCI Repository 

Ames Housing 

Automunge [68] 

Hyperimpute [89] 

claims, housing 

price (regression) √ analytical  Allstate Claims 

Tabular (classification)  AutoML UCI 

MPII, 

Image 

(keypoint localization) 

√ Deep learned  CUB -200 -2011, 

ATRW, MS -COCO, 

AnimalPose 

cleanTS [62] Tabular (time series) √ V √ AutoML Power, Temperature, 

GP -VAE [88] 

Image, digitized 

signals (binary 

format) 

Generative 

modeling (VAE) 

Physionet, 

Healing MNIST, 

SPRITES 

DataAssist [125] Tabular √ √ √ √ Deep learned Various 

Tabular 

(classification) 

DiffPrep [126] Tabular 

Tabular 

Generative 

modeling 

AutoML 

(gradient -based) 

√

UCI 

OpenML 

Various 

BERT -Sort [99]  (regression, 

classification) 

Deep learned  (UCI, Kaggle) 

Glycan, Peptide 

BioAutoMATED [127] Sequence -based 

Text, tabular; 

√ √ AutoML 

√ √ Deep learned 

sequencies, 

Ribosome 

binding 

TextCLS, 

Rotom [108]  Entity matching, 

NLP  (meta -learning)  EDT, EM 

Learn2clean [104] Text, multimedia (web data) √ √ Deep 

AlphaClean [128] Tabular √ AutoML 

House Prices, 

Google Play Store 

Apps, Users 

LAQ, Physician, 

Hospital 

BoostClean [105] 

Image, text 

(from the web) 

Tabular 

(classification) 

Deep learned 

(RL) 

√ √ Deep learned 

Custom (web data) 

Various (UCI, 

Kaggle, USCensus, 

NFL, Titanic, etc). 

AutoData [53] 

Images from same category 

Noisy image dataset 

Releveat images 

Tower 

Bridge 

Tree 

River 

Bridge 

Tower 

Noisy image 

filtering 

.. 

.

> Sky
> Bridge
> Tree
> R

.i.v.er Image distance 

metric learning 

Category 

labels 

Sky 

Jet 

Building 

Tag propagation 

Image feature 

extraction 

Input image 

Predicted tags 

MIWAE [87]  √

PLACL [63] 

√

Gain [81]  √

√ √ √

√ √11 

Figure 11. Data Labeling by LADA: The LADA framework proposed in [130] acquires unlabeled training data from the real world. An augmentation 

policy is then used to carry out initial preprocessing and generate additional data by learning relevant transformation opera tions. With the aid of a 

classification network, the optimum policy is determined using the classification loss. Finally, an oracle is used to lablel the unlabeled raw data. 

Table 2 

General approaches for automating data preprocessing tasks. The approaches vary in sophistication, from basic to fully automa ted methods 

based on AutoML pipelines.               

> Method Description and main characteristics Example works
> Standalone preprocessing, usually manual or with the aid of generic
> Basic data processing tools (e.g., image processing libraries for manipulating
> images). This approach is largely manual.
> [131], [132]
> Automated processing within deep learning pipelines using explicitly
> formulated analytical relations. [45], [49], [133]
> DL Deeply learned processing modules within machine learning pipelines. [51], [83], [130]
> End -to -end processing, model selection and hypaprameter tuning using
> AutoML.
> [53], [61], [126],
> [54], [68], [134]

common tasks such as data cleaning, labeling and imputa -

tion. Because of the highly varied implementation details, it 

is difficult to compare performance across different studies. 

The second major difficulty is that data preprocessing 

is rarely accomplished by a single algorithmic operation. 

Each of the preprocessing procedures (i.e., data imputation, 

labeling, cleaning, etc.) often involves a set of multiple 

sequential or parallel processing operations. Comparing dif -

ferent methods requires defining consistent pipelines, which 

can be very complex given the many possible combinations 

of preprocessing procedures. 

Another important challenge is that most data prepro -

cessing procedures involve subjective considerations. For 

example, determining outliers or selecting which data el -

ements to retain can be influenced by domain knowledge 

and researcher bias. This subjectivity also makes it difficult 

to obtain fair and objective comparisons. 

## 4 AUTOMATED DATA AUGMENTATION 

Data augmentation involves creating variations of the orig -

inal training data by applying context -appropriate transfor -

mation operations. For example, in computer vision, these 

may involve image scaling, shearing, rotation, reflection and 

blurring operations. Text augmentation methods include 

synonym replacement, random insertion and deletion. The 

creation of such variations lead to a more representative 

data for the given task, and enhances the machine learning 

model’s ability to generalize well on unseen data. Figure 12 

summarizes the common data augmentation operations for 

common machine learning tasks. Typically, for a particular 

task, data augmentation may involve applying all of these 

basic operations together with more advanced techniques. 

As explained earlier, in most cases data augmentation is 

achieved by manipulating existing data in such a way as 

to create adequate variability. Most state -of -the -art methods 

for automated data augmentation rely on AutoML tech -

niques.There are, however, practical situations where train -

ing data does not exist in any form [135], and augmentation 

methods are required to synthesize novel data from scratch. 

Generative artificial intelligence techniques have excelled in 

this area. 

Presently, for data manipulation, automation has largely 

been successful in computer vision and image understand -

ing domains. It has been particularly challenging to ex -

tend these methods to time series forecasting and natural 

language processing domains owing to the difficulty in 

achieving semantic -preserving transformations without hu -

man supervision. Only a very few works (e.g., [136], [137]) 

have reported successful application of automated data 

augmentation techniques in natural language processing 

domains. Recently approaches for generative AI techniques 

have achieved remarkable results in text generation. 

> Analytical
> AutoML 12

Mac hine lea rn ing TaSk Ba Sic da ta augmentati on meth odS

Image processing and 

computer vision (image 

classification, object 

detection, visual tracking, 

e.tc.)                   

> •Rotation :Rotating images by various angles to simulate different viewpoints.
> •Flipping: Horizontally flipping images to create mirror images
> •Cropping : Randomly cropping images to focus on different parts of the scene
> •Zooming : Applying random zoom to create variations in image scale
> •Color JiMering : Modifying the brightness, contrast, saturation, and hue of images
> •Gaussian Noise: Adding random Gaussian noise to images to simulate real -world variations.
> •Blurring: Applying Gaussian blur or other blurring techniques to images

Text recognition & language  • Synonym Replacement : Replacing words with their synonyms to create variations. 

understanding (machine 

translation, text -to -voice 

conversion, etc.)                

> •Random Deletion: Randomly removing words to change sentence length and structure
> •Random Insertion: Randomly inserting additional words into a sentence to simulate natural language variation
> •Text Paraphrasing: Rephrasing sentences while preserving the original meaning
> •Character -Level Perturbations: Swapping, inserting, or deleting characters to simulate typographical errors or
> variations in spelling
> •Contextualized Word Embedding: Replacing words with alternatives that are contextually relevant

Time series forecasting 

(e.g., stock price prediction, 

economic performance 

forecasting, sales forecasting, 

etc.)                

> •Dynamic Time Warping: Stretching or compressing the time axis of a time series to induce variations in the
> temporal scale of the data
> •Window Slicing: Dividing the original time series into overlapping or non -overlapping windows of varying
> lengths can provide the model with multiple perspectives of the data
> •Noise Addition : Introducing random noise to the time series data
> •Data Interpolation : Filling missing data points using interpolation techniques
> •Outlier Injection : Introducing outliers or anomalies into the time series data can help the model learn how to
> identify and handle unexpected events.
> •Time Shiƒing: ShiGing the entire time series or specific portions of it forward or backward in time to create new
> instances with shiGed patterns.

Figure 12. Common data augmentation functions for various machine learning tasks. Note that this list is by no means complete .

4.1 Automated data augmentation in AutoML pipelines 

AutoML has emerged as a powerful augmentation approach 

for large -scale machine learning and Big Data applica -

tions. AutoML -based data augmentation methods generally 

aimed to automate the process of creating and applying ap -

propriate transformation operations on the training data.In 

addition to data augmentation, the AutoML models perform 

a number of additional tasks, such as machine learning 

model development (e.g., model selection, hyperparame -

ter tuning, performance validation, etc.). In an AutoML 

framework, a large number of augmentation operations are 

usually created manually or in an automated way, and the 

data augmentation task reduces to a simple search for the 

best transformation operations and their associated hyper -

parameters. The basic concept of AutoML approach to data 

augmentation is shown in Figure 13. 

4.1.1  Generation of augmentation operations 

The first step in the augmentation process is to generate 

diverse transformation operations to manipulate the in -

put data. In image augmentation, for example, the basic 

transformations are typically standard geometric and pho -

tometric image processing functions like rotations, flipping, 

scaling, shearing, color jittering, solarizaion, noise addition 

and contrast adjustment. Transformation magnitudes may 

include scale factors, rotation angels, translation offsets, 

color intensity and brightness levels, etc. The various trans -

formation operations and their corresponding magnitudes 

constitute a search space. 

The effectiveness of automated augmentation strategies 

is based largely on the ability to compose a comprehensive 

set of operations that are representative of the tasks under 

consideration. The general approach to automating data 

augmentation tasks involves creating the set of required 

transformation operations and then applying the created 

operations on data and algorithmically determining the 

most useful augmentations using various optimization tech -

niques. We briefly discuss common approaches to creating 

augmentations as well as techniques for optimizing the 

augmentation strategies. 

Analytical Methods 

The most basic approach to automating data 

augmentation is the use of semi -automated or analytical 

methods to create augmented data. The analytical method 

involves defining a set of explicit mathematical relations 

or heuristics to perform required transformations on the 

training data. These relations can be domain -specific and 

depend on the characteristics of the given data type. This 

class of methods generally perform standard augmentation 

functions such as rotations, shearing, flipping, and color 

jittering. Many works (e.g., [138], [139], [140], [141], [142] 

involve defining different types of basic transformations 

functions to apply, as well as specifying the degree or 

magnitudes of these operations within an applicable range. 

While simple, the approaches can be effective for certain 

tasks [143], [144], they may not be able to account for 

unknown transformations. 13    

> Figure 13. General scheme and process of automated data augmentation methods. A portion of the training set is sampled by a s pecified
> algorithmda, ta sampler, for augmentation. The selected sample undergoes a series of transformations by an augmenter sub -module. The
> augmented sample is then added to the original data batch ifor training. The training loss is fed to both the sampler and augmenter to fine -
> tune their performance. Over time, this results in better sampling and augmentations, leading to overall improved performance.

Deep -learned transformations 

Some recent works [46], [145], [146], [147], [148] 

have suggested moving from predefined augmentation 

operations to learned mechanisms, which allow 

augmentation operations to be learned from input data 

rather than relying on explicitly formulated relations. The 

basic idea is to implement data transformation sub -modules 

within deep learning pipelines whose parameters and 

hyperparameters can be leaned and applied on input data 

in the training process. This method offers a higher degree 

of automation than approaches based on explicit analytical 

formulations. They also allow arbitrary or unknown 

augmentation styles to be achieved. A common technique 

widely employed for learning transformations is the spatial 

transformation network (STN) [45] (Figure 14A). 

Generative machine learning techniques 

Another common way to automate the construction 

of data augmentation operations is based on generative 

modeling [145], [149], [150], [151]. This approach creates 

transformed samples without explicitly requiring analyt -

ical formulas, or even requiring implicit transformation 

parameters to be learned. The basic idea is to first learn 

the distribution of real data by training a discriminator -

generator pair on the original dataset. Knowledge about 

the distribution can then help to generate new but slightly 

varied samples that simulate data diversity. Thus, in the 

automated data processing pipeline, the generative model 

is employed as an intermediate data transformation unit. 

Its parameters are learned jointly with the CNN model in a 

bi -level optimization scheme (see Figure 14 B). 

Generative machine learning approaches based on 

GANs are particularly common in computer vision do -

mains, where they enable realistic image samples to be gen -

erated with desired visual features. This family of methods 

is also useful in applications where it is necessary to transfer 

knowledge from an existing domain to a different domain 

(e.g., in [152]). Generative methods are also used to align the 

distributions of data from different sources, enabling deep 

learning models to perform better when applied to new 

data that may come from a slightly different distribution. 

However, despite the usefulness of the approach, generative 

models themselves require careful tuning, greatly increasing 

labor demands, and thus may limit the full benefits of 

automation. 

4.1.2  Optimization of augmentation strategies 

Optimization techniques are typically employed to search 

for the the best augmentation strategy. The search task 

involve finding not only relevant transformations but also 

the optimum levels of transformations. For image augmen -

tation these levels may be rotation angles, translation offsets, 

saturation values, etc. 

Because the search space is inherently complex and 

discontinuous, many works utilize black -box optimization 

techniques such as reinforcement learning (e.g., in [138], 

[140], [140], [142], [153], [154]), Bayesian optimization (e.g., 

in [142], [155], [156]) and evolutionary computation algo -

rithms (e.g., in [157], [158], [159], [160] to search for good 

augmentation strategies These techniques have produced 

impressive results but are noticeably slow when dealing 

with very large data. 

To overcome this limitation, approaches for optimizing 

the discrete augmentation search space using the concepts 

of approximation gradients [161] have also been proposed 

[149], [162], [163], [164]. The use of these concepts makes 

it possible to develop methods for solving optimization 

problems for discontinuous functions present in the search 

space. Approaches based on approximate techniques are 

generally more efficient than black -box search techniques. 

Alternative search spaces providing the ability to find ef -

fective augmentation policies without extensive search have 

also been considered [165], [166]. In some cases (e.g., [167]) 

these approaches eliminate the need for the search stage 

altogether. For instance, researchers in [166] contend that it 

is not necessary to conduct exhaustive search for extensive 

range of possible augmentations and hyperparameters in 

a combinatorial manner and, instead, propose to reduce the 

search space to a linear search space of uniform probabilities 

and augmentation parameters that can be traversed by a

simple grid search. The approaches, despite the significant 

reduction in compute time, have shown competitive perfor - 

> Optimization of augmentation operations
> Samples data from training batch

i 

> Augmentor
> Sampled data

is   

> Data sampler +
> Loss
> Parameter tuning Batch

i        

> Training data
> Deep learning framework Original data (ith batch) 14
> Generative model intermmediate
> transformations Model training

Input  CNN 

> GAN tuning

Generative modeling -based automated augmentation 

B

GAN 

A Localization Network Grid Generator 

# θ τθ( G)

Output 

Input  Spatial Transformer Network (STN)  Sampler 

Output     

> Figure 14. Approaches to generating basic transformations within automated data augmentation frameworks: (A) -learned transformation using
> spatial transformation network (STN) [45], and (B) -generative -modeling -based synthesis of transformed image data.

mance. However, there seems to be little room to further 

extend their predictive performance. 

4.2 Data synthesis by large language models (LLMs) 

and diffusion models 

The automated data augmentation methods covered in 

Subsection 4.1.1 utilize generative modeling techniques to 

perform intermediate data processing in AutoML pipelines. 

While generative methods can be used to perform data 

transformation operations in that manner, approaches based 

on variational autoencoders [168], generative adversarial 

networks [169], diffusion models [170], autoregressive mod -

els [171], [172] and large language models (LLMs) [173] are 

commonly designed to directly synthesize data by them -

selves, thereby bypassing all intermediate processing steps. 

Recent advances in deep generative AI -based data synthesis 

methods, especially diffusion models and large language 

models (LLMs), have enabled the possibility of generating 

clean data from scratch or from noisy data in an end -to -end 

manner. Data generated this way can be used to augment ex -

isting data (e.g., in [174], [175], [176]) or completely replace 

natural data in situations where datasets are inadequate or 

are inaccessible for training machine learning models (e.g., 

in [177], [178]). 

Large language models are a special type of deep learn -

ing systems based on the concept of transformer [179] and 

are primarily designed to perform NLP tasks. On the other 

hand, diffusion models are a particularly versatile class of 

generative methods that synthesize data in a similar way to 

generative adversarial networks – a technique that allows 

deep neural networks to generate data using random noise. 

They are based on nonequilibrium thermodynamics [180]. 

Diffusion models are realized by first defining a Markov 

chain of diffusion steps that gradually increase the random 

noise component to input data, and then implement a re -

verse diffusion process to re -create the desired data samples 

from the input noise (Fig 15). 

Generative methods based on LLMs [174], [181] and dif -

fusion models [182], [183], [184] have become very useful in 

augmenting data for natural language processing and com -

puter vision applications, respectfully. In NLP applications, 

additional text data commonly generated by prompting 

LLMs such as ChatGPT [185], LLaMA [186] and BERT [187] 

to complete input sentences with missing words or phrases 

so that different variations of the original sentences could 

be created. For instance, Ubani et al. in [188] propose to 

manipulate text data with the help of ChatGPT to produce 

augmented data for training deep learning networks for 

NLP -based tasks by giving appropriate prompts to generate 

syntactically different variations of the original sentences. 

In computer vision applications, generative AI -based data 

augmentation typically involves manipulating existing im -

ages to generate novel styles, poses, background contexts 

and views. DiffusionCLIP [184] introduces an approach to 

perform various image transformations using text -guided 

prompts with a diffusion model developed on the basis 

of CLIP [189]. The approach achieved photorealistic styl -

izations of different images in a wide range of novel con -

texts. Figure 16 shows the result of various image styl -

izations methods using DiffusionCLIP [184] and two other 

text -based generative data manipulation models –StyleCLIP 

[190] and StyleGAN -NADA [191]. 

While diffusion models and LLMs can both perform data 

processing – e.g., text manipulation [188] and image quality 

enhancement (super -resolution) [182], [192] , denoising [183] 

and styling [193], [194], their most powerful use -case has 

been the task of automatically generating high quality data 

from “scratch” [195], [196]. Modern generative AI tech -

niques based on diffusion models can be used to synthesize 

many different types of data – including images [197], [198], 

videos [199], [200], [201], text [188], [202], audio [203], [204], 

time series [205] and tabular data [206], [207], [208] –using 

only text prompts without additional input data. 

Language models, when trained on pairs of data (e.g., 

images) and corresponding descriptive texts (e.g., captions), 

can learn the association between the textual descriptions 

and the underlying data. The trained model can then be 

used to generate desired images based on descriptive text 

inputs. In this regard, large language models and diffusion 

models play a complementary role in the data generation 

process. Indeed, the popularity of large -scale diffusion mod -

els has been driven largely by the development of powerful 

generative tools that generate data based on intuitive text 

prompts enabled by LLM techniques. Some of the most 

prominent data generation models in this category include 

Dalle -2 [209], Stable Diffusion [210], Glide [193] and Imagen 15                         

> Figure 15. The idea of diffusion models is to learn to generate data by gradually adding random perturbations to the input da ta, and then perform
> the reverse process of denoising the data until a clean data output is achieved. Image by authors.
> Figure 16. Visual comparison of results of different generative AI -based image stylization methods: DiffusionCLIP [184], StyleCLIP [190] and
> StyleGAN -NADA [191]. The models attempt to style achurch building with respect to different visual contexts. Image is from [184]. We have applied
> some minor changes in the text fonts to make it clearer.

[211]. Figure 17 depicts photorealistic images generated 

by different diffusion models based on user’s supplied 

prompts. 

4.3 Dataset distillation methods 

While the approaches discussed in Subsections 4.1 rely on 

generating extra data to improve machine learning models, 

an alternative approach, known as dataset distillation [214], 

[215], seeks to select an informative subset from a large -scale 

training dataset that retains the important properties and, 

importantly, the generalization performance of the original 

dataset. That is, the aim is to produce a leaner dataset that 

still performs satisfactorily on the target task. The approach 

can also be used to distill data labels from a large set of 

labels (e.g., in [216], [217]) or from noisy labels (e.g., in 

[218]). While traditional approaches to dataset distillation 

rely on manually engineered procedures, some recent works 

[219], [220] have proposed mechanisms for automating the 

process. Although these approaches are well motivated, and 

can potentially lead to high performing but leaner deep 

learning models, at present their level of automation is 

limited. Consequently, their popularity is relatively low. 

4.4 Performance of automated data augmentation 

methods 

Results from several studies show that models trained on 

data generated by automated techniques outperform those 

trained on data generated using manual approaches. In 

Table 3 we compare the performance of ten state -of -the -

art automated data augmentation techniques and ten of 

the best traditional augmentation methods across several 

datasets (CIFAR -10 [221] , CIFAR -100 [221] and ImageNet 

[222]). All results are obtained using Wide ResNet (WRN -

28 -10) [223] backbone trained until convergence -roughly 

about the same number of epochs (300). This provides a

fair ground for comparison of the different methods. In 

Figure 18 we summarize the comparative performance by 

averaging the classification scores as well as the percentage 

improvement over the baseline (i.e., models trained without 

augmentation). These results convincingly demonstrate the 

effectiveness of automated data augmentation approaches. 

While the performance of automated augmentation tech -

niques is in no doubt, an important challenge is the enor -

mous computational resources typically required for train -

ing these models. Also, since the augmentation process is 

not guided by human intuition, some augmentation opera -

tions may unduly alter the original semantic meaning of the 

data, which may not be observed at training time and can 

later lead to catastrophic failure. For instance, modifying the 

label of an image or applying overly aggressive transforma -

tions in such a way that they transform its semantic meaning 

can result in erroneous training signals with potentially 

harmful implications.       

> Noise Data generation by stepwise denoising
> Noise
> Data perturbation by gradual noise addition
> Final output data
> Original input data
> Recon Wooden house Asian tower Department store Original Red brick Golden Sunset Recon Original
> StyleCLIP  DiffusionCLIP StyleGAN -NADA 16

Figure 17. Examples of images generated by state -of -the -art diffusion models: Blended Diffusion [212], Multidiffusion [213] and Stable Inpainting 

[210]. The images have been generated using text prompts together with simple foreground images as inputs. Image is from Mult idiffusion [213]. 

Table 3 

Performance results of automated and traditional data augmentation methods on classification tasks. The values represent perc entage accuracy 

on CIFAR -10, CIFAR -100 and ImageNet datasets. Bold is highest score, italic is second highest, and underline is third highest.                                                   

> Method Classification accuracy on various datasets
> CIFAR -10 CIFAR -100 ImageNet (top 1) ImageNet (top 5)
> Baseline 96.1 81.2 76.3 92.1
> Automated data augmentation methods
> TrivialAA [224] 97.5 84.3 78.1 93.9
> DivAug [225] 98.1 84.2 78.0 -
> AWS [153] 98.0 84.7 79.4 94.5
> MetaAug [226] 97.7 83.8 79.7 94.6
> PBA [159] 97.4 83.3 77.2 93.4
> AdvAA [149] 98.1 84.5 79.9 94.5
> A2 -Aug [227] 98.0 85.2 79.2 -
> FastAA [142] 97.3 82.8 77.6 93.7
> KeepAA [228] 98.7 -78.0 93.0
> DeepAA [229] 97.4 83.7 78.3 -
> Traditional data augmentation methods

## 5 AUTOMATED FEATURE ENGINEERING 

Feature engineering is a set of low -level data processing 

tasks that aim to increase the representativeness of input 

data by indirectly manipulating extracted feature vectors in 

deep learning layers. The goal is to obtain the minimum 

subset of features that can adequately encode all necessary 

information about the data without loss of predictive per -

formance. 

Traditional approaches to feature engineering [239] rely 

on assumptions about the underlying data and feature 

statistics to enrich feature representation. They typically                                      

> MultiDiffusion  Blended Diffusion  Stable  Inpainting  Input data and  prompt
> StochasticDepth [230] --77.5 93.7
> RE [231] 96.9 82.3 77.3 93.3
> RICAP [232] 97.2 82.0 78.6 -
> SalencyMix [233] 96.0 80.5 78.7 -
> GridMask [144] 97.2 83.4 77.9 -
> SmoothMix [234] --77.7 93.7
> Manifold Mixup [235] 77.5 93.8
> CutOut [236] 96.9 81.6 77.1
> MixUp [237] 97.3 82.1 77.9 93.9
> FMix [238] 96.4 82.0 77.7 -17

∈

∈ 

> Figure 18. Comparison of automated data augmentation and traditional methods. Results for each class of approaches are based on 10 state -of -
> the -art techniques.

employ analytical formulations of feature transformation 

operations to enhance the representativeness of extracted 

features. These techniques, apart from being enormously 

computationally expensive and heavily dependent on do -

main expertise, do not often lead to an optimal solution. For 

this reason, automated machine learning is considered as a 

more viable approach. 

5.1 General concept of automated feature engineering 

The basic idea of automated feature engineering is to create 

a large search space of all possible feature processing op -

erations and then apply optimization techniques to search 

for an optimum feature engineering strategy. An optimum 

strategy is the best possible combination of the elementary 

operations (i.e., the set of operations that result in the best 

performance for the specific dataset and task). 

Formally, for a target task Tsk , the feature engineering 

problem for a given dataset D consisting of a set of features 

F f 1, f 2, f 3, ...f n involves defining and applying a set of 

suitable transformations T t 1, t 2, t 3, ...t n on the original 

feature set F to generate additional feature set Φ and, with 

the help of a search mechanism, finding the best possible 

subset of features over the feature space Ω = F + Φ that 

maximize performance on the target task Tsk .

Because of the need to run and evaluate an unusually 

large number of operations on the given data, and owing 

to complex interactions among these operations, it is often 

challenging to obtain a good solution in reasonable time, 

especially when dealing with large datasets. Instead of deal -

ing with the entire search space at a go, it is often possible 

to create smaller search spaces which can be evaluated first 

before testing various combinations of the best performing 

strategies. A number of recent works, for example, [240], 

[241], have tended to favor such an approach. For example, 

VolcanoML, introduced by Li et al. in [240], aims to allevi -

ate the challenge of computational burden of large search 

spaces by proposing a more scalable search space design 

that allows users to construct custom textitexecution plans 

for their specific AutoML tasks. 

Instead of attempting to search on the entire search space 

for the optimum solution, the authors suggest decomposing 

it (the search space) into smaller components and then learn 

a strategy to select the most appropriate ones for a given 

task and dataset. To achieve this, they developed a high -

level structural representation of the search space consisting 

of small atomic subspaces that can be used as building 

blocks for larger search spaces. The small search spaces 

form a tree structure and can be combined in different 

permutations. This provides a more flexible mechanism that 

allows selecting a final search space configuration based on 

computational budget and training time constraints. The 

authors demonstrated the ability of the approach to com -

pose more efficient AutoML pipelines than with traditional 

search space configurations. NAS -based feature engineer -

ing approaches [59], [60] differ from conventional AutoML 

methods in the sense that they do not only optimize pa -

rameters and hyperparameters for predefined model ar -

chitectures but are also concerned with creating, training 

and validating new neural network topologies based on the 

input data. The goal is to find fundamentally new model 

structures which perform better than manually designed 

architectures. In this case, for the construction of the search 

space, dynamic feature processing operations are defined 

together with elementary building blocks for generating 

surrogate neural architectures. Search techniques are the 

applied to jointly optimize the resulting neural architecture 

and the feature processing operations. 

While a wide variety of AutoML tools (e.g., [97], [242], 

[243]) exist for generic data processing, dedicated AutoML -

based feature engineering utilities and software packages 

[244], [245], [246], [247] have also been introduced that per -

form automated feature extraction as their core functions. A

large number of these tools are aimed at processing features 

from time series and tabular data. Tools for time series 

data include TSFEL [248], tsfresh [246], tsfeaturex [247]. 

Featuretools [244] is aimed at extracting useful features 

from hierarchical and relational databases. This is achieved 

using a so -called deep feature synthesis that mines and 

combines features from multiple database tables at different 

hierarchical levels. 

5.2 Common feature engineering tasks and ap -

proaches for automation 

In the literature, three main feature engineering tasks can 

be distinguished [249]: feature extraction, feature synthesis             

> A
> 120 Classification accuracy (percentage scores)
> 100
> 80
> 60
> 40
> 20
> 0
> CIFAR -10 CIFAR -100 ImageNet -Top 1% ImageNet -Top 5%
> BaseLine Automated Traditional
> Improvement as a percentage of baseline performance
> 4
> 3.5
> 3
> 2.5
> 2
> 1.5
> 1
> 0.5
> 0
> CIFAR -10 CIFAR -100 ImageNet -Top 1% ImageNet -Top 5%
> BaseLine Automated Traditional
> B
> 96.1
> 97.52
> 96.72
> 81.2
> 84.1
> 83.35
> 76.3
> 78.54
> 77.76
> 93.1
> 93.94
> 93.76
> 0 1.48
> 0.65
> 0 3.57
> 2.65
> 0
> 2.94
> 1.91
> 0
> 0.90
> 0.71 18

(often referred to as feature construction in many literature 

sources (e.g., [250], [251], [252], [253]), and feature selection. 

There is no consistent definition of these terms. Also, the 

scope and interrelationships among these three processes 

often vary widely. While many authors treat all three tasks 

as distinct processes, some authors (e.g., [254]) consider 

feature synthesis and selection to be aspects of the same 

process - feature extraction. And while authors such as Horn 

et al. [245] treat the extraction and synthesis subtasks as the 

same task, other literature sources (e.g., [255]) regard fea -

ture construction and extraction as a single process distinct 

from feature selection. In this work, we use the following 

designation:  

> •

Feature extraction - relates to the process of manip -

ulating input data or intermediate feature sets to 

obtain more generalizable, robust, representative and 

compact features for the particular task.  

> •

Feature synthesis – also commonly termed feature 

construction [254], is the creation of new feature sets 

based on the available data or intermediate features 

to better characterize the dataset and enhance the 

performance of the resulting model.  

> •

Feature selection – is the process of choosing and 

further processing only the best set of features that 

result in noticeable performance improvement. 

Despite this clear delineation, the boundaries between 

these processes can be very blur, and in some cases, the 

terms can be used interchangeably. Moreover, they are often 

applied simultaneously in a given machine learning prob -

lem. 

5.2.1  Automated feature extraction 

For many large -scale machine learning problems, feature 

extraction is the first and most important stage in the feature 

engineering process. It allows to obtain more compact and 

representative feature set from raw data. Since the input 

data in these ML application settings are usually char -

acterized by a large number of variables, it is generally 

useful to simplify feature representation by maintaining 

only the important variables that account for performance. 

The process of feature extraction results in new features 

with reduced computational and memory footprint but with 

superior characteristics necessary to encode the underlying 

data. Feature extraction can also enhance interpretability 

since the reduced set generally contains only significant 

features useful for the given task. The extracted features are 

generally obtained by applying various linear combinations 

of the original ones. This allows the dimensionality of 

identified set of features to be reduced while maintaining 

the information encoded by the original dataset. Traditional 

feature extraction techniques rely on algorithms such as 

principal component analysis (PCA) [256], [257], indepen -

dent component analysis (ICA) [258], [259], linear discrim -

inant analysis (LDA) [260] and locally linear embedding 

(LLE) [261], [262]. In AutoML pipelines, feature extraction 

is usually achieved by using simplified analytical functions 

based on some variations of these standard algorithms to 

perform basic feature extraction operations (e.g., in [263], 

[264]) within a bi -level optimization scheme, where the 

best performing operations and the corresponding hyper -

parameter values are automatically determined. Because of 

the potential for excessive explosion of features, most of 

these approaches fuse the extraction and selection stages 

as a single process. Yang et al. [265], for instance, employ 

simple mathematical operations to first extract data and 

then utilize Boruta algorithm proposed in [264] to select the 

most effective ones. 

Some new approaches [266], [267], [268], instead of 

focusing solely on utilizing analytical functions to extract 

features from data, propose to rather learn effective network 

architectures for applying the necessary feature extraction 

operations. These approaches are based on the concept 

of neural architecture search (NAS). For instance, Meta -

learning for Tabular Data (METABU) [267] employs a NAS -

based technique to automatically extrct useful features by 

finding the best machine learning model and the corre -

sponding hyperparameter values. Firstly, the authors manu -

ally constructed a large number of meta -features (135 in all). 

They then employ optimal transport (OT) [269] technique to 

perform linear transformation on these manually designed 

meta -features in order to generate new features that are 

linear combinations of the basic meta -features. Finally, Au -

toML technique is used to find the best model configuration 

and hyperparameter values that results in the best features 

based on performance as measured on a validation set. 

Also in [266], Lopes et al. propose an automatic feature 

extraction method for sentiment analysis problem based 

on Neural Architecture Search (see Figure 19). The authors 

constructed diverse machine learning models, each of which 

independently performs feature extraction in different ways. 

They then used random search techniques to find the best 

performing model based on the performance scores on 

validation data. The approach uses a three -stage structure 

to accomplish the task of predicting sentiments based on 

enhanced features extracted from input data. The model 

takes as input images obtained from the internet and their 

corresponding textual descriptions. After an initial prepro -

cessing stage, image and text data are classified separately 

by predefined models in the search space. The image and 

textual features are then fused together to obtained a com -

posite feature representation before categorizing the under -

lying inputs into one of the following: Positive, Neutral 

and Negative. The fusion process, in this formulation, is 

essentially a feature extraction mechanism that produces 

useful features from combined text and image data. 

Transferable AutoMl (Tr -AutoML) [268] utilizes a meta -

feature extraction method that allows useful features 

learned from previously trained, multiple AutoML pipelines 

to be mined, aggregated and then transferred to new 

tasks. The approach combines meta -learning and architec -

ture search for feature extraction from multiple but related 

datasets. It relies on transferring already -learned architec -

tures that previously showed good results in performing 

feature extraction functions on new datasets and tasks. 

This significantly reduces the search space since a common 

architecture can serve for multiple feature extraction tasks. 

5.2.2  Automated feature synthesis 

The feature synthesis or construction stage creates addi -

tional features to further enhance the performance of the 19 

> Figure 19. Functional diagram of the automated feature extraction techniques proposed in [266]. The approach employs a three -stage neural
> network framework to perform sentiment analysis by first extracting and separating online images and their corresponding labe ls. Different models
> are then used to perform preprocessing before sentiment classification, also by independent sub -models. Different combinations of these models
> are then evaluated and the best settings selected according to the performance on the target task.

quires repetitive trial and error using different combina -

tions of operations with different hyperprameter settings. 

Automated feature synthesis [38], [245], [272], [273], [274] is 

particularly valuable in situations where the scarcity of data 

restricts the adequacy of features that are mined through  

> ←

feature extraction process. For a given feature set derived 

from a training data, the automated feature synthesis task 

seeks to determine and optimize the best combination of 

basic feature -level processing operations and their associ -

ated hyperparameters to apply in order to generate new  

> ∪

features that, when incorporated into the original feature 

set, maximize the performance of the model on the target 

data. Some automated feature synthesis methods (e.g., [274], 

[275], [276]) apply all operations at once to expand feature 

set and then search for the best features within the expanded 

set. Kanter and Veeramachaneni [274], for instance, simulta -

neously apply all transformation operations in the search 

space and then utilize different classification sub -models to 

select the most effective features. The main drawback of this              

> Figure 20. Basic principle of feature synthesis and selection approach in
> Explorekit [270]. This relies on drastically increasing the feature set by
> generating additional features and then searching for a small subset of
> features the yield the best performance.

resulting model. The synthesis process generally exploits 

the statistical distribution of features extracted from the data 

to generate new, complementary features. This is useful in 

cases where extracted features are insufficient to provide 

an adequate representation of the input data. Typically, 

feature synthesis is achieved by performing manipulations 

on accessible features with the help of predefined trans -

formation operations. Apart from generating new features, 

feature synthesis models [40], [253], [271] typically incor -

porate mechanisms to perform feature selection as not all 

artificially constructed features would be useful for the 

given task. However, these approaches, as discussed in this 

subsection, differ from feature selection methods as their 

main focus is in generating good features as oppose to 

merely selecting useful features from existing set extracted 

directly from input data. 

Common techniques used to accomplish this task in -

clude feature transformation, interpolation, averaging and 

mixing. Traditional approaches to solving this problem re -

approach is the high memory requirements that results from 

the need to store a large pool of features. 

Another line of works (e.g., [270], [273]), instead of 

aggressively expanding the search space, iteratively apply 

small number of operations to expand, test and select small 

subsets of features in batches. While this approach over -

comes the high memory demand of techniques such as [274], 

[275], [276], testing for effective features at each iteration 

can lead to expensive computational overheads. Moreover, 

useful features that could form the basis to further generate 

more effective features can be lost in the selection process 

if they do not directly yield good performance. Thus, tech -

niques based on this concept can fail to achieve optimum 

feature„. In order to reduce the high computational cost of 

evaluating all candidate features, more recent approaches 

[271], [277] suggest using meta -learning techniques to pre -

dict effective transformations that generate the best features 

and to prioritize them for the synthesis and evaluation 

stages. 

Different from many of the feature synthesis paradigms 

[271], [277] that employ a bi -level optimization mechanism 

to generate useful features, some approaches [270], [276], 

[278] utilized more conventional deep neural networks to 

accomplish feature engineering. These models learn to pre -

dict the effectiveness of different transformation operations  

> Dataset
> Candidate Feature
> Generation
> Meta -Features
> Classifier
> Candidate Feature
> Evaluation &
> selection
> Add Selected
> Candidate to
> Generated
> Features Set
> Candidate Feature
> Ranking Generate Candidate
> Features Ranking
> Model
> Previously Analyzed
> Datasets
> Input data 20

through previous training on other datasets. Some works 

such as [59] have demonstrated the effectiveness of NAS 

techniques to synthesize useful features. These works con -

struct the search space consisting of basic neural models and 

then employ neural architecture search approaches to find 

the best models capable of applying feature transformations 

to generate new features. 

5.2.3  Automated feature selection 

The overall goal of feature selection is to find a minimal 

set of features that provides the best predictive perfor -

mance using the least possible computational resources. 

The process, thus, effectively simplifies the resulting model 

by eliminating irrelevant features. The overall performance 

can be drastically enhanced since irrelevant features often 

harms performance. In addition, feature selection helps to 

determine the effectiveness of different feature sets on the 

ML model and hence can provide additional insights about 

the important factors that influence performance [57]. More -

over, the reduction of features by selecting only the most 

useful ones leads to a more simplified and interpretable 

models. Feature selection, thus, performs four important 

functions:  

> •

Model size reduction by the elimination of redun -

dant and less useful features, leading to a reduction 

in computational and memory demand  

> •

Predictive performance improvement as a result of 

the elimination of potentially detrimental features  

> •

Model structure simplification as a result of reduced 

feature set  

> •

Knowledge discovery (i.e., provides insights into the 

internal representation)  

> •

Interpretability of data and resulting machine learn -

ing model 

A major difficulty with feature selection is the fact that 

it is often not straightforward to determine the minimum 

subset of features that can provide optimum performance. 

A set of features that may not be useful when applied alone 

may turn out to be the optimal set when used in com -

bination with another set of features. This makes the fea -

ture selection process an inherently combinatorial problem. 

Moreover, there may be multiple sets of features that result 

in optimal performance. Feature selection approaches aimed 

at knowledge discovery [57], [279], [280], [281], [282]) often 

need to identify all such minimal feature sets. However, in 

some scenarios it may be necessary to identify all such sets 

of features. For example, feature selection for knowledge 

discovery applications generally aims to identify all useful 

sets of features. On the other hand, for problems such as 

regression, classification and time series forecasting, it is 

important to be able to quickly find only one out of the most 

useful sets of minimal features that can comprehensively 

encode the given data. 

Typically, to carry out feature selection, a subset of fea -

tures is drawn from the set of learned features based on a

given criterion and then evaluated to assess the efficacy of 

the chosen subset on the end task. The process is repeated 

until a small subset of features is obtained that can provide 

optimal performance. For automated feature selection, like 

in most automated machine learning tasks, optimization 

methods are usually employed to find the optimum feature 

sets. Explorekit [270] (20) uses a learned mechanism that 

employs a ranking classifier to identify useful features for 

further exploitation. In conventional deep learning models, 

regularization techniques based on feature dropout methods 

[283] are a common way to eliminate irrelevant features 

and retain more useful ones. But hyperparameter selection 

for these dropout mechanisms is generally accomplished 

manually, making it challenging to find the optimum set of 

features to drop in order to retain only the most effective 

set. Inspired by these feature -level information dropout 

regularization techniques, some recent feature engineering 

methods [283], [284] are aimed at learning optimum dropout 

patterns based on automated hyperparameter tuning tech -

niques. These approaches can be considered as an alterna -

tive variant of automated feature selection methods since, 

by eliminating irrelevant features, they practically allow 

only the most useful subset of features to be selected for 

the given task. AutoDropout [285], proposed by Pham and 

Le, learns effective feature -level information dropout using 

AutoML -based hyperparameter tuning. To accomplish this, 

the authors created primitive dropout patterns using ad -

justable hyperparameters that define important properties 

of the dropout pattern, including the dropout size, stride, 

and the number of times to repeat the particular pattern. 

In addition, hyperparameters describing geometric opera -

tions, viz. rotations and shearing, as well as additional flags 

(binary variables) that determine whether or not dropout 

patterns are shared across channels or are applied on the 

residual branches of ResNet, are also specified. In the op -

timization stage, optimum settings for variables describ -

ing these hyperparameters are then automatically learned 

through feedback of the loss signal. 

5.3 End -to -end automated feature engineering 

While individual feature processing tasks can be automat -

ing separately, many modern automated feature processing 

approaches [134], [274], [275], [286], [287], [288] are aimed at 

performing the full range of feature engineering tasks (i.e., 

feature extraction, synthesis of additional features, and se -

lection of useful features) in a holistic and end -to -end man -

ner. In this works, the performance of the various subtasks 

may not be distinctive. Most approaches generally follow 

the so -called expansion -reduction paradigm, where various 

transformations are applied to the extracted features to cre -

ate additional features that are then evaluated together with 

the original features and the best set of features selected. 

These idea is to utilize a composition of primitive trans -

formation operations within a machine learning pipeline 

to generate diverse features which are jointly optimized 

with model hyperparameters in the training process using 

various search mechanisms. For instance, Scalable Auto -

matic Feature Engineering (SAFE) [40] (Figure 21) employs 

a search strategy based on XBoost [289] to find the best 

combinations of useful features. The search is conditioned 

on maximizing information gain. Evolutionary Automated 

Feature Engineering (EAAFE) [286] and Ring Theory based 

Harmony Search (RTHS) [290] employ evolutionary com -

putational algorithms to find useful features. The use of 

reinforcement learning approaches is also common [276]. 21 

to generate useful transformations. The technique first aims 

to find a neural network model capable of performing 

the most effective feature transformations. The resulting 

features are selected and then enhanced by applying sub -

sequent feature engineering operations. This process alle -

viates the problem of feature explosion where redundant 

and less useful features are spawn that adversely affect 

predictive performance while at the same time contributing 

to computational complexity and memory demands. Simi -

larly, Rakotoarison et al. in [267] propose to learn the best 

values for model hyperparameters in order to optimize the 

selection of manually composed meta -features. The authors 

show that meta -features learned using AutoML techniques 

can outperform state -of -the -art feature extraction methods 

based on traditional machine learning techniques.                      

> Figure 21. Scalable Automatic Feature Engineering (SAFE) [40] exploits
> the relationships among the initial feature set to obtain areduced feature
> set which is then expanded and further filtered according to the perfor -
> mance of the various combinations of features generated.

Neural feature search (NFS) [59] uses a reinforcement 

learning -based RNN controllers to learn effective transfor -

mation policies for generating useful features. Differentiable 

Automated Feature Engineering (DIFER) [288] formulates 

the automated feature engineering task as a differentiable 

problem by mapping discrete features into a continuous 

vector space with the help of a so -called encoder -predictor -

decoder -based feature optimizer and then finds effective fea -

tures by gradient ascent. The general architecture of their 

framework is shown in Figure 22. The approach uses ran -

dom sampling to generate initial population of features 

which are further transformed by applying a sequence of 

transformation operations. 

5.4 Automated feature engineering based on NAS 

frameworks 

Automated feature engineering methods [59], [59], [291] 

have also been proposed that employ Neural Architecture 

Search (NAS) techniques for feature selection. For instance, 

Chen et al. in [291] employ a NAS mechanism to search 

for and select the most effective set of features for a mul -

timodal person re -identification system based on RGB and 

infrared images. Their technique incorporates a two -level 

search space construction, where spatial and channel -wise 

features are independently selected. Neural Feature Search 

(NFS) [59] proposes a method that utilizes NAS technique 

to accomplish feature engineering (actually extraction and 

selection). The authors utilized reinforcement learning algo -

rithm to train a Recurrent Neural Network -based controller 

5.5 Performance of automated feature engineering 

methods 

In this section, we compare performance results of auto -

mated and traditional feature engineering techniques com -

monly employed in practice and in the scientific literature. 

The first set of results (shown in Table 5) show classification 

accuracies (F -1 scores [292]) obtained from experimental 

evaluations by several authors –specifically, [59], [278], [288], 

[293]. The feature engineering methods compared are the 

following. Raw – this entails using the original feature 

set without any further feature processing. Random (Ran )

involves iteratively running through the feature set and 

applying a random set of transformation operations (or 

no transformation) on the features in each iteration. In the 

case of ME , the feature engineering algorithm selects only 

one transformation operation – the most effective operation 

– for each sample set. Brute -force ( BF ) applies all trans -

formation operations on the entire feature set and then 

finds the best subset of features from the expanded set. 

In the literature, this approach is commonly referred to as 

Expansion -Reduction . The rest of the methods are LFE [278], 

NFS [59], AutoFeat [245] and DiFFER [288]. We selected 

23 datasets where performance results are widely available 

for many of the aforementioned automamted feature en -

gineering approaches. These datasets are from UCI [294], 

[295] and OpenML [296] repositories. For more detailed 

information about their basic characteristics, the reader may 

refer to their repositories [294] and Github pages [295], [297]. 

Random Forest model is employed as the base model for the 

classification tasks using ten -fold cross validation. 

From the table, it can be seen that the automated feature 

engineering techniques clearly outperform basic methods 

based on traditional paradigms. However, it is worth noting 

that the high performance of automated methods is not 

consistent for all datasets. Particularly, spambase , autos and 

convex show little or no improvements with automated 

feature engineering methods. 

We present a composite table (Table 6) about the exper -

imental performance evaluations in [293]. The results show 

the comparative performance of state -of -the -art traditional 

feature engineering methods (DCN -V2 [298] and FCTree 

[299]) and automated engineering methods (FETCH [300], 

AutoCross [301], SAFE [40], OpenFE [293] and AutoFeat 

[245]). Baseline results (i.e., task performance without fea - 

> Original feature s

22 

Figure 22. The basic design of differentiable Automated Feature Engineering (DIFER) [288] is made up of three main components : (1) population 

initialization module which generates feature by sampling randomly from the initial set, (2) feature evolution module for gro wing and introducing 

diversity into the feature set, and (3) feature selection which aims to reduce the overall feature count by selecting only th e best set of features. 

Table 4 

Degrees of automation of feature engineering tasks. 

Method Description and main characteristics Example works 

Basic and rigid automation based on fixed analytical formulas to 

separately extract, synthesize and select useful features 

Deep learning -based approach to generate and select useful features 

given a set of raw data in an end -to -end manner. 

[260], [261], [262] 

[270], [276], [278] 

Fully 

automated 

AutoML and NAS -based approaches to holistically generate useful 

features directly from raw data, together with model selection and 

hyperparameter optimization. 

[274], [286], [287] 

Table 5 

Performance of state -of -the -art feature engineering methods. The results are classification accuracies (F -1 scores) on selected datasets from the 

UCI [294], [295] and OpenML [296] repositories. Bold is highest score, italic is second highest, and underline is third highest. NB: Feat. -number 

of features, Inst. - number of samples.                                                                                                                                                                                                                                          

> Dataset Feat. Inst. Raw ME BF Ran NFS Auto -
> Feat LFE Differ
> AP -0mentum -lung 10936 203 0.883 0.915 0.925 0.908 0.981 0.929
> AP -0mentum -ovary 10936 275 0.724 0.775 0.801 0.745 0.873 -0.811 0.8724
> autos 48 4562 0.946 0.95 0.944 0.929 --0.96 -
> Balance -scale 8369 0.884 0.916 0.892 0.881 --0.919 -
> convex 784 50000 0.82 0.5 0.913 0.5 -0.819 -
> Credit -a6690 0.753 0.647 0.521 0.643 0.803 0.8391 0.771 0.8826
> dbworld -bodies 2100 0.93 0.939 0.927 0.909 --0.961 -
> diabetes 8768 0.745 0.694 0.737 0.719 0.786 0.762
> fertility 9100 0.854 0.872 0.861 0.832 0.913 0.7900 0.873 0.9098
> gisette 5000 2100 0.941 0.601 0.741 0.855 0.959 -0.942 0.9635
> hepatitis 6155 0.747 0736 0.753 0.727 0.905 0.7677 0.807 0.8339
> higgs -boson -subset 28 5000 0.676 0.584 0.661 0.663 0.827 0.827
> ionospare 34 351 0.931 0.918 0.912 0.907 0.972 0.9117 0.932 0.9770
> labor 857 0.856 0.827 0.855 0.806 0.960 -0.896 -
> lymph 10936 138 0.673 0.664 0.534 0.666 0.987 -0.757 -
> madelon 500 780 0.612 0.549 0.585 0.551 0.836 -0.617 -
> Megawatt1 37 253 0.873 0.874 0.882 0.869 0.933 0.8893 0.894 0.9171
> Pima -indians 8768 0.74 0.687 0.751 0.726 0.790 0.7631 0.745 0.7865
> Secom 590 470 0.917 0.917 0.913 0.915 0.934 -0.918 -
> sonar 60 208 0.808 0.763 0.468 0.462 0.839 -0.801 -
> spambase 57 4601 0.948 0.737 0.39 0.413 0.948 -0.947 -
> Spectf -heart 43 80 0.941 0.955 0.881 0.942 --0.955 -
> Twitter -absolute 77 140707 0.964 0.866 0.946 0.958 --0.964 -

ture engineering) are included for comparison. The evalu -

ated models are implemented using LightGBM [302]. Details 

of the datasets are presented in [293], [303], [304]. The 

models are run ten times on each of the datasets. It can be 

observed that in most of the cases the traditional approaches 

fail to outperform the baseline. While traditional approaches 

have shown strong performance over baseline results on UC 

Irvine and OpenML datasets in several works, including 

[59], [278], [288], [293], they appear to perform very poor 

per [293] (shown in Table 6). This is due to the fact that 

feature engineering process is highly dataset -sensitive, and 

the approaches in DCN -V2 [298] and FCTree [299] were opti -

mized for datasets, which are fundamentally different from 

the UC Irvine and OpenML datasets. Indeed, for a number 

of the dataset in Table 6, the performance of the automated 

methods also dropped significantly, sometimes below the 

baseline. Li et al. [300] acknowledged this problem, and 

their approach, Feature Set Data -Driven Search (FETCH), 

aims to specifically tackle this issue by developing a more 

generalizeable feature generation and selection mechanism. 

DL -based 

Analytical 23 

# ?    

> Data in the wild
> Model
> selection
> Final
> Model Model
> validation Model training
> Model
> construction
> Data processing
> Feature
> selection
> Feature Feature
> extraction synthesis
> Data
> pre -processing
> Relevant data
> identificatn
> Data acquisition

Task 

> Raw training data

Table 6 

Performance of automated feature engineering methods (FETCH [300], AutoCross [301], SAFE [40], OpenFE [293] and AutoFeat [245 ]) against 

traditional and baseline methods methods (DCN -V2 [298] and FCTree [299]). Bold is highest score, italic is second highest, and underline is third 

highest. NB: No.S (k) - number of samples (in 1000s); CL - number of classes; Cat, Ord and Num - numbers of categorical, ordinal and numerical 

features.             

> Dataset No.S Cl Features Metric Raw DCN -FC -SAFE Auto -Auto -Open -FET -
> Housing
> Machine

## 6 HOLISTIC , END -TO -END WORKFLOW OF DATA 

PROCESSING IN MACHINE LEARNING 

In Sections 3 through 5, we describe approaches for au -

tomating specific low -level data processing tasks such as 

data cleaning, labeling and feature engineering. Many au -

tomated machine learning techniques incorporate manual 

processing in some stages of the pipeline. For example, 

feature engineering may be fully automated but appropriate 

transformations can be manually selected to pre -process 

input data according to the specific details of the task and 

the properties of the underlying data (e.g., in [40], [286], 

[288]). Recently, there is a trend toward full automation of 

the entire machine learning pipeline from data acquisition 

to model deployment, where not only data -centric tasks are 

automated, but also the realization of the overall big data 

solution (Figure 23). 

Figure 23. Simplified architechture of an end -to -end AutoML framework. 

End -to -end frameworks aim to automatically collect and 

process data, perform feature engineering and also fix vari -

ous errors inherent in the data (for example, missing meta -

data) or errors that may arise in the course of processing. 

In addition, these frameworks can simultaneously carry out 

hyperparameters optimization, model architecture construc -

tion, selecting of evaluation metrics, prediction, analysis of 

results, and performing other machine learning tasks. This 

enhances the ability to discover valuable semantic informa -

tion from large volumes of unstructured data for big data 

applications. 

To enable a single end -to -end AutoML framework to 

solve a wide range of big data problems, several compo -

nents are often used to create AutoML systems. In particular, 

different blocks of processing units are typically used to 

handle different tasks such as data cleaning, augmenta -

tion and feature engineering. These functional blocks are 

often arranged sequentially in the form of a structured 

pipeline. To deal with the different processing requirements 

of diverse data formats and tasks, multiple pipelines with 

different structures incorporating several machine learning 

models are often used. 

Since the types of computational blocks required for a

particular problem depends on the nature of dataset and 

task, the selection of the structure and parameters of the 

specific pipeline to use is an important part of the auto -

mated data processing and machine learning tasks. Thus, 

the automation problem reduces to the task of combining 

and optimizing the various models in the pipeline in order 

to achieve the best possible performance. The pipeline is 

often represented as a computational graph, and its struc -

ture is determined using various functions to numerically 

evaluate the performance of different configurations. While 

some end -to -end AutoML systems such as ATM [305], ML -

Plan [306] and Hyperopt -Sklearn [307] utilize fixed 

pipelines, a large number of new approaches (e.g., AutoDES 

[308], FLAML [309], RECIPE [310] and H2O AutoML [100]) 

employ variable -length pipelines. Approaches employing 

variable pipelines typically use evolutionary computational 

algorithms to create a population of basic processing opera -

tions and iteratively adapt the simple algorithms to generate 

new and better algorithms through mutation and crossover. 

Recently, many large -scale generic AutoML tools have been 

developed to allow off -the -shelf application of machine 

learning models for various data -centric tasks. The main 

categories, characteristics and functions of these systems are 

presented in Section 8. 

AutoSmart [312] (Figure 25) is another example of a

fully automated machine learning framework that performs                                                                                                                                                      

> (K) Cat Ord Num V2 Tree Feat Cross Fe CH
> Vehicle 98.5 200100 AUC ^0.925 0.924 0.926 0.925 0.925 0.921 0.928 0.927
> Diabetes 102 234 10 3AUC ^0.731 0.717 0.731 0.730 0.732 0.732 0.888 0.731
> Telecom 51 222 14 21 AUC ^0.671 0.661 0.670 0.673 0.672 0.651 0.680 0.673
> California 20.6 -017RMSE _0.432 0.479 0.432 -0.444 -0.421 0.430
> Microsoft 1200 -025 111 RMSE _0.744 0.750 0.744 -0.744 -0.738 X
> Nomao 34.4 229 55 34 AUC ^0.996 0.992 0.996 0.996 0.996 0.993 0.997 0.996
> Broken 900 2027 31 AUC ^0.756 0.748 0.750 0.750 0.750 0.765 0.786 X
> Jannis 83.7 40054 Acc. ^0.721 0.720 0.719 -0.721 -0.729 0.720
> Covertype 581 7045 9Acc. ^0.969 0.966 0.719 -0.968 -0.974 X
> Medical 163 -605RMSE _1128 1413.7 1089 -1172 -982.0 1130.4 24
> Figure 24. AutoPrognosis [311] uses patient data to perform end -to -end automated data processing. The system is aimed at enhancing clinicians’
> decision making by making accurate prognosis of health outcomes and also providing explanations for predictions.

several data -centric tasks in an end -to -end manner. These in -

clude data preprocessing, data integration (i.e., table merg -

ing), feature synthesis and selection, as well as ensemble 

learning and model hyperparameter tuning. The framework 

incorporates a computational time and memory controller 

for managing the usage of computational resources required 

for the task. There are many off -the -shelf tools built on the 

basis of end -to -end automated machine learning that are 

even more complex and functional more capable than the 

models described above. We describe these models in more 

detail in Section 8. 

## 7 GENERIC AUTO ML TOOLS FOR DATA PROCESS - 

> ING AND FEATURE ENGINEERING

Generic AutoML tools are aimed at enabling a fully auto -

mated workflow in model development. That is, allow non -

professional users to use AutoML system to take raw input 

data, perform all necessary processing functions and then 

generate suitable models for an end application. Thus, they 

provide non -expert users the opportunity to use state -of -

the -art machine level techniques to solve complex problems 

without explicitly processing data and building models. 

These tools are generally characterized by a high degree 

of flexibility. They allow to construct ML pipelines using 

various types of input data. The most commonly supported 

input formats are tabular, text, time series and image data 

types. Many of the tools offer a way to customize and 

manage model performance and complexity. Some of the 

most widely used generic AutoML frameworks include 

TPOT [96], Auto -WEKA [243], Amazon SageMaker Autopi -

lot [313], DataRobot [314], AutoKeras [97], AutoGluon [315] 

and H2O AutoML [100]. We summarize the main features 

of these frameworks in Table 7. 

While most of the most popular tools are designed to 

tackle general problems related to Big Data, some Au -

toML tools –e.g., Google AutoML Vision [242], AutoProg -

nosis [311], JADBio [57], GAMA [316] and MedicMind [27], 

Microsoft Azure Custom Vision [317] –are domain -specific. 

AutoPrognosis [311] and MedicMind [318], for instance, 

are specifically designed to deal with healthcare related 

data. AutoPrognosis [311] (shown in Figure 24) is concerned 

with using electronic health records to aid diagnosis while 

MedicMind [318] deals with medical image analysis. Google 

AutoML Vision [242] and Microsoft Azure Custom Vision 

[317] are aimed at solving computer vision problems such 

as image classification and object recognition. 

7.1 Common functions supported by AutoML tools 

In the course of training, the autoML framework creates 

several alternative pipelines that test, evaluate and val -

idate different sets of machine learning algorithms and 

hyperparameter settings for solving specific problems. To 

accomplish this, the frameworks are usually sub -divided 

into specialized components dealing with specific tasks 

and data types: image, text or tabular data. There may 

also be special modules to support feature generation and 

preprocessing. AutoM tools typically handle multiple data 

processing tasks, including data collection and preliminary 

preparation, processing and post -processing: They can au -

tomatically find missing data, identify incorrect labels, and 

select the desired subsets of data required for a given task. 

The tools generally come with user -friendly graphic user 

interfaces (GUIs) with production -ready tools. The main 

functions include task analysis to understand the needs of 

the particular user; problem recognition – to discover the 

specific problem the user intends to solve and generate a

set of potential machine learning tasks capable of solving 

the problem; model evaluation and validation are carried 

out to arrive at an the optimal solution. The GUI and 

interface allow users to easily accomplish all these steps 

by performing intuitive actions to achieve desired results in 

an almost automated way; given a particular dataset, with 

minimal user input, the system is able to specify plausible 

problems that can be exploited to automatically generate 

models that produce end solutions. AutoML systems also 

provide intuitive information about the generated models 

and the data manipulation processes in a way that provides 

additional insights to guide non -expert users. Based on this 

information and their specific goals and preferences, users 

are able to refine the automatically defined problems and 

solution sets. Many AutoML platforms are readily com -

patible and interoperable with standard tools, allowing the 

results of the processing stage to be seamlessly integrated 

with external applications [18]. In addition to data process -

ing functions, commercial AutoML tools provide additional 

functionalities such as data visualization. Tools like Pecan 

AI [332] allow users to seamlessly interact with their models 

and data without any coding, whatsoever. 

Another important function commonly supported by 

advanced AutoML tools is visualization. Visualization util -

ities typically provide all necessary information needed to 

understand and make useful decisions about the task in 

relation to the available data. Such capabilities facilitate 25 

tabular 

time series 

time series 

text, time series 

image, text 

time series 

text 

time series 

time series 

Figure 25. The general architecture of AutoSmart [312]. The approach performs several data preprocessing and feature engineer ing tasks in an 

end -to -end manner. 

Table 7 

A summary of the main features of popular generic AutoML tools indicating the main tasks they perform: CV -computer vision; NLP -natural 

language processing, Class. - classification, and Regres. -regression. 

Data types 

supported 

AutoKeras [97] Open source text, images, 

Main domains and tasks 

Amazon 

SageMaker 

Autopilot [313] 

Google 

Proprietary 

Tabular, time 

series, images, 

texts 

tabular, image, 

Vertex AI [319] Proprietary 

Google Cloud 

AutoML [242] 

Auto -WEKA [243] Open source 

text, and video 

data 

tabular, image, 

text, and video 

data 

Tabular 

BigML OptiML [320] Open source Tabular, images, 

PyCaret [321] Open source Tabular, text, 

DataBricks [322] Proprietary Tabular, text, 

AutoGluon 

(Amazon) [315] 

image, text, 

tabular data 

Auto -sklearn [323] Open source Tabular 

MS Azure 

AutoML [324] 

time series, 

images, text 

DataRobot [314] Proprietary time series, 

images, text 

H2O AutoML [100] Proprietary Tabular, Texts 

H2O 

Driverless AI [325]  Proprietary Time series, tabular, 

TPOT [96] Open source Tabular, text*, 

image* 

Darwin [326] Proprietary Tabular 

IBM Watson 

AutoAI [327]  Proprietary Tabular, text, 

Tabular, time 

FEDOT [328] Open source 

MLBox [329] 

NNI (Microsoft) [330] Open source 

series, images, 

text 

Tabular 

Tabular, images 

Ludwig [331] Open source Tabular, text, 

FLAML (Microsoft) [309] Proprietary Tabular, text, 

ATM [305] Open source Tabular, image, 

Time & Memory Control Unit 

Automated Table Merging  Automated Feature Engineering       

> 1-1
> 1-1Feature Gneration
> M-1First -order Feature Transformation
> 1-MSecond -order Feature Combination
> M-MHigher -order Feature Combination
> Categorical Feature Transformation

Input  

> Fast Feature
> Selection

Prediction 

Automated Model Tuning 

Automated Data Processing 

> Related
> Table 3
> Related
> Table 2
> Related
> Table 1
> Ensemble Learning
> Hyperparameter Tuning
> Non -Categorical Feature Processing
> Categorical Feature Processing
> Feature Definition
> Related
> Table 5
> Main
> Table
> Related
> Table 4
> Risk
> Assessment
> Code
> Optimization
> Column
> Sampling
> Row
> Sampling

Tool Type 

Proprietary 

Proprietary 

Proprietary 

Time 

CV  NLP  Class.  Regr. 

series 

√ √ √ √ √

× × √ √ √

√ √ √ √ √

√ √ √ √ √

× × × √ √

√ √ √ √ √

√ √ √ √ √

√ X √ √ √

√ √ √ √ √

√ √ √ √ √

√* √ √ √ √

√ √ √ √ √

√ √ √ √ √

√ √ √ √ √

√*

×

×

×

×

×

X

√

√

√

×

√

√

√

√

√ √ √ √ √

×√ ×

×

×

×

√

√

√

√

√ √ √ √ √

× × √ √ √26 

interpretability and provides useful insights into complex 

machine learning mechanisms. This is useful for knowledge 

discovery applications. In medicine and healthcare, these 

applications include the discovery of novel drug targets, 

or the design ofbetter assays with minimal field measuring 

and testing requirements. Visualization capabilities can also 

allow developers to better assess the robustness and overall 

quality of a model before deploying in practical applica -

tions. For instance, one can examine the effect of missing 

specific features by deliberately removing features of inter -

est from a given model (e.g., in [57]). Moreover, visualization 

of the AutoML process can help to enrich the knowledge of 

data scientists or machine learning researchers by providing 

new insights [58]. It also enhances trust in AutoML solutions 

[333], [334] since developers can visually probe and validate 

pertinent issues related to trust and reliability in an intuitive 

manner. 

Since many large -scale AutoML tools integrate several 

technologies, and multiple data pipelines into a single plat -

form, they significantly increase synergy and effectiveness 

of solutions. The use of highly integrated platforms also 

speeds up development time by eliminating additional 

stages of development, testing and deployment of big data 

solutions. 

7.2 Main categories and features of AutoML tools 

AutoML tools can broadly be divided into two categories: 

open source and proprietary tools. 

7.2.1  Open source AutoML tools 

Common open source systems include Auto -Weka [243], 

AutoKeras [97], PyCaret [321], TROT [96], FEDOT [328] and 

Auto -sklearn [323]. The development of open source Au -

toML tools is characterized by the involvement of very large 

community of developers who contribute and update differ -

ent functionalities independently. They are generally built 

on open source machine learning libraries and frameworks 

such us PyTorch [335], Scikit -learn [336]and TensorFlow 

[337]. Because the codebases for such systems are accessible 

to users and developers, custom modifications can easily be 

implemented, unlike proprietary systems. They may also 

offer seamless interoperation or integration options with 

minimal additional requirements for working with other 

open source tools. Another important feature of open source 

AutoML tools is the wide community involvement in the 

development and extension of functionalities. Generally, 

open source AutoML tools are more restricted in terms of 

the level of automation. They typically involve some level of 

manual work in the implementation of data processing tasks 

and model development. In particular, users are required 

to have some competence in the underlying programming 

languages (e.g., C++, Python or R) used to build the tools. 

It also requires a level of understanding of the modeling 

process – correct formulation of big data problem and an 

intuitive choice of possible models, as well as how to assess 

the suitability of resulting models. These tools are designed 

with flexibility and ease of adaptation and extension in 

mind. Compared to proprietary tools, they provide less 

intuitive graphical user interfaces (GUIs), and generally 

more difficult for non -expert users to processing data and 

building models for various tasks. 

7.2.2  Proprietary AutoML tools 

Some of the largest and most popular AutoML tools are 

proprietary solutions. Prominent among these include MS 

Azure AutoML [324], FLAML [309], H2O AutoML [100], 

Google Cloud AutoML [242], AutoGluon [315], IBM Watson 

AutoAI [327] and DataBricks [322]. Among these tools, 

one can also distinguish systems developed by technology 

start -ups (e.g., Darwin [326] by SparkCognition, OptiML 

[320] by BigML) and those owned by very big companies, 

so -called tech giants (e.g., AutoGluon [315] by Amazon 

and Google Cloud AutoML [242] by Alphabet). Solutions 

provided by start -ups are generally stand -alone products 

aimed at relatively narrow scope of applications. Those 

provided by technology giants tend to be more generic and 

focus on providing a broad range of tools to meet diverse 

business needs. The tools developed by tech giants are 

typically cloud -based platforms capable of handling large 

amounts of data for businesses as well as individual users. 

Prominent among these category of AutoML tools include 

Amazon SageMaker Autopilot [313], Google Cloud AutoML 

[242], Google Vertex AI [319], Microsoft Azure ML [324], 

IBM Watson AutoAI [327]. The tools provide easy -to -use 

graphical user interfaces and a rich set of tools within ad -

vanced integrated development environments that support 

complex tasks and processes. The cloud -based platforms 

are generally highly scalable and easily allow integration of 

other cloud -based utilities. Microsoft Azure ML Microsoft 

Azure ML [324], for instance, provides a suit of cloud -

based AutoML tools such as Machine Learning API Service, 

Microsoft Azure Custom Vision and the Machine Learning 

Studio. In addition to providing utilities and APIs on the 

cloud that can be leverage by non -expert users to process 

training data and build machine learning models for various 

tasks, it allows one to seamlessly work directly with other 

Microsoft tools such as Microsoft Azure HDInsight and 

MS SQL Server. Similarly, Google Cloud AutoML provides 

modules such as AutoML Vision, AutoML Data Science and 

AutoML Natural Language that perform computer vision, 

Big Data processing and analytics, and NLP tasks, respec -

tively. These toolsets are cloud -enabled, and can further 

integrate and interact with other Google products through 

the cloud. 

Proprietary frameworks are designed to particularly tar -

get business users with subject matter knowledge about 

their technical job areas or business operations but without 

data science expertise. Since such users cannot easily under -

stand what data processing tasks need to be performed, or 

create appropriate queries to explore and analyze data, this 

type of AutoML tools provide typically intuitive graphic 

user interfaces and visualization utilities to allow users to 

perform these data processing, exploration and analysis 

functions at a higher level. 

## 8 IMPLICATIONS FOR INDUSTRY AND COMMERCE 

The techniques discussed in this survey seek to automate 

mundane machine learning tasks and reduce errors associ -

ated with manual data processing procedures. Specifically, 

they eliminate all stages of data preparation and manual 

model selections, tuning and evaluation. This has led to 

greater productivity and improved accuracy of results in 27 

many areas. The automation of low -level data processing 

and model development steps allows business users to 

move straight to generating results from their raw input 

data. This reduces the time required for products and ser -

vices to move from development to market. The enormous 

power of automated AI technologies also helps industries 

to process extremely large volumes of heterogeneous data. 

This allows them to better discover and exploit new insights 

to accelerate innovation, leading to the development of new 

products, services and even entirely new business models. 

The transformative role of AI in business and industry 

is undisputed [338]. However, implementing practical AI 

solutions for industry and business requires highly qualified 

professionals with strong expertise in Statistics, Mathemat -

ics and Computer Science. Unfortunately, there is a severe 

shortage of qualified personnel [339], [340]. State -of -the -art 

AutoML and generative AI tools have simplified the devel -

opment and application of AI solutions by encapsulating 

all complex algorithmic implementations behind intuitive 

user interfaces which support high -level interaction. The 

technologies have enabled the development of powerful AI 

tools and platforms that eliminate the need for AI special -

ists and allow non -expert users to be able to effortlessly 

perform otherwise complex data processing and machine 

learning tasks. They effectively eliminate the need for firms 

to hire expensive and scarce personnel. In practice, this 

means lowering the entry barrier for businesses, and thus 

empowering small businesses and industries with power -

ful AI capabilities. The immergence of cloud -based tools 

further expands the potential of businesses without the 

needed physical infrastructure to leverage AI capabilities for 

productivity. The simplification and significant reduction of 

cost of new models will undoubtedly expand the scope of AI 

and machine learning technologies in business and industry. 

We discuss the implications of these technologies for specific 

industries, namely: healthcare, agriculture, manufacturing, 

and retail, banking and finance. 

8.1 Healthcare 

The implications of automated data processing and the 

emergence of predictive analytical tools in healthcare are 

enormous. With the ability to process large volumes of 

medical data with minimal human intervention and provide 

useful insights and recommendations, these technologies 

enable medical professionals to make accurate diagnoses 

and provide appropriate treatment plans. Specifically, au -

tomated AI tools with access to large volumes of medical 

data –e.g., from wearable medical devices and IoT sensors, 

electronic medical records, scans, clinical tests and genetic 

profiles –together with advanced scientific knowledge bases 

(e.g., in the form of generative AI -based models like LLMs) 

can quickly analyze any new medical case, provide high -

level, human -understandable information about the diagno -

sis and treatment options. Indeed, specialized LLM -based 

medical tools such as Med -PaLM 2 [341] and MedAlpaca 

[342] are already capable of expert -level medical question 

answering, and in some cases outperforming medical pro -

fessionals in medical licensing examination [341]. 

The use of advanced data processing tools greatly helps 

to eliminates human factors from medical care, minimizing 

the risk of costly errors that humans sometimes commit as 

a result of distraction and fatigue. 

Automated AI solutions also serve as a catalyst for 

innovation in medical treatment. For instance, they are 

able to analyze existing treatment options and reconfigure 

them to work better or to fight new medical conditions. 

They are able to provide more suitable treatment plans for 

individuals with peculiar underlying physiological issues. 

These data -driven tools are also important for tasks like 

drug discovery as they help in the identification of new 

drug targets, analysis of their effectiveness and prediction 

of potential drug reactions and side effects. The use of these 

modern technologies significantly increases the precision 

and efficiency of medical care while at the same time sig -

nificantly reducing the costs. 

In addition to extending the capabilities and service 

quality of medical professionals, these technologies, through 

online conversational AI tools (e.g., chatbots), also offer pa -

tients the means to access personalized healthcare anytime 

and anywhere; individuals who experience symptoms can 

consult chatbots for personalized medical advice and direc -

tion. This capability is especially useful as supportive care 

system for elderly people. This also frees precious time for 

medical professionals, allowing them to focus on important 

medical problems instead of taking patients’ medical com -

plaints and providing consultancy services in non -critical 

situations. 

8.2 Agriculture 

In agriculture, these techniques will simplify the complex 

task of obtaining and processing heterogeneous data for 

various field variables and conditions. Additionally, farm -

ers will be able to utilize advanced digital tools to auto -

mate complex agricultural tasks that would have otherwise 

required technical expertise in AI and machine learning. 

Specifically, they will be able to develop advanced predic -

tive analytics models using AutoML and generative AI tools 

to predict and manage risks such as disease and pest infes -

tation, changes in weather and climate patterns, and other 

factors for crop failure. Using these tools, lay farmers will be 

able forecast market demand for their produce and predict 

the expected crop yield, as well as provide better service to 

their suppliers, distributors and downstream customers. 

8.3 Retail, banking and finance 

Another important area of significant impact of automated 

data processing techniques is retail, banking and finance. 

Here, AI -enabled technologies have simplified and auto -

mated important business processes (e.g., processing and 

analysis of financial data, management related tasks, analy -

sis of documents to aid compliance with regulatory require -

ments). They offer a cheap and an effective means to dis -

cover important business patterns, trends and opportunities 

while simultaneously providing useful and actionable di -

rections to responsible personnel to respond appropriately. 

With the help of automated data processing techniques 

companies are able to focus on business problems instead 

of on data processing and analysis, thereby increasing their 

productivity and profitability. By processing customer data 

in advanced analytics engines, companies are also able to 28 

predict customer behavior and preference, and thus provide 

tailored products and services to better address their needs. 

Generative AI tools developed on the basis of business -

specific data also enhance customer experience by automat -

ing customer engagement tasks in the form of advanced 

virtual assistants that can provide context -specific informa -

tion and personalized financial recommendations to clients 

in real -time. 

8.4 Manufacturing 

In manufacturing, automated AI tools can also help to 

streamline overall factory operations as well as automate, 

optimize and manage energy consumption, logistics, and 

production. Data gathered from IoT devices and sensors 

can be analyzed by automated machine learning models 

to provide useful information about potential hazards and 

recommend preventive actions. Generative AI techniques 

allows manufacturing companies to design new products 

simply by specifying a set of desired attributes. This partic -

ular capability has a huge potential in manufacturing as it 

can lead to innovative product concepts and products with 

qualitatively better characteristics than their counterparts 

designed by traditional methods. For instance, such prod -

ucts may be more efficient, cheaper and environmentally -

friendly. 

## 9 DISCUSSIONS 

In the era of big data, the volume and complexity of data 

that machine learning systems typically handle have in -

creased substantially. Consequently, collecting and process -

ing the data into a form that is suitable for machine learn -

ing tasks is a challenging undertaking. Approaches based 

on conventional machine learning concepts are extremely 

laborious and require enormous development time. Auto -

mated processing methods, especially approaches based on 

AutoML, have greatly automated these laborious processes 

and, thus, have simplified and accelerated the development 

cycle of deep learning models. As the size data and the 

complexity of machine learning problems increase, such 

approaches are expected to become a general practice, es -

pecially for generic machine learning tasks such as Big Data 

analytics and data visualization. 

9.1 Main challenges 

We discuss some of the most important challenges modern 

automated data processing systems have to deal with. 

9.1.1  Growing volume and complexity of data 

While more and more data processing tasks are becoming 

increasingly automated, today’s AutoML methods are still 

limited in terms of the complexity of data and tasks that 

can be handled. Handling very complex data still requires 

the intervention of human developers at some stages in the 

development process. While many of the recent works have 

focused primarily on reducing the amount of computational 

resources needed to implement automated data processing 

solutions, the techniques continue to evolve steadily and 

many new workarounds are expected be introduced in the 

foreseeable future to incrementally extend the scope of tasks 

that can be automated. Future AutoML systems will have 

the ability to create end solutions for complex problems 

by automating the full range of tasks from input data 

acquisition to model construction and validation. 

Since many data processing tools are intended for use 

by different types of users and the resulting models typically 

need to meet several performance objectives simultaneously, 

albeit within an acceptable cost, it is often challenging to 

ensure that the right balance of performance and complexity 

is achieved. To address this issue, it is possible to develop a 

flexible data processing scheme which can be tweaked per 

the specific user or application requirements. For instance, 

Tsamardinos et al. in [57] devised different model configu -

ration options, with each configuration prioritizing a spe -

cific machine learning objective: interpretability, predictive 

performance, minimization of data size. These configura -

tion settings allow users to customize model performance 

according to their priorities. This approach is used by data 

processing engines in many general -purpose automated ma -

chine learning systems that handle large -scale data. How -

ever, these state -of -the -art approaches still require users to 

have an understanding of the underlying requirements and 

to manually select the best settings for the particular task. 

Future work could employ context knowledge to automate 

this process, too. 

9.1.2  Complexity of data processing problems 

The application of machine learning in more diverse ap -

plication areas, coupled with the rapidly increasing com -

plexity of data pipelines has made it challenging to solve 

many modern problems using automated machine learning. 

Preprocessing tasks such as labeling and categorical data 

annotation are particularly difficult to automate. Because of 

these challenges, in practical applications, AutoML systems 

usually automate only some data processing tasks in the ma -

chine learning pipeline and provide baseline results that can 

reveal further insights into possible avenues for improve -

ment by manual means. In the near future, the introduction 

of a wide variety of new data processing techniques and 

algorithms for their optimization will provide a means for 

solving complex problems whose solutions are unattainable 

at present. 

9.1.3  Context -awareness 

Approaches based of state -of -the -art AutoML often generate 

a large pool of intermediate features and attributes in the 

search space. The performance of the end model ultimately, 

to a large extent, depends on the search space. Because the 

generation process is inherently “blind”, generated features 

might not be informative or relevant to the target task or 

could introduce redundant information. This could, even 

with good optimization methods, lead to poor performance. 

9.1.4  Overly conservative results 

To avoid making bizarre errors, generative models are 

typically designed to be very conservative and tend to 

produce outputs that align well within the distribution of 

the encoded training data, thus avoiding extreme cases 

and outliers. In many real -world domains, however, data 

is often not clean. Consequently, the output data of gener -

ative models often lack the wide variability of real -world 29 

data. For instance, large language models generally fail 

to produce context -specific responses and resort to overly 

generic answers. Training with such data may result in less -

than -acceptable outcomes in some cases. Moreover, because 

of the enormous sizes, complexity and opacity of gener -

ative AI -based automated data processing and synthesis 

techniques such as diffusion models and LLMs, they can 

sometimes produce unreliable and potentially harmful data 

that may be difficult to detect. The approaches are also prone 

to the so -called hallucination effect [343]. These problems 

severely limit the utility of generative AI models in high -

stakes tasks, especially in application domains such as fi -

nance, security and healthcare. 

9.1.5  Adaptability and transferability 

Because the processing procedures are not based on intu -

itive and well -grounded mechanisms, data generated by 

automated techniques might be specifically optimized for 

a singlr task or a narrow set of tasks. This may produce 

good results for the specific models and datasets they have 

been tuned for but fail to generalize well in unseen data 

and new problem domains. The problem of transferablility 

and adaptability of automatically generated data has not 

been investigated in the literature. Also, as data in many 

application settings (e.g., data about pandemics, economic 

or market conditions) evolve over time, automated process -

ing methods need to be able to adapt accordingly. Moreover, 

it is often useful to have a single solution that can address 

a set of related problems in a broad application domain. 

Current approaches are not able to meet this need. 

9.1.6  Balancing multiple requirements and trade -offs 

Modern automated data processing problems often involve 

very large datasets with complex relationships and interac -

tions. At the same time, models that are trained on them 

are required to meet multiple performance requirements. 

In particular, there is the need to navigate delicate trade -

offs among different aspects of performance, such as re -

ducing data size versus retaining important information, 

achieving higher predictive accuracy versus interpretability, 

robustness and algorithmic fairness. Methods that that en -

hance performance in one aspect might harm performance 

in another. These complex trade -offs make it difficult for 

automated machine learning algorithms to strike the right 

balance independent of human input, as users may pri -

oritize different aspects of performance according to their 

specific goals. 

9.1.7  Reliability and trust 

With the widespread adoption of AI in diverse areas, their 

reliability – and consequently trust in their solutions –has 

become an important issue. This is particularly more serious 

with automated learning methods since they rely on black -

box approaches automated hyperparameter optimization 

and there is very little human supervision of the overall 

learning process. The automated optimization of hyperpa -

rameters might result in complex settings and potentially 

harmful interactions that are difficult to predict or diagnose 

at design time but may manifest later under deployment. 

Moreover, with these black box methods, it is challenging 

to guarantee that the end systems will perform as intended 

and align solutions with broader user objectives while re -

specting societal values and expectations – i.e., make accu -

rate inferences while at the same time being constrained by 

wider issues such as safety, privacy and ethics. It is currently 

not possible to incorporate these high -level concepts in the 

automated learning process. 

9.1.8  Lack of comprehensive valuation metrics for some 

processing tasks 

Unlike many machine learning tasks where standard model 

configurations, datasets and evaluation metrics have been 

created to test different methods, for many data processing 

tasks, especially preprocessing, there is a general lack of 

standardized settings and metrics that can be universally 

applied across different tasks and datasets. This leads to 

researchers using varying settings and metrics, thereby 

making direct comparison of methods very challenging. 

9.1.9  Scalability 

Scalability is another major limitation of automated data 

processing methods. White these techniques work well for 

large -scale problems involving huge datasets, they typically 

perform very poorly on small data; training effective when 

large datasets used. In situations where data is scarce, 

traditional processing techniques often outperform auto -

mated methods. Furthermore, automated processing meth -

ods are inherently computationally expensive. Therefore, 

while automated data processing approaches are promising, 

the choice between automated methods and traditional ap -

proaches ultimately depends on factors such as dataset size, 

problem domain, and available computational resources. 

9.1.10  Limited scope of application 

Existing AutoML tools are limited in terms of the range of 

tasks and data types they support. Presently, most generic 

solutions work best for tabular data. Also, most automated 

data processing techniques support classification and re -

gression problems, while offering little in problem domains 

such as natural language processing and time series fore -

casting. 

9.2 Future prospects 

Since the concept of automated data processing based on 

AutoML methods is fairly new, one would assume that we 

are just at the initial stage of realizing the vast potential 

this approach presents. At the same time, it is important to 

exercise caution when making assessments about its future 

possibilities as undue expectations can ultimately lead to 

disappointment, which can potentially result in “automated 

data processing winter” [344]. Also, because automated data 

acquisition and processing methods are characteristically re -

source intensive, their potential will depend on the progress 

in other areas such as computer hardware technology and 

the development and extensive use of cloud computing 

infrastructure. The main prospects for the foreseeable future 

include the following. 30 

9.2.1  Support for higher -level functions 

New data preprocessing algorithms that also perform qual -

ity control and budget management functions taking into 

consideration the specific application requirements as well 

as the objectives and priorities of the end user will be devel -

oped. This will involve more advanced user interface (UI) 

and user experience (UX) solutions with intuitive features 

that support high -level semantic interaction. 

9.2.2  Extension of generic AutoML methods to handle 

more complex problems and diverse data types 

While specialized automated data processing methods have 

partially tackled a variety of data types and machine learn -

ing problems, presently, generic AutoML solutions are de -

signed primarily to work with tabular data; their support 

for other data types such as images, audio, video and point 

clouds is very limited. Also, they mostly solve general clas -

sification and regression tasks, and poorly handle data pro -

cessing for applications in time series and natural language 

processing domains. In the near future, new approaches 

that extend the scope of automated data processing meth -

ods, especially those based on AutoML frameworks, to a

wider problem domain, including time series and natural 

language processing, are envisaged. 

9.2.3  Advanced human -in -the -loop (HITL) automated data 

processing 

Future research will concentrate on developing methods to 

provide user -friendly interfaces and more intuitive explana -

tions for human actors based on techniques such as feature 

importance visualization, counterfactual explanations, and 

attention mechanisms. In turn, researchers will develop 

machine learning algorithms that can learn from human 

feedback through, for example, explicit labeling and rein -

forcement signals. Together, these two sets of approaches 

can be leveraged to provide synergy between humans and 

machines. This will ultimately lead to improved perfor -

mance, reliability, robustness, and the ability to tackle more 

challenging problems that are beyond the capability of 

either humans or machines when working independently. 

It is conceivable that more advanced HITL systems will 

enable support for interaction of multiple users or user -

expert groups with automated machine learning systems. 

Businesses, for example, will be able to model high -level in -

formation such as broad organizational goals and priorities, 

available budgetary resources, and the overall bottom line. 

Systems based on this approach will also align better with 

broader human needs, values, and expectations. 

Humans in the machine learning loop will be particu -

larly useful for more complex data processing tasks such as 

categorical encoding and data annotation. Humans will also 

play a vital role in model validation and refinement. as well 

as the incorporation of high -level concepts such as 

9.2.4  New and dedicated infrastructure for automated data 

processing 

Dedicated tools that specifically cater to the needs of big 

and complex data problems will be developed. These will 

be specialized platforms (e.g., cloud -based infrastructure), 

tools (e.g., libraries and open -source software), and frame -

works (e.g., generic data processing models) designed to 

support and streamline different aspects of automated data 

processing. This infrastructure will simplify various pro -

cessing tasks and make it easier for both data scientists and 

non -experts to effectively use automated machine learning 

models for data processing. This type of infrastructure will 

seamlessly provide needed functions while taking care of 

contemporary challenges such as privacy, transparency, and 

data security. 

9.2.5  Progressive self -refinement of synthetic data 

It is important to note that while automated data processing 

models can perform complex data processing and can even 

synthesize relevant data automatically, paradoxically, their 

training requires large volumes of high -quality data that 

correctly captures the distribution of data in the target 

domains. For many domains data are not often available 

naturally. However, it is conceivable that in the future, 

in cases where domain -relevant data may be inaccessible, 

more generic generative AI models could be used to synthe -

size rudimentary data and then perform progressive self -

refinement by continually generating, filtering and reusing 

the generated data for subsequent retraining and fine -tuning 

until an acceptable quality of data is attained. This approach 

is already being studied [345], and preliminary results are 

promising. 

9.2.6  Explainable data synthesis by generative AI tech -

niques 

In the near future, researchers will focus on developing 

models that not only deliver high performance but also pro -

vide insights for their actions. Explainable AutoML frame -

works designed for data processing will be able to perform 

data processing tasks in an end -to -end machine learning 

pipeline while providing explanations about the intermedi -

ate processes and resulting outcomes. This approach will be 

particularly useful for complex data processing tasks such 

as categorical encoding and data annotation. Explainability 

will also play a vital role in allowing researchers to better 

evaluate and validate results, as well as to perform further 

refinements. 

Generative AI techniques will be particularly useful in 

solving the problem of opacity associated with the data 

generation process of traditional AutoML methods. For 

instance, the massive volume of knowledge embedded in 

LLMs and diffusion models can be leveraged to explain 

the underlying AutoML model’s decisions regarding gener -

ated data. Specifically, these models will be able to create 

additional metadata about the generated data, including 

information about the relationships among different data 

elements and attributes. In addition, intuitive user inter -

faces and high -level interaction mechanisms will allow de -

velopers to incorporate human -understandable information 

about the target tasks for which the dataset is to be gener -

ated. This can help to mitigate inadequacies and potential 

flaws in the data generated by today’s generative AI meth -

ods. 

## 10  CONCLUSION 

The importance of automated data processing has increased 

remarkably in the last few years. This is largely driven by 31 

the increasing demand for machine learning solutions in 

many areas, coupled with the large volumes of data that 

need to be processed for these machine learning tasks. 

In this work, we survey state -of -the -art approaches for 

automating data processing for deep learning and big data 

tasks. We first present methods for realizing individual 

data processing solutions. These include data preprocess -

ing (e.g., data cleaning, imputation, labeling, categorical 

encoding, etc.), data augmentation and feature engineering 

(specifically, feature extraction, construction and selection). 

We also discuss approaches to implementing all processing 

steps holistically within a single end -to -end deep learning 

framework. We summarized the main characteristics and 

functions of generic AutoML frameworks designed for big 

data applications. Furthermore, we discuss future develop -

ments that are likely to have a significant impact on the 

success of automated data processing. 

The survey shows that while many data processing tasks 

can already be seamlessly automated in state -of -the -art au -

tomated machine learning pipelines, challenges still remain 

regarding the full automation certain tasks. The need to 

address a wide scope of problems makes it particularly dif -

ficult for machine learning systems to incorporate effective 

search mechanisms that allow relevant data to be collected 

and exploited in a context -dependent manner. 

## ACKNOWLEDGMENTS 

The authors would like to thank... 

## REFERENCES 

[1]  A. L’heureux, K. Grolinger, H. F. Elyamany, and M. A. Capretz, 

“Machine learning with big data: Challenges and approaches,” 

Ieee Access , vol. 5, pp. 7776 –7797, 2017. 

[2]  M. M. Najafabadi, F. Villanustre, T. M. Khoshgoftaar, N. Seliya, 

R. Wald, and E. Muharemagic, “Deep learning applications and 

challenges in big data analytics,” Journal of big data , vol. 2, no. 1, 

pp. 1 –21, 2015. 

[3]  P. Rouzrokh, B. Khosravi, S. Faghani, M. Moassefi, D. V. Vera Gar -

cia, Y. Singh, K. Zhang, G. M. Conte, and B. J. Erickson, “Mit -

igating bias in radiology machine learning: 1. data handling,” 

Radiology: Artificial Intelligence , vol. 4, no. 5, p. e210290, 2022. 

[4]  S. Carta, A. S. Podda, D. R. Recupero, and R. Saia, “A local feature 

engineering strategy to improve network anomaly detection,” 

Future Internet , vol. 12, no. 10, p. 177, 2020. 

[5]  S. Lu, X. Wei, Y. Li, and L. Wang, “Detecting anomaly in big data 

system logs using convolutional neural network,” in 2018 IEEE 

16th Intl Conf on Dependable, Autonomic and Secure Computing, 

16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl 

Conf on Big Data Intelligence and Computing and Cyber Science and 

Technology Congress (DASC/PiCom/DataCom/CyberSciTech) . IEEE, 

2018, pp. 151 –158. 

[6]  S. García, S. Ramírez -Gallego, J. Luengo, J. M. Benítez, and 

F. Herrera, “Big data preprocessing: methods and prospects,” Big 

Data Analytics , vol. 1, no. 1, pp. 1 –22, 2016. 

[7]  J. Waring, C. Lindvall, and R. Umeton, “Automated machine 

learning: Review of the state -of -the -art and opportunities for 

healthcare,” Artificial intelligence in medicine , vol. 104, p. 101822, 

2020. 

[8]  G. Liu, D. Lu, and J. Lu, “Pharm -automl: An open -source, end -

to -end automated machine learning package for clinical outcome 

prediction,” CPT: pharmacometrics & systems pharmacology , vol. 10, 

no. 5, pp. 478 –488, 2021. 

[9]  J. Drozdal, J. Weisz, D. Wang, G. Dass, B. Yao, C. Zhao, M. Muller, 

L. Ju, and H. Su, “Trust in automl: exploring information needs 

for establishing trust in automated machine learning systems,” 

in Proceedings of the 25th International Conference on Intelligent User 

Interfaces , 2020, pp. 297 –307. 

[10]  Z. Li, H. Guo, W. M. Wang, Y. Guan, A. V. Barenji, G. Q. Huang, 

K. S. McFall, and X. Chen, “A blockchain and automl approach 

for open and automated customer service,” IEEE Transactions on 

Industrial Informatics , vol. 15, no. 6, pp. 3642 –3651, 2019. 

[11]  A. Crisan and B. Fiore -Gartland, “Fits and starts: Enterprise use 

of automl and the role of humans in the loop,” in Proceedings of 

the 2021 CHI Conference on Human Factors in Computing Systems ,

2021, pp. 1–15. 

[12]  X. He, K. Zhao, and X. Chu, “Automl: A survey of the state -of -

the -art,” Knowledge -Based Systems , vol. 212, p. 106622, 2021. 

[13]  M. -A. Zöller and M. F. Huber, “Benchmark and survey of auto -

mated machine learning frameworks,” Journal of artificial intelli -

gence research , vol. 70, pp. 409 –472, 2021. 

[14]  K. Blom, A. Serban, H. Hoos, and J. Visser, “Automl adoption in 

ml software,” in 8th ICML Workshop on automated machine learning ,

2021. 

[15]  U. Gain and V. Hotti, “Low -code automl -augmented data 

pipeline –a review and experiments,” in Journal of Physics: Con -

ference Series , vol. 1828, no. 1. IOP Publishing, 2021, p. 012015. 

[16]  A. Alsharef, K. Aggarwal, M. Kumar, A. Mishra et al. , “Review 

of ml and automl solutions to forecast time -series data,” Archives 

of Computational Methods in Engineering , vol. 29, no. 7, pp. 5297 –

5311, 2022. 

[17]  C. Shorten and T. M. Khoshgoftaar, “A survey on image data 

augmentation for deep learning,” Journal of big data , vol. 6, no. 1, 

pp. 1 –48, 2019. 

[18]  S. Y. Feng, V. Gangal, J. Wei, S. Chandar, S. Vosoughi, T. Mita -

mura, and E. Hovy, “A survey of data augmentation approaches 

for nlp,” arXiv preprint arXiv:2105.03075 , 2021. 

[19]  Q. Wen, L. Sun, F. Yang, X. Song, J. Gao, X. Wang, and H. Xu, 

“Time series data augmentation for deep learning: A survey,” 

arXiv preprint arXiv:2002.12478 , 2020. 

[20]  V. Jane et al. , “Survey on iot data preprocessing,” Turkish Journal of 

Computer and Mathematics Education (TURCOMAT) , vol. 12, no. 9, 

pp. 238 –244, 2021. 

[21]  S. Ramrez -Gallego, B. Krawczyk, S. Garca, M. Woniak, and 

F. Herrera, “A survey on data preprocessing for data stream 

mining,” Neurocomputing , vol. 239, no. C, pp. 39 –57, 2017. 

[22]  S. Khalid, T. Khalil, and S. Nasreen, “A survey of feature selection 

and feature extraction techniques in machine learning,” in 2014 

science and information conference . IEEE, 2014, pp. 372 –378. 

[23]  N. Pudjihartono, T. Fadason, A. W. Kempa -Liehr, and J. M. 

O’Sullivan, “A review of feature selection methods for machine 

learning -based disease risk prediction,” Frontiers in Bioinformatics ,

vol. 2, p. 927312, 2022. 

[24]  S. Meisenbacher, M. Turowski, K. Phipps, M. Rätz, D. Müller, 

V. Hagenmeyer, and R. Mikut, “Review of automated time series 

forecasting pipelines,” Wiley Interdisciplinary Reviews: Data Min -

ing and Knowledge Discovery , vol. 12, no. 6, p. e1475, 2022. 

[25]  N. K. Chauhan and K. Singh, “A review on conventional machine 

learning vs deep learning,” in 2018 International conference on 

computing, power and communication technologies (GUCON) . IEEE, 

2018, pp. 347 –352. 

[26]  S. Marsland, Machine learning: an algorithmic perspective . Chap -

man and Hall/CRC, 2011. 

[27]  Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature , vol. 

521, no. 7553, pp. 436 –444, 2015. 

[28]  L. Deng, D. Yu et al. , “Deep learning: methods and applications,” 

Foundations and trends® in signal processing , vol. 7, no. 3 –4, pp. 

197 –387, 2014. 

[29]  L. R. Koetzier, D. Mastrodicasa, T. P. Szczykutowicz, N. R. 

van der Werf, A. S. Wang, V. Sandfort, A. J. van der Molen, 

D. Fleischmann, and M. J. Willemink, “Deep learning image 

reconstruction for ct: technical principles and clinical prospects,” 

Radiology , vol. 306, no. 3, p. e221257, 2023. 

[30]  L. Budach, M. Feuerpfeil, N. Ihde, A. Nathansen, N. Noack, 

H. Patzlaff, F. Naumann, and H. Harmouch, “The effects of 

data quality on machine learning performance,” arXiv preprint 

arXiv:2207.14529 , 2022. 

[31]  M. O. Ojo and A. Zahid, “Improving deep learning classifiers 

performance via preprocessing and class imbalance approaches 

in a plant disease detection pipeline,” Agronomy , vol. 13, no. 3, p. 

887, 2023. 

[32]  K. J. Kubota, J. A. Chen, and M. A. Little, “Machine learning 

for large -scale wearable sensor data in parkinson’s disease: Con -

cepts, promises, pitfalls, and futures,” Movement disorders , vol. 31, 

no. 9, pp. 1314 –1326, 2016. 32 

[33]  V. Iosifidis and E. Ntoutsi, “Dealing with bias via data augmen -

tation in supervised learning scenarios,” Jo Bates Paul D. Clough 

Robert Jäschke , vol. 24, no. 11, 2018. 

[34]  Y. Shi, T. ValizadehAslani, J. Wang, P. Ren, Y. Zhang, M. Hu, 

L. Zhao, and H. Liang, “Improving imbalanced learning by pre -

finetuning with data augmentation,” in Fourth International Work -

shop on Learning with Imbalanced Domains: Theory and Applications .

PMLR, 2022, pp. 68 –82. 

[35]  Q. Wang, F. Meng, and T. P. Breckon, “Data augmentation with 

norm -ae and selective pseudo -labelling for unsupervised domain 

adaptation,” Neural Networks , vol. 161, pp. 614 –625, 2023. 

[36]  Y. Bengio, A. Courville, and P. Vincent, “Representation learning: 

A review and new perspectives,” IEEE transactions on pattern 

analysis and machine intelligence , vol. 35, no. 8, pp. 1798 –1828, 2013. 

[37]  M. Cerrada, L. Trujillo, D. E. Hernández, H. A. Correa Zevallos, 

J. C. Macancela, D. Cabrera, and R. Vinicio Sánchez, “Automl 

for feature selection and model tuning applied to fault severity 

diagnosis in spur gearboxes,” Mathematical and Computational 

Applications , vol. 27, no. 1, p. 6, 2022. 

[38]  S. Chang, C. Wang, and C. Wang, “Automated feature engineer -

ing for fraud prediction in online credit loan services,” in 2022 

13th Asian Control Conference (ASCC) . IEEE, 2022, pp. 738 –743. 

[39]  Q. Liu, Z. Liu, H. Zhang, Y. Chen, and J. Zhu, “Mining cross 

features for financial credit risk assessment,” in Proceedings of 

the 30th ACM International Conference on Information & Knowledge 

Management , 2021, pp. 1069 –1078. 

[40]  Q. Shi, Y. -L. Zhang, L. Li, X. Yang, M. Li, and J. Zhou, “Safe: 

Scalable automatic feature engineering framework for industrial 

tasks,” in 2020 IEEE 36th International Conference on Data Engineer -

ing (ICDE) . IEEE, 2020, pp. 1645 –1656. 

[41]  A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, 

“Cnn features off -the -shelf: an astounding baseline for recogni -

tion,” in Proceedings of the IEEE conference on computer vision and 

pattern recognition workshops , 2014, pp. 806 –813. 

[42]  M. A. Hall, “Correlation -based feature selection for machine 

learning,” Ph.D. dissertation, The University of Waikato, 1999. 

[43]  Z. Che, S. Purushotham, R. Khemani, and Y. Liu, “Distilling 

knowledge from deep networks with applications to healthcare 

domain,” arXiv preprint arXiv:1512.03542 , 2015. 

[44]  D. Dai, T. Xu, X. Wei, G. Ding, Y. Xu, J. Zhang, and H. Zhang, 

“Using machine learning and feature engineering to characterize 

limited material datasets of high -entropy alloys,” Computational 

Materials Science , vol. 175, p. 109618, 2020. 

[45]  M. Jaderberg, K. Simonyan, A. Zisserman et al. , “Spatial trans -

former networks,” Advances in neural information processing sys -

tems , vol. 28, 2015. 

[46]  S. Mounsaveng, I. Laradji, I. Ben Ayed, D. Vazquez, and M. Ped -

ersoli, “Learning data augmentation with online bilevel opti -

mization for image classification,” in Proceedings of the IEEE/CVF 

Winter Conference on Applications of Computer Vision , 2021, pp. 

1691 –1700. 

[47]  H. Luo, W. Jiang, X. Fan, and C. Zhang, “Stnreid: Deep convolu -

tional networks with pairwise spatial transformer networks for 

partial person re -identification,” IEEE Transactions on Multimedia ,

vol. 22, no. 11, pp. 2905 –2913, 2020. 

[48]  H. T. Vu and C. -C. Huang, “A multi -task convolutional neural 

network with spatial transform for parking space detection,” 

in 2017 IEEE International Conference on Image Processing (ICIP) .

IEEE, 2017, pp. 1762 –1766. 

[49]  A. Karargyris, “Color space transformation network,” arXiv 

preprint arXiv:1511.01064 , 2015. 

[50]  P. Tarasiuk and M. Pryczek, “Geometric transformations em -

bedded into convolutional neural networks,” Journal of Applied 

Computer Science, Vol. 24, No. 3, Wydawnictwo Politechniki Łódzkiej, 

Łódz´ 2016, ISSN 1507 -0360. , 2016. 

[51]  Y. Xu, Z. Zhang, L. You, J. Liu, Z. Fan, and X. Zhou, “scigans: 

single -cell rna -seq imputation using generative adversarial net -

works,” Nucleic acids research , vol. 48, no. 15, pp. e85 –e85, 2020. 

[52]  A. Nazabal, P. M. Olmos, Z. Ghahramani, and I. Valera, “Han -

dling incomplete heterogeneous data using vaes,” Pattern Recog -

nition , vol. 107, p. 107501, 2020. 

[53]  J. Liu, F. Zhu, C. Chai, Y. Luo, and N. Tang, “Automatic data ac -

quisition for deep learning,” Proceedings of the VLDB Endowment ,

vol. 14, no. 12, pp. 2739 –2742, 2021. 

[54]  L. Santos and L. Ferreira, “Atlantic -automated data preprocess -

ing framework for supervised machine learning,” Software Im -

pacts , vol. 17, p. 100532, 2023. 

[55]  A. Karras, C. Karras, N. Schizas, M. Avlonitis, and S. Sioutas, 

“Automl with bayesian optimizations for big data management,” 

Information , vol. 14, no. 4, p. 223, 2023. 

[56]  H. Jin, F. Chollet, Q. Song, and X. Hu, “Autokeras: An automl 

library for deep learning,” Journal of Machine Learning Research ,

vol. 24, no. 6, pp. 1 –6, 2023. 

[57]  I. Tsamardinos, P. Charonyktakis, G. Papoutsoglou, G. Bor -

boudakis, K. Lakiotaki, J. C. Zenklusen, H. Juhl, E. Chatzaki, 

and V. Lagani, “Just add data: automated predictive modeling 

for knowledge discovery and feature selection,” NPJ precision 

oncology , vol. 6, no. 1, pp. 1 –17, 2022. 

[58]  M. Francia, J. Giovanelli, and G. Pisano, “Hamlet: A framework 

for human -centered automl via structured argumentation,” Fu -

ture Generation Computer Systems , 2022. 

[59]  X. Chen, Q. Lin, C. Luo, X. Li, H. Zhang, Y. Xu, Y. Dang, 

K. Sui, X. Zhang, B. Qiao et al. , “Neural feature search: A neural 

architecture for automated feature engineering,” in 2019 IEEE 

International Conference on Data Mining (ICDM) . IEEE, 2019, pp. 

71 –80. 

[60]  T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: 

A survey,” The Journal of Machine Learning Research , vol. 20, no. 1, 

pp. 1997 –2017, 2019. 

[61]  Z. Y. -C. Liu, S. Roychowdhury, S. Tarlow, A. Nair, S. Badhe, 

and T. Shah, “Autodc: Automated data -centric processing,” arXiv 

preprint arXiv:2111.12548 , 2021. 

[62]  M. K. Shende, A. E. Feijoo -Lorenzo, and N. D. Bokde, “cleants: 

Automated (automl) tool to clean univariate time series at mi -

croscales,” Neurocomputing , 2022. 

[63]  C. Wang, S. Jin, Y. Guan, W. Liu, C. Qian, P. Luo, and W. Ouyang, 

“Pseudo -labeled auto -curriculum learning for semi -supervised 

keypoint localization,” arXiv preprint arXiv:2201.08613 , 2022. 

[64]  Y. Liu, X. Zhang, J. Kauttonen, and G. Zhao, “Uncertain label 

correction via auxiliary action unit graphs for facial expression 

recognition,” arXiv preprint arXiv:2204.11053 , 2022. 

[65]  J. T. Hancock and T. M. Khoshgoftaar, “Survey on categorical 

data for neural networks,” Journal of Big Data , vol. 7, no. 1, pp. 1–

41, 2020. 

[66]  K. Mallikharjuna Rao, G. Saikrishna, and K. Supriya, “Data pre -

processing techniques: emergence and selection towards machine 

learning models -a practical review using hpa dataset,” Multime -

dia Tools and Applications , pp. 1 –20, 2023. 

[67]  P. Cerda and G. Varoquaux, “Encoding high -cardinality string 

categorical variables,” IEEE Transactions on Knowledge and Data 

Engineering , vol. 34, no. 3, pp. 1164 –1176, 2020. 

[68]  N. J. Teague, “Missing data infill with automunge,” arXiv preprint 

arXiv:2202.09484 , 2022. 

[69]  S. -H. Jung, H. -S. Lee, J. -Y. Kim, and C. -B. Sim, “A novel on auto 

imputation and analysis prediction model of data missing scope 

based on machine learning,” Journal of Korea Multimedia Society ,

vol. 25, no. 2, pp. 257 –268, 2022. 

[70]  N. J. Teague, “Tabular engineering with automunge.” 

[71]  X. Chu, I. F. Ilyas, S. Krishnan, and J. Wang, “Data cleaning: 

Overview and emerging challenges,” in Proceedings of the 2016 

international conference on management of data , 2016, pp. 2201 –2206. 

[72]  T. N. Minh, M. Sinn, H. T. Lam, and M. Wistuba, “Automated 

image data preprocessing with deep reinforcement learning,” 

arXiv preprint arXiv:1806.05886 , 2018. 

[73]  W. Liang, G. A. Tadesse, D. Ho, L. Fei -Fei, M. Zaharia, C. Zhang, 

and J. Zou, “Advances, challenges and opportunities in creating 

data for trustworthy ai,” Nature Machine Intelligence , vol. 4, no. 8, 

pp. 669 –677, 2022. 

[74]  S. Krishnan, J. Wang, E. Wu, M. J. Franklin, and K. Goldberg, 

“Activeclean: Interactive data cleaning for statistical modeling,” 

Proceedings of the VLDB Endowment , vol. 9, no. 12, pp. 948 –959, 

2016. 

[75]  N. Erickson, J. Mueller, A. Shirkov, H. Zhang, P. Larroy, M. Li, 

and A. Smola, “Autogluon -tabular: Robust and accurate automl 

for structured data,” arXiv preprint arXiv:2003.06505 , 2020. 

[76]  M. Lam, S. Awasthi, H. J. Watson, J. Goldstein, G. Pana -

giotaropoulou, V. Trubetskoy, R. Karlsson, O. Frei, C. -C. Fan, 

W. De Witte et al. , “Ricopili: rapid imputation for consortias 

pipeline,” Bioinformatics , vol. 36, no. 3, pp. 930 –933, 2020. 

[77]  J. Barnard and X. -L. Meng, “Applications of multiple imputation 

in medical studies: from aids to nhanes,” Statistical methods in 

medical research , vol. 8, no. 1, pp. 17 –36, 1999. 

[78]  M. J. Puma, R. B. Olsen, S. H. Bell, and C. Price, “What to do when 

data are missing in group randomized controlled trials. ncee 33 

2009 -0049.” National Center for Education Evaluation and Regional  [102]  M. K. Dahouda and I. Joe, “A deep -learned embedding tech -

Assistance , 2009.  nique for categorical features encoding,” IEEE Access , vol. 9, pp. 

[79] “Pychemflow: an automated pre -processing pipeline in python  114 381 –114 391, 2021. 

for reproducible machine learning on chemical data.”  [103]  P. Li, X. Rao, J. Blase, Y. Zhang, X. Chu, and C. Zhang, “Cleanml: 

[80] M. Bilal, G. Ali, M. W. Iqbal, M. Anwar, M. S. A. Malik, and R. A.  A study for evaluating the impact of data cleaning on ml classi -

Kadir, “Auto -prep: efficient and automated data preprocessing  fication tasks,” in 2021 IEEE 37th International Conference on Data 

pipeline,” IEEE Access , vol. 10, pp. 107 764 –107 784, 2022.  Engineering (ICDE) . IEEE, 2021, pp. 13 –24. 

[81] J. Yoon, J. Jordon, and M. Schaar, “Gain: Missing data imputation  [104]  L. Berti -Equille, “Learn2clean: Optimizing the sequence of tasks 

using generative adversarial nets,” in International conference on  for web data preparation,” in The world wide web conference , 2019, 

machine learning . PMLR, 2018, pp. 5689 –5698.  pp. 2580 –2586. 

[82] L. Gondara and K. Wang, “Multiple imputation using deep de - [105]  S. Krishnan, M. J. Franklin, K. Goldberg, and E. Wu, “Boostclean: 

noising autoencoders,” arXiv preprint arXiv:1705.02737 , vol. 280,  Automated error detection and repair for machine learning,” 

2017.  arXiv preprint arXiv:1711.01299 , 2017. 

[83] Z. Dai, Z. Bu, and Q. Long, “Multiple imputation via genera - [106]  F. Neutatz, B. Chen, Z. Abedjan, and E. Wu, “From cleaning 

tive adversarial network for high -dimensional blockwise missing  before ml to cleaning for ml.” IEEE Data Eng. Bull. , vol. 44, no. 1, 

value problems,” in 2021 20th IEEE International Conference on  pp. 24 –41, 2021. 

Machine Learning and Applications (ICMLA) . IEEE, 2021, pp. 791 – [107]  I. Gemp, G. Theocharous, and M. Ghavamzadeh, “Automated 

798.  data cleansing through meta -learning,” in Twenty -Ninth IAAI 

[84] S. Zhang, J. Chen, J. Chen, X. Chen, and H. Huang, “Data im - Conference , 2017. 

putation in iot using spatio -temporal variational auto -encoder,”  [108]  Z. Miao, Y. Li, and X. Wang, “Rotom: A meta -learned data 

Neurocomputing , vol. 529, pp. 23 –32, 2023.  augmentation framework for entity matching, data cleaning, text 

[85] Q. Ma, X. Li, M. Bai, X. Wang, B. Ning, and G. Li, “Mivae: Mul - classification, and beyond,” in Proceedings of the 2021 International 

tiple imputation based on variational auto -encoder,” Engineering  Conference on Management of Data , 2021, pp. 1303 –1316. 

Applications of Artificial Intelligence , vol. 123, p. 106270, 2023.  [109]  M. M. Morsali, H. Mohammadzade, and S. B. Shouraki, “Face: 

[86] D. -H. Shin, S. -E. Lee, B. -U. Jeon, and K. Chung, “Missing value  Fast, accurate and context -aware audio annotation and classifica -

imputation model based on adversarial autoencoder using spa - tion,” arXiv preprint arXiv:2303.03666 , 2023. 

tiotemporal feature extraction.” Intelligent Automation & Soft Com - [110]  J. Zhang, M. A. Mazurowski, B. C. Allen, and B. Wildman -

puting , vol. 37, no. 2, 2023.  Tobriner, “Multistep automated data labelling procedure (mad -

[87] P. -A. Mattei and J. Frellsen, “Miwae: Deep generative modelling  lap) for thyroid nodules on ultrasound: An artificial intelligence 

and imputation of incomplete data sets,” in International confer - approach for automating image annotation,” Artificial Intelligence 

ence on machine learning . PMLR, 2019, pp. 4413 –4423.  in Medicine , vol. 141, p. 102553, 2023. 

[88] V. Fortuin, D. Baranchuk, G. Rätsch, and S. Mandt, “Gp -vae:  [111]  A. Kaushik, D. Dunham, Z. He, M. Manohar, M. Desai, K. C. 

Deep probabilistic time series imputation,” in International con - Nadeau, and S. Andorf, “Cyanno: a semi -automated approach 

ference on artificial intelligence and statistics . PMLR, 2020, pp.  for cell type annotation of mass cytometry datasets,” Bioinformat -

1651 –1661.  ics , vol. 37, no. 22, pp. 4164 –4171, 2021. 

[89] D. Jarrett, B. C. Cebere, T. Liu, A. Curth, and M. van der Schaar,  [112]  K. G. Ince, A. Koksal, A. Fazla, and A. A. Alatan, “Semi -

“Hyperimpute: Generalized iterative imputation with automatic  automatic annotation for visual object tracking,” in Proceedings 

model selection,” in International Conference on Machine Learning . of the IEEE/CVF international conference on computer vision , 2021, 

PMLR, 2022, pp. 9916 –9937.  pp. 1233 –1239. 

[90] M. Feurer, K. Eggensperger, S. Falkner, M. Lindauer, and F. Hut - [113]  A. Chakravarty, J. Sivaswamy et al. , “An assistive annotation sys -

ter, “Auto -sklearn 2.0: The next generation,” arXiv preprint  tem for retinal images,” in 2015 IEEE 12th International Symposium 

arXiv:2007.04074 , vol. 24, 2020.  on Biomedical Imaging (ISBI) . IEEE, 2015, pp. 1506 –1509. 

[91] O. Kramer, “Scikit -learn,” in Machine learning for evolution strate - [114]  X. Ke, J. Zou, and Y. Niu, “End -to -end automatic image annota -

gies . Springer, 2016, pp. 45 –53.  tion based on deep cnn and multi -label data augmentation,” IEEE 

[92] R. L Esteve, “Databricks,” in The Azure Data Lakehouse Toolkit . Transactions on Multimedia , vol. 21, no. 8, pp. 2093 –2106, 2019. 

Springer, 2022, pp. 83 –139.  [115]  X. Qian, C. Liu, X. Qi, S. -C. Tan, E. Lam, and N. Wong, “Context -

[93] A. Elangovan, J. He, and K. Verspoor, “Memorization vs. general - aware transformer for 3d point cloud automatic annotation,” 

ization: quantifying data leakage in nlp performance evaluation,”  arXiv preprint arXiv:2303.14893 , 2023. 

arXiv preprint arXiv:2102.01818 , 2021.  [116]  A. Ligocki, A. Jelinek, L. Zalud, and E. Rahtu, “Fully automated 

[94] T. T. Khuat, D. J. Kedziora, and B. Gabrys, “The roles and  dcnn -based thermal images annotation using neural network 

modes of human interactions with automated machine learning  pretrained on rgb data,” Sensors , vol. 21, no. 4, p. 1552, 2021. 

systems,” arXiv preprint arXiv:2205.04139 , 2022.  [117]  W. Zhang, H. Hu, H. Hu, and J. Yu, “Automatic image annotation 

[95] B. Wu and C. A. Knoblock, “Maximizing correctness with min - via category labels,” Multimedia Tools and Applications , vol. 79, pp. 

imal user effort to learn data transformations,” in Proceedings of  11 421 –11 435, 2020. 

the 21st International Conference on Intelligent User Interfaces , 2016,  [118]  D. A. Wood, J. Lynch, S. Kafiabadi, E. Guilhem, A. Al Busaidi, 

pp. 375 –384.  A. Montvila, T. Varsavsky, J. Siddiqui, N. Gadapa, M. Townend 

[96] R. S. Olson and J. H. Moore, “Tpot: A tree -based pipeline opti - et al. , “Automated labelling using an attention model for radiol -

mization tool for automating machine learning,” in Workshop on  ogy reports of mri scans (alarm),” in Medical Imaging with Deep 

automatic machine learning . PMLR, 2016, pp. 66 –74.  Learning . PMLR, 2020, pp. 811 –826. 

[97] H. Jin, Q. Song, and X. Hu, “Auto -keras: An efficient neural ar - [119]  X. Chen, B. Mersch, L. Nunes, R. Marcuzzi, I. Vizzo, J. Behley, and 

chitecture search system,” in Proceedings of the 25th ACM SIGKDD  C. Stachniss, “Automatic labeling to generate training data for 

international conference on knowledge discovery & data mining , 2019,  online lidar -based moving object segmentation,” IEEE Robotics 

pp. 1946 –1956.  and Automation Letters , vol. 7, no. 3, pp. 6107 –6114, 2022. 

[98] F. Neutatz, B. Chen, Y. Alkhatib, J. Ye, and Z. Abedjan, “Data  [120]  T. Anwar, “Covid19 diagnosis using automl from 3d ct scans,” in 

cleaning and automl: Would an optimizer choose to clean?”  Proceedings of the IEEE/CVF International Conference on Computer 

Datenbank -Spektrum , pp. 1 –10, 2022.  Vision , 2021, pp. 503 –507. 

[99] M. Bahrami, W. -P. Chen, L. Liu, and M. Prasad, “Bert -sort: A  [121]  M. Guillermo, R. K. Billones, A. Bandala, R. R. Vicerra, 

zero -shot mlm semantic encoder on ordinal features for automl,”  E. Sybingco, E. P. Dadios, and A. Fillone, “Implementation of 

in International Conference on Automated Machine Learning . PMLR,  automated annotation through mask rcnn object detection model 

2022, pp. 11 –1.  in cvat using aws ec2 instance,” in 2020 IEEE region 10 conference 

[100] E. LeDell and S. Poirier, “H2o automl: Scalable automatic ma - (TENCON) . IEEE, 2020, pp. 708 –713. 

chine learning,” in Proceedings of the AutoML Workshop at ICML , [122]  M. Jiu and H. Sahbi, “Context -aware deep kernel networks for 

vol. 2020, 2020.  image annotation,” Neurocomputing , vol. 474, pp. 154 –167, 2022. 

[101] A. Vakhrushev, A. Ryzhkov, M. Savchenko, D. Simakov,  [123]  G. Yang, J. Zhang, Y. Zhang, B. Wu, and Y. Yang, “Probabilistic 

R. Damdinov, and A. Tuzhilin, “Lightautoml: Automl solu - modeling of semantic ambiguity for scene graph generation,” 

tion for a large financial services ecosystem,” arXiv preprint  in Proceedings of the IEEE/CVF Conference on Computer Vision and 

arXiv:2109.01528 , 2021.  Pattern Recognition , 2021, pp. 12 527 –12 536. 34 

[124]  B. Martinez and M. F. Valstar, “Advances, challenges, and oppor -

tunities in automatic facial expression recognition,” Advances in 

face detection and facial image analysis , pp. 63 –100, 2016. 

[125]  K. Goyle, Q. Xie, and V. Goyle, “Dataassist: A machine learn -

ing approach to data cleaning and preparation,” arXiv preprint 

arXiv:2307.07119 , 2023. 

[126]  P. Li, Z. Chen, X. Chu, and K. Rong, “Diffprep: Differentiable data 

preprocessing pipeline search for learning over tabular data,” 

Proceedings of the ACM on Management of Data , vol. 1, no. 2, pp. 1 –

26, 2023. 

[127]  J. A. Valeri, L. R. Soenksen, K. M. Collins, P. Ramesh, G. Cai, 

R. Powers, N. M. Angenent -Mari, D. M. Camacho, F. Wong, 

T. K. Lu et al. , “Bioautomated: An end -to -end automated machine 

learning tool for explanation and design of biological sequences,” 

Cell Systems , vol. 14, no. 6, pp. 525 –542, 2023. 

[128]  S. Krishnan and E. Wu, “Alphaclean: Automatic generation of 

data cleaning pipelines,” arXiv preprint arXiv:1904.11827 , 2019. 

[129]  S. Chen, N. Tang, J. Fan, X. Yan, C. Chai, G. Li, and X. Du, 

“Haipipe: Combining human -generated and machine -generated 

pipelines for data preparation,” Proceedings of the ACM on Man -

agement of Data , vol. 1, no. 1, pp. 1 –26, 2023. 

[130]  Y. -Y. Kim, K. Song, J. Jang, and I. -C. Moon, “Lada: Look -ahead 

data acquisition via augmentation for deep active learning,” 

Advances in Neural Information Processing Systems , vol. 34, pp. 

22 919 –22 930, 2021. 

[131]  D. Sakkos, E. S. Ho, H. P. Shum, and G. Elvin, “Image editing -

based data augmentation for illumination -insensitive 

background subtraction,” Journal of Enterprise Information Man -

agement , no. ahead -of -print, 2020. 

[132]  S. Schelter and J. Stoyanovich, “Taming technical bias in machine 

learning pipelines,” Bulletin of the Technical Committee on Data 

Engineering , vol. 43, no. 4, 2020. 

[133]  R. Jiang and S. Mei, “Polar coordinate convolutional neural 

network: From rotation -invariance to translation -invariance,” in 

2019 IEEE International Conference on Image Processing (ICIP) .

IEEE, 2019, pp. 355 –359. 

[134]  B. Hilprecht, C. Hammacher, E. S. Reis, M. Abdelaal, and C. Bin -

nig, “Diffml: End -to -end differentiable ml pipelines,” in Proceed -

ings of the Seventh Workshop on Data Management for End -to -End 

Machine Learning , 2023, pp. 1–7. 

[135]  A. Mumuni and F. Mumuni, “Data augmentation: A comprehen -

sive survey of modern approaches,” Array , p. 100258, 2022. 

[136]  T. Niu and M. Bansal, “Automatically learning data augmenta -

tion policies for dialogue tasks,” arXiv preprint arXiv:1909.12868 ,

2019. 

[137]  S. Ren, J. Zhang, L. Li, X. Sun, and J. Zhou, “Text autoaugment: 

Learning compositional augmentation policy for text classifica -

tion,” arXiv preprint arXiv:2109.00523 , 2021. 

[138]  E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le, 

“Autoaugment: Learning augmentation strategies from data,” in 

Proceedings of the IEEE/CVF Conference on Computer Vision and 

Pattern Recognition , 2019, pp. 113 –123. 

[139]  R. Hataya, J. Zdenek, K. Yoshizoe, and H. Nakayama, “Faster 

autoaugment: Learning augmentation strategies using backprop -

agation,” in European Conference on Computer Vision . Springer, 

2020, pp. 1–16. 

[140]  S. Lin, T. Yu, R. Feng, X. Li, X. Jin, and Z. Chen, “Local patch 

autoaugment with multi -agent collaboration,” arXiv e -prints , pp. 

arXiv –2103, 2021. 

[141]  E. Cubuk, B. Zoph, J. Shlens, Q. R. Le, and Randaugment, 

“Practical automated data augmentation with a reduced search 

space,” in Proceedings of the IEEE/CVF Conference on Computer 

Vision and Pattern Recognition Workshops (CVPRW) , pp. 3008 –3017. 

[142]  S. Lim, I. Kim, T. Kim, C. Kim, and S. Kim, “Fast autoaugment,” 

Advances in Neural Information Processing Systems , vol. 32, 2019. 

[143]  A. Liu, Z. Huang, Z. Huang, and N. Wang, “Direct differentiable 

augmentation search,” in Proceedings of the IEEE/CVF International 

Conference on Computer Vision , 2021, pp. 12 219 –12 228. 

[144]  P. Chen, S. Liu, H. Zhao, and J. Jia, “Gridmask data augmenta -

tion,” arXiv preprint arXiv:2001.04086 , 2020. 

[145]  Z. Tang, Y. Gao, L. Karlinsky, P. Sattigeri, R. Feris, and D. Metaxas, 

“Onlineaugment: Online data augmentation with less domain 

knowledge,” in European Conference on Computer Vision . Springer, 

2020, pp. 313 –329. 

[146]  Y. Gao, Z. Tang, M. Zhou, and D. Metaxas, “Enabling data diver -

sity: efficient automatic augmentation via regularized adversarial 

training,” in International Conference on Information Processing in 

Medical Imaging . Springer, 2021, pp. 85 –97. 

[147]  H. Miao and L. T. Rahman, “Multi -class traffic sign classification 

using autoaugment and spatial transformer.” 

[148]  A. Zhao, G. Balakrishnan, F. Durand, J. V. Guttag, and A. V. Dalca, 

“Data augmentation using learned transformations for one -shot 

medical image segmentation,” in Proceedings of the IEEE/CVF 

conference on computer vision and pattern recognition , 2019, pp. 

8543 –8553. 

[149]  X. Zhang, Q. Wang, J. Zhang, and Z. Zhong, “Adversarial au -

toaugment,” arXiv preprint arXiv:1912.11188 , 2019. 

[150]  V. Chinbat and S. -H. Bae, “Ga3n: Generative adversarial autoaug -

ment network,” Pattern Recognition , vol. 127, p. 108637, 2022. 

[151]  X. Peng, Z. Tang, F. Yang, R. S. Feris, and D. Metaxas, “Jointly 

optimize data augmentation and network training: Adversarial 

data augmentation in human pose estimation,” in Proceedings of 

the IEEE Conference on Computer Vision and Pattern Recognition ,

2018, pp. 2226 –2234. 

[152]  Y. Liu, W. Tian, and S. Li, “Meta -data augmentation based search 

strategy through generative adversarial network for automl 

model selection,” in Pacific -Asia Conference on Knowledge Discovery 

and Data Mining . Springer, 2021, pp. 312 –324. 

[153]  K. Tian, C. Lin, M. Sun, L. Zhou, J. Yan, and W. Ouyang, “Im -

proving auto -augment via augmentation -wise weight sharing,” 

Advances in Neural Information Processing Systems , vol. 33, pp. 

19 088 –19 098, 2020. 

[154]  C. -T. Chu, M. Rohmatillah, C. -H. Lee, and J. -T. Chien, “Aug -

mentation strategy optimization for language understanding,” 

in ICASSP 2022 -2022 IEEE International Conference on Acoustics, 

Speech and Signal Processing (ICASSP) . IEEE, 2022, pp. 7952 –7956. 

[155]  C. Zhang, X. Li, Z. Zhang, J. Cui, and B. Yang, “Bo -aug: learning 

data augmentation policies via bayesian optimization,” Applied 

Intelligence , pp. 1 –16, 2022. 

[156]  T. -Y. Hu, A. Shrivastava, J. -H. R. Chang, H. Koppula, S. Braun, 

K. Hwang, O. Kalinli, and O. Tuzel, “Sapaugment: Learning a 

sample adaptive policy for data augmentation,” in ICASSP 2021 -

2021 IEEE International Conference on Acoustics, Speech and Signal 

Processing (ICASSP) . IEEE, 2021, pp. 4040 –4044. 

[157]  A. Terauchi and N. Mori, “Evolutionary approach for autoaug -

ment using the thermodynamical genetic algorithm,” in Proceed -

ings of the AAAI Conference on Artificial Intelligence , vol. 35, no. 11, 

2021, pp. 9851 –9858. 

[158]  S. Cheng, Z. Leng, E. D. Cubuk, B. Zoph, C. Bai, J. Ngiam, 

Y. Song, B. Caine, V. Vasudevan, C. Li et al. , “Improving 3d object 

detection through progressive population based augmentation,” 

in European Conference on Computer Vision . Springer, 2020, pp. 

279 –294. 

[159]  D. Ho, E. Liang, X. Chen, I. Stoica, and P. Abbeel, “Popu -

lation based augmentation: Efficient learning of augmentation 

policy schedules,” in International Conference on Machine Learning .

PMLR, 2019, pp. 2731 –2741. 

[160]  T. -H. Cheung and D. -Y. Yeung, “Modals: Modality -agnostic au -

tomated data augmentation in the latent space,” in International 

Conference on Learning Representations , 2020. 

[161]  R. J. Williams, “Simple statistical gradient -following algorithms 

for connectionist reinforcement learning,” Machine learning ,

vol. 8, no. 3, pp. 229 –256, 1992. 

[162]  C. Lin, M. Guo, C. Li, X. Yuan, W. Wu, J. Yan, D. Lin, 

and W. Ouyang, “Online hyper -parameter learning for auto -

augmentation strategy,” in Proceedings of the IEEE/CVF Interna -

tional Conference on Computer Vision , 2019, pp. 6579 –6588. 

[163]  R. Hataya, J. Zdenek, K. Yoshizoe, and H. Nakayama, “Meta 

approach to data augmentation optimization,” in Proceedings of 

the IEEE/CVF Winter Conference on Applications of Computer Vision ,

2022, pp. 2574 –2583. 

[164]  Y. Li, G. Hu, Y. Wang, T. Hospedales, N. M. Robertson, and 

Y. Yang, “Dada: Differentiable automatic data augmentation,” 

arXiv preprint arXiv:2003.03780 , 2020. 

[165]  M. Momeny, A. A. Neshat, A. Gholizadeh, A. Jafarnezhad, 

E. Rahmanzadeh, M. Marhamati, B. Moradi, A. Ghafoorifar, and 

Y. -D. Zhang, “Greedy autoaugment for classification of mycobac -

terium tuberculosis image via generalized deep cnn using mixed 

pooling based on minimum square rough entropy,” Computers in 

Biology and Medicine , vol. 141, p. 105175, 2022. 

[166]  A. Naghizadeh, M. Abavisani, and D. N. Metaxas, “Greedy 

autoaugment,” Pattern Recognition Letters , vol. 138, pp. 624 –630, 

2020. 35 

[167]  T. C. LingChen, A. Khonsari, A. Lashkari, M. R. Nazari, J. S. 

Sambee, and M. A. Nascimento, “Uniformaugment: A search -

free probabilistic data augmentation approach,” arXiv preprint 

arXiv:2003.14348 , 2020. 

[168]  A. Caillon and P. Esling, “Rave: A variational autoencoder for 

fast and high -quality neural audio synthesis,” arXiv preprint 

arXiv:2111.05011 , 2021. 

[169]  N. Park, M. Mohammadi, K. Gorde, S. Jajodia, H. Park, and 

Y. Kim, “Data synthesis based on generative adversarial net -

works,” arXiv preprint arXiv:1806.03384 , 2018. 

[170]  F. -A. Croitoru, V. Hondru, R. T. Ionescu, and M. Shah, “Diffusion 

models in vision: A survey,” IEEE Transactions on Pattern Analysis 

and Machine Intelligence , 2023. 

[171]  J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasude -

van, A. Ku, Y. Yang, B. K. Ayan et al. , “Scaling autoregressive 

models for content -rich text -to -image generation,” arXiv preprint 

arXiv:2206.10789 , vol. 2, no. 3, p. 5, 2022. 

[172]  D. Lee, C. Kim, S. Kim, M. Cho, and W. -S. Han, “Autoregressive 

image generation using residual quantization,” in Proceedings of 

the IEEE/CVF Conference on Computer Vision and Pattern Recogni -

tion , 2022, pp. 11 523 –11 532. 

[173]  Y. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu, H. Chen, L. Yang, 

X. Yi, C. Wang, Y. Wang et al. , “A survey on evaluation of large 

language models,” arXiv preprint arXiv:2307.03109 , 2023. 

[174]  K. M. Yoo, D. Park, J. Kang, S. -W. Lee, and W. Park, “Gpt3mix: 

Leveraging large -scale language models for text augmentation,” 

arXiv preprint arXiv:2104.08826 , 2021. 

[175]  B. Peng, C. Zhu, M. Zeng, and J. Gao, “Data augmentation for 

spoken language understanding via pretrained language mod -

els,” arXiv preprint arXiv:2004.13952 , 2020. 

[176]  J. Kulhánek, V. Hudecek, T. Nekvinda, and O. Dušek, “Augpt: 

Dialogue with pre -trained language models and data augmenta -

tion,” arXiv preprint arXiv:2102.05126 , vol. 26, pp. 532 –535, 2021. 

[177]  T. Schick and H. Schütze, “Generating datasets with pretrained 

language models,” arXiv preprint arXiv:2104.07540 , 2021. 

[178]  N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert -Voss, 

K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson et al. ,

“Extracting training data from large language models,” in 30th 

USENIX Security Symposium (USENIX Security 21) , 2021, pp. 

2633 –2650. 

[179]  A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. 

Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” 

Advances in neural information processing systems , vol. 30, 2017. 

[180]  J. Sohl -Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, 

“Deep unsupervised learning using nonequilibrium thermody -

namics,” in International conference on machine learning . PMLR, 

2015, pp. 2256 –2265. 

[181]  L. Bonifacio, H. Abonizio, M. Fadaee, and R. Nogueira, “Inpars: 

Data augmentation for information retrieval using large language 

models,” arXiv preprint arXiv:2202.05144 , 2022. 

[182]  H. Li, Y. Yang, M. Chang, S. Chen, H. Feng, Z. Xu, Q. Li, and 

Y. Chen, “Srdiff: Single image super -resolution with diffusion 

probabilistic models,” Neurocomputing , vol. 479, pp. 47 –59, 2022. 

[183]  S. Welker, H. N. Chapman, and T. Gerkmann, “Driftrec: Adapting 

diffusion models to blind image restoration tasks,” arXiv preprint 

arXiv:2211.06757 , 2022. 

[184]  G. Kim, T. Kwon, and J. C. Ye, “Diffusionclip: Text -guided diffu -

sion models for robust image manipulation,” in Proceedings of the 

IEEE/CVF Conference on Computer Vision and Pattern Recognition ,

2022, pp. 2426 –2435. 

[185]  OpenAI, “Introducing chatgpt.” 

[186]  H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. -A. Lachaux, 

T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar et al. ,

“Llama: Open and efficient foundation language models,” arXiv 

preprint arXiv:2302.13971 , 2023. 

[187]  J. Devlin, M. -W. Chang, K. Lee, and K. Toutanova, “Bert: Pre -

training of deep bidirectional transformers for language under -

standing,” arXiv preprint arXiv:1810.04805 , 2018. 

[188]  S. Ubani, S. O. Polat, and R. Nielsen, “Zeroshotdataaug: Gener -

ating and augmenting training data with chatgpt,” arXiv preprint 

arXiv:2304.14334 , 2023. 

[189]  A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agar -

wal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al. , “Learning 

transferable visual models from natural language supervision,” 

in International conference on machine learning . PMLR, 2021, pp. 

8748 –8763. 

[190]  O. Patashnik, Z. Wu, E. Shechtman, D. Cohen -Or, and D. Lischin -

ski, “Styleclip: Text -driven manipulation of stylegan imagery,” in 

Proceedings of the IEEE/CVF International Conference on Computer 

Vision , 2021, pp. 2085 –2094. 

[191]  R. Gal, O. Patashnik, H. Maron, A. H. Bermano, G. Chechik, and 

D. Cohen -Or, “Stylegan -nada: Clip -guided domain adaptation of 

image generators,” ACM Transactions on Graphics (TOG) , vol. 41, 

no. 4, pp. 1 –13, 2022. 

[192]  H. Sahak, D. Watson, C. Saharia, and D. Fleet, “Denoising dif -

fusion probabilistic models for robust image super -resolution in 

the wild,” arXiv preprint arXiv:2302.07864 , 2023. 

[193]  A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mc -

Grew, I. Sutskever, and M. Chen, “Glide: Towards photorealistic 

image generation and editing with text -guided diffusion mod -

els,” arXiv preprint arXiv:2112.10741 , 2021. 

[194]  C. V. Krishna, K. Venkatesh, S. Nibin, H. Vishakh, and M. G. Nair, 

“Segment -based, user -generated image styling with neural style 

transfer,” in 2023 International Conference on Distributed Computing 

and Electrical Circuits and Electronics (ICDCECE) . IEEE, 2023, pp. 

1–6. 

[195]  Q. Nguyen, T. Vu, A. Tran, and K. Nguyen, “Dataset diffusion: 

Diffusion -based synthetic dataset generation for pixel -level se -

mantic segmentation,” arXiv preprint arXiv:2309.14303 , 2023. 

[196]  Y. Yu, Y. Zhuang, J. Zhang, Y. Meng, A. Ratner, R. Krishna, 

J. Shen, and C. Zhang, “Large language model as attributed 

training data generator: A tale of diversity and bias,” arXiv 

preprint arXiv:2306.15895 , 2023. 

[197]  N. Liu, S. Li, Y. Du, A. Torralba, and J. B. Tenenbaum, “Com -

positional visual generation with composable diffusion models,” 

in European Conference on Computer Vision . Springer, 2022, pp. 

423 –439. 

[198]  S. -I. Cheng, Y. -J. Chen, W. -C. Chiu, H. -Y. Tseng, and H. -Y. Lee, 

“Adaptively -realistic image generation from stroke and sketch 

with diffusion model,” in Proceedings of the IEEE/CVF Winter 

Conference on Applications of Computer Vision , 2023, pp. 4054 –4062. 

[199]  J. Z. Wu, Y. Ge, X. Wang, S. W. Lei, Y. Gu, Y. Shi, W. Hsu, Y. Shan, 

X. Qie, and M. Z. Shou, “Tune -a-video: One -shot tuning of image 

diffusion models for text -to -video generation,” in Proceedings of 

the IEEE/CVF International Conference on Computer Vision , 2023, 

pp. 7623 –7633. 

[200]  Z. Luo, D. Chen, Y. Zhang, Y. Huang, L. Wang, Y. Shen, D. Zhao, 

J. Zhou, and T. Tan, “Videofusion: Decomposed diffusion models 

for high -quality video generation,” in Proceedings of the IEEE/CVF 

Conference on Computer Vision and Pattern Recognition , 2023, pp. 

10 209 –10 218. 

[201]  K. Mei and V. Patel, “Vidm: Video implicit diffusion models,” in 

Proceedings of the AAAI Conference on Artificial Intelligence , vol. 37, 

no. 8, 2023, pp. 9117 –9125. 

[202]  S. Gong, M. Li, J. Feng, Z. Wu, and L. Kong, “Diffuseq: Sequence 

to sequence text generation with diffusion models,” arXiv preprint 

arXiv:2210.08933 , 2022. 

[203]  Y. Leng, Z. Chen, J. Guo, H. Liu, J. Chen, X. Tan, D. Mandic, L. He, 

X. Li, T. Qin et al. , “Binauralgrad: A two -stage conditional diffu -

sion probabilistic model for binaural audio synthesis,” Advances 

in Neural Information Processing Systems , vol. 35, pp. 23 689 –23 700, 

2022. 

[204]  Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, “Dif -

fwave: A versatile diffusion model for audio synthesis,” arXiv 

preprint arXiv:2009.09761 , 2020. 

[205]  Y. Tashiro, J. Song, Y. Song, and S. Ermon, “Csdi: Conditional 

score -based diffusion models for probabilistic time series impu -

tation,” Advances in Neural Information Processing Systems , vol. 34, 

pp. 24 804 –24 816, 2021. 

[206]  N. Suh, X. Lin, D. -Y. Hsieh, M. Honarkhah, and G. Cheng, “Au -

todiff: combining auto -encoder and diffusion model for tabular 

data synthesizing,” arXiv preprint arXiv:2310.15479 , 2023. 

[207]  A. Kotelnikov, D. Baranchuk, I. Rubachev, and A. Babenko, 

“Tabddpm: Modelling tabular data with diffusion models,” in 

International Conference on Machine Learning . PMLR, 2023, pp. 

17 564 –17 579. 

[208]  T. Sattarov, M. Schreyer, and D. Borth, “Findiff: Diffusion models 

for financial tabular data generation,” in 4th ACM International 

Conference on AI in Finance , 2023, pp. 64 –72. 

[209]  A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hi -

erarchical text -conditional image generation with clip latents,” 

arXiv preprint arXiv:2204.06125 , vol. 1, no. 2, p. 3, 2022. 36 

[210]  R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, 

“High -resolution image synthesis with latent diffusion models,” 

in Proceedings of the IEEE/CVF conference on computer vision and 

pattern recognition , 2022, pp. 10 684 –10 695. 

[211]  C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, 

K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans 

et al. , “Photorealistic text -to -image diffusion models with deep 

language understanding,” Advances in Neural Information Process -

ing Systems , vol. 35, pp. 36 479 –36 494, 2022. 

[212]  O. Avrahami, O. Fried, and D. Lischinski, “Blended latent dif -

fusion,” ACM Transactions on Graphics (TOG) , vol. 42, no. 4, pp. 

1–11, 2023. 

[213]  O. Bar -Tal, L. Yariv, Y. Lipman, and T. Dekel, “Multidiffusion: 

Fusing diffusion paths for controlled image generation,” 2023. 

[214]  T. Wang, J. -Y. Zhu, A. Torralba, and A. A. Efros, “Dataset distilla -

tion,” arXiv preprint arXiv:1811.10959 , 2018. 

[215]  R. Yu, S. Liu, and X. Wang, “Dataset distillation: A comprehen -

sive review,” arXiv preprint arXiv:2301.07014 , 2023. 

[216]  O. Bohdal, Y. Yang, and T. Hospedales, “Flexible dataset 

distillation: Learn labels instead of images,” arXiv preprint 

arXiv:2006.08572 , 2020. 

[217]  I. Sucholutsky and M. Schonlau, “Soft -label dataset distillation 

and text dataset distillation,” in 2021 International Joint Conference 

on Neural Networks (IJCNN) . IEEE, 2021, pp. 1–8. 

[218]  Y. Li, J. Yang, Y. Song, L. Cao, J. Luo, and L. -J. Li, “Learning 

from noisy labels with distillation,” in Proceedings of the IEEE 

international conference on computer vision , 2017, pp. 1910 –1918. 

[219]  L. Li, P. Dong, Z. Wei, and Y. Yang, “Automated knowledge 

distillation via monte carlo tree search,” in Proceedings of the 

IEEE/CVF International Conference on Computer Vision , 2023, pp. 

17 413 –17 424. 

[220]  H. He, X. Shi, J. Mueller, S. Zha, M. Li, and G. Karypis, “Towards 

automated distillation: A systematic study of knowledge distilla -

tion in natural language processing,” 2022. 

[221]  A. Krizhevsky, G. Hinton et al. , “Learning multiple layers of 

features from tiny images,” 2009. 

[222]  J. Deng, W. Dong, R. Socher, L. -J. Li, K. Li, and L. Fei -Fei, 

“Imagenet: A large -scale hierarchical image database,” in 2009 

IEEE conference on computer vision and pattern recognition . Ieee, 

2009, pp. 248 –255. 

[223]  S. Zagoruyko and N. Komodakis, “Wide residual networks,” 

arXiv preprint arXiv:1605.07146 , 2016. 

[224]  S. G. Müller and F. Hutter, “Trivialaugment: Tuning -free yet state -

of -the -art data augmentation,” in Proceedings of the IEEE/CVF 

International Conference on Computer Vision , 2021, pp. 774 –782. 

[225]  Z. Liu, H. Jin, T. -H. Wang, K. Zhou, and X. Hu, “Divaug: Plug -in 

automated data augmentation with explicit diversity maximiza -

tion,” in Proceedings of the IEEE/CVF International Conference on 

Computer Vision , 2021, pp. 4762 –4770. 

[226]  F. Zhou, J. Li, C. Xie, F. Chen, L. Hong, R. Sun, and Z. Li, 

“Metaaugment: Sample -aware data augmentation policy learn -

ing,” arXiv preprint arXiv:2012.12076 , 2020. 

[227]  L. Li and A. Li, “A2 -aug: Adaptive automated data augmenta -

tion,” in Proceedings of the IEEE/CVF Conference on Computer Vision 

and Pattern Recognition , 2023, pp. 2266 –2273. 

[228]  C. Gong, D. Wang, M. Li, V. Chandra, and Q. Liu, “Keepaugment: 

A simple information -preserving data augmentation approach,” 

in Proceedings of the IEEE/CVF Conference on Computer Vision and 

Pattern Recognition , 2021, pp. 1055 –1064. 

[229]  Y. Zheng, Z. Zhang, S. Yan, and M. Zhang, “Deep autoaugment,” 

arXiv preprint arXiv:2203.06172 , 2022. 

[230]  G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, 

“Deep networks with stochastic depth,” in European conference 

on computer vision . Springer, 2016, pp. 646 –661. 

[231]  Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang, “Random 

erasing data augmentation,” in Proceedings of the AAAI conference 

on artificial intelligence , vol. 34, no. 07, 2020, pp. 13 001 –13 008. 

[232]  R. Takahashi, T. Matsubara, and K. Uehara, “Ricap: Random 

image cropping and patching data augmentation for deep cnns,” 

in Asian conference on machine learning . PMLR, 2018, pp. 786 –798. 

[233]  A. Uddin, M. Monira, W. Shin, T. Chung, S. -H. Bae et al. , “Salien -

cymix: A saliency guided data augmentation strategy for better 

regularization,” arXiv preprint arXiv:2006.01791 , 2020. 

[234]  J. -H. Lee, M. Z. Zaheer, M. Astrid, and S. -I. Lee, “Smoothmix: A 

simple yet effective data augmentation to train robust classifiers,” 

in Proceedings of the IEEE/CVF conference on computer vision and 

pattern recognition workshops , 2020, pp. 756 –757. 

[235]  V. Verma, A. Lamb, C. Beckham, A. Najafi, I. Mitliagkas, 

D. Lopez -Paz, and Y. Bengio, “Manifold mixup: Better represen -

tations by interpolating hidden states,” in International Conference 

on Machine Learning . PMLR, 2019, pp. 6438 –6447. 

[236]  T. DeVries and G. W. Taylor, “Improved regularization of 

convolutional neural networks with cutout,” arXiv preprint 

arXiv:1708.04552 , 2017. 

[237]  H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez -Paz, 

“mixup: Beyond empirical risk minimization,” arXiv preprint 

arXiv:1710.09412 , 2017. 

[238]  E. Harris, A. Marcu, M. Painter, M. Niranjan, A. Prügel -Bennett, 

and J. Hare, “Fmix: Enhancing mixed sample data augmenta -

tion,” arXiv preprint arXiv:2002.12047 , 2020. 

[239]  B. A. Pimentel and A. C. De Carvalho, “A new data character -

ization for selecting clustering algorithms using meta -learning,” 

Information Sciences , vol. 477, pp. 203 –219, 2019. 

[240]  Y. Li, Y. Shen, W. Zhang, C. Zhang, and B. Cui, “Volcanoml: 

speeding up end -to -end automl via scalable search space decom -

position,” The VLDB Journal , pp. 1 –25, 2022. 

[241]  S. Liu, P. Ram, D. Vijaykeerthy, D. Bouneffouf, G. Bramble, 

H. Samulowitz, D. Wang, A. Conn, and A. Gray, “An admm 

based framework for automl pipeline configuration,” in Proceed -

ings of the AAAI Conference on Artificial Intelligence , vol. 34, no. 04, 

2020, pp. 4892 –4899. 

[242]  E. Bisong, “Google automl: cloud vision,” in Building Machine 

Learning and Deep Learning Models on Google Cloud Platform .

Springer, 2019, pp. 581 –598. 

[243]  C. Thornton, F. Hutter, H. H. Hoos, and K. Leyton -Brown, “Auto -

weka: Combined selection and hyperparameter optimization of 

classification algorithms,” in Proceedings of the 19th ACM SIGKDD 

international conference on Knowledge discovery and data mining ,

2013, pp. 847 –855. 

[244]  “Featuretools. available online:  https://www .featur etools.com/ 

(accessed on 10 march 2022).” 2020. 

[245]  F. Horn, R. Pack, and M. Rieger, “The autofeat python library 

for automated feature engineering and selection,” in Joint Euro -

pean Conference on Machine Learning and Knowledge Discovery in 

Databases . Springer, 2019, pp. 111 –120. 

[246]  M. Christ, N. Braun, J. Neuffer, and A. W. Kempa -Liehr, “Time 

series feature extraction on basis of scalable hypothesis tests 

(tsfresh –a python package),” Neurocomputing , vol. 307, pp. 72 –77, 

2018. 

[247]  N. A. Roque and N. Ram, “tsfeaturex: An r package for automat -

ing time series feature extraction,” Journal of open source software ,

vol. 4, no. 37, 2019. 

[248]  M. Barandas, D. Folgado, L. Fernandes, S. Santos, M. Abreu, 

P. Bota, H. Liu, T. Schultz, and H. Gamboa, “Tsfel: Time series 

feature extraction library,” SoftwareX , vol. 11, p. 100456, 2020. 

[249]  B. Peng, S. Wan, Y. Bi, B. Xue, and M. Zhang, “Automatic feature 

extraction and construction using genetic programming for rotat -

ing machinery fault diagnosis,” IEEE transactions on cybernetics ,

vol. 51, no. 10, pp. 4909 –4923, 2020. 

[250]  A. De Brabandere, P. Robberechts, T. Op De Beéck, and J. Davis, 

“Automating feature construction for multi -view time series 

data,” in ECMLPKDD Workshop on Automating Data Science .

N/A, 2019, pp. 1–16. 

[251]  K. Lillywhite, D. -J. Lee, B. Tippetts, and J. Archibald, “A fea -

ture construction method for general object recognition,” Pattern 

Recognition , vol. 46, no. 12, pp. 3300 –3314, 2013. 

[252]  U. Kamath, K. De Jong, and A. Shehu, “Effective automated 

feature construction and selection for classification of biological 

sequences,” PloS one , vol. 9, no. 7, p. e99982, 2014. 

[253]  A. De Brabandere, T. Op De Beéck, K. Hendrickx, W. Meert, and 

J. Davis, “Tsfuse: Automated feature construction for multiple 

time series data,” Machine Learning , pp. 1 –56, 2022. 

[254]  I. Guyon, S. Gunn, M. Nikravesh, and L. A. Zadeh, Feature 

extraction: foundations and applications . Springer, 2008, vol. 207. 

[255]  I. Mierswa and K. Morik, “Automatic feature extraction for 

classifying audio data,” Machine learning , vol. 58, no. 2, pp. 127 –

149, 2005. 

[256]  N. Sumonja, B. Gemovic, N. Veljkovic, and V. Perovic, “Au -

tomated feature engineering improves prediction of protein –

protein interactions,” Amino Acids , vol. 51, no. 8, pp. 1187 –1200, 

2019. 

[257]  J. Zhang, “Machine learning with feature selection using prin -

cipal component analysis for malware detection: a case study,” 

arXiv preprint arXiv:1902.03639 , 2019. 37 

[258]  L. Pion -Tonachini, K. Kreutz -Delgado, and S. Makeig, “Iclabel: 

An automated electroencephalographic independent component 

classifier, dataset, and website,” NeuroImage , vol. 198, pp. 181 –

197, 2019. 

[259]  W. Long, Z. Lu, and L. Cui, “Deep learning -based feature engi -

neering for stock price movement prediction,” Knowledge -Based 

Systems , vol. 164, pp. 163 –173, 2019. 

[260]  V. P. Rathi and S. Palani, “Brain tumor mri image classification 

with feature selection and extraction using linear discriminant 

analysis,” arXiv preprint arXiv:1208.2128 , 2012. 

[261]  M. Li, X. Luo, J. Yang, and Y. Sun, “Applying a locally linear 

embedding algorithm for feature extraction and visualization of 

mi -eeg,” Journal of Sensors , vol. 2016, 2016. 

[262]  X. Wang, Y. Zheng, Z. Zhao, and J. Wang, “Bearing fault diagnosis 

based on statistical locally linear embedding,” Sensors , vol. 15, 

no. 7, pp. 16 225 –16 247, 2015. 

[263]  A. Agrapetidou, P. Charonyktakis, P. Gogas, T. Papadimitriou, 

and I. Tsamardinos, “An automl application to forecasting bank 

failures,” Applied Economics Letters , vol. 28, no. 1, pp. 5 –9, 2021. 

[264]  M. B. Kursa, A. Jankowski, and W. R. Rudnicki, “Boruta –a system 

for feature selection,” Fundamenta Informaticae , vol. 101, no. 4, pp. 

271 –285, 2010. 

[265]  F. Yang, Y. Qiao, Y. Qi, J. Bo, and X. Wang, “Bacs: blockchain 

and automl -based technology for efficient credit scoring classifi -

cation,” Annals of Operations Research , pp. 1 –21, 2022. 

[266]  V. Lopes, A. Gaspar, L. A. Alexandre, and J. Cordeiro, “An 

automl -based approach to multimodal image sentiment anal -

ysis,” in 2021 International Joint Conference on Neural Networks 

(IJCNN) . IEEE, 2021, pp. 1–9. 

[267]  H. Rakotoarison, L. Milijaona, A. Rasoanaivo, M. Sebag, and 

M. Schoenauer, “Learning meta -features for automl,” in Interna -

tional Conference on Learning Representations , 2022. 

[268]  C. Xue, J. Yan, R. Yan, S. M. Chu, Y. Hu, and Y. Lin, “Transferable 

automl by model sharing over grouped datasets,” in Proceedings 

of the IEEE/CVF Conference on Computer Vision and Pattern Recog -

nition , 2019, pp. 9002 –9011. 

[269]  G. Peyré, M. Cuturi et al. , “Computational optimal transport: 

With applications to data science,” Foundations and Trends® in 

Machine Learning , vol. 11, no. 5 -6, pp. 355 –607, 2019. 

[270]  G. Katz, E. C. R. Shin, and D. Song, “Explorekit: Automatic 

feature generation and selection,” in 2016 IEEE 16th International 

Conference on Data Mining (ICDM) . IEEE, 2016, pp. 979 –984. 

[271]  A. Harari and G. Katz, “Automatic features generation and 

selection from external sources: A dbpedia use case,” Information 

Sciences , vol. 582, pp. 398 –414, 2022. 

[272]  A. Fatima, F. A. Khan, A. Raza, and A. B. Kamran, “Automated 

feature synthesis from relational database for data science related 

problems,” in 2018 International Conference on Frontiers of Informa -

tion Technology (FIT) . IEEE, 2018, pp. 71 –75. 

[273]  U. Khurana, D. Turaga, H. Samulowitz, and S. Parthasrathy, 

“Cognito: Automated feature engineering for supervised learn -

ing,” in 2016 IEEE 16th International Conference on Data Mining 

Workshops (ICDMW) . IEEE, 2016, pp. 1304 –1307. 

[274]  J. M. Kanter and K. Veeramachaneni, “Deep feature synthesis: 

Towards automating data science endeavors,” in 2015 IEEE inter -

national conference on data science and advanced analytics (DSAA) .

IEEE, 2015, pp. 1–10. 

[275]  H. T. Lam, J. -M. Thiebaut, M. Sinn, B. Chen, T. Mai, and O. Alkan, 

“One button machine for automating feature engineering in 

relational databases,” arXiv preprint arXiv:1706.00327 , 2017. 

[276]  U. Khurana, H. Samulowitz, and D. Turaga, “Feature engineering 

for predictive modeling using reinforcement learning,” in Pro -

ceedings of the AAAI Conference on Artificial Intelligence , vol. 32, 

no. 1, 2018. 

[277]  I. Reyes -Amezcua, D. Flores -Araiza, G. Ochoa -Ruiz, A. Mendez -

Vazquez, and E. Rodriguez -Tello, “Macfe: A meta -learning and 

causality based feature engineering framework,” arXiv preprint 

arXiv:2207.04010 , 2022. 

[278]  F. Nargesian, H. Samulowitz, U. Khurana, E. B. Khalil, and D. S. 

Turaga, “Learning feature engineering for classification.” in Ijcai ,

2017, pp. 2529 –2535. 

[279]  J. Fan and R. Li, “Statistical challenges with high dimension -

ality: Feature selection in knowledge discovery,” arXiv preprint 

math/0602133 , 2006. 

[280]  H. Liu and H. Motoda, Feature selection for knowledge discovery and 

data mining . Springer Science & Business Media, 2012, vol. 454. 

[281]  G. Borboudakis and I. Tsamardinos, “Extending greedy feature 

selection algorithms to multiple solutions,” Data Mining and 

Knowledge Discovery , vol. 35, no. 4, pp. 1393 –1434, 2021. 

[282]  A. R. Statnikov, “Algorithms for discovery of multiple markov 

boundaries: Application to the molecular signature multiplicity 

problem,” Ph.D. dissertation, 2008. 

[283]  H. Li, T. Fu, J. Dai, H. Li, G. Huang, and X. Zhu, “Autoloss -

zero: Searching loss functions from scratch for generic tasks,” 

in Proceedings of the IEEE/CVF Conference on Computer Vision and 

Pattern Recognition , 2022, pp. 1009 –1018. 

[284]  V. Dodballapur, R. Calisa, Y. Song, and W. Cai, “Automatic 

dropout for deep neural networks,” in International Conference on 

Neural Information Processing . Springer, 2020, pp. 185 –196. 

[285]  H. Pham and Q. Le, “Autodropout: Learning dropout patterns to 

regularize deep networks,” in Proceedings of the AAAI Conference 

on Artificial Intelligence , vol. 35, no. 11, 2021, pp. 9351 –9359. 

[286]  G. Zhu, S. Jiang, X. Guo, C. Yuan, and Y. Huang, “Evolution -

ary automated feature engineering,” in Pacific Rim International 

Conference on Artificial Intelligence . Springer, 2022, pp. 574 –586. 

[287]  R. P. Bonidia, A. P. A. Santos, B. L. de Almeida, P. F. Stadler, U. N. 

da Rocha, D. S. Sanches, and A. C. de Carvalho, “Bioautoml: 

automated feature engineering and metalearning to predict non -

coding rnas in bacteria,” Briefings in Bioinformatics , vol. 23, no. 4, 

p. bbac218, 2022. 

[288]  G. Zhu, Z. Xu, C. Yuan, and Y. Huang, “Difer: differentiable 

automated feature engineering,” in International Conference on 

Automated Machine Learning . PMLR, 2022, pp. 17 –1. 

[289]  T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting sys -

tem,” in Proceedings of the 22nd acm sigkdd international conference 

on knowledge discovery and data mining , 2016, pp. 785 –794. 

[290]  S. Ahmed, K. K. Ghosh, P. K. Singh, Z. W. Geem, and R. Sarkar, 

“Hybrid of harmony search algorithm and ring theory -based 

evolutionary algorithm for feature selection,” IEEE Access , vol. 8, 

pp. 102 629 –102 645, 2020. 

[291]  Y. Chen, L. Wan, Z. Li, Q. Jing, and Z. Sun, “Neural feature search 

for rgb -infrared person re -identification,” in Proceedings of the 

IEEE/CVF Conference on Computer Vision and Pattern Recognition ,

2021, pp. 587 –597. 

[292]  M. Sokolova, N. Japkowicz, and S. Szpakowicz, “Beyond accu -

racy, f-score and roc: a family of discriminant measures for per -

formance evaluation,” in Australasian joint conference on artificial 

intelligence . Springer, 2006, pp. 1015 –1021. 

[293]  T. Zhang, Z. A. Zhang, Z. Fan, H. Luo, F. Liu, Q. Liu, W. Cao, and 

L. Jian, “Openfe: Automated feature generation with expert -level 

performance,” in International Conference on Machine Learning .

PMLR, 2023, pp. 41 880 –41 901. 

[294]  A. Asuncion and D. Newman, “Uci machine learning repository,” 

2007. 

[295]  “Uc irvine machine learning repository. available online: 

https://archive.ics.uci.edu/ (accessed on feb. 2023).” 

[296]  J. Vanschoren, J. N. Van Rijn, B. Bischl, and L. Torgo, “Openml: 

networked science in machine learning,” ACM SIGKDD Explo -

rations Newsletter , vol. 15, no. 2, pp. 49 –60, 2014. 

[297]  “Open machine learning github page. available online: 

https://github.com/openml/openml/ (accessed on july 2023).” 

[298]  R. Wang, R. Shivanna, D. Cheng, S. Jain, D. Lin, L. Hong, and 

E. Chi, “Dcn v2: Improved deep & cross network and practical 

lessons for web -scale learning to rank systems,” in Proceedings of 

the web conference 2021 , 2021, pp. 1785 –1797. 

[299]  W. Fan, E. Zhong, J. Peng, O. Verscheure, K. Zhang, J. Ren, 

R. Yan, and Q. Yang, “Generalized and heuristic -free feature 

construction for improved accuracy,” in Proceedings of the 2010 

SIAM International Conference on Data Mining . SIAM, 2010, pp. 

629 –640. 

[300]  S. Li, X. Ao, F. Pan, and Q. He, “Learning policy scheduling for 

text augmentation,” Neural Networks , vol. 145, pp. 121 –127, 2022. 

[301]  Y. Luo, M. Wang, H. Zhou, Q. Yao, W. -W. Tu, Y. Chen, W. Dai, 

and Q. Yang, “Autocross: Automatic feature crossing for tabular 

data in real -world applications,” in Proceedings of the 25th ACM 

SIGKDD International Conference on Knowledge Discovery & Data 

Mining , 2019, pp. 1936 –1945. 

[302]  G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T. -

Y. Liu, “Lightgbm: A highly efficient gradient boosting decision 

tree,” Advances in neural information processing systems , vol. 30, 

2017. 38 

[303]  Y. Gorishniy, I. Rubachev, and A. Babenko, “On embeddings for 

numerical features in tabular deep learning,” Advances in Neural 

Information Processing Systems , vol. 35, pp. 24 991 –25 004, 2022. 

[304]  L. Grinsztajn, E. Oyallon, and G. Varoquaux, “Why do tree -based 

models still outperform deep learning on typical tabular data?” 

Advances in Neural Information Processing Systems , vol. 35, pp. 507 –

520, 2022. 

[305]  T. Swearingen, W. Drevo, B. Cyphers, A. Cuesta -Infante, A. Ross, 

and K. Veeramachaneni, “Atm: A distributed, collaborative, scal -

able system for automated machine learning,” in 2017 IEEE 

international conference on big data (big data) . IEEE, 2017, pp. 151 –

162. 

[306]  F. Mohr, M. Wever, and E. Hüllermeier, “Ml -plan: Automated 

machine learning via hierarchical planning,” Machine Learning ,

vol. 107, no. 8, pp. 1495 –1515, 2018. 

[307]  B. Komer, J. Bergstra, and C. Eliasmith, “Hyperopt -sklearn: au -

tomatic hyperparameter configuration for scikit -learn,” in ICML 

workshop on AutoML , vol. 9. Citeseer, 2014, p. 50. 

[308]  Y. Zhao, “Autodes: Automl pipeline generation of classifica -

tion with dynamic ensemble strategy selection,” arXiv preprint 

arXiv:2201.00207 , 2022. 

[309]  C. Wang, Q. Wu, M. Weimer, and E. Zhu, “Flaml: A fast and 

lightweight automl library,” Proceedings of Machine Learning and 

Systems , vol. 3, pp. 434 –447, 2021. 

[310]  A. G. de Sá, W. J. G. Pinto, L. O. V. Oliveira, and G. L. Pappa, 

“Recipe: a grammar -based framework for automatically evolving 

classification pipelines,” in European Conference on Genetic Pro -

gramming . Springer, 2017, pp. 246 –261. 

[311]  A. Alaa and M. Schaar, “Autoprognosis: Automated clinical 

prognostic modeling via bayesian optimization with structured 

kernel learning,” in International conference on machine learning .

PMLR, 2018, pp. 139 –148. 

[312]  Z. Luo, Z. He, J. Wang, M. Dong, J. Huang, M. Chen, and 

B. Zheng, “Autosmart: An efficient and automatic machine learn -

ing framework for temporal relational data,” in Proceedings of 

the 27th ACM SIGKDD Conference on Knowledge Discovery & Data 

Mining , 2021, pp. 3976 –3984. 

[313]  P. Das, N. Ivkin, T. Bansal, L. Rouesnel, P. Gautier, Z. Karnin, 

L. Dirac, L. Ramakrishnan, A. Perunicic, I. Shcherbatyi et al. ,

“Amazon sagemaker autopilot: a white box automl solution at 

scale,” in Proceedings of the fourth international workshop on data 

management for end -to -end machine learning , 2020, pp. 1 –7. 

[314]  “Datarobot automated machine learning, available online: 

https://www .datar obot.com/platform/automated -machine -

learning/ (accessed on 26 february 2022).” 

[315]  W. Qi, C. Xu, and X. Xu, “Autogluon: A revolutionary framework 

for landslide hazard analysis,” Natural Hazards Research , vol. 1, 

no. 3, pp. 103 –108, 2021. 

[316]  P. Gijsbers and J. Vanschoren, “Gama: genetic automated machine 

learning assistant,” Journal of Open Source Software , vol. 4, no. 33, 

p. 1132, 2019. 

[317]  “What is custom vision?” 

[318]  E. Korot, Z. Guan, D. Ferraz, S. K. Wagner, G. Zhang, X. Liu, 

L. Faes, N. Pontikos, S. G. Finlayson, H. Khalid et al. , “Code -free 

deep learning for multi -modality medical image classification,” 

Nature Machine Intelligence , vol. 3, no. 4, pp. 288 –298, 2021. 

[319]  “Google vertexai. available online: 

https://cloud.google.com/vertex -ai (accessed on 25 july 

2022).” 

[320]  A. Sujeeth, H. Lee, K. Brown, T. Rompf, H. Chafi, M. Wu, 

A. Atreya, M. Odersky, and K. Olukotun, “Optiml: an implic -

itly parallel domain -specific language for machine learning,” in 

Proceedings of the 28th International Conference on Machine Learning 

(ICML -11) , 2011, pp. 609 –616. 

[321]  M. Ali, “Pycaret: An open source, low -code machine learning 

library in python,” PyCaret version , vol. 2, 2020. 

[322]  L. Etaati, “Azure databricks,” in Machine Learning with Microsoft 

Technologies . Springer, 2019, pp. 159 –171. 

[323]  M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, 

and F. Hutter, “Efficient and robust automated machine learn -

ing,” Advances in neural information processing systems , vol. 28, 

2015. 

[324]  R. Barga, V. Fontama, W. H. Tok, and L. Cabrera -Cordon, Pre -

dictive analytics with Microsoft Azure machine learning . Springer, 

2015. 

[325]  P. Hall, N. Gill, M. Kurka, and W. Phan, “Machine learning 

interpretability with h2o driverless ai,” H2O. ai , 2017. 

[326]  “Sparkcognition’s release of darwin 2.0 brings increased 

speed and scalability to automated model building, available 

online:  https://www .sp arkcognition.com/sparkcognitions -

release -of -darwin -2-0-brings -increased -speed -and -scalability -to -

automated -model -building/ (accessed on 3 november 2022).” 

[327]  D. Wang, P. Ram, D. K. I. Weidele, S. Liu, M. Muller, J. D. 

Weisz, A. Valente, A. Chaudhary, D. Torres, H. Samulowitz et al. ,

“Autoai: Automating the end -to -end ai lifecycle with humans -

in -the -loop,” in Proceedings of the 25th International Conference on 

Intelligent User Interfaces Companion , 2020, pp. 77 –78. 

[328]  N. O. Nikitin, P. Vychuzhanin, M. Sarafanov, I. S. Polonskaia, 

I. Revin, I. V. Barabanova, G. Maximov, A. V. Kalyuzhnaya, 

and A. Boukhanovsky, “Automated evolutionary approach for 

the design of composite machine learning pipelines,” Future 

Generation Computer Systems , vol. 127, pp. 109 –125, 2022. 

[329]  “Mlbox, machine learning box, available online: 

https://mlbox.readthedocs.io/en/latest/ (accessed on 20 

august 2022).” 

[330]  “Microsoft neural network intelligence. available online: 

https://mlbox.readthedocs.io/en/latest/ (accessed on 20 august 

2022).” 

[331]  “Ludwig: a declarative machine learning framework available 

online: https://ludwig.ai/latest/ (accessed on 13 june february 

2022).” 

[332]  “Predictive analytics with automl, available online: 

https://www.pecan.ai/content/predictive -analytics -automl/ 

(accessed on 3 november 2022).” 

[333]  M. -A. Zöller, W. Titov, T. Schlegel, and M. F. Huber, “Xautoml: A 

visual analytics tool for establishing trust in automated machine 

learning,” arXiv preprint arXiv:2202.11954 , 2022. 

[334]  D. Wang, J. D. Weisz, M. Muller, P. Ram, W. Geyer, C. Dugan, 

Y. Tausczik, H. Samulowitz, and A. Gray, “Human -ai collab -

oration in data science: Exploring data scientists’ perceptions 

of automated ai,” Proceedings of the ACM on Human -Computer 

Interaction , vol. 3, no. CSCW, pp. 1 –24, 2019. 

[335]  A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, 

T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al. , “Pytorch: 

An imperative style, high -performance deep learning library,” 

Advances in neural information processing systems , vol. 32, 2019. 

[336]  F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, 

O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al. ,

“Scikit -learn: Machine learning in python,” the Journal of machine 

Learning research , vol. 12, pp. 2825 –2830, 2011. 

[337]  M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, 

G. S. Corrado, A. Davis, J. Dean, M. Devin et al. , “Tensorflow: 

Large -scale machine learning on heterogeneous distributed sys -

tems,” arXiv preprint arXiv:1603.04467 , 2016. 

[338]  A. Sestino and A. De Mauro, “Leveraging artificial intelligence 

in business: Implications, applications and methods,” Technology 

Analysis & Strategic Management , vol. 34, no. 1, pp. 16 –29, 2022. 

[339]  C. Pompa and T. Burke, “Data science and analytics skills 

shortage: equipping the apec workforce with the competencies 

demanded by employers,” APEC Human Resource Development 

Working Group , 2017. 

[340]  D. Dolezel and A. McLeod, “Big -data skills: Bridging the data 

science theory -practice gap in healthcare,” Perspectives in Health 

Information Management , vol. 18, no. Winter, 2021. 

[341]  K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, 

K. Clark, S. Pfohl, H. Cole -Lewis, D. Neal et al. , “Towards expert -

level medical question answering with large language models,” 

arXiv preprint arXiv:2305.09617 , 2023. 

[342]  T. Han, L. C. Adams, J. -M. Papaioannou, P. Grundmann, T. Ober -

hauser, A. Löser, D. Truhn, and K. K. Bressem, “Medalpaca –an 

open -source collection of medical conversational ai models and 

training data,” arXiv preprint arXiv:2304.08247 , 2023. 

[343]  Y. Zhou, C. Cui, J. Yoon, L. Zhang, Z. Deng, C. Finn, M. Bansal, 

and H. Yao, “Analyzing and mitigating object hallucination in 

large vision -language models,” arXiv preprint arXiv:2310.00754 ,

2023. 

[344]  J. Hendler, “Avoiding another ai winter,” IEEE Intelligent Systems ,

vol. 23, no. 02, pp. 2 –4, 2008. 

[345]  J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and 

J. Han, “Large language models can self -improve,” arXiv preprint 

arXiv:2210.11610 , 2022.
