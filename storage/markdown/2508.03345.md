Title: Adaptive AI Agent Placement and Migration in Edge Intelligence Systems

URL Source: http://arxiv.org/pdf/2508.03345v1

Published Time: Wed, 06 Aug 2025 00:48:26 GMT

Markdown Content:
# Adaptive AI Agent Placement and Migration in Edge Intelligence Systems 

Xingdan Wang 2, 1, Jiayi He 2, 1, Zhiqing Tang 1B, Jianxiong Guo 1,Jiong Lou 3, Liping Qian 4, Tian Wang 1, Weijia Jia 11Institute of Artificial Intelligence and Future Networks, Beijing Normal University, China 

> 2

Faculty of Arts and Sciences, Beijing Normal University, China 

> 3

Department of Computer Science and Engineering, Shanghai Jiao Tong University, China 

> 4

College of Information Engineering, Zhejiang University of Technology, China 

{wangxingdan, jiayihe }@mail.bnu.edu.cn, {jianxiongguo, zhiqingtang }@bnu.edu.cn, lj1994@sjtu.edu.cn, lpqian@zjut.edu.cn, {tianwang, jiawj }@bnu.edu.cn 

Abstract —The rise of LLMs such as ChatGPT and Claude fuels the need for AI agents capable of real-time task handling. However, migrating data-intensive, multi-modal edge workloads to cloud data centers, traditionally used for agent deployment, introduces significant latency. Deploying AI agents at the edge improves efficiency and reduces latency. However, edge envi-ronments present challenges due to limited and heterogeneous resources. Maintaining QoS for mobile users necessitates agent migration, which is complicated by the complexity of AI agents coordinating LLMs, task planning, memory, and external tools. This paper presents the first systematic deployment and man-agement solution for LLM-based AI agents in dynamic edge environments. We propose a novel adaptive framework for AI agent placement and migration in edge intelligence systems. Our approach models resource constraints and latency/cost, leveraging ant colony algorithms and LLM-based optimization for efficient decision-making. It autonomously places agents to optimize resource utilization and QoS and enables lightweight agent migration by transferring only essential state. Implemented on a distributed system using AgentScope and validated across globally distributed edge servers, our solution significantly re-duces deployment latency and migration costs. 

Index Terms —Edge intelligence, AI agent, placement, migra-tion. 

I. I NTRODUCTION 

The advancement of large language models (LLMs) and AI, such as ChatGPT, Claude, and DeepSeek, has made AI agents more convenient and effective at completing tasks, leading to their growing popularity [8]. Current research extensively explores AI agent deployment in cloud data centers to address various challenges, such as self-driving cars, intelligent robots, and voice assistants [5]. In multi-modal edge scenarios, trans-mitting large volumes of edge user data, such as real-time video, point clouds, and sensor data, to the cloud introduces significant latency. Therefore, processing and aggregating data and tasks at the edge is very urgent [6]. Deploying AI agents at the edge facilitates autonomous AI task planning and execution [10]. This edge-based architecture offers scalability and supports greater device and user access  

> This work is supported by the National Natural Science Foundation of China (NSFC) under Grant 62302048. (Corresponding author: Zhiqing Tang.)

compared to cloud-based deployments [11], offering many advantages. Firstly, it can reduce the distance of data trans-mission, thereby reducing latency and providing more real-time responses [14]. Secondly, by planning and executing tasks locally, edge servers can reduce the amount of data transmitted to the cloud, thereby saving bandwidth and reducing costs [9]. Furthermore, AI agents performing tasks on edge servers can appropriately reduce the frequency of sensitive data transmit-ted to the cloud, thereby improving data privacy and security [15]. Finally, edge AI agents ensure continuous application operation and improves system reliability, even with unstable network connections [3]. However, few studies address the deployment of AI agents in edge intelligence systems. Moreover, unlike the centralized processing in cloud data centers, the mobility of edge users necessitates the migration of AI agents to maintain optimal QoS, as agents should follow users as they move [3], [4]. Unlike traditional service deployment, AI agents often have various procedures such as LLM invocation, task planning, memory storage, and tool calling [5]. Therefore, the placement and migration procedures of AI agents become much more complex [1], [2]. Traditional service or container deployment provides static services. Agent deployment, conversely, adap-tively deploys agents, incorporating service caching, resource allocation, etc. [7]. Deploying AI agents in edge intelligence systems is challenging due to complex environmental interac-tions and dynamics [16]. The first challenge is how to optimize AI agent placement considering the edge servers’ limited computing, storage, and communication capabilities [12]. Computing resources affect the decision-making and computation speed of agents. Storage resources affect the type of agents. For example, multi-modal agents must be deployed on edge servers with relatively abundant storage resources so that they can handle a large amount of data [17]. Communication resources can affect the communication delay among agents. Agents heavily reliant on LLMs should be located on servers near their dependencies, whereas agents transferring large files benefit from servers with ample storage and communication resources. 

> arXiv:2508.03345v1 [cs.AI] 5 Aug 2025

The second challenge is how to consider the mobility requirements of edge users and migrate the agents accordingly. Dynamic environment configuration is crucial because the environmental context directly influences LLM-based agent responses [15]. Maintaining environmental context integrity during and after migration is therefore essential [13]. More-over, unlike traditional service or container migration, AI agent migration only requires transferring the memory and config-uration files, rather than the entire code base. This unique characteristic should be leveraged to minimize migration costs. To address these challenges, we propose a novel framework for adaptive AI agent placement and migration in edge intel-ligence systems. We first model the AI agent in edge comput-ing, fully considering the latency and cost during placement and migration, including transmission latency, initialization latency, migration latency, and computational costs, storage costs, etc. Then, based on the ant colony algorithm, we propose AI agent placement and migration algorithms, and introduce an LLM-based autonomous optimization scheme to further improve the algorithms’ performance. We have implemented a distributed edge intelligence system based on AgentScope and deployed AI agents on edge servers distributed around the world [5]. We implement the placement and migration of AI agents on the edge intelligence system, and the experimental results show that our algorithm reduces the deployment latency by 9.5% and the migration cost by 11.5% on average. The main contributions are summarized as follows: 1) We propose the deployment of AI agents in edge in-telligence systems. Our objective is to minimize task execution and agent migration times while maximizing edge resource utilization. 2) Our agent placement algorithm optimizes for computing and storage resources, and geographical distribution. Task scheduling and agent placement decisions are made based on the resource availability and current agent distribution. 3) We design an adaptive AI agent migration algorithm, con-sidering computing and communication resources. This algorithm autonomously decides whether to migrate the agent based on user movement, aiming to improve QoS. 4) We have implemented our edge agent system and veri-fied the effectiveness of our algorithms through a large number of experiments. II. S YSTEM MODEL AND PROBLEM FORMULATION 

A. System Model 

As shown in Fig. 1, our edge intelligence system dynami-cally places and migrates AI agents to meet delay requirements of user tasks arriving at any time. Agents are deployed on edge servers for real-time processing. To minimize startup latency and communication latency due to user mobility, agents are strategically migrated. The core components are defined below. 1) AI Agent: Dynamic and intelligent entities, denoted as 

a ∈ A = {a1, a 2, . . . , a |A|}, are deployed to different edge servers. Among them, |·| is used to indicate the num-ber of elements in the set, and |A| is the number of AI 

Fig. 1 An overview of our edge intelligence system. agents. Each AI Agent a has a memory requirement Ma

(GB), computing resource requirements Ca (CPU cycles), storage resource requirements Sa (GB), communication resource requirements Va (Mbps), edge server location 

Ea (server number), etc. 2) Edge System: The edge server set is defined as E =

{e1, e 2, . . . , e |E|}. Each edge server e ∈ E has resource capacities such as memory capacity Me (GB), computing resource capacity Ce (CPU cycles), storage resource capacity Se (GB), and communication resource capacity 

Ve (Mbps). Furthermore, the AI Agent running on each edge server e is defined as the set Ae.3) Task: The task currently being executed is defined as the set R = {r1, r 2, . . . , r |R|}. Each user task r has a prompt 

pr and file fr , and requires a set of AI Agents Ar .

B. Cost 1) Latency: The latency encompasses file transfer, agent startup, migration, task processing, and result return. 

Transmit Latency: It comprises file upload/reception and prompt transmission, defined as: 

T tra  

> r

= sr 

> ηe

+ Tp, (1) where sr represents the size of the file, ηe represents the actual network bandwidth of the edge node, and Tp represents the time of receiving the prompt. 

Migration Latency: Migration involves exporting and transferring memory, followed by initiating the AI agent and importing the memory. Memory transfer time is calculated as: 

T tra m = Ma

> ηe2
> e1

, (2) where Ma represents memory size and ηe2 

> e1

denotes the net-work bandwidth during memory transmission. The migration delay is defined as: 

T mig  

> r

= T export  

> e1

+ T tra m + T initiation  

> e2

+ T load  

> e2

, (3) where T export  

> e1

is the time when the edge node e1 exports memory, and T initiation  

> e2

is the time when the edge node e2initiates the AI agent. T load  

> e2

is the time when the edge node 

e2 is imported into memory. 

Initiation Latency: The initial delay, caused by the aggre-gate startup times of all AI agents, increases with the number of agents. Therefore, it is defined as: 

T Ini  

> r

= P 

> a∈Ar

Tinit 

 |Ae(a)| , (4) where Ar is the set of AI agents required for task r, and eais the server where AI Agent a is located. 

Processing Latency: It depends on task requirements. If file processing is necessary, latency varies based on file type and quantity. Otherwise, only LLM reasoning time is relevant: 

T pro  

> r

= Tf pro + TLLM , (5) where Tf pro refers to the file processing time, and TLLM refers to the thinking time of LLM, indicating that total processing delay comprises both file processing and LLM computation. Finally, the total task duration is defined as: 

Tr = T tra  

> r

+ T mig  

> r

+ T Ini  

> r

+ T pro  

> r

. (6) 

2) Cost: The cost reflects resource utilization, encompass-ing computing, storage, and communication usage. 

Computing Cost: Computing resources refer to CPU usage: 

OCP U r = Tr − ( sr 

> ηe

+ T tra  

> m

). (7) 

Storage Cost: It encompasses all necessary task files and the AI agent’s memory, defined as: 

OStor r = P|R| 

> i=1

Sr + P|A| 

> j=1

Ma, (8) where |R| is the number of tasks, Sr is the storage space for task r, and Ma is the memory usage of AI Agent a.

Communication Cost: Interdependent AI agents commu-nicate and transfer files using communication resources: 

OComm r = Pmi=1 Ci, (9) where Ci represents the communication resource usage for each of the m events. The total cost is defined as: 

Or = OCP U r + OStor r + OComm r . (10) 

3) Constraints: For each agent a assigned to edge server 

e, resource requirements must be met: 

P 

> a∈Ar

Maza,e ≤ M rem  

> e

, ∀e ∈ N (ec),P 

> a∈Ar

Saza,e ≤ Srem  

> e

, ∀e ∈ N (ec),P 

> a∈Ar

Vaza,e ≤ V rem  

> e

, ∀e ∈ N (ec),

(11) 

4) Problem Formulation: The Edge Agent Deployment (EAD) problem is defined as: 

Problem EAD. 

min z

P

> a∈Ar

 Ma

> ηeec

+ T init 

> e



+ P 

> e∈N (ec)

 P 

> aCaza,e
> Crem
> e



+θ P

> a∈Ar

P  

> (ai,a j)∈D a

Tcomm (eai , e aj )

s.t. P  

> e∈N (ec)

za,e = 1 , ∀a ∈ Ar

zai,e = zaj ,e , ∀(ai, a j ) ∈ D a, ∀a ∈ Ar

Algorithm 1: ALP 

Input: Task list T , Edge server set E, Resource status 

R, LLM model MLLM 

Output: Final deployment strategy Sfinal  

> 1

Initialize pheromone matrix τ and heuristic matrix η; 

> 2

for each ant k = 1 to m do  

> 3

for each task ti ∈ T do  

> 4

Select a server ej ∈ E′ using probability: 

pij = [τij ]α · [ηij ]β

P 

> e∈E′

[τie ]α · [ηie ]β

Assign task ti to server ej ; 

> 5

end  

> 6

Compute deployment cost Ck; 

> 7

end  

> 8

Select the best deployment: S∗ = arg min k Ck; 

> 9

Update pheromone matrix τ ; 

> 10

Post-Optimization: Use MLLM to refine and validate 

S∗; 

> 11

Obtain final optimized deployment Sfinal ;The EAD problem seeks to minimize initial expected cost, considering resource constraints. Moreover, the Edge Agent Migration (EAM) problem is: 

Problem EAM. 

Net Gain (ea → e′) = ∆ Tlatency 

| {z }

> delay benefits

− (Tmig + γO overhead )

| {z }

> migration costs

− θdep · ∆Tdep 

| {z }

> dependency damage costs

s.t. ∆Tlatency = E[Tcomm (ec, e a)] − E[Tcomm (ec, e ′)] ∆Tdep = P 

> a′∈D a

(Tcomm (e′, e a′ ) − Tcomm (ea, e a′ )) 

The EAM problem is triggered when: 1) Position Deviation Threshold: The number of hops 

H(ec, e a) to the server ea hosting the AI Agent, caused by the client’s movement, exceeds Hth .2) Resource Bottleneck Warning: The available resources of server ea fall below the security threshold. Our edge AI system, addressing both EAD and EAM, can be modeled as a time-scaled boxing problem. Initial deployment allocates resources, and subsequent migration dynamically adjusts AI agent placement within edge server capacity limits. III. A LGORITHMS 

We present AntLLM, an algorithm comprising AntLLM Placement (ALP) and AntLLM Migration (ALM), which uses the Ant Colony Algorithm for initial placement and migration, then refines these decisions with LLM assistance for optimized placement and migration strategies. 

A. AntLLM Placement Algorithm (ALP) 

The ALP algorithm’s process is shown in Algorithm 1, and the main process is as follows. 1) Path Modeling. Frame deployment as a path selection problem: ants navigate from the first agent, choosing an edge server for each agent. Path nodes are represented as 

(a, e ), and path length is |Ar|.2) Pheromone Matrix. A two-dimensional matrix, 

pheromone [a][ e], represents the attractiveness of assigning AI agent a to edge node e.3) Heuristic Function. The heuristic function η, based on Problem EAD, evaluates a server’s suitability for an agent. It combines an initial score, a resource score, and a communication score. 4) Probability Selection Rules. The probability of an ant agent selecting deployment target e is determined by: 

Pa,e = [τa,e ]α·[ηa,e ]β    

> P
> e′[τa,e ′]α·[ηa,e ′]β

, (12) where τa,e represents the pheromone concentration de-posited by agent a at node e, and ηa,e is a heuristic value. Parameters α and β weigh the influence of pheromones and the heuristic, respectively. 5) Path Construction. Each ant builds a complete deploy-ment plan, verifying resource limits and avoiding illegal plans during construction. 6) Path Evaluation. Score each deployment path based on: total deployment delay, total resource expenditure, and communication dependency cost between agents. 7) Pheromone Update. After all ants have completed de-ployment, update the pheromone matrix. Pheromones volatilize, and trails are reinforced based on evaluation: 

τa,e ← (1 − ρ) · τa,e + ∆ τa,e , (13) 8) Iterative Update. Iterate pheromone construction, evalua-tion, and update to determine the optimal placement. 

B. AntLLM Migration Algorithm (ALM) 

The ALM algorithm is illustreted in Algorithm 2. The details are as follows. 1) State Space Modeling. The search space consists of all transferable nodes N (ea) adjacent to the agent’s current server ea. Each ant attempts to migrate from ea to a server 

e′ within this space. 2) Pheromone Initialization. Initialize the pheromone matrix 

τe′ to reflect target node attractiveness. 3) Heuristic Function. The heuristic η uses Problem EAM to optimize latency reduction, migration time, and depen-dency latency. 4) Probabilistic Transfer Selection. Each ant selects the target server according to the following equation: 

Pe′ = [τe′ ]α·[ηe′ ]β    

> P
> e′′ ∈N(ea )[τe′′ ]α·[ηe′′ ]β

, (14) where α and β represent the importance of pheromones and inspiring information, respectively. 5) Construction of the Migration Plan. Each ant plans its migration. If the target node e′ lacks sufficient resources, the plan is invalid and penalized. 

Algorithm 2: ALM 

Input: Current deployment S, Resource status R,Performance threshold C, LLM model MLLM 

Output: Final migration plan Mfinal  

> 1

if performance degradation condition C is triggered 

then  

> 2

Initialize pheromone matrix τ and heuristic matrix 

η; 

> 3

for each ant k = 1 to m do  

> 4

Initialize empty migration plan Mk; 

> 5

for each task ti to migrate do  

> 6

Select a target server ej ∈ Ecand ; 

> 7

Assign ti to ej , update Mk; 

> 8

end  

> 9

Compute migration cost Ck; 

> 10

end  

> 11

Select the optimal plan: M ∗ = arg min k Ck; 

> 12

Update pheromone matrix τ ; 

> 13

Post-Optimization: Use MLLM to refine and validate M ∗; 

> 14

Obtain final optimized migration plan Mfinal ; 

> 15

end 

6) NetGain Evaluation. Prioritize schemes with higher mi-gration benefits: Fitness (e′) = ∆ Tlatency − (Tmig + γO overhead ), (15) Fitness decreases if resource constraints are violated. 7) Pheromone Update. It evaporates after each ant traversal: 

τe′ ← (1 − ρ) · τe′ , (16) Obtain the optimal scheme reinforcement: 

τe′ ← τe′ + Q 

> 1+ Cost (e′)

, (17) 8) Iterative optimization. Iterate the process and output the optimal migration target server, e∗.IV. S YSTEM IMPLEMENTATION 

Our system is implemented through AgentScope [5]. The following outlines the step-by-step system planning for de-ployment and migration, enabling autonomous deployment and migration of the distributed AI agent system. 

A. System Deployment Steps 

1) Task Analysis and Planning. An AI agent analyzes the user’s task document and uses a DialogAgent to automatically refine the goal, decompose it into sub-tasks handled by multiple agents, generate resource require-ments (CPU/Memory/Storage/Network) for each agent, and establish agent dependencies. 2) Obtain Real-time Server Resources. Paramiko is used to connect to the edge server and execute re-mote commands, retrieving real-time resources. Specif-2 3 4Number of Nodes                         

> 0
> 10
> 20
> 30
> 40
> 50 Initiation  Time  (s)
> AntLLM Gr eedy Polling Random 234Number of Nodes
> 0
> 20
> 40
> 60
> 80 Migration  Time  (s)
> AntLLM Gr eedy Polling Random 234Number of Nodes
> 0
> 5
> 10
> 15
> 20
> 25
> 30 Pr ocessing  Time  (s)
> AntLLM Gr eedy Polling Random 234Number of Nodes
> 0
> 20
> 40
> 60
> 80
> 100
> 120
> 140
> 160 Total  Time  (s)
> AntLLM Gr eedy Polling Random

(a) Initial Time (b) Migration Time (c) Process Time (d) Total Time Fig. 2 Delay Performance with different number of nodes 2 3 4Number of Nodes                         

> 0
> 1
> 2
> 3
> 4
> 5
> 6
> 7CPU  Usage  (%)
> AntLLM Gr eedy Polling Random 234Number of Nodes
> 0
> 5
> 10
> 15
> 20
> 25 Initiation  Time  (s)
> AntLLM Gr eedy Polling Random 234Number of Nodes
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0 Disk  Usage  (MB)
> AntLLM Gr eedy Polling Random 234Number of Nodes
> 0.0000
> 0.0005
> 0.0010
> 0.0015
> 0.0020
> 0.0025
> 0.0030
> 0.0035 VMs  Usage  (Mbps)
> AntLLM Gr eedy Polling Random

(a) CPU Usage (b) Memory Usage (c) Disk Usage (d) VMs Usage Fig. 3 Cost Performance with different number of nodes ically, network throughput is calculated by reading 

/proc/net/dev twice. 3) Optimal Resource Allocation. We calculate the server efficiency score using a defined function, then optimize deployment solutions with the AntLLM algorithm to generate the best option. 4) Remote Service Deployment. We start the AI agent by executing remote commands via SSH, verifying success-ful startup by checking the process ID. 5) Task Execution Coordination. Recursively invoke the execute function to process dependencies. 

B. Dynamic Migration Steps 

The system resumes the task after dynamic migration. 1) Triggering Conditions. Driven by user motivation, peri-odic detection, and event triggers. 2) Migration Decision. Re-evaluate candidate servers using the fetch server function, then determine the migration decision using the AntLLM algorithm. 3) Migration Execution. Migrate agent memory data via export/import functions to initiate a new instance on the target server while releasing the original resource. V. E XPERIMENTS 

A. Experimental Settings 

We evaluate our algorithms using Content-based image retrieval (CBIR) tasks. Edge servers, deployed on Huawe-icloud in Beijing, Shanghai, Guiyang, and Singapore, are configured with 2 cores and 2 GB of memory to meet AI agent requirements. Beijing, Shanghai, and Guiyang servers have 40 GB of disk space and 20 Mbps peak bandwidth, while the Singapore server has 70 GB of disk space and 30 Mbps peak bandwidth. Inter-server distances are calculated using Euclidean distance. The number of AI agents is determined by actual needs and allows flexible deployment and migration across edge servers. Balancing throughput and performance, each server can host a maximum of 10 concurrent AI agents. Each agent can access models like “gpt-4o”, “qwen-max”, “claude 3-7-sonnet 20250404”, “glm-4-plus”, and “yi-lightning” to utilize any API within “gpt-4o-mini” for task completion. 

Baselines: We evaluate performance against these baselines. 1) Greedy: It chooses the server with the highest score based on server capacity and AI agent resource requirements. 2) Random: AI agents are randomly assigned to servers. 3) Polling: Deploy AI agents sequentially. For each agent, select the first candidate server with sufficient resources. 

B. Experimental Results 

Performance with different number of servers. Fig.2 and Fig. 3 illustrate the various latencies and resource costs when the number of edge servers increases. Initialization and pro-cessing times decrease with more servers, while migration time increases. This indicates that the accumulation of migration time caused by more options brought by the increase in the number of servers, and the total time is greatly affected by the cumulative migration time. Moreover, Fig.2 and Fig. 3 show a trend of increasing with the increase in the number of servers. As the number of servers increases, CPU, memory, and disk usage per node decrease, while communication resource demands rise. This suggests that adding more servers reduces individual node resource con-sumption but increases inter-server communication needs. The performance hierarchy among different algorithms is: AntLLM 

< Greedy < Polling < Random. Despite minor differences among the baselines, the AntLLM algorithm reduces the total delay by an average of 10.31% and the resource consumption by 38.56% compared to the baseline algorithms. 5 6 7Number of Tasks                         

> 0
> 10
> 20
> 30
> 40
> 50 Initiation  Time  (s)
> AntLLM Gr eedy Polling Random 567Number of Tasks
> 0
> 20
> 40
> 60
> 80
> 100
> 120 Migration  Time  (s)
> AntLLM Gr eedy Polling Random 567Number of Tasks
> 0
> 5
> 10
> 15
> 20 Pr ocessing  Time  (s)
> AntLLM Gr eedy Polling Random 567Number of Tasks
> 0
> 25
> 50
> 75
> 100
> 125
> 150
> 175 Total  Time  (s)
> AntLLM Gr eedy Polling Random

(a) Initial Time (b) Migration Time (c) Process Time (d) Total Time Fig. 4 Delay Performance with different number of tasks 5 6 7Number of Tasks                         

> 0
> 1
> 2
> 3
> 4
> 5
> 6
> 7CPU  Usage  (%)
> AntLLM Gr eedy Polling Random 567Number of Tasks
> 0.0
> 2.5
> 5.0
> 7.5
> 10.0
> 12.5
> 15.0
> 17.5
> 20.0 Memory  Usage  (MB)
> AntLLM Gr eedy Polling Random 567Number of Tasks
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8 Disk  Usage  (MB)
> AntLLM Gr eedy Polling Random 567Number of Tasks
> 0.0000
> 0.0005
> 0.0010
> 0.0015
> 0.0020
> 0.0025
> 0.0030
> 0.0035 VMs  Usage  (Mbps)
> AntLLM Gr eedy Polling Random

(a) CPU Usage (b) Memory Usage (c) Disk Usage (d) VMs Usage Fig. 5 Cost Performance with different number of tasks 

Performance with different number of tasks. Fig. 4 and Fig. 5 illustrate the various latencies and resource costs when the number of tasks increases. As the number of tasks increases, both latency and resource costs rise. There are differences among each baseline, but the performance of the AntLLM algorithm is always optimal. Overall, the structural hierarchy is: AntLLM < Greedy < Polling < Random. Compared with the baseline algorithm, the AntLLM algorithm reduces the total delay by an average of 10.64% and the resource consumption by 49.61%. VI. C ONCLUSION 

This paper presented a distributed AI agent system capable of independently deploying and migrating agents to fulfill user tasks based on user requirements. To minimize deployment and migration overhead, we implemented AntLLM, a hybrid algorithm combining ant colony optimization with LLM as-sistance. The experimental results showed that the AntLLM algorithm is effective. In future work, we will further consider the online incremental deployment mechanism to achieve real-time policy updates when task requirements change. REFERENCES [1] J. Lou, Z. Tang, and W. Jia, “Energy-efficient joint task assignment and migration in data centers: A deep reinforcement learning approach,” IEEE Transactions on Network and Service Management, vol. 20, no. 2, pp. 961–973, 2022. [2] F. Mou, J. Lou, Z. Tang, Y. Wu, W. Jia, Y. Zhang, and W. Zhao, “Adaptive digital twin migration in vehicular edge computing and networks,” IEEE Transactions on Vehicular Technology, 2024. [3] Z. Tang, F. Mou, J. Lou, W. Jia, Y. Wu, and W. Zhao, “Multi-user layer-aware online container migration in edge-assisted vehicular networks,” IEEE/ACM Transactions on Networking, vol. 32, no. 2, pp. 1807–1822, 2023. [4] Z. Tang, X. Zhou, F. Zhang, W. Jia, and W. Zhao, “Migration mod-eling and learning algorithms for containers in fog computing,” IEEE Transactions on Services Computing, vol. 12, no. 5, pp. 712–725, 2018. [5] D. Gao, Z. Li, X. Pan, W. Kuang, Z. Ma, B. Qian, F. Wei, W. Zhang, Y. Xie, D. Chen, L. Yao, H. Peng, Z. Zhang, L. Zhu, C. Cheng, H. Shi, Y. Li, B. Ding, and J. Zhou, “Agentscope: A flexible yet robust multi-agent platform,” 2024. [6] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, “Edge intelligence: Paving the last mile of artificial intelligence with edge computing,” Proceedings of the IEEE, vol. 107, no. 8, pp. 1738–1762, 2019. [7] L. Chen, C. Shen, P. Zhou, and J. Xu, “Collaborative service placement for edge computing in dense small cell networks,” IEEE Transactions on Mobile Computing, vol. 20, no. 2, pp. 377–390, 2021. [8] DeepSeek-AI and et al., “Deepseek-v3 technical report,” 2025. [9] T. Bai, C. Pan, Y. Deng, M. Elkashlan, A. Nallanathan, and L. Hanzo, “Latency minimization for intelligent reflecting surface aided mobile edge computing,” IEEE Journal on Selected Areas in Communications, vol. 38, no. 11, pp. 2666–2682, 2020. [10] Z. Yao, Z. Tang, W. Yang, and W. Jia, “ Enhancing LLM QoS through Cloud-Edge Collaboration: A Diffusion-based Multi-Agent Reinforce-ment Learning Approach ,” IEEE Transactions on Services Computing, no. 01, pp. 1–17, Apr. 2025, doi: 10.1109/TSC.2025.3562362. [11] X. Chi, H. Chen, G. Li, Z. Ni, N. Jiang, and F. Xia, “Edsp-edge: Efficient dynamic edge service entity placement for mobile virtual reality systems,” IEEE Transactions on Wireless Communications, vol. 23, no. 4, pp. 2771–2783, 2024. [12] F. Liu, G. Tang, Y. Li, Z. Cai, X. Zhang, and T. Zhou, “A survey on edge computing systems and tools,” Proceedings of the IEEE, vol. 107, no. 8, pp. 1537–1562, 2019. [13] S. Wang, C. Yuen, W. Ni, Y. L. Guan, and T. Lv, “Multiagent deep rein-forcement learning for cost- and delay-sensitive virtual network function placement and routing,” IEEE Transactions on Communications, vol. 70, no. 8, pp. 5208–5224, 2022. [14] S. Greengard, The internet of things. MIT press, 2021. [15] T. Wang, Y. Mei, W. Jia, X. Zheng, G. Wang, and M. Xie, “Edge-based differential privacy computing for sensor–cloud systems,” Journal of Parallel and Distributed Computing, vol. 136, pp. 75–85, 2020. [16] S. Liu, L. Liu, J. Tang, B. Yu, Y. Wang, and W. Shi, “Edge computing for autonomous driving: Opportunities and challenges,” Proceedings of the IEEE, vol. 107, no. 8, pp. 1697–1716, 2019. [17] S. Tuli, F. Mirhakimi, S. Pallewatta, S. Zawad, G. Casale, B. Javadi, F. Yan, R. Buyya, and N. R. Jennings, “Ai augmented edge and fog computing: Trends and challenges,” Journal of Network and Computer Applications, vol. 216, p. 103648, 2023.
