Title: 

URL Source: http://arxiv.org/pdf/2309.14365v1

Published Time: Sat, 30 Sep 2023 00:02:58 GMT

Markdown Content:
# An In-depth Survey of Large Language Model-based Artificial Intelligence Agents 

## Pengyu Zhao ∗, Zijian Jin ∗, Ning Cheng 

Beijing Jiaotong University, New York University, zj2076@nyu.edu 

{pengyuzhao, ningcheng }@bjtu.edu.cn 

Abstract 

Due to the powerful capabilities demonstrated by large language model (LLM), there has been a recent surge in efforts to integrate them with AI agents to enhance their performance. In this paper, we have explored the core differences and characteristics between LLM-based AI agents and traditional AI agents. Specifically, we first compare the fundamental characteristics of these two types of agents, clarifying the significant advantages of LLM-based agents in handling natural language, knowledge storage, and reasoning capabilities. Subsequently, we conducted an in-depth analysis of the key components of AI agents, including planning, memory, and tool use. Particularly, for the crucial component of memory, this paper introduced an innovative classification scheme, not only departing from traditional classification methods but also providing a fresh perspective on the design of an AI agent’s memory system. We firmly believe that in-depth research and understanding of these core components will lay a solid foundation for the future advancement of AI agent technology. At the end of the paper, we provide directional suggestions for further research in this field, with the hope of offering valuable insights to scholars and researchers in the field. 

Keywords: AI agents, Survey, Large language model 

## 1. Introduction 

The notion of intelligent agents can trace its roots back to the research of the mid to late 20th century. Pioneering contributions in this realm encompass Hewitt’s Actor model (Hewitt et al., 1973) and Minsky’s innovative conceptualization in the ’So-ciety of Mind’ (Minsky, 1988) which still trigger some new ideas recently eg: ”Mindstorms in Nat-ural Language-Based Societies of Mind” (Zhuge and et al., 2023).In the 1990s, Russell introduced the framework for intelligent and rational agents (Russell and Norvig, 2010), which has since be-come a foundational theory in this field. The ad-vent of deep neural networks post-2012 marked a significant shift in the AI landscape. Leveraging the power of backpropagation (Rumelhart et al., 1986) for training deep models, researchers be-gan to explore more sophisticated agent behaviors, transcending beyond traditional rule-based meth-ods. Among the emergent methodologies, Rein-forcement Learning (RL) stood out as a paradigm where agents learn optimal behavior through inter-actions with the environment and receiving feed-back in the form of rewards or penalties. In 2013, DeepMind (Mnih et al., 2013) used RL to play the Atair Game and win humans’ performance which indicates that AI Agents are available to outper-form human capabilities in specific areas. The in-corporation of neural networks into RL, often re-ferred to as Deep Reinforcement Learning (DRL) (Li, 2017), allowed for the tackling of previously in-

> ∗

Equal contribution. 

tractable problems, bridging the gap between high-dimensional input spaces and complex decision-making processes (Arulkumaran et al., 2017). De-spite the promising advancements offered by DRL, certain challenges persist. Chief among these is the issue of generalization. Many reinforcement learn-ing agents, especially those trained in simulated environments, struggle to transfer their learned be-havior to new or slightly altered scenarios, often termed as domain adaptation (Arndt et al., 2020). Training these agents can also be computationally intensive, often requiring vast amounts of inter-actions to achieve satisfactory performance. Fur-thermore, Reinforcement learning training strug-gles with convergence and the design of reward functions can be challenging, particularly in real-world scenarios, and can be a daunting and often unfeasible task. This hampers the rapid develop-ment and deployment of RL-based agents in di-verse environments. In 2020, OpenAI released GPT3 (Brown et al., 2020) with 175 billion parameters, making it the largest publicly available language model at the time. These models, characterized by their im-mense size and capacity, have shown exceptional prowess in generalization across a myriad of tasks. The ability of LLMs to understand and gener-ate language allows them to act as a foundational model for a wide range of applications (Huang and Chang, 2022). Their inherent generalization capabilities make them ideal candidates to serve as base models for universal agents. By harness-

> arXiv:2309.14365v1 [cs.CL] 23 Sep 2023

ing the vast knowledge embedded within LLMs, researchers are now exploring hybrid models, in-tegrating the strengths of reinforcement learning with the generalization capacities of LLMs (Hu et al., 2023). This symbiotic combination promises to pave the way for more robust, adaptable, and efficient intelligent agents in the future. In order to assist readers in quickly understanding the research history of AI agents and to further in-spire research in AI agents, in this paper, we offer a comprehensive and systematic review of AI agents based on the components 1 and applications. 

## 2. LLM vs. Traditional Agents 

Traditional agents were designed specifically to ad-dress certain problems. They primarily relied on predetermined algorithms or rule sets, excelling in tasks they were built for. However, they often struggled with generalization and reasoning when confronted with tasks outside their initial scope. The introduction of Large Language Models (LLMs) has brought significant changes to AI agent design. These agents, trained on the exten-sive corpus, are not only proficient in understand-ing and generating natural language but also dis-play strong generalization abilities. This capability allows them to easily integrate with various tools, enhancing their versatility. On the other hand, the emergent abilities of Large Language Models (Wei et al., 2022a) shows that LLMs are also good at reasoning which can help them learn from fault behavior. Taking game exploration as an example, espe-cially in the Minecraft setting, the differences be-tween LLM-based agents like VOYAGER (Wang et al., 2023a) and traditional RL agents are ev-ident. LLM agents, with their rich pre-trained knowledge, have an advantage in decision-making strategies even without task-specific training. On the other hand, traditional RL agents often need to start from scratch in new environments, rely-ing heavily on interaction to learn. In this sce-nario, VOYAGER showcases better generalization and data efficiency. 

## 3. Components of AI Agents 

3.1. Overview 

The LLM-powered AI agent system relies on LLM to function as its brain, which is supported by sev-eral crucial components that deploy various impor-tant functions. These functions, including plan-ning, memory, and tool use, have been studied in-dependently and thoughtfully in the past and have a well-established history. In this survey, we will 

> 1The key components of AI agents were originally defined at https://lilianweng.github.io/posts/2023-06-23-agent/

introduce the research history of each individual functional model, mainstream methods, combina-tion methods with the AI agent, and potential di-rections for the future. We hope that this historical information will serve as an inspiration for the fu-ture development of AI agents. It is worth noting that the integration of these three functional mod-els is still a relatively new concept. 

3.2. Planning 

The goal of planning is to design a series of ac-tions to facilitate state transitions and ultimately achieve the desired task. As shown in the left of Figure 1, this component, functioning as an in-dividual module, has been integrated in various applications, such as robot manipulations (Chen et al., 2021), robot navigation (Lo et al., 2018), and service robots (Li and Ding, 2023). And the existing works, such as methods using the planning domain description language (PDDL) (Aeronau-tiques et al., 1998; Fox and Long, 2003; Jiang et al., 2019) and hierarchical planning frameworks (Erol et al., 1994; Su´ arez-Hern´ andez et al., 2018; Guo et al., 2023), have greatly propelled the advance-ment of planning systems. Recently, with signif-icant successes achieved by LLMs in various do-mains, numerous studies have been exploring the utilization of LLMs to enhance the planning and execution capabilities of AI agents. Benefiting from the powerful inference capabilities of LLM, LLM-based AI agents can efficiently decompose complex tasks or instructions into a series of sub-tasks or simpler instructions (i.e., planning). For instance, as shown in the top right of Figure 1, the LLM-based agent decomposes the complex instruc-tion “Put the banana on the counter” into a se-ries of simpler instructions which are easier for the agent to accomplish. Further, taking actions solely based on the initial plan formulated by the agent without considering external environmental feed-back may limit the performance of the agent. For example, as shown in the bottom right of Figure 1, an agent creates a plan for the instruction “Put the bat on the bed”, and the first step in the initial planning is “Pick up the baseball bat”, which may fail to execute when there is no ’bat’ nearby. How-ever, if the agent can self-reflection based on the feedback, it can refine the first step to ”Walk to the side of the baseball bat”, and then progressively work towards achieving the goal. Therefore, dur-ing the execution process, reflecting on and analyz-ing past behaviors and feedback, and subsequently adjusting the plan, are equally pivotal for the suc-cessful execution of tasks by AI agents. Next, we will introduce relevant works that utilize LLM for task decomposition and self-reflection. Agent 

Planning 

> Task decomposition
> Self-reflection

Put the banana on the counter 

> Step 2: Go to the counter .
> Step 3: Put down the banana.

Put a bat on the bed  

> Step 1: Walk to the side of the baseball bat.
> Step 2: Pick up the baseball bat.
> Step 3: Walk to the bed.
> Step 4: Lean the bat on bed.
> Step 1: Pick up the baseball bat.
> Refinement planning
> Step 1: Pick up the banana .
> Robot manipulations
> Applications
> Robot navigation
> Service robot
> .......
> LLM-based methods
> Representative
> works
> Plan domain
> description language
> Hierarichical planning
> framework
> .......
> You can only pick up the baseball
> bat if you're next to it, but it's not
> currently beside you.

Feedback Figure 1: Overview of the planning component of AI agent. Left introduces some applications and representative methods of planning. Right provides an example illustrating the working mechanism of an AI agent with task decomposition and self-reflection. 

3.2.1. Task Decomposition 

Task decomposition aims to decompose the com-plex task or instruction into a series of simpler sub-goals or sub-instructions for performing the task. For example, as shown in the top right of Fig-ure 1, given a task instruction ”Put the banana on the counter”, the agent will split it into three steps: 1. Pick up the banana. 2. Go to the counter. 3. Put down the banana. The exist-ing works mainly perform task decomposition by chain or tree of thought (Wei et al., 2022b; Ko-jima et al., 2022; Yao et al., 2023a) and PDDL with LLM (Liu et al., 2023a). Chain of thought can utilize a few examples or simple instructions to progressively guide LLM reasoning, in order to decompose complex tasks into a series of sim-pler tasks (Wei et al., 2022b; Zhang et al., 2022; Huang et al., 2022a; Wang et al., 2023b). Zhang et al. (Zhang et al., 2022) proposed a method for au-tomatically generating chain of thought samples. They first clustered the problems and then, for each cluster, selected representative questions to generate chain of thought samples in a zero-shot manner. Huang et al. (Huang et al., 2022a) uti-lized high-level tasks related to the given task and their decomposed planning steps as examples, and combined these examples with input information to construct prompts. Then, they employed LLM to predict the next steps of planning and added the generated steps to the original prompts, con-tinuing the prediction until the entire task was completed. Wang et al. (Wang et al., 2023b) pro-posed that by guiding LLM to first construct a series of plans and then progressively execute so-lutions, it can effectively alleviate the issue of in-termediate plans disappearing during the reason-ing process. Unlike linear thinking, the Tree of Thought (Long, 2023; Yao et al., 2023a) generates multiple branches of thoughts at each step to cre-ate a tree-like structure. Subsequently, searching on this tree of thought is conducted using meth-ods like breadth-first search or depth-first search. For evaluating each state, reasoning can be facili-tated using a ”value prompt” or assessment results can be generated through a voting mechanism. In addition, some research efforts consider combining LLM with PDDL for the purpose of planning tar-get problems (Xie et al., 2023; Liu et al., 2023a; Guan et al., 2023). For example, Liu et al. (Liu et al., 2023a) first conveyed the task description in the form of natural language to LLM for translat-ing to PDDL format by in-context learning, then they employed the classical planners to generate plans and converted them into natural language format by LLM again. 

3.2.2. Self-Reflection 

During the process of interacting with the environ-ment, AI agents can enhance their planning ability by reflecting on past actions by receiving feedback. There are many works attempt to combine LLM-based agents with the self-reflection (Yao et al., 2022; Huang et al., 2022b; Shinn et al., 2023; Liu et al., 2023b; Sun et al., 2023; Singh et al., 2023; Yao et al., 2023b; Chen and Chang, 2023). For ex-ample, Yao et al. (Yao et al., 2022) integrated ac-tions with the chain of thought, leveraging thought to formulate planning that guides the agent’s exe-cution of acts. Simultaneously, interactive execu-tion of actions in the environment further enhances the agent’s planning ability. Shinn et al. (Shinn et al., 2023) introduced a framework named Reflex-ion, in which the approach first generates actions through the Actor module and evaluates them. Then utilizes the self-reflection module to gener-ate feedback and store it in memory. When errors occur, this method can infer the actions that led to the errors and correct them, thereby continuously enhancing the agent’s capabilities. Liu et al. (Liu et al., 2023b) first rated the various outputs of the model based on human feedback, then they used prompt templates to construct these ratings into natural language forms and combined them with the outputs for fine-tuning the model, thereby en-abling it to learn self-reflection. Singh et al. (Singh et al., 2023) utilize Pythonic program and annota-tions to generate planning, wherein assertion func-tions are used to obtain feedback from the envi-ronment. When assertions are false, error recovery can be performed. Sun et al. (Sun et al., 2023) proposed a model named AdaPlanner, which uti-lizes two refiners to optimize and refine plans. One of the refiners collects information from the envi-ronment after executing an action, which is then utilized for subsequent actions. The other one ad-justs the existing plan based on feedback obtained from the external environment when the executed action fails to achieve its intended outcome. Simi-larly, Yao et al (Yao et al., 2023b). first finetuned a small language model as a retrospective model to generate feedback for past failures, and then ap-pended this feedback to the actor prompt as input of the large LLM for preventing the recurrence of similar errors and predicting the next action. 

3.3. Memory 

Memory can help individuals integrate past learned knowledge and experience events with their cur-rent state, thereby assisting in making more appro-priate decisions. In general, human memory can be categorized into three primary types: sensory memory, short-term memory, and long-term mem-ory (Camina and G¨ uell, 2017). Sensory memory is the collection of information through the senses of touch, hearing, vision, and other senses, and it has an extremely brief lifespan (Wan et al., 2020; Jung et al., 2019). Short-term memory refers to the pro-cess of handling information within a brief period, and it is typically carried out by working mem-ory (Hunter, 1957; Baddeley, 1983, 1997). In con-trast, long-term memory refers to memories that can be stored for an extended period, which en-compasses episodic memory and semantic memory. Episodic memory refers to the memory capacity for events that individuals have personally experi-enced, and it is often able to closely associate these events with contextual information (Tulving et al., 1972; Tulving, 1983). Semantic memory refers to the factual knowledge that individuals know, and this type of memory is unrelated to specific events and personal experiences (Tulving et al., 1972). Similarly, memory, as a key component of AI agents, can assist them in learning valuable knowl-edge from past information, thereby helping the agents perform tasks more effectively. To fully uti-lize the stored information in memory, some re-search has attempted to integrate AI agents with short-term memory (Kang et al., 2023; Peng et al., 2023), long-term memory (Vere and Bickmore, 1990; Kazemifard et al., 2014), and a combination of both (Nuxoll and Laird, 2007; Kim et al., 2023; Yao et al., 2023b; Shinn et al., 2023). In addition, since sensory memory can be regarded as the em-bedded representation of inputs such as text and images, similar to a sensory buffer, we consider sen-sory memory not to be part of the memory module of the AI agent. With the emergence of large lan-guage models (LLM), some works devoted to drive the development of AI agents using LLM. Consid-ering the characteristics of LLM, as shown in Fig-ure 2, we further redefine the concepts of memory types for AI agents and classify them into training memory, short-term memory, and long-term mem-ory. Training memory refers to the knowledge and facts that a model learns during the pre-training pro-cess, and this information is stored through model parameters. Existing research has shown that models can learn world knowledge (Rogers et al., 2021), relational knowledge (Petroni et al., 2019; Safavi and Koutra, 2021), common sense knowl-edge (Davison et al., 2019; Da et al., 2021; Bian et al., 2023), semantic knowledge (Tang et al., 2023), and syntactic knowledge (Chiang et al., 2020) during the pre-training phase. Therefore, by employing LLM for reasoning, the AI agent can implicitly recall this knowledge to enhance the model’s performance. Short-term memory refers to the temporary infor-mation that AI agents process during task execu-tion, such as the example information involved in the in-context learning process and the intermedi-ate results generated during LLM inference. Dur-ing the inference process, LLM temporarily stores and processes in-context information or intermedi-ate results, using them to improve the ability of the model. This is similar to human working memory, which temporarily holds and processes informa-tion in the short-term to support complex cognitive tasks (Gong et al.). Some works utilize in-context learning to improve the performance of LLM. They first combine some examples with input informa-tion to construct a prompt and then send this prompt to LLM to utilize short-term memory (Li et al., 2023b; Logeswaran et al., 2022; Omidvar and An, 2023). For example, Li et al. (Li et al., 2023b) pointed out that when provided with a con-text that is relevant to the task, it is important to ensure that its working memory is controlled by the context. Otherwise, the model should rely on the world knowledge obtained during the pre-Memory 

Intelligent Agent with LLM    

> Input Embedding
> Training Memory
> Stored through model parameters.
> Short-term Memory
> Temporary information that LLM process
> during task execution
> Long-term Memory
> Stored in an external storage system
> The knowledge and facts that LLM learns
> during the pre-training process.

Human's Memory 

> Sensory Memory
> Short-term Memory
> Episodic Memory
> Semantic Memory
> Long-term Memory

Figure 2: Mapping Structure of Memory: Left illustrates memory categories in human memory, while the right depicts memory categories in AI agents, which have been redefined based on the characteristics of LLM. training phase. Logeswaran et al. (Logeswaran et al., 2022) first combined some examples with input instructions as a prompt, and then gener-ated multiple candidate sub-goal plans using LLM. Subsequently, they employed a re-rank model to se-lect the most suitable plan from these candidates. Some works prompt LLM to output its thinking process and results in the form of chain-of-thought, or to feed the intermediate results from LLM’s inference into LLM for further reasoning (Huang et al., 2022a; Akyurek et al., 2023; Chen et al., 2023b,a; Zhang et al., 2023a; Chen et al., 2023c). For example, Zhang et al. (Zhang et al., 2023a) first guided the model to generate a chain of thought by engaging it in multi-turn dialogues based on the given context. Subsequently, they combined the context with the generated chain of thought to form samples, which are then used to assist the model in reasoning and prediction under new con-textual situations. Akyurek et al. (Akyurek et al., 2023) proposed a multi-agent collaborative system that includes two LLMs. One LLM is responsible for generating answers based on the input content, while the other LLM generates a textual critique based on the input and output of the first LLM to assist in error correction. Long-term memory refers to the information stored in an external storage system, and when AI agents use this memory, they can retrieve information rel-evant to the current context from the external stor-age. The utilization of long-term memory can be divided into three steps: information storage, in-formation retrieval, and information updating. In-formation storage aims to store essential informa-tion from the interactions between the agent and its environment. For example, Shuster et al. (Shus-ter et al., 2022) first generated a summary of the last interaction. If the generated summary is ”no persona,” it is not stored; otherwise, the summary information is stored in long-term memory. Zhang et al. (Zhang et al., 2023b) utilized a tabular for-mat to store memory in the form of key-value pairs. In this format, the observations and states serve as the keys, and the actions and their corresponding Q-values are stored as values. Liang et al. (Liang et al., 2023a) stored the relevant information from the interactions between the agent and the environ-ment. The information from the last interaction is stored in the flash memory for quick retrieval. The rest of the information is stored in the action mem-ory as long-term memory. Information retrieval aims to retrieve information relevant to the cur-rent context from long-term memory to assist the agent in performing tasks. For example, Lee et al. (Lee et al., 2023) first clarified the input infor-mation, then they employed dense passage retriev-ers to select relevant information from long-term memory. Afterward, they combined the selected information with the input information and used methods like chain-of-thought or few-shot learning to choose the most relevant information for task execution. Zhang et al. (Zhang et al., 2023b) first computed the similarity between the received in-formation and the keys stored in the long-term memory, and then selected the top k records with the highest similarity to assist the LLM’s decision-making. Information updating aims to update the stored long-term memory. For example, Zhong et al. (Zhong et al., 2023) designed a forgetting mech-anism based on the Ebbinghaus forgetting curve to simulate the updating process of human long-term memory. 

3.4. Tool Use 

Recent works have greatly propelled the devel-opment of LLMs, however, LLMs still fail to achieve satisfactory performance in certain sce-narios involving up-to-date information, computa-tional reasoning, and others. For example, when a user asks, ’Where is the global premiere of Op-penheimer?’, ChatGPT is unable to answer this question because the movie ’Oppenheimer’ is the latest information and is not included in the train-ing corpus of the LLM. To bridge these gaps, many efforts have been dedicated to integrating LLM with external tools to extend its capabilities. Some works aim to integrate LLM with specific tools such as web search (Nakano et al., 2021), translation (Thoppi-lan et al., 2022), calculators (Cobbe et al., 2021), and some plugins of ChatGPT 2. Some other works consider teaching LLMs to choose suitable tools or combine various tools to accomplish tasks. For example, Karpas et al. (Karpas et al., 2022) imple-mented a system named MRKL, which mainly con-sists of a language model, an adapter, and multiple experts (e.g., model or tools), where the adapter is utilized to select the appropriate expert to assist the language model in processing input requests. Parisi et al. (Parisi et al., 2022) designed an iter-ative self-play algorithm to assist LM in learning how to utilize external APIs by fine-tuning LM. In self-play, they first fine-tuned LM with a few sam-ples and then utilized it to generate the tool in-put for invoking the tool API to generate results, followed by an LM to infer an answer. If the re-ferred answer is similar to the golden answer, the task input and predicted results (i.e., tool input, tool result, and predicted answer) are appended to the corpus sets for further fine-tuning and itera-tion in the next round. Patil et al. (Patil et al., 2023) first constructed a dataset with the format of instruct-API pairs, and then fine-tuned LLM based on the dataset for aiding LLM to employ tools with zero-shot and retriever-aware. Similarly, Schick et al. (Schick et al., 2023) fine-tuned the LLM on a dataset containing API calls to help the LLM learn the ability to invoke APIs. Paranjape et al. (Paranjape et al., 2023) first retrieved the related examples with the input task as a prompt and then employed the LLM to implement infer-

> 2https://openai.com/blog/chatgpt-plugins

ence with chain reasoning. In this process, if the immediate step requires tools, the inference process is paused to execute the tools, and the output of the tools is inserted into the inference process. Li et al. (Li et al., 2023c) proposed the API bank to eval-uate the LLM’s ability to utilize tools and devised a tool-augmented LLM paradigm to alleviate the limitation of in-context length. Shen et al. (Shen et al., 2023) proposed a method to combine LLM with HuggingFace to enhance the performance of LLM. Specifically, the method first employs LLM to decompose complex tasks into a series of sub-tasks and then sequentially selects suitable models from HuggingFace to perform these sub-tasks. Lu et al. (Lu et al., 2023) designed a plug-and-play compositional reasoning method, which first plans the schedule of input tasks and then composes mul-tiple tools to execute sub-tasks for achieving the original task. Liang et al. (Liang et al., 2023b) first applied a multi-model foundation model to under-stand and plan the given instructions for selecting suitable APIs from the API platform, and then uti-lized an action executor to generate results based on the selected APIs. Besides, they also exploited the feedback of humans to optimize the ability of planning and choose APIs of LLM, and the docu-ment of API in API platform. Different from the above approaches, Cai et al. (Cai et al., 2023) first employed an LLM to generate tool for input task, and then utilized an LLM to perform task based on the generated tool. Specifically, for an incoming task, if the tool required by the task has been gen-erated, the tool will be invoked directly, otherwise, the LLM will first generates tool, and then uses it. 

## 4. Application 

AI Agent is not an emergent concept. As early as 1959, the world’s first complete artificial intelli-gence system, advice taker (McCarthy, 1959), was proposed. Subsequently, John McCarthy and oth-ers began to use the term Agent to describe the role that a computing program can play in a scene to achieve certain tasks in artificial intelligence. With reinforcement learning coming into promi-nence, the field of artificial intelligence has seen a number of notable AI agents based on reinforce-ment learning and gaming strategies, such as Al-phaGo (Silver et al., 2016), a Go agent launched by DeepMind in 2014. Similarly, OpenAI launched 

OpenAI Five (Berner and et al., 2019) for playing the game of Dota 2 in 2017 and DeepMind an-nounced AlphaStar (Vinyals et al., 2019) for play-ing StarCraft II. Recently, the emergence of Chat-GPT has made AI agents active once again. The LLM-based Agent also keeps emerging. In this pa-per, we focus on the latest LLM-based AI Agent applications and talk about the applications of AI Agent from seven aspects: chatbot, game, design, Category Application Description                                  

> Chatbot Pi Inflection’s chatting AI agent known for its emotional companion-ship and high emotional intelligence Game Voyager (Wang et al., 2023a) The first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention Coding GPT Engineer A AI coding agent that can generate an entire codebase based on a prompt Design Diagram An AI-powered and automatable design platform Research ChemCrow (Bran et al., 2023) An LLM chemistry agent designed to accomplish tasks across or-ganic synthesis, drug discovery, and materials design Agent (Boiko et al., 2023) An intelligent agent system that combines multiple large language models for autonomous design, planning, and execution of scien-tific experiments Collaboration DialOp (Lin et al., 2023a) AI assistants collaborating with one or more humans via natural language to help them make complex decisions MindOS An engine creating autonomous AI agents for users’ professional tasks MetaGPT An multi-agent framework assigning different roles to GPTs to form a collaborative software entity for complex tasks Multi-GPT An experimental multi-agent system where multiple “expertG-PTs” collaborate to perform a task and each has their own short and long-term memory and the ability to communicate with each other. Generative Agents (Park et al., 2023) Multiple AI agents for the interactive simulacra of human behavior General purpose Auto-GPT An AI agent chaining LLM “thoughts” together to autonomously achieve whatever goal users set BabyAGI An task-driven autonomous agent leveraging GPT-4 language model, Pinecone vector search, and the LangChain framework to perform a wide range of tasks across diverse domains SuperAGI A developer-centric open-source framework to build, manage and run useful Autonomous AI Agents AgentGPT A framework allow users to configure and deploy Autonomous AI agents rapidly

Table 1: LLM-based AI Agent applications. research, coding, collaboration, and general pur-pose, as shown in Tab. 1. 

4.1. Chatbot 

Pi 3 is a typical LLM-based chatting AI agent re-leased by Inflection. Like ChatGPT 4 and Claude 5,users can talk directly with Pi, but Pi not only serves productivity needs such as searching or an-swering questions but also focuses on emotional companionship. Pi is known for its high emotional intelligence. Users can communicate with Pi as naturally as they would with a close friend. 

4.2. Game 

No other LLM-based gaming intelligence has recently received more attention than Voy-ager (Wang et al., 2023a). Voyager is an AI agent with access to GPT-4 (OpenAI, 2023). Voyager shows remarkable proficiency in playing the game of Minecraft and is able to utilize a learned skill library to solve new tasks from scratch without hu-man intervention, demonstrating strong in-context lifelong learning capabilities. 

4.3. Coding 

Developers have always wanted to have a code generator to help improve programming efficiency. 

> 3

https://pi.ai/talk 

> 4

https://chat.openai.com 

> 5

https://www.anthropic.com/index/claude-2 

LLM-based agents are naturally used in code gen-eration. A very attractive coding agent is GPT Engineer 6, which can generate an entire codebase according to a prompt. GPT Engineer even learns the developer’s coding style and lets the devel-oper finish the coding project in just a few min-utes. What makes GPT Engineer unique is that GPT Engineer asks many detailed questions to al-low developers to clarify missing details instead of accepting these requests unconditionally made by developers. 

4.4. Design 

The idea of AI Agent has also been applied to de-sign. Diagram 7 is a representative AI-powered and automatable design platform with many products, including Magician, Genius, Automator, and UI-AI, for designing high-quality charts and graphs. Taking Genius and UI-AI as examples. Genius is equivalent to a design assistant, helping to trans-form users’ ideas into designs. Users only need to provide a product description and Genius can create fully editable UI designs. In addition, Ge-nius can provide design suggestions to help improve productivity. UI-AI contains a series of user inter-face AI models made for designers that leverage the latest advancements in AI combined with creative 

> 6

https://github.com/AntonOsika/gpt-engineer 

> 7

https://diagram.com/ prompting or multimodal prompts to generate de-sign assets. 

4.5. Research 

A number of AI agents for autonomous scientific research have emerged. ChemCrow (Bran et al., 2023) is an LLM chemistry agent designed to ac-complish various tasks such as organic synthesis, drug discovery, and materials design. It integrates 17 expert-designed chemistry tools and operates by prompting GPT-4 to provide specific instructions about the task and the format required. Specifi-cally, a set of tools is created by using a variety of chemistry-related packages and software. These tools and user prompts are provided to GPT-4 and GPT-4 determines its behavioral path before arriv-ing at the final answer through an automated, it-erative chain-of-thought process. Throughout the process, ChemCrow serves as an assistant to expert chemists while simultaneously lowering the entry barrier for non-experts by offering a simple inter-face to access accurate chemical knowledge. Agent (Boiko et al., 2023) is an exploration of emerging autonomous scientific research capabil-ities of large language models. It binds multiple LLMs together for autonomous design, planning, and execution of scientific experiments (eg., the synthesis experiment of ibuprofen and the cross-coupling experiment of Suzuki and Sonogashira reaction). Specifically, autonomous scientific re-search is accomplished through a series of tools for surfing the Web, reading documents, executing code, etc., and several LLMs for well-timed calls. 

4.6. Collaboration 

Collaboration is one of the most significant appli-cations of AI agents. Many researchers have al-ready started to develop the application by allow-ing different AI agents to collaborate with each other, such as AI lawyers, AI programmers, and AI finance to form a team to complete complex tasks together. DialOp (Lin et al., 2023a) de-scribes a simple collaborative morphology, in which AI assistants collaborate with one or more hu-mans via natural language to help them make com-plex decisions. The autonomous AI agents cur-rently created by MindOS 8 are also used for sim-ple human-agent collaboration to assist users with professional tasks. Compared to DialOp and Min-dOS, MetaGPT 9and Multi-GPT 10 allow multiple agents can automatically divide up the work and collaborate with each other to accomplish a task, with MetaGPT focusing more on software industry tasks. 

> 8https://mindos.com/marketplace
> 9https://github.com/geekan/MetaGPT
> 10 https://github.com/sidhq/Multi-GPT

Additionally, Generative Agents (Park et al., 2023) are introduced to simulate human behavior. By ex-tending LLMs, complete records of the experiences of the generative agents are stored using natural language, and over time these memories are syn-thesized to form higher-level reflections that are dynamically retrieved to plan behavior. End-users can interact with a town of 25 generative agents using natural language. The architecture behind these generative agents is expected to be applied in collaborative scenarios. 

4.7. General purpose 

In addition to specific applications, some AI agents are developed for general purposes. These AI agents generally perform a wide range of tasks across diverse domains and attempt to reach the goal by thinking of tasks to do, executing them, and learning from the results. Auto-GPT 11 

is one of the first examples of GPT-4 running fully autonomously. The feature of completing tasks autonomously without human intervention attracts people’s attention. Similar to Auto-GPT, BabyAGI 12 is a task-driven autonomous AI agent. BabyAGI constructs a task list dedicated to achiev-ing the goal, derives further tasks based on the pre-vious results, and executes these tasks in order of priority until the overall goal is achieved. More-over, SuperAGI 13 and AgentGPT 14 support the building and deployment of autonomous AI agents, and have it embark on any goal imaginable. Al-though these AI agents are not so perfect and even have some deficiencies, their presentation is cer-tainly an important step towards artificial general intelligence. 

4.8. Vision-Language model-based agent application 

LLM has already demonstrated outstanding capa-bilities in language-only scenarios. However, in some application scenarios, agents need to deal with multi-modal information, especially vision-language modalities. In such cases, modeling only the language information may not achieve satisfactory performance. Recent work considers equipping agents with the Vision-language model (VLM) to handle multi-modal information. In this subsection, we introduce some latest VLM-based agent applications. Some works attempt to ap-ply VLM in the field of embodied AI and robotics that are based on visual and language modalities. For example, Khandelwal et al. (Khandelwal et al., 

> 11 https://github.com/Significant-Gravitas/ Auto-GPT
> 12 https://github.com/yoheinakajima/babyagi
> 13 https://github.com/TransformerOptimus/ SuperAGI
> 14 https://github.com/reworkd/AgentGPT

2022) introduced CLIP (Radford et al., 2021) into Embodied Agents, and demonstrated that CLIP can effectively enhance the task performance of em-bodied AI. Driess et al. (Driess et al., 2023) com-bined ViT and PaLM to construct a multi-modal model named PaLM-E, which is applied in embod-ied reasoning. PaLM-E takes a multi-modal se-quence (i.e., text and image) as input and converts it into text and image embeddings. Specifically, the image embedding is generated by the ViT and a projector encode images. Then, the text and im-age embeddings serve as input to PaLM for infer-ring the decisions that the robot needs to execute. Finally, the decisions are transformed into actions by a low-level policy or planner. Some works fo-cus on the navigation task. For instance, Dorbala et al. (Dorbala et al., 2022) first used GPT-3 to break down navigation instructions into a series of sub-instructions. Then, at each time step, they utilized CLIP to select an image from the cur-rent panoramic view that corresponded to the sub-instructions, serving as the direction for the next navigation step. This process continued until the agent reached its target location. ZSON (Majum-dar et al., 2022) is an object-goal navigation agent designed to locate specific objects within an en-vironment. Besides, some works consider applied LVM in the field of multi-model conversational. For example, Video-ChatGPT (Maaz et al., 2023) is a video-based conversational agent fine-tuned us-ing video instruction data. It first employs the vi-sual encoder from CLIP to encode video frames into temporal and spatial features. Then, it uti-lizes a trainable adapter to map these features into the language space and combines them with query representations as inputs of LLM to generate re-sponses. Li et al.(Li et al., 2023a) introduce a conversational assistant for the biomedical field, named LLaVA-Med. It is continuously trained by LLaVA on multimodal biomedical datasets. 

## 5. Benchmarking 

Recently, LLM-based AI agents have attracted sig-nificant research interest. In order to evaluate the performance of the proposed agents, some works focus on designing more suitable benchmarks. For example, Valmeekam et al. (Valmeekam et al., 2023) focused on assessing the planning ability of LLMs, which is a key component of AI agents. Liu et al. (Liu et al., 2023d) designed a benchmark based on the WebShop and HotPotQA environ-ment. Their goal is to compare the performance of multiple agent architectures equipped with differ-ent LLMs. Li et al. (Li et al., 2023c) constructed a benchmark, named API Bank, to evaluate the ability of LLMs to use tools. Fan et al. (Fan et al., 2022) proposed a simulator based on Minecraft to assess the performance of open-ended embod-ied agent. Xu et al. (Xu et al., 2023) designed a benchmark, named GentBench, which consists of public and private sections, with the aim of com-prehensively evaluating the performance of agents. Specifically, GentBench includes a series of com-plex tasks that promote LLMs to employ exter-nal tools for addressing these challenges. Baner-jee (Banerjee et al., 2023) introduced an end-to-end benchmark that evaluates the performance of LLM-based chatbots by comparing generated answers with the gold answer. Lin et al. (Lin et al., 2023b) presented a task-based evaluation method, which assesses the capabilities of agents based on their task completion within the interac-tive environment. Liu et al. (Liu et al., 2023c) in-troduced a multi-dimensional benchmark, named AgentBench, which evaluates the performance of LLM across multiple environments. 

## 6. Conclusion 

In this paper, we presented a comprehensive and systematic survey of the LLM-based agents. We first introduced the difference between agents based on LLM and traditional methods, then re-viewed the related works from the perspectives of components and application of AI agents. Fur-thermore, we have explored some pressing issues that require solutions and valuable research direc-tions. With the development of LLM, an increas-ing amount of research attention has been directed toward the field of AI agents, resulting in the emer-gence of numerous new technologies and methods. Through this review, we aim to assist readers in swiftly grasping the key information and applica-tions of AI agents, and also provide insights into future research directions. 

## 7. Bibliographical References 

Constructions Aeronautiques, Adele Howe, Craig Knoblock, ISI Drew McDermott, Ashwin Ram, Manuela Veloso, Daniel Weld, David Wilkins SRI, Anthony Barrett, Dave Christianson, et al. 1998. Pddl— the planning domain definition lan-guage. Technical Report, Tech. Rep. 

Afra Feyza Akyurek, Ekin Akyurek, Ashwin Kalyan, Peter Clark, Derry Tanti Wijaya, and Niket Tandon. 2023. RL4F: Generating natural language feedback with reinforcement learning for repairing model outputs. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics , pages 7716–7733. Karol Arndt, Murtaza Hazara, Ali Ghadirzadeh, and Ville Kyrki. 2020. Meta reinforcement learn-ing for sim-to-real domain adaptation. In 2020 IEEE International Conference on Robotics and Automation (ICRA) , pages 2725–2731. IEEE. Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. 2017. Deep reinforcement learning: A brief survey. 

IEEE Signal Processing Magazine , 34(6):26–38. Alan D Baddeley. 1997. Human memory: Theory and practice . psychology press. Alan David Baddeley. 1983. Working mem-ory. Philosophical Transactions of the Royal Society of London. B, Biological Sciences ,302(1110):311–324. Debarag Banerjee, Pooja Singh, Arjun Avad-hanam, and Saksham Srivastava. 2023. Bench-marking llm powered chatbots: Methods and metrics. arXiv preprint arXiv:2308.04624 .Christopher Berner and Brockman et al. 2019. Dota 2 with large scale deep reinforcement learn-ing. arXiv preprint arXiv:1912.06680 .Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yao-jie Lu, and Ben He. 2023. Chatgpt is a knowl-edgeable but inexperienced solver: An investiga-tion of commonsense problem in large language models. arXiv preprint arXiv:2303.16421 .Daniil A Boiko, Robert MacKnight, and Gabe Gomes. 2023. Emergent autonomous scientific research capabilities of large language models. 

arXiv preprint arXiv:2304.05332 .Andres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller. 2023. Chemcrow: Augment-ing large-language models with chemistry tools. 

arXiv preprint arXiv:2304.05376 .Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sas-try, Amanda Askell, et al. 2020. Language mod-els are few-shot learners. Advances in neural in-formation processing systems , 33:1877–1901. Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. 2023. Large lan-guage models as tool makers. arXiv preprint arXiv:2305.17126 .Eduardo Camina and Francisco G¨ uell. 2017. The neuroanatomical, neurophysiological and psy-chological basis of memory: Current models and their origins. Frontiers in pharmacology , 8:438. Jingkai Chen, Brian C Williams, and Chuchu Fan. 2021. Optimal mixed discrete-continuous plan-ning for linear hybrid systems. In Proceedings of the 24th International Conference on Hybrid Systems: Computation and Control , pages 1–12. Jiuhai Chen, Lichang Chen, Heng Huang, and Tianyi Zhou. 2023a. When do you need chain-of-thought prompting for chatgpt? arXiv preprint arXiv:2304.03262 .Liting Chen, Lu Wang, Hang Dong, Yali Du, Jie Yan, Fangkai Yang, Shuang Li, Pu Zhao, Si Qin, Saravan Rajmohan, et al. 2023b. Introspective tips: Large language model for in-context deci-sion making. arXiv preprint arXiv:2305.11598 .Po-Lin Chen and Cheng-Shang Chang. 2023. Interact: Exploring the potentials of chat-gpt as a cooperative agent. arXiv preprint arXiv:2308.01552 .Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, and Ji-Rong Wen. 2023c. Chatcot: Tool-augmented chain-of-thought reasoning on \\ chat-based large lan-guage models. arXiv preprint arXiv:2305.14323 .Cheng-Han Chiang, Sung-Feng Huang, and Hung-Yi Lee. 2020. Pretrained language model em-bryology: The birth of albert. In Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 6813–6828. Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-ian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 .Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, and Antoine Bosselut. 2021. Analyzing common-sense emergence in few-shot knowledge models. 

arXiv preprint arXiv:2101.00297 .Joe Davison, Joshua Feldman, and Alexander M Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the conference on empirical methods in natural lan-guage processing and the 9th international joint conference on natural language processing , pages 1173–1178. Vishnu Sashank Dorbala, Gunnar Sigurdsson, Robinson Piramuthu, Jesse Thomason, and Gaurav S Sukhatme. 2022. Clip-nav: Using clip for zero-shot vision-and-language naviga-tion. arXiv preprint arXiv:2211.16649 .Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yev-gen Chebotar, Pierre Sermanet, Daniel Duck-worth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. 2023. Palm-e: An embodied multimodal language model. In Proceedings of the International Con-ference on Machine Learning , pages 8469–8488. Kutluhan Erol, James Hendler, and Dana S Nau. 1994. Htn planning: complexity and expres-sivity. In Proceedings of the Twelfth AAAI National Conference on Artificial Intelligence ,pages 1123–1128. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, An-drew Tang, De-An Huang, Yuke Zhu, and An-ima Anandkumar. 2022. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems , 35:18343–18362. Maria Fox and Derek Long. 2003. Pddl2. 1: An extension to pddl for expressing temporal plan-ning domains. Journal of artificial intelligence research , 20:61–124. Dongyu Gong, Xingchen Wan, and Dingmin Wang. Working memory capacity of chatgpt: An empir-ical study. Lin Guan, Karthik Valmeekam, Sarath Sreedha-ran, and Subbarao Kambhampati. 2023. Lever-aging pre-trained large language models to con-struct and utilize world models for model-based task planning. arXiv preprint arXiv:2305.14909 .Huihui Guo, Fan Wu, Yunchuan Qin, Ruihui Li, Keqin Li, and Kenli Li. 2023. Recent trends in task and motion planning for robotics: A survey. 

ACM Computing Surveys .Carl Hewitt, Peter Bishop, and Richard Steiger. 1973. A universal modular actor formalism for artificial intelligence. In Proceedings of the 3rd international joint conference on Artificial intel-ligence , pages 235–245. Bin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang, Zenglin Xu, and Bin Liu. 2023. Enabling efficient interaction between an algo-rithm agent and an llm: A reinforcement learn-ing approach. arXiv preprint arXiv:2306.03604 .Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. arXiv preprint arXiv:2212.10403 .Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022a. Language mod-els as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning , pages 9118– 9147. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. 2022b. Inner monologue: Em-bodied reasoning through planning with lan-guage models. arXiv preprint arXiv:2207.05608 .Ian ML Hunter. 1957. Memory: Facts and fallacies. Yu-qian Jiang, Shi-qi Zhang, Piyush Khandel-wal, and Peter Stone. 2019. Task planning in robotics: an empirical comparison of pddl-and asp-based systems. Frontiers of Information Technology & Electronic Engineering , 20:363– 373. Yei Hwan Jung, Byeonghak Park, Jong Uk Kim, and Tae-il Kim. 2019. Bioinspired electronics for artificial sensory systems. Advanced Materials ,31(34):1803637. Jikun Kang, Romain Laroche, Xindi Yuan, Adam Trischler, Xue Liu, and Jie Fu. 2023. Think before you act: Decision transformers with internal working memory. arXiv preprint arXiv:2305.16338 .Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, et al. 2022. Mrkl systems: Amodular, neuro-symbolic architecture that com-bines large language models, external knowledge sources and discrete reasoning. arXiv preprint arXiv:2205.00445 .Mohammad Kazemifard, Nasser Ghasem-Aghaee, Bryan L Koenig, and Tuncer I ¨Oren. 2014. An emotion understanding framework for intelligent agents based on episodic and semantic memo-ries. Autonomous agents and multi-agent sys-tems , 28:126–153. Apoorv Khandelwal, Luca Weihs, Roozbeh Mot-taghi, and Aniruddha Kembhavi. 2022. Sim-ple but effective: Clip embeddings for embodied ai. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,pages 14829–14838. Taewoon Kim, Michael Cochez, Vincent Fran¸ cois-Lavet, Mark Neerincx, and Piek Vossen. 2023. A machine with short-term, episodic, and semantic memory systems. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 37, pages 48–56. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. 

Advances in neural information processing sys-tems , 35:22199–22213. Gibbeum Lee, Volker Hartmann, Jongho Park, Dimitris Papailiopoulos, and Kangwook Lee. 2023. Prompted llms as chatbot modules for long open-domain conversation. arXiv preprint arXiv:2305.04533 .Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. 2023a. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. 

arXiv preprint arXiv:2306.00890 .Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar. 2023b. Large language models with controllable working memory. In 

Findings of the Association for Computational Linguistics: ACL , pages 1774–1793. Haizhen Li and Xilun Ding. 2023. Adaptive and intelligent robot task planning for home service: A review. Engineering Applications of Artificial Intelligence , 117:105618. Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023c. Api-bank: A benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244 .Yuxi Li. 2017. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274 .Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhou-jun Li. 2023a. Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. arXiv preprint arXiv:2304.13343 .Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, et al. 2023b. Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis. arXiv preprint arXiv:2303.16434 .Jessy Lin, Nicholas Tomlin, Jacob Andreas, and Jason Eisner. 2023a. Decision-oriented dia-logue for human-ai collaboration. arXiv preprint arXiv:2305.20076 .Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, and Qin Chen. 2023b. Agentsims: An open-source sandbox for large language model evaluation. arXiv preprint arXiv:2308.04026 .Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023a. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477 .Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023b. Chain of hindsight aligns lan-guage models with feedback. arXiv preprint arXiv:2302.02676 , 3. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xu-anyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023c. Agent-bench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688 .Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, et al. 2023d. Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents. arXiv preprint arXiv:2308.05960 .Shih-Yun Lo, Shiqi Zhang, and Peter Stone. 2018. Petlon: planning efficiently for task-level-optimal navigation. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems , pages 220–228. Lajanugen Logeswaran, Yao Fu, Moontae Lee, and Honglak Lee. 2022. Few-shot subgoal plan-ning with language models. arXiv preprint arXiv:2205.14288 .Jieyi Long. 2023. Large language model guided tree-of-thought. arXiv preprint arXiv:2305.08291 .Pan Lu, Baolin Peng, Hao Cheng, Michel Gal-ley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-play compositional reasoning with large lan-guage models. arXiv preprint arXiv:2304.09842 .Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424 .Arjun Majumdar, Gunjan Aggarwal, Bhavika De-vnani, Judy Hoffman, and Dhruv Batra. 2022. Zson: Zero-shot object-goal navigation using multimodal goal embeddings. Advances in Neural Information Processing Systems , pages 32340–32352. J McCarthy. 1959. Programs with common sense. In Proc. Teddington Conference on the Mecha-nization of Thought Processes, 1959 , pages 75– 91. Marvin L. Minsky. 1988. The Society of Mind . Si-mon & Schuster, New York. Volodymyr Mnih, Koray Kavukcuoglu, David Sil-ver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. 2013. Play-ing atari with deep reinforcement learning. 

CoRR , abs/1312.5602. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332 .Andrew M Nuxoll and John E Laird. 2007. Extend-ing cognitive architecture with episodic memory. In Proceedings of the 22nd national conference on Artificial intelligence-Volume 2 , pages 1560– 1565. Amin Omidvar and Aijun An. 2023. Empowering conversational agents using semantic in-context learning. In Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023) , pages 766–771. OpenAI. 2023. Gpt-4 technical report. Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. 2023. Art: Automatic multi-step reasoning and tool-use for large lan-guage models. arXiv preprint arXiv:2303.09014 .Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm: Tool augmented language models. arXiv preprint arXiv:2205.12255 .Joon Sung Park, Joseph C O’Brien, Carrie JCai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442 .Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. 2023. Gorilla: Large lan-guage model connected with massive apis. arXiv preprint arXiv:2305.15334 .Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. 2023. Check your facts and try again: Improv-ing large language models with external knowl-edge and automated feedback. arXiv preprint arXiv:2302.12813 .Fabio Petroni, Tim Rockt¨ aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the Confer-ence on Empirical Methods in Natural Language Processing and the 9th International Joint Con-ference on Natural Language Processing , pages 2463–2473. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In 

Proceedings of the 38th International Conference on Machine Learning , pages 8748–8763. Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2021. A primer in bertology: What we know about how bert works. Trans-actions of the Association for Computational Linguistics , 8:842–866. David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning represen-tations by back-propagating errors. nature ,323(6088):533–536. Stuart Russell and Peter Norvig. 2010. Artifi-cial Intelligence: A Modern Approach , 3 edition. Prentice Hall. Tara Safavi and Danai Koutra. 2021. Relational world knowledge representation in contextual language models: A review. arXiv preprint arXiv:2104.05837 .Timo Schick, Jane Dwivedi-Yu, Roberto Dess` ı, Roberta Raileanu, Maria Lomeli, Luke Zettle-moyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761 .Yongliang Shen, Kaitao Song, Xu Tan, Dong-sheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chat-gpt and its friends in huggingface. arXiv preprint arXiv:2303.17580 .Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an autonomous agent with dy-namic memory and self-reflection. arXiv preprint arXiv:2303.11366 .Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al. 2022. Blenderbot 3: a deployed conversational agent that continually learns to responsibly en-gage. arXiv preprint arXiv:2208.03188 .David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc-tot, et al. 2016. Mastering the game of go with deep neural networks and tree search. nature ,529(7587):484–489. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Di-eter Fox, Jesse Thomason, and Animesh Garg. 2023. Progprompt: Generating situated robot task plans using large language models. In Pro-ceedings of IEEE International Conference on Robotics and Automation , pages 11523–11530. Alejandro Su´ arez-Hern´ andez, Guillem Aleny` a, and Carme Torras. 2018. Interleaving hierarchical task planning and motion constraint testing for dual-arm manipulation. In 2018 IEEE/RSJ In-ternational Conference on Intelligent Robots and Systems , pages 4061–4066. Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. 2023. Adaplanner: Adaptive planning from feedback with language models. arXiv preprint arXiv:2305.16653 .Chao Tang, Dehao Huang, Wenqi Ge, Weiyu Liu, and Hong Zhang. 2023. Graspgpt: Leverag-ing semantic knowledge from a large language model for task-oriented grasping. arXiv preprint arXiv:2307.13204 .Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language mod-els for dialog applications. arXiv preprint arXiv:2201.08239 .Endel Tulving. 1983. Elements of episodic memory. Endel Tulving et al. 1972. Episodic and semantic memory. Organization of memory , 1(381-403):1. Karthik Valmeekam, Sarath Sreedharan, Matthew Marquez, Alberto Olmo, and Subbarao Kamb-hampati. 2023. On the planning abilities of large language models (a critical investigation with a proposed benchmark). arXiv preprint arXiv:2302.06706 .Steven Vere and Timothy Bickmore. 1990. A basic agent. Computational intelligence , 6(1):41–60. Oriol Vinyals, Igor Babuschkin, Wojciech M Czar-necki, Micha¨ el Mathieu, Andrew Dudzik, Jun-young Chung, David H Choi, Richard Pow-ell, Timo Ewalds, Petko Georgiev, et al. 2019. Grandmaster level in starcraft ii us-ing multi-agent reinforcement learning. Nature ,575(7782):350–354. Changjin Wan, Pingqiang Cai, Ming Wang, Yan Qian, Wei Huang, and Xiaodong Chen. 2020. Artificial sensory memory. Advanced Materials ,32(15):1902434. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291 .Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023b. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In Proceedings of the 61st An-nual Meeting of the Association for Computa-tional , pages 2609–2634. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raf-fel, Barret Zoph, Sebastian Borgeaud, Dani Yo-gatama, Maarten Bosma, Denny Zhou, Don-ald Metzler, et al. 2022a. Emergent abili-ties of large language models. arXiv preprint arXiv:2206.07682 .Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Pro-cessing Systems , 35:24824–24837. Yaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong, and Harold Soh. 2023. Trans-lating natural language to planning goals with large-language models. arXiv preprint arXiv:2302.05128 .Binfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao, and Dongkuan Xu. 2023. Gentopia: A collaborative platform for tool-augmented llms. arXiv preprint arXiv:2308.04030 .Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliber-ate problem solving with large language models. 

arXiv preprint arXiv:2305.10601 .Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 .Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, et al. 2023b. Retroformer: Retrospective large language agents with policy gradient opti-mization. arXiv preprint arXiv:2308.02151 .Bowen Zhang, Xianghua Fu, Daijun Ding, Hu Huang, Yangyang Li, and Liwen Jing. 2023a. Investigating chain-of-thought with chatgpt for stance detection on social media. arXiv preprint arXiv:2304.03087 .Danyang Zhang, Lu Chen, Situo Zhang, Hong-shen Xu, Zihan Zhao, and Kai Yu. 2023b. Large language model is semi-parametric re-inforcement learning agent. arXiv preprint arXiv:2306.07929 .Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. In Proceed-ings of the Eleventh International Conference on Learning Representations .Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. 2023. Memorybank: Enhancing large language models with long-term memory. 

arXiv preprint arXiv:2305.10250 .Mingchen Zhuge and Haozhe Liu et al. 2023. Mind-storms in natural language-based societies of mind.
