Title: Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models

URL Source: http://arxiv.org/pdf/2505.08803v1

Published Time: Thu, 15 May 2025 00:00:51 GMT

Markdown Content:
# Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models 

# Zizhao Hu 

University of Southern California Los Angeles, California, USA zizhaoh@usc.edu 

# Mohammad Rostami 

University of Southern California Los Angeles, California, USA rostamim@usc.edu 

# Jesse Thomason 

University of Southern California Los Angeles, California, USA jessetho@usc.edu 

## Abstract 

Recent research has highlighted the risk of generative model col-lapse, where performance progressively degrades when continually trained on self-generated data. However, existing exploration on model collapse is limited to single, unimodal models, limiting our understanding in more realistic scenarios, such as diverse multi-modal AI agents interacting autonomously through synthetic data and continually evolving. We expand the synthetic data training and model collapse study to multi-modal vision-language generative systems, such as vision-language models (VLMs) and text-to-image diffusion models, as well as recursive generate-train loops with multiple models. We find that model collapse, previously observed in single-modality generative models, exhibits distinct characteris-tics in the multi-modal context, such as improved vision-language alignment and increased variance in VLM image-captioning task. Additionally, we find that general approaches such as increased de-coding budgets, greater model diversity, and relabeling with frozen models can effectively mitigate model collapse. Our findings pro-vide initial insights and practical guidelines for reducing the risk of model collapse in self-improving multi-agent AI systems and curating robust multi-modal synthetic datasets. 

## 1 Introduction 

The key to training large-scale generative AI models is the avail-ability of large-scale training datasets. As models grow in size and capability, the amount of available human-authored data for training becomes a significant bottleneck. This bottleneck has prompted the exploration of synthetic data for training AI models. In the realm of LLMs, synthetic data has been used for pretrain-ing [ 1 , 14 , 24 ], continued pre-training [ 47 ], supervised finetuning for general tasks [ 25 ], or specialized tasks such as math [ 12 , 29 , 45 , 48 ]and coding [ 30 , 46 ], and post-training RL [ 4, 11 , 13 , 20 ]. Other than LLMs, synthetic data has also been used to train VLMs [ 28 ]. In continual learning, synthetic data has also been used to reduce catastrophic forgetting [40]. In model training, synthetic data offers advantages in terms of quantity, controllability, and privacy. However, because genera-tive models are often used to generate synthetic data for training generative models, model collapse through recursive training on self-generated data becomes a risk [ 3, 42 ]. We directly address sev-eral limitations of the existing literature studying such generative model collapse. In particular, existing works: (1) focus on characterizing model collapse in single-modality tasks (e.g., language generation or unconditional image gen-eration), providing limited insight into how collapse mani-fests in multi-modal generative systems; (2) focus on a single model’s self-consuming training loop, with-out providing insights on more realistic scenarios where multiple models, from multiple generative paradigms, are within such a loop; and (3) offer solutions to model collapse primarily dependent on mixing synthetic data with human-authored data, without exploring how to improve the robustness of synthetic data. These limitations are particularly concerning given that a grow-ing proportion of labeled web content is generated by two classes of multi-modal models: text-to-image diffusion models [ 7, 35 –37 ] and VLMs [ 5, 8 , 22 , 27 , 28 , 32 ]. In this study, we address these limitations by expanding the investigation of synthetic data training and model collapse to multi-modal, multi-agent scenarios, answering: (1) How does recursive synthetic data training impact multi-modal generative systems? (2) What properties characterize multi-modal model collapse? (3) Do model interactions make collapse better or worse? (4) How can we improve generative synthetic data to mitigate model collapse? Through extensive experiments on VLMs and diffusion models, we make several key discoveries: (1) Balancing synthetic data bias-variance trade-off should be the focus for reducing model collapse rather than increasing variance only, since variance does not always collapse for certain tasks, such as VLM image captioning; (2) increased decoding budgets generate more robust synthetic datasets, which can mitigate model collapse despite no sig-nificant change in data quality; (3) commonly used statistical quality measures correlate with the robustness of synthetic data against model collapse; and (4) adding more models to the recursive training loop can make the model collapse worse, unless the added models are frozen and pretrained on human-authored data. Our analysis centers around recursive synthetic data training under two paradigms: image-captioning VLMs and text-to-image diffusion models. Our work provides insight into the generate-train self-consuming loop in a multi-modal and multi-agent environment. 

## 2 Related work 

In this section, we briefly cover some representative existing works studying model training with synthetic data and model collapse, and provide background on multi-modal generative models. 

Model training using generative synthetic data. Generative models have been used for synthetic data curation for various down-stream NLP tasks [ 12 , 29 , 30 , 45 , 46 , 48 ] and vision tasks [ 9, 10 , 41 ]. 

> arXiv:2505.08803v1 [cs.LG] 10 May 2025

Two 

giraffes ... 

MSCOCO 

VLM 

Gen 0 

VLM 

Gen 1 

T2I 

Gen 0 

T2I 

Gen 1 

VLM  T2I 

Gen 0 

T2I 

Gen 1 

(1) Single model  (2)  Relabeling 

VLM 

Gen 1 

T2I 

Gen 0 

T2I 

Gen 1 

(3) Joint 

VLM 

Gen 0 

# Recursively train more generations, Gen 2, Gen 3, ... 

Two 

giraffes ... 

Synthetic Image 

> A vibrant
> picture of
> giraffes ...

Synthetic Caption 

VLM 

Gen 0  T2I 

VLM 

Gen 1 Figure 1: Recursive finetuning configurations for investigating model collapse: (1) Single model recursive finetuning (“T2I” stands for text-to-image diffusion model), (2) Finetuning with VLM or diffusion relabeling, and (3) Joint recursive finetuning. (1) allows independent study of the effect of recursive training on VLMs and text-to-image diffusion models. (2) enables information integration from another generative paradigm, which simulates a multi-agent environment where a frozen grounding model is available and used to label synthetic data. (3) simulates a multi-agent environment where all models have full freedom to update their weights using available synthetic data. Model collapse. Model collapse is a concept first coined by Shu-mailov et al. [ 42 ]. It is a phenomenon in which models recursively trained on self-generated data experience performance degradation. It is hypothesized that model collapse happens because synthetic data lacks the diversity and nuances of real-world data. This issue has been identified in both language models [ 39 , 42 , 43 ] and image-generation models [ 3]. These works characterize model collapse by reduced perplexity and increased repetition in outputs generated by LLMs and diversity and quality degradation in output images of diffusion models. Addressing model collapse is critical, considering that using AI-generated synthetic data for training large AI models is becoming unavoidable as we exhaust human-authored data. 

Text-to-image diffusion models and VLMs. Text-to-image mod-els like DALL-E [ 7, 35 , 36 ], Stable Diffusion [ 37 ], Imagen [ 19 ], and Midjourney have revolutionized image generation because they enable users to generate high-quality images directly from textual descriptions. VLMs such as CLIP [ 34 ] and BLIP [ 22 , 23 ] have ad-vanced vision-language understanding. The broader class of VLMs or multi-modal LLMs can handle more diverse tasks, including VQA [ 2 ], semantic segmentation [ 21 ], and even autoregressive im-age generation [ 44 ]. However, we concentrate our study of VLMs’ model collapse on the image-captioning task. 

## 3 Experiment settings 

Here, we detail the specific setups for the model collapse experi-ment and the model diversity experiment in separate subsections. Then, we describe the evaluation methods employed in our analysis. Finally, we explain the training settings and dataset used. 

## 3.1 Model collapse 

We consider three settings in which to analyze model collapse (see Figure 1). In setting 1, we study the characteristics of model collapse in a single diffusion model or VLM. In settings 2 and 3, we study the interaction between VLMs and diffusion models. Setting 2 adds a frozen VLM or diffusion model to relabel the diffusion or VLM syn-thetic data in the recursive loop. Setting 3 makes the frozen models in setting 2 also trainable, simulating a joint generating-training environment. For detailed data processing, hyperparameter setting, and model specifications, see Appendix B. 

Setting 1: Single model recursive fine-tuning. For the diffu-sion model, we use Stable Diffusion 1.5 [ 37 ]. Images are generated from a fixed subset of MSCOCO training captions. Newly generated images are paired with the original captions to fine-tune the model in the next generation. For the VLM, we use BLIP-2 [ 22 ], and the captions are generated from a fixed subset of MSCOCO training, guided by a constant prefix prompt. Newly generated captions are paired with the original images to fine-tune the model in the next generation. The results and analysis for this setting are presented in Sections 4.1–4.5, where we investigate model collapse character-istics in diffusion models and VLMs, variance behaviors, and the effects of hyperparameters and decoding budgets. 

Setting 2: Two-model recursive fine-tuning with frozen re-labeling. This setting simulates a scenario where a model of a different paradigm trained on human-authored data serves as a labeler to relabel the synthetic data. The two models that we use are the same as setting 1, but we enable interaction between the VLM and the Diffusion model. Specifically, before passing the gen-erated synthetic data to the next generation of training, we use a frozen VLM or Diffusion model to relabel the generated images or 

> 2

captions. Consequently, the conditioning modalities are updated for every generation. The effectiveness of this “relabeling” approach in mitigating model collapse is analyzed in Section 4.7. 

Setting 3: Two-model joint recursive fine-tuning. This set-ting simulates a scenario where a VLM and a diffusion model both dynamically evolve on synthetic data. This setting is similar to setting 2, except the grounding model is first fine-tuned on the syn-thetic data pairs before providing relabeling for the next generation. The results for this joint fine-tuning approach and its impact on model collapse are presented in Section 4.8. 

## 3.2 Model diversity 

To study how model diversity affects model collapse, we set up two different types of diversity settings: 1. hyperparameter diversity and 2. architectural diversity, defined as follows: 

Hyperparameter diversity. Compared with the baseline settings, we use different hyperparameters to generate the same amount of data for synthetic data training. For fair comparison, we choose the hyperparameters that do not impact decoding cost. For image generation, we use 5 different classifier-free guidance (CFG) levels (1,3,7,10,20) of Stable Diffusion 1.5, to generate 1000 images for each generation, with 200 images each. For image-captioning, we do the same with 5 different temperature values (0.6, 0.7, 0.8, 0.9, 1). 

Architectural diversity. We use additional diffusion models (Sta-ble Diffusion 1.4 and Flux.1) and additional VLMs (BLIP, MiniCPM-Llama3-V 2.5) in our experiments. To simplify the process, the additional models are frozen and only used for generating synthetic data, which is mixed with the synthetic data generated during the BLIP-2 and Stable Diffusion 1.5 recursive finetuning process. 

## 3.3 Evaluation settings 

We use a comprehensive set of evaluation metrics, aiming to pin-point the key characteristics of model collapse. 

Inherent properties. We use saturation, contrast, brightness, color standard deviation for images, and average length, and vocabulary size for captions. For definitions please refer to Appendix Section A. 

Repensentational variance. To quantify the variance of a modal-ity, we calculate its CLIP embedding variance. The shared CLIP embedding space allows direct comparison of the image and text. 

Quality. We use FID [ 16 ] and IS [ 49 ] for images, and BLEU [ 33 ], METEOR [ 6], and BERTScore [ 50 ] for captions. For definitions please refer to Appendix Section A. FID and BLEU are also used to quantify model collapse intensity. 

Vision-language alignment. We use CLIP Score [ 34 ] to mea-sure the alignment between captions and images. Following the modality gap [ 26 ] study in contrastive VLMs, we also included L2 modality gap (L2M) [ 26 ] and relative modality gap (RMG) [ 38 ]. These metrics allow an understanding of the embedding distribu-tion shift. Since the modality gap has been found to correlate with information imbalance between modalities [ 38 ], we use it to mea-sure information balance change between modalities. 

Gender biases. When diffusion models are given a gender-neutral prompt about an occupation, and asked to generate multiple im-ages of a person in this occupation, the genders of the generated persons might be biased towards male or female. To study how this bias changes during recursive finetuning, we use a gender clas-sifier (described in Appendix Section A) to classify the gender of the person in the recursively generated images, given 100 gender neutral prompts for 10 different occupations each. The classifier outputs male, female. The ratio of female and male predictions is then calculated, and the metric is denoted as 𝑛 − 𝐵 , where 𝑛 is a ratio ranging from 0.5 to 1, with 𝐵 indicating the direction of bias: 

𝑀 signifying a bias towards male and 𝐹 towards female. 

## 3.4 Training settings 

We employ full finetuning to update all parameters for all exper-iments. While efficient low-resource finetuning approaches such as LoRA [ 18 ] are viable solutions for updating large models, we opt for full model finetuning to study model collapse. Full model finetuning allows our findings to be extendable to continued pre-training scenarios. In our model collapse investigation, we consider a “ Recursive Finetuning ” setting, where the model generates data, fine-tunes on the generated data, and continues this loop for several generations. In addition, we have two baseline settings: “Real Finetuning ”, where the synthetic data is replaced with the human-authored data, and “ Gen 0 Finetuning ”, where the recur-sively generated synthetic data is replaced with the synthetic data generated by the “generation 0” model (i.e., the pretrained model). 

## 3.5 Dataset 

We use an MSCOCO subset of 1000 samples for creating initial con-ditions for generation and setting up the fine-tuning baselines. This also enables meaningful evaluation on the MSCOCO evaluation set. For the subset configurations, please see Section B.1 

## 4 Analysis 

We first study the characteristics of model collapse in text-to-image diffusion models (Section 4.1) and VLMs (Section 4.2). We then challenge the commonly believed universal variance reduction in model collapse by providing evidence that it does not happen in VLM image-captioning tasks (Section 4.3). Next, we study what aspects of synthetic data can slow down model collapse (Section 4.4). We also show that decoding budgets correlate with less model collapse, shown as better FID at recursive generation 10 (Section 4.5). Finally, we provide several insights on how to reduce model collapse with synthetic data using diverse model interactions (Sections 4.6– 4.8). 

## 4.1 Characteristics of model collapse in text-to-image diffusion models 

Loss of variance but improved alignment. We observe that recursive finetuning has a similar trend as “Gen 0 Finetuning”, but with more significant shifts in terms of saturation, RMG, L2M, CLIP variance, FID, and CLIP Score (Figure 2). Among these metric shifts, 

> 3

Figure 2: Recursive finetuning shifts generated image properties. Mean (solid lines) and standard deviation (shaded area) are calculated from 5 groups of 200 evaluation samples. Two baselines are shown: “Gen 0 Finetune” stands for finetuning only on the synthetic data generated by the generation 0 (pretrained) model. “Real Finetune” stands for finetuning on the human-authored data. The most prominent effect of recursive finetuning compared to the other two baselines is the shifted saturation (1), reduced CLIP embedding variance (7), increased modality gaps (5, 6), and increased CLIP Score (10). 

the most prominent is that the image representational variance col-lapses faster in recursive finetuning than the other two baselines, as shown by a 4 times faster CLIP variance drop rate compared to MSCOCO data finetuning. The constantly improved caption-image alignment is driven by this reduced variance. This effect removes objects and high-frequency details that are not present in the cap-tions. These effects can be seen in Figure 9 as the details in the backgrounds vanish. 

Increased modality gaps. Both RMG (Figure 2 subfigure 5) and L2M (subfigure 6) show an increasing trend. However, such in-creases in modality gaps do not cause a drop in CLIP Score, which is a pair-wise measure. These combined suggest that the generated image distribution centroid shifts away from the centroid of the caption, but pair-wisely are more aligned with the captions. 

Saturation-shift driven gender biases. We observe an up-shift of image saturation in all finetuning settings, where a more drastic change for recursive finetuning is observed. This up-shift trend is established as early as generation 1. We also observed a gender shift in the generated images across generations (Figure 3), although the initial finetuning captions do not contain gendered words or information. To quantify the gender bias shift, we selected 10 occu-pations as prompts for the diffusion model to generate 100 images for each. Then, we used a pretrained gender classifier to predict the genders for each image. Figure 4 shows that compared to the model at generation 0, there is a general shift towards females, even in cases where female representation was already heavily biased. This observation indicates that such a shift is not driven by the distributional properties of the finetuning set, but by some unidirec-tional drifting properties in the finetuning process. We hypothesize that the up-shift of saturation is more related to female and female-confounding concepts. To test this hypothesis, we manually tuned down the saturation after each recursive finetuning generation for the synthetic images (Figure 4 “Gen 10 sat adjusted”). We observe that for most occupations, the gender bias was reversed towards the distribution at generation 0, confirming our hypothesis. 

Figure 3: Gender bias shift: In this example, we observe a shift towards females in tandem with saturation upshift. 

## 4.2 Characteristics of model collapse in VLMs 

Large performance discrepancy between synthetic training and MSCOCO data training at generation 1. We observe a large metrics gap at generation 1 between synthetic and MSCOCO data training settings, where a full episode of synthetic data finetuning is finished in both recursive finetuning and “Gen 0 Finetuning”. This gap is caused by the large discrepancies between the MSCOCO captions and the zero-shot (generation 0) synthetic captions. This gap is only seen in VLMs because the zero-shot image generated by the diffusion model is similar to the MSCOCO images, while the VLMs zero-shot caption is significantly longer than the MSCOCO captions. Thus, without the knowledge from the MSCOCO set cap-tions, the synthetic finetunings quickly shift the distribution away 

> 4

Figure 4: Gender bias shift: we see a shift to the female side after 10 generations of recursive finetuning on the diffusion model. After manually downshifting the saturation after each generation, we see a reduced gender bias shift, indicat-ing that saturation drives the gender bias shift in recursive finetuning of the Stalbe Diffusion 1.4 model. 

from MSCOCO caption finetuning, and maintain such gaps for the remaining generations. 

Different collapsing behavior compared to LLMs. Although sharing the same generative domain in language, VLMs in the image-captioning recursive finetuning show a different behavior compared to LLMs, as observed in previous research. For the vocab-ulary used for the test images, the recursive finetuning uses about 200 more tokens than “Real Finetuning” and 100 more than “Gen 0 Finetuning” at generation 10 (Figure 5 subfigure 2). This observation suggests that the recursive finetuning causes the model to explore more tokens to explain the image, which is also reflected by the per-plexity explosion in subfigure 3, indicating a more drastic softmax entropy increase. This observation is the opposite of the previously observed LLM collapse, where the vocabulary and perplexity used are significantly reduced [ 42 , 43 ]. As for the captioning quality measure, we observe a more drastic drop of BLEU-4, ROUGE-L, METEOR, and SPICE scores in recursive finetuning, but a slower drop in terms of CIDEr which suggests that the synthetic data finetuning, including recursive finetuning, is able to maintain the semantics, but loses fluency or generates grammatically incorrect sentences. Since CIDEr is less sensitive to fluency, grammaticality, and details, which is the information lost in model collapse, the improved semantic alignment with the image can counteract and slow down the CIDEr drop. 

Variance increase. Different from the diffusion model collapse, we observe a steady increase in the variance of the generated cap-tions (Figure 6) due to the softmax entropy increase, reflected by the perplexity and vocabulary size increase. This observation also differs from the observation in single modality generative model col-lapse, where a variance collapse is a main property. The improved alignment is a shared property also seen in diffusion recursive fine-tuning. This phenomenon is due to the low sensitivity of CLIP embeddings towards grammar and more focus on object and prop-erty matching [ 38 ], which are improved through higher entropy and vocabulary usage. 

Alignment peaks then drops. In Figure 5 subfigures 7 and 9, recursive finetuning setting shows an initial increase in CIDEr through generation 5 and a drop thereafter. Similar trend is also seen in the CLIP Score through generation 3. The initial increase of these metrics reflects a higher alignment between the two modali-ties, even than that of the real image caption pairs, due to boosted object alignment. The later slow decrease is a compounding ef-fect of overfitting seen in the real finetuning (orange line), and the decrease in n-gram matching due to higher vocabulary size with similar average lengths. Both synthetic finetuning cases boost the alignment above the real image and caption baseline, with the “Gen 0 Finetuning” having a peak at generation 1 and recursive fine-tuning at generation 3. These early generation peaks suggest that synthetic data training can be used to improve modality alignment when the recursive loop is shallow. 

## 4.3 Variance shift during recursive finetuning 

Recent works [ 42 , 43 ] have discovered that the variance decreases during LLMs’ recursive finetuning. However, our experiments on multimodal models suggests that variance can be increased by the cross-modal interactions in VLM recursive finetuning. The different trend of variance shift in diffusion models and VLMs (Figure 7) can be explained through the lens of information imbalance. As suggested in a recent work [ 38 ], during contrastive VLM training, a model is trying to match the information content between vision and language. We claim that such an effect can also be extended to other vision-language generative paradigms, for example, the VLM and diffusion models. Both objectives explicitly or implicitly maximize the conditional likelihood from one modality to the other, and this process will bridge the variance gap between the two modalities. When the generation is from image to text, the high variance image will “drag” the variance of the text to a higher variance. 

## 4.4 Balanced bias-variance tradeoff mitigates model collapse 

Both VLM and diffusion generative paradigms provide hyperpa-rameters to control the bias-variance tradeoff during inference time without requiring additional training or affecting test-time com-plexity. This flexibility allows us to study how different trade-offs 

> 5

Figure 5: Recursive finetuning effect on the generated caption properties: Mean and standard deviation are calculated from 10 groups of 200 evaluation images randomly selected from the COCO evaluation set. Two baselines are shown: “Gen 0 Finetune” stands for finetuning on the synthetic data by the base (Generation 0) model. “Real Finetune” stands for finetuning on the MSCOCO data. The most prominent effect of recursive finetuning compared to the other two baselines is the vocabulary size and perplexity explosion (2, 3). 

Figure 6: Generations from VLM recursive finetuning show increased language variance, details, but reduced grammati-cal coherence. 

between bias and variance can affect the model collapse. We control the classifier-free guidance (CFG) scale [ 17 ] and the temperature of the diffusion model and VLM during the decoding process, respec-tively. A higher CFG scale and lower temperature generate images and captions with higher bias and lower variance. In Figure 8 we discover that, in general, the shifting bias and variance towards one 

Figure 7: PCA of CLIP embeddings of VLM and diffusion recursive finetuning experiments. Variance changes in dif-ferent directions for VLM and diffusion recursive finetuning. 

direction (first row) does not correlate well with less model collapse, but the better generation quality in terms of FID and BLEU-4 score (peaks at a balanced bias-variance trade-off) in Gen 0 has a high correlation with less model collapse. These correlations indicate that simple quality measures such as FID for images and BLEU-4 for captions can be good indicators for synthetic data robustness against model collapse, and the optimal values for both metrics require a balanced bias-variance trade-off. 

## 4.5 More decoding budgets mitigate model collapse 

Decoding budgets have been a recent focus on LLMs’ inference time scaling [ 11 , 31 , 32 ]. This trend drives us to study the correlation between the decoding budgets and the model’s robustness towards model collapse. We first notice that generated samples have the best FIDs at 50 denoising steps for diffusion models, while the FID remains stable for more denoising steps. However, at generation 10 of recursive finetuning, the image quality consistently increases as we increase the number of generation steps (Figure 9). 

> 6

Figure 8: Correlation between bias-variance trade-off hyper-parameters vs. Gen 10 quality measures. 

Figure 9: Increasing denoising budget for diffusion genera-tion reduces model collapse, as seen in text-to-image genera-tions for "a cat is sitting on top of a vehicle." The best quality (FID) of the generated image is the one with 50 denoising steps. 

## 4.6 Model and hyperparameter diversity mitigates model collapse 

In our model diversity experiments, we observed that incorporat-ing a variety of models with different hyperparameters and ar-chitectures led to a more robust recursive training process. Both approaches mitigates the model collapse in VLMs and diffusion recursive finetuning, as shown in Table 1. Note that this diversity study does not mix VLMs with diffusion models, which is studied as relabeling and joint finetuning in the following sections. 

## 4.7 Relabeling mitigates model collapse 

Relabeling. By introducing VLM models into the process of dif-fusion recursive training and vice versa, we made the conditional modality dynamic by relabeling it after every generation. For ex-ample, in the diffusion recursive training, a frozen VLM updates the caption of the newly generated synthetic image. This approach mitigates model collapse, shown as improved quality after 10 gen-erations of recursive training (Table 2). 

Table 1: Effect of model diversity on quality metrics at gen-eration 10. Both hyperparameter diversity and architectural diversity mitigate model collapse.                                   

> Settings Gen 10 FID ↓Gen 10 BLEU@4 ↑
> Diffusion Model Recursive Finetuning
> SD v1.4 253.2 ±5.4 -
> Hyperparameter Diversity
> SD v1.4 (2 CFGs, 3, 7) 241.3 ±6.2 -
> Architectural Diversity
> SD v1.4 “Gen 0 Finetune” 242.1 ±5.8 -SD v1.4 + SD v1.5 207.3 ±7.5 -SD v1.4 + Flux.1 194.2 -
> VLM Model Recursive Finetuning
> BLIP-2 -0.017 ±0.012
> Hyperparameter Diversity
> BLIP-2 (2 temperatures, 1, 0.7) -0.024 ±0.014
> Architectural Diversity
> BLIP-2 “Gen 0 Finetune” -0.039 ±0.0012 BLIP-2 + MiniCPM v2.6 -0.071

Table 2: Quality metrics at generation 10 after recursive fine-tuning, demonstrating relabeling reduces model collapse in both diffusion and VLM models. 

Settings Gen 10 FID ↓ Gen 10 BLEU@4 ↑

Diffusion Model Recursive Fine-tuning 

Pure Recursive 253.2 ± 5.4 -VLM Relabeling 223.1 ± 8.2 -

With Prefix Tags 

"Synthetic image of" 244.5 ± 4.3 -"#" 245.2 ± 4.8 -"Synthetic N image of" 239.8 ± 11.0 -"A bright image of" 238.9 ± 6.5 -

VLM Model Recursive Fine-tuning 

Pure Recursive - 0.017 ± 0.012 Diffusion Relabeling - 0.032 ± 0.018 

Prefix tag. In addition to relabeling with generative models, we provide two simple baselines that also change the conditioning cap-tion in diffusion pretraining by rules. We set up 2 approaches: 1. We simply append the prefix word "Synthetic" to all gen 1 to gen 10 cap-tions, indicating the generated images are synthetic. The intuition behind the setting is that we assume the diffusion model already has some pretrained knowledge on the word “synthetic image of”, and we provide better alignment by adding this information. 2. We append "Synthetic 1", "Synthetic 2" to all gen 1 to gen 10 captions to provide additional information about the synthetic generations. In addition, we also conduct ablations on using other placeholder tokens such as “#” or semantic tokens such as “bright.” Similarly, in VLM recursive training, the frozen diffusion model generates 

> 7

images from the newly generated synthetic captions. Since the base-line and ablation experiment designs are not as intuitive, we do not provide them for VLM training. Theoretical basis for relabeling effectiveness. The effectiveness of relabeling stems from providing a novel grounding for the synthetic data, distinct from the model’s prior association with the conditioning texts or images. Instead of introducing a contradicting "one-to-many" mapping from the conditions to the synthetic data recursively, relabeling introduces a fresh condition. This can alleviate the competition inherent in forcing synthetic data into pre-established real-data relationships. The success of both relabeling and even simpler prefix tagging supports this idea, as both introduce a new conditioning signal. However, relabeling with a generative model offers a grounding that is semantically closer to the synthetic data, thereby guiding the model’s distribution shift in a less drastic manner compared to mere tagging. 

## 4.8 Joint finetuning exacerbates model collapse 

In our Setting 3 experiments (Figure 1 subfigure 3), we jointly fine-tune both the diffusion model and VLM on synthetic data pairs. This approach differs from the relabeling method (Setting 2) in that both models evolve together rather than having one model remain frozen. Surprisingly, our findings indicate that joint fine-tuning actually worsens model collapse compared to the relabeling approach. When both models are allowed to adapt to each other’s out-puts simultaneously, we observe an accelerated co-degradation process. Without a stable reference point (frozen model), both mod-els reinforce each other’s biases and errors, leading to more rapid distributional drift. This mutual adaptation creates a feedback loop that amplifies rather than mitigates the collapse process. Table 3 summarizes the results of joint fine-tuning compared to both single-model recursive fine-tuning and relabeling approaches. The joint approach shows significant degradation in generation quality across iterations, with faster deterioration of FID scores and lower BLEU@4 scores at generation 10 compared to the relabeling method. 

Table 3: Comparison of generation quality at generation 10 across different fine-tuning approaches. Joint fine-tuning of both models shows accelerated degradation in quality metrics compared to relabeling with a frozen model. 

Approach Gen 10 FID ↓ Gen 10 BLEU@4 ↑

Single Model Recursive 253.2 0.017 With Relabeling 223.1 0.032 

Joint Fine-tuning 312.5 0.009 

The joint fine-tuning results demonstrate that maintaining at least one frozen model is crucial when using synthetic data in multi-modal contexts. The frozen model serves as an anchor that prevents both models from drifting too far from the original data distribution. This finding highlights the importance of stability in multi-modal generative systems and suggests that completely self-contained recursive learning loops without external anchoring can rapidly destabilize. 

## 5 Conclusion 

This study investigates multimodal synthetic data finetuning and multi-modal model collapse. Our analysis provides insights into un-derstanding and mitigating model collapse in recursive fine-tuning. We highlight several key findings: (1) variance does not always collapse during model collapse, as evidenced in VLM image cap-tioning (Section 4.3); (2) increased decoding budgets generate more robust synthetic datasets (Section 4.5); (3) commonly used quality measures correlate well with robustness against model collapse (Section 4.4); and (4) model diversity is crucial for maintaining stability in recursive fine-tuning (Section 4.6). In addition, our experiments demonstrate that relabeling with a frozen model significantly slows model collapse compared to single-model recursive fine-tuning, while joint fine-tuning without a frozen model accelerates model collapse. This finding highlights the critical importance of maintaining human-authored data-grounded, frozen models in an autonomous multi-agent self-training loop to mitigate model collapse. 

## 6 Safe and Responsible Innovation Statement 

Our research investigates the risks of model collapse in multimodal vision-language generative systems when trained on synthetic data. While synthetic data offers benefits, its uncontrolled recursive use can lead to performance degradation. This work addresses poten-tial societal impacts by exploring methods to mitigate this collapse, ensuring the reliability of future AI systems that may rely on self-generated data. Ethical considerations include the potential for synthetic data to perpetuate or amplify biases present in the ini-tial models or training data. Our findings on the benefits of model diversity and frozen grounding models contribute to responsible deployment by suggesting architectural strategies to maintain sta-bility and prevent uncontrolled drift in autonomous multi-agent AI environments. By providing guidelines for robust synthetic data training, we aim to encourage responsible innovation in multimodal interaction research, minimizing the risks of misuse or unintended negative consequences arising from model collapse. 

## References  

> [1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ro-nen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio César Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Lil-iang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024. Phi-3
> 8

Technical Report: A Highly Capable Language Model Locally on Your Phone. arXiv:2404.14219 [cs.CL] https://arxiv.org/abs/2404.14219 [2] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, and Devi Parikh. 2016. VQA: Visual Question Answering. arXiv:1505.00468 [cs.CL] https://arxiv.org/abs/1505.00468 [3] Sadegh Alemohammad, Ernest Boix-Adsera, Riccardo Schneider, Antonia Creswell, Karel Lenc, Elan Rosenfeld, Scott Linderman, Alec Radford, J Zico Kolter, Durk P Kingma, et al . 2023. Self-consuming generative models go MAD. 

arXiv preprint arXiv:2307.01852 (2023). [4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen Technical Report. arXiv:2309.16609 [cs.CL] https://arxiv.org/abs/2309.16609 [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Jun-yang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv:2308.12966 [cs.CV] https://arxiv.org/abs/2308.12966 [6] Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization . 65–72. [7] James Betker, Gabriel Goh, Li Jing, † TimBrooks, Jianfeng Wang, Linjie Li, † Lon-gOuyang, † JuntangZhuang, † JoyceLee, † YufeiGuo, † WesamManassra, † Praful-laDhariwal, † CaseyChu, † YunxinJiao, and Aditya Ramesh. [n. d.]. Improving Im-age Generation with Better Captions. https://api.semanticscholar.org/CorpusID: 264403242 [8] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. 2025. Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling. arXiv:2412.05271 [cs.CV] https://arxiv.org/abs/2412.05271 [9] Aaron S Coyner, Jimmy S Chen, Ken Chang, Praveer Singh, Susan Ostmo, RV Paul Chan, Michael F Chiang, Jayashree Kalpathy-Cramer, J Peter Campbell, Imaging, Informatics in Retinopathy of Prematurity Consortium, et al . 2022. Synthetic medical images for robust, privacy-preserving training of artificial intelligence: application to retinopathy of prematurity diagnosis. Ophthalmology Science 2, 2 (2022), 100126. [10] Deniz Daum, Richard Osuala, Anneliese Riess, Georgios Kaissis, Julia A Schn-abel, and Maxime Di Folco. 2024. On Differentially Private 3D Medical Image Synthesis with Controllable Latent Diffusion Models. In MICCAI Workshop on Deep Generative Models . Springer, 139–149. [11] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 [cs.CL] https://arxiv.org/abs/2501.12948 [12] Leo Gao, John Schulman, and Jacob Hilton. 2023. GL-LAVA: Solving geometric problems with large language models. arXiv preprint arXiv:2307.09289 (2023). [13] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab-hishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind That-tai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasu-den Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsim-poukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Va-sic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Ro-han Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold-schlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ram-chandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowl-ing, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Cag-gioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Han-nah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, 

> 9

Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Has-son, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navy-ata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Niko-lay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyag-ina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Ran-gaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robin-son, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783 [14] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023. Textbooks Are All You Need. arXiv:2306.11644 [cs.CL] https://arxiv.org/abs/ 2306.11644 [15] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2022. CLIPScore: A Reference-free Evaluation Metric for Image Captioning. doi:10.48550/arXiv.2104.08718 arXiv:2104.08718 [cs]. [16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems 30 (2017). [17] Jonathan Ho and Tim Salimans. 2022. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 (2022). [18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2022). [19] Imagen-Team-Google, :, Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Lluis Castrejon, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, Zach Eaton-Rosen, Hongliang Fei, Nando de Freitas, Yilin Gao, Evgeny Gladchenko, Sergio Gómez Colmenarejo, Mandy Guo, Alex Haig, Will Hawkins, Hexiang Hu, Huilian Huang, Tobenna Peter Igwe, Christos Ka-planis, Siavash Khodadadeh, Yelin Kim, Ksenia Konyushkova, Karol Langner, Eric Lau, Rory Lawton, Shixin Luo, Soňa Mokrá, Henna Nandwani, Yasumasa Onoe, Aäron van den Oord, Zarana Parekh, Jordi Pont-Tuset, Hang Qi, Rui Qian, Deepak Ramachandran, Poorva Rane, Abdullah Rashwan, Ali Razavi, Robert Riachi, Hansa Srinivasan, Srivatsan Srinivasan, Robin Strudel, Benigno Uria, Oliver Wang, Su Wang, Austin Waters, Chris Wolff, Auriel Wright, Zhisheng Xiao, Hao Xiong, Keyang Xu, Marc van Zee, Junlin Zhang, Katie Zhang, Wenlei Zhou, Konrad Zolna, Ola Aboubakar, Canfer Akbulut, Oscar Akerlund, Isabela Albuquerque, Nina Anderson, Marco Andreetto, Lora Aroyo, Ben Bariach, David Barker, Sherry Ben, Dana Berman, Courtney Biles, Irina Blok, Pankil Botadra, Jenny Brennan, Karla Brown, John Buckley, Rudy Bunel, Elie Bursztein, Christina Butterfield, Ben Caine, Viral Carpenter, Norman Casagrande, Ming-Wei Chang, Solomon Chang, Shamik Chaudhuri, Tony Chen, John Choi, Dmitry Churbanau, Nathan Clement, Matan Cohen, Forrester Cole, Mikhail Dektiarev, Vincent Du, Praneet Dutta, Tom Eccles, Ndidi Elue, Ashley Feden, Shlomi Fruchter, Frankie Garcia, Roopal Garg, Weina Ge, Ahmed Ghazy, Bryant Gipson, Andrew Goodman, Dawid Górny, Sven Gowal, Khyatti Gupta, Yoni Halpern, Yena Han, Susan Hao, Jamie Hayes, Jonathan Heek, Amir Hertz, Ed Hirst, Emiel Hoogeboom, Tingbo Hou, Heidi Howard, Mohamed Ibrahim, Dirichi Ike-Njoku, Joana Iljazi, Vlad Ionescu, William Isaac, Reena Jana, Gemma Jennings, Donovon Jenson, Xuhui Jia, Kerry Jones, Xiaoen Ju, Ivana Kajic, Christos Kaplanis, Burcu Karagol Ayan, Jacob Kelly, Suraj Kothawade, Christina Kouridi, Ira Ktena, Jolanda Kumakaw, Dana Kurniawan, Dmitry Lagun, Lily Lavitas, Jason Lee, Tao Li, Marco Liang, Maggie Li-Calis, Yuchi Liu, Javier Lopez Alberca, Matthieu Kim Lorrain, Peggy Lu, Kris-tian Lum, Yukun Ma, Chase Malik, John Mellor, Thomas Mensink, Inbar Mosseri, Tom Murray, Aida Nematzadeh, Paul Nicholas, Signe Nørly, João Gabriel Oliveira, Guillermo Ortiz-Jimenez, Michela Paganini, Tom Le Paine, Roni Paiss, Alicia Par-rish, Anne Peckham, Vikas Peswani, Igor Petrovski, Tobias Pfaff, Alex Pirozhenko, Ryan Poplin, Utsav Prabhu, Yuan Qi, Matthew Rahtz, Cyrus Rashtchian, Charvi Rastogi, Amit Raul, Ali Razavi, Sylvestre-Alvise Rebuffi, Susanna Ricco, Felix Riedel, Dirk Robinson, Pankaj Rohatgi, Bill Rosgen, Sarah Rumbley, Moonkyung Ryu, Anthony Salgado, Tim Salimans, Sahil Singla, Florian Schroff, Candice Schu-mann, Tanmay Shah, Eleni Shaw, Gregory Shaw, Brendan Shillingford, Kaushik Shivakumar, Dennis Shtatnov, Zach Singer, Evgeny Sluzhaev, Valerii Sokolov, Thibault Sottiaux, Florian Stimberg, Brad Stone, David Stutz, Yu-Chuan Su, Eric Tabellion, Shuai Tang, David Tao, Kurt Thomas, Gregory Thornton, Andeep Toor, Cristian Udrescu, Aayush Upadhyay, Cristina Vasconcelos, Alex Vasiloff, Andrey Voynov, Amanda Walker, Luyu Wang, Miaosen Wang, Simon Wang, Stanley Wang, Qifei Wang, Yuxiao Wang, Ágoston Weisz, Olivia Wiles, Chenxia Wu, Xingyu Federico Xu, Andrew Xue, Jianbo Yang, Luo Yu, Mete Yurtoglu, Ali Zand, Han Zhang, Jiageng Zhang, Catherine Zhao, Adilet Zhaxybay, Miao Zhou, Shengqi Zhu, Zhenkai Zhu, Dawn Bloxwich, Mahyar Bordbar, Luis C. Cobo, Eli Collins, Shengyang Dai, Tulsee Doshi, Anca Dragan, Douglas Eck, Demis Hass-abis, Sissie Hsiao, Tom Hume, Koray Kavukcuoglu, Helen King, Jack Krawczyk, Yeqing Li, Kathy Meier-Hellstern, Andras Orban, Yury Pinsky, Amar Subramanya, Oriol Vinyals, Ting Yu, and Yori Zwols. 2024. Imagen 3. arXiv:2408.07009 [cs.CV] https://arxiv.org/abs/2408.07009 [20] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. 2025. Tulu 3: Pushing Frontiers in Open Language Model Post-Training. arXiv:2411.15124 [cs.CL] https://arxiv.org/abs/2411.15124 [21] Feng Li, Hao Zhang, Pei Sun, Xueyan Zou, Siyi Liu, Jianwei Yang, Chun yue Li, Lei Zhang, and Jianfeng Gao. 2023. Semantic-SAM: Segment and Recognize Anything at Any Granularity. ArXiv abs/2307.04767 (2023). https://api.semanticscholar. org/CorpusID:259501973 [22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. BLIP-2: Bootstrap-ping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023). [23] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086 (2022). [24] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks Are All You Need II: phi-1.5 technical report. arXiv:2309.05463 [cs.CL] https://arxiv.org/abs/2309.05463 [25] Yuanzhi Li, Weizhe Wang, Wenhan Zhou, Xinyun Hong, Ruoxi Yan, Heng Ji, Nebojsa Jojic, Michael S Bernstein, Zhongjun Wu, and Dekai Wu. 2024. Training language models from scratch with synthetic data. arXiv preprint arXiv:2401.10021 

(2024). [26] Paul Pu Liang, Yiwei Chen, Xiang Ding, Zhengyang Ren, Chun-Liang Fu, and Tong Zhao. 2022. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. Advances in Neural Information Processing Systems 35 (2022), 12943–12956. [27] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. 2024. VILA: On Pre-training for Visual Language Models. arXiv:2312.07533 [cs.CV] https://arxiv.org/abs/2312. 07533 [28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruc-tion Tuning. arXiv:2304.08485 [cs.CV] https://arxiv.org/abs/2304.08485 [29] Haipeng Luo, Ming Gong, Wenhan Zhou, Xinyun Hong, Ruoxi Yan, Heng Ji, Nebojsa Jojic, Michael S Bernstein, Zhongjun Wu, and Dekai Wu. 2023. Wiz-ardMath: Empowering mathematical reasoning for large language models via evol-instruct. arXiv preprint arXiv:2308.09583 (2023). [30] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenhu Hu, Chongyang Tao, Jian-Guang Ma, and Jian-Guang Lou. 2023. WizardCoder: Empowering Large Language Models with Code Evolution. arXiv preprint arXiv:2306.08568 (2023). [31] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. arXiv:2501.19393 [cs.CL] https: 

> 10

//arxiv.org/abs/2501.19393 [32] OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Mądry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Car-ney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braun-stein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, An-drew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob Mc-Grew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian O’Connell, Ian O’Connell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schul-man, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Made-laine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Mu-rat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shad-well, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Wal-ters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. 2024. GPT-4o System Card. arXiv:2410.21276 [cs.CL] https://arxiv.org/abs/2410.21276 [33] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics . 311–318. [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al . 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning . 8748–8763. [35] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 1, 2 (2022), 3. [36] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. 

arXiv preprint arXiv:2102.12092 (2021). [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In 

Proceedings of the IEEE/CVF conference on computer vision and pattern recognition .10684–10695. [38] Simon Schrodi, David T. Hoffmann, Max Argus, Volker Fischer, and Thomas Brox. 2024. Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Models. https://api. semanticscholar.org/CorpusID:273233140 [39] Mohamed El Amine Seddik, Suei-Wen Chen, Soufiane Hayou, Pierre Youssef, and Merouane Debbah. 2024. How Bad is Training on Synthetic Data? A Statistical Analysis of Language Model Collapse. arXiv:2404.05090 [cs.LG] https://arxiv. org/abs/2404.05090 [40] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. 2017. Continual Learning with Deep Generative Replay. arXiv:1705.08690 [cs.AI] https://arxiv. org/abs/1705.08690 [41] Hoo-Chang Shin, Neil A Tenenholtz, Jameson K Rogers, Christopher G Schwarz, Matthew L Senjem, Jeffrey L Gunter, Katherine P Andriole, and Mark Michalski. 2018. Medical image synthesis for data augmentation and anonymization using generative adversarial networks. In Simulation and Synthesis in Medical Imaging: Third International Workshop, SASHIMI 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings 3 . Springer, 1–11. [42] Ilia Shumailov, Zakhar Gao, Rohin Anil, Vedant Misra, Yee Whye Teh, and Nicolas Papernot. 2023. The curse of recursion: Training on generated data makes models forget. In International Conference on Machine Learning .[43] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. 2024. AI models collapse when trained on recursively generated data. Nature 631 (2024), 755 – 759. https://api.semanticscholar.org/CorpusID: 271448069 [44] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. 2024. Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation. ArXiv abs/2406.06525 (2024). https://api.semanticscholar.org/ CorpusID:270371603 [45] Trieu H Trinh and Quoc V Le. 2024. Solving open-ended problems with large language models. arXiv preprint arXiv:2401.10023 (2024). [46] Yue Wang, Weizhe Wang, Wenhan Zhou, Xinyun Hong, Ruoxi Yan, Heng Ji, Nebojsa Jojic, Michael S Bernstein, Zhongjun Wu, and Dekai Wu. 2024. Magicoder: Empowering Code Generation with Source Code. arXiv preprint arXiv:2401.10026 

(2024). [47] Kai Yang, Dan Klein, Pieter Abbeel, and Sergey Levine. 2024. Synthetic data for continued pre-training of large language models. arXiv preprint arXiv:2401.10020 

(2024). [48] Tao Yu, Wenhan Zhou, Xinyun Hong, Ruoxi Yan, Heng Ji, Nebojsa Jojic, Michael S Bernstein, Zhongjun Wu, and Dekai Wu. 2024. MetaMath: Bootstrap mathe-matical questions for large language models. arXiv preprint arXiv:2401.10022 

(2024). [49] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition .586–595. [50] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019). 

> 11

## A Evaluation Metrics 

This appendix details the evaluation metrics used to assess the performance of our image generation models. We employed a com-bination of quantitative metrics to evaluate various aspects of the generated images, including image quality, diversity, and alignment with textual descriptions. 

## A.1 Contrast 

The contrast of the generated images was measured using the Root Mean Square (RMS) contrast, defined as: Contrast = 𝜎 𝑔𝑟𝑎𝑦 

𝜇 𝑔𝑟𝑎𝑦 

where 𝜎 𝑔𝑟𝑎𝑦 is the standard deviation of the grayscale image, and 𝜇 𝑔𝑟𝑎𝑦 is its mean. 

## A.2 Brightness 

The brightness of the generated images was calculated as the av-erage pixel intensity of the grayscale image, scaled to the range 

[0, 255 ]:Brightness = 𝜇 𝑔𝑟𝑎𝑦 × 255 

## A.3 Color Distribution 

The color distribution of the generated images was analyzed by calculating the average RGB values, color saturation, and color standard deviation. The average RGB values were computed as: Avg R = 𝜇 𝑅 

Avg G = 𝜇 𝐺 

Avg B = 𝜇 𝐵 

where 𝜇 𝑅 , 𝜇 𝐺 , and 𝜇 𝐵 are the means of the red, green, and blue color channels, respectively. The color saturation was calculated as the mean difference between the maximum and minimum channel values: Saturation = 𝜇 (max (𝑅, 𝐺, 𝐵 ) − min (𝑅, 𝐺, 𝐵 )) 

The color standard deviation was computed as the average stan-dard deviation of the RGB channels: Color Std = 𝜎 𝑅 + 𝜎 𝐺 + 𝜎 𝐵 

3

where 𝜎 𝑅 , 𝜎 𝐺 , and 𝜎 𝐵 are the standard deviations of the red, green, and blue color channels, respectively. 

## A.4 Fréchet Inception Distance (FID) 

The Fréchet Inception Distance (FID) was used to measure the similarity between the distribution of generated images and the distribution of real images. FID is defined as: FID = || 𝜇 𝑥 − 𝜇 𝑔 || 2 + Tr (Σ𝑥 + Σ𝑔 − 2(Σ𝑥 Σ𝑔 )1/2)

where 𝜇 𝑥 and 𝜇 𝑔 are the mean feature vectors of the real and generated images, respectively, and Σ𝑥 and Σ𝑔 are their covariance matrices. 

## A.5 Inception Score (IS) 

The Inception Score (IS) was used to evaluate the quality and diver-sity of the generated images. IS is defined as: IS = exp (𝐸 𝑥 ∼𝑝 𝑔 [𝐷 𝐾𝐿 (𝑝 (𝑦 |𝑥 )|| 𝑝 (𝑦 ))]) 

where 𝑝 𝑔 is the distribution of generated images, 𝑝 (𝑦 |𝑥 ) is the conditional class distribution given an image 𝑥 , and 𝑝 (𝑦 ) is the marginal class distribution. 

## A.6 CLIP Score 

The CLIP score was used to measure the alignment between the generated images and their textual descriptions. The CLIP score is defined as the cosine similarity between the CLIP embeddings of the generated images and their corresponding captions. The implementation is identical to the original paper [15]. 

## A.7 Relative Modality Gap (RMG) 

The Relative Modality Gap (RMG) measures the relative dissimi-larity between image and text embeddings. The implementation is identical to the original paper [38]. 

## A.8 L2 Distance Modality Gap (L2M ) 

L2M is the L2 distance between the mean of the image and text embedding features. The implementation is identical to the original paper [26]. 

## A.9 CLIP Variance 

CLIP Variance measures the variance of CLIP image embeddings. 

## B Detailed Experimental Setup 

This section provides comprehensive details about our experimen-tal setup, including data preprocessing, hyperparameter configura-tions, and model specifications. 

## B.1 Data Preprocessing 

For our experiments, we used a subset of the MSCOCO training set, which contains 1000 image-caption pairs. We preprocessed the dataset as follows: (1) Images were resized to 256 ×256 pixels for input to the diffu-sion models (2) We removed instances that are not gender-neutral for gender bias analysis Data augmentations were minimized to ensure that any observed effects were due to the recursive fine-tuning process rather than data preprocessing artifacts. 

## B.2 Hyperparameter Settings 

The following hyperparameters were used in our experiments: 

Stable Diffusion 1.4: 

• Learning rate: 1e-5 

• Batch size: 8 

• Training steps per generation: 2000 

• CFG scale default: 7.0 (unless specified otherwise in experi-ments) 

> 12

• Diffusion steps: 50 (unless specified otherwise in experi-ments) 

BLIP-2: 

• Learning rate: 5e-6 

• Batch size: 16 

• Training steps per generation: 2000 

• Beam search size: 4 (unless specified otherwise in experi-ments) 

• Temperature: 1.0 (unless specified otherwise in experiments) 

## B.3 Model Specifications 

Diffusion Models: We primarily used Stable Diffusion 1.5, which is a latent diffusion model trained on billions of image-text pairs. For architectural diversity experiments, we also included: 

• Stable Diffusion 1.4: An earlier version with 860M parameters 

• Flux.1: A newer diffusion model with improved handling of certain visual properties 

Vision-Language Models: Our main VLM was BLIP-2, which uses a frozen image encoder and a large language model for cap-tioning. For diversity experiments, we also used: 

• BLIP: The predecessor to BLIP-2 with a different architecture 

• MiniCPM-Llama3-V 2.5: A smaller but efficient multi-modal model All models were implemented using PyTorch and trained and run on a NVIDIA A100 GPU with 40GB of memory and a NVIDIA RTX 4090 GPU.
