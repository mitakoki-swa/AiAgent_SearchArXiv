Title: Online Training of Large Language Models: Learn while Chatting

URL Source: http://arxiv.org/pdf/2403.04790v1

Published Time: Mon, 11 Mar 2024 01:01:52 GMT

Markdown Content:
# Online Training of Large Language Models: Learn while Chatting 

JUHAO LIANG âˆ—, ZIWEI WANG âˆ—, and ZHUOHENG MA âˆ—, The Chinese University of Hong Kong, Shenzhen, China 

JIANQUAN LI, ZHIYI ZHANG, and XIANGBO WU, The Chinese University of Hong Kong, Shenzhen, China 

BENYOU WANG, The Chinese University of Hong Kong, Shenzhen & Shenzhen Research Institute of Big Data, China Large Language Models (LLMs) have dramatically revolutionized the field of Natural Language Processing (NLP), offering remarkable capabilities that have garnered widespread usage. However, existing interaction paradigms between LLMs and users are constrained by either inflexibility, limitations in customization, or a lack of persistent learning. This inflexibility is particularly evident as users, especially those without programming skills, have restricted avenues to enhance or personalize the model. Existing frameworks further complicate the model training and deployment process due to their computational inefficiencies and lack of user-friendly interfaces. To overcome these challenges, this paper introduces a novel interaction paradigm-â€™Online Training using External Interactionsâ€™-that merges the benefits of persistent, real-time model updates with the flexibility for individual customization through external interactions such as AI agents or online/offline knowledge bases 

> 1

.CCS Concepts: â€¢ Human-centered computing â†’ Human computer interaction (HCI) ; User interface toolkits ; â€¢ Computing methodologies â†’ Natural language processing ;Additional Key Words and Phrases: Large Language Model, User Interaction, Natural Language Processing 

ACM Reference Format: 

Juhao Liang, Ziwei Wang, Zhuoheng Ma, Jianquan Li, Zhiyi Zhang, Xiangbo Wu, and Benyou Wang. 2023. Online Training of Large Language Models: Learn while Chatting. 1, 1 (March 2023), 15 pages. https://doi.org/ 10.1145/nnnnnnn.nnnnnnn 

1 INTRODUCTION 

Large language models (LLMs) [ 26 , 62 , 69 , 78 ] has witnessed remarkable advancements in recent years, revolutionizing various natural language processing (NLP) tasks [ 2, 7 , 14 ]. In a world where knowledge and user requirements are constantly shifting, itâ€™s critical for these models to engage in incremental learning [ 85 ] . Existing work [ 29 , 63 ] tried to improve language models by self-consistency or self-reflection; however such improvement is limited to be in the scenarios where 

> âˆ—

Both authors contributed equally to this research. 

> 1

https://github.com/FreedomIntelligence/Online-Training.git Authorsâ€™ addresses: Juhao Liang, juhaoliang1997@gmail.com; Ziwei Wang, ziweiwang2@link.cuhk.edu.cn; Zhuoheng Ma, zhuohengma@link.cuhk.edu.cn, The Chinese University of Hong Kong, Shenzhen, China; Jianquan Li; Zhiyi Zhang; Xiangbo Wu, The Chinese University of Hong Kong, Shenzhen, China; Benyou Wang, The Chinese University of Hong Kong, Shenzhen & Shenzhen Research Institute of Big Data, China, wangbenyou@cuhk.edu.cn. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. 

Â© 2023 Association for Computing Machinery. XXXX-XXXX/2023/3-ART $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn , Vol. 1, No. 1, Article . Publication date: March 2023.  

> arXiv:2403.04790v1 [cs.CL] 4 Mar 2024 2Juhao Liang, Ziwei Wang, Zhuoheng Ma, Jianquan Li, Zhiyi Zhang, Xiangbo Wu, and Benyou Wang

deterministic rule based check could be used like coding and numerical answers, which might be used in open-world problems. We argue that using LLMs to improve themselves is limited; interactions between LLMs and environment are essential incremental learning. There are two typical ways for incremental learning: offline incremental training and online in-context learning . Offline incremental training in language models involves training the system with new sets of annotated data, allowing for some level of adaptability, such as supervised fine-tuning [ 46 ] and reinforcement learning with human feedback [ 11 , 88 ]. However, this approach is fraught with several limitations. First, it suffers from a lagging nature, meaning there is a delay between the emergence of new information and the modelâ€™s update to include it, rendering the model less reliable for immediate or evolving scenarios. Second, the method is generally non-personalized; it updates the model based on broad data sets rather than tailoring to individual user preferences or specific contextual needs. This methodâ€™s high computational cost and static nature post-update make it inflexible for adapting to real-time changes or diverse user requirements. Another approach in the realm of incremental learning is known as â€™ online in-context learning â€™. In this setup, a LLM typically interacts with an AI agent or connects to either offline or online knowledge sources. For instance, the model might retrieve information from a given knowledge base or from web content, a.k.a, Retrieval-Augmented Generation (RAG) [ 30 , 36 , 74 ]. Such methods often occur within the paradigm of in-context learning (ICL) [ 8, 40 ]. However, a significant limitation of this approach is its lack of knowledge persistency. When the session changes, the learned information is not retained, leading to a loss of any updates or learning that took place. To address the shortcomings of both â€™Offline Incremental Trainingâ€™ and â€™Online In-Context Learning,â€™ we introduce a cutting-edge paradigm, â€™Online Training using External Interactions.â€™ This new approach offers the benefits of persistent model updates and real-time learning. Unlike traditional methods, it necessitates external interactions during the learning process, such as interfacing with AI agents or connecting to offline or online knowledge sources. These capabilities allow the model to continually adapt and stay updated, effectively addressing the limitations seen in previous frameworks. Furthermore, this innovative human-computer interaction paradigm presents a unique opportunity for ordinary users to modify LLMs themselves, thereby shifting from a developer-owned model to a more user-centric approach. This paradigm could mark the beginning of a broader practical application of LLMs in the everyday lives of an increasing number of people. The key contributions of this work are as follows: 

â€¢ We propose a novel paradigm for human interaction with language models, shifting from static to adaptable models. Instead of humans adapting to fixed model parameters, we introduce models that evolve in response to human input, fostering a dynamic and sometimes reciprocal relationship. 

â€¢ The classification of interaction paradigm and proposal of a novel paradigm: Online Training using External Interactions, which is a user-friendly incremental learning methodology. 

2 RELATED WORK AND MOTIVATION 

From the usersâ€™ perspective, there are currently two well-known interaction paradigms with LLMs: offline parameter-variant and online parameter-invariant. In this section, we first introduce these two prevalent user interaction paradigms and their applications, along with their applications, and detail their respective advantages and disadvantages in Sec. 2.1. Following this, in comparison to existing works, we outline the research objectives and motivation behind the proposed novel interaction paradigm in Sec. 2.2.  

> , Vol. 1, No. 1, Article . Publication date: March 2023. Online Training of Large Language Models: Learn while Chatting 3

Table 1. Comparison of interaction paradigms. Online and offline refer to whether the model is serving, while training and parameter-invariant and parameter-variant indicate whether the parameter of the model changed. To assess and compare various interaction approaches, we focus on five key attributes: 1) Knowledge Persistency, which indicates if updated information remains accessible across different sessions; 2) Flexibility, evaluating if LLMs become static post-training; 3) Efficient Update, gauging the time and computational costs involved in model updates; 4) Knowledge Timeliness, assessing if the modelâ€™s information is current; and 5) Knowledge Quality, which verifies the accuracy and reliability of the modelâ€™s information. In the context of traditional training, LLMs are fine-tuned using a specific set of annotated data.                  

> Paradigm Methodology Knowledge Persistency Flexibility Efficient Update Knowledge Timeliness Knowledge Quality
> Offline Parameter-Variant Traditional Training [8, 44, 61] âœ“âœ“
> Online Parameter-Invariant Retrieval-based methods [30, 36, 74] âœ“âœ“
> Prompt-based methods [8, 40] âœ“âœ“
> Tool-based methods [19, 23, 35, 54, 70, 73, 80] âœ“
> Online Parameter-Variant Online Training using External Interactions âœ“âœ“âœ“âœ“âœ“

2.1 Related Work 

Offline parameter-variant paradigm . The offline parameter-variant paradigm is the most commonly used interaction paradigm, wherein models are updated during periods of non-service. This paradigm comprises methods that are trained on a given labeled dataset. The dataset can be compiled solely by humans, as suggested by Ouyang et al. (2022) [ 46 ], which represents a conventional method of model training, specifically referred to as supervised fine-tuning (SFT). Alternatively, the process can be assisted by a retriever, as explored by [ 28 , 42 , 43 ]. Interaction with external knowledge during training can enhance the modelâ€™s representation by integrating a larger volume of factual knowledge. For developers, offline parameter-variant methods are the most reliable and effective options for training a language model from scratch or for model updates due to their â€™once and for allâ€™ characteristic, leading to high knowledge persistency and quality. However, from the userâ€™s perspective, the entire model training process can be complex, inflexible and time-consuming, ranging from data collection to computing resource configuration and model training. 

Online parameter-invariant paradigm . For online parameter-invariant paradigm, there are three kinds of techniques, retrieval-based methods [ 30 , 36 , 74 ], prompt-based methods [ 8 , 40 ], and 

tool-based methods [ 19 , 23 , 35 , 54 , 70 , 73 , 80 ]. RAG [ 36 ] is a representation of retrieval-based methods ,which emphasizes the use of external knowledge sources to augment language models during inference time. Interaction with the knowledge base during inference can aid the language model in generating more precise, contextually relevant, and informed responses by dynamically leveraging external knowledge sources based on the specific input or query at hand. RAG amalgamates pre-trained parametric and non-parametric memory for language generation. Designed to enhance the factual accuracy of dialogue agents, it aims to mitigate the issue of knowledge hallucination. Whereas, its performance heavily relies not only on the quality of the knowledge but also on the effectiveness of the retrieval method. The primary objective of the prompt-based method is to sustain real-time, continuous engagement, making it ideal for application scenarios such as dialogue systems, real-time translations, and multi-round question answering. This iterative interaction process allows the modelâ€™s output to incrementally adjust to meet user demands. Typically, this interaction paradigm does not modify the  

> , Vol. 1, No. 1, Article . Publication date: March 2023. 4Juhao Liang, Ziwei Wang, Zhuoheng Ma, Jianquan Li, Zhiyi Zhang, Xiangbo Wu, and Benyou Wang

modelâ€™s parameters during the interaction, instead necessitating users to incessantly input or revise prompts to draw more meaningful responses from the language model. In-context learning [ 8] is a method of prompt-based, which operates without access to any external memory or knowledge beyond its pre-training phase. Consequently, while generating responses, it primarily relies on the immediate context of the conversation or task for information. Consequently, conversations can become rigid and labor-intensive due to the necessity for prompt engineering or dialogue engineering. The drawbacks of this interaction pattern are the inefficiency of proper prompts construction and failures in non-textual tasks. By fragmenting a downstream task into multiple steps, tool-based methods can assign specific stages to external tools or APIs, such as those specializing in mathematical computations, web searches, image generation, and etc. Typically, tasks that emphasize fidelity and accuracy, such as real facts, complex mathematical operations, and tasks that transcend the LLM training corpus including up-to-date knowledge, low-resource languages, and image generation, are more effectively resolved using external tools than LLMs. ToolLLM [ 54 ] is a well-known tool-enhanced method. It constitutes a comprehensive framework for tool-use, offering tangible tools and components for LLMs. It is specifically engineered to empower LLMs to execute higher-level tasks, such as adhering to human instructions for utilizing external tools (APIs). However, it suffers from challenges related to invoking tools at the appropriate time and determining the most suitable tool to utilize. 

Pros and Cons of Existing Paradigms . The two existing User-LLM interaction paradigms are extensively utilized in both research and practical applications. Offline parameter-variant approaches excel in knowledge persistency and quality, a result of the substantial engineering effort required before model training. This includes thorough data collection and meticulous training configuration. However, such a workload leads to inflexibility and high costs in model updates. Moreover, these methods inherently lack timeliness in knowledge updates due to their demanding workload. On the other hand, online parameter-invariant methods enable the enhancement of trained LLMs without extra training costs through prompting. However, as observed in previous research [ 41 ], the efficacy of prompting is not consistently positive. It may significantly degrade, especially if the relevant informationâ€™s position varies in long context, even in models designed for long-context scenarios. Additionally, online parameter-invariant methods, like retrieval-based and tool-based approaches, often require extensive systems support, such as external databases, posing a burden for system development and sharing. Furthermore, the integrated knowledge in these methods has a short effective lifespan, expiring at the sessionâ€™s end or upon context removal. Overall, the pros and cons of existing paradigms are listed in Table 1. 

2.2 Motivation 

Motivation . In the context of knowledge persistence, the offline parameter-variant paradigm can be likened to the human brainâ€™s long-term memory, which necessitates extensive training and preparation for embedding specific knowledge. Conversely, the online parameter-invariant paradigm resembles short-term memory, where knowledge or skills can be rapidly acquired through quick learning but are not retained in the model for an extended period. This analogy highlights the strengths and weaknesses of these existing interaction paradigms. Motivated by this, we propose an intermediate interaction paradigm that amalgamates the benefits of both: the â€™Online Parameter-Variantâ€™ method. This novel approach aims to reduce training costs compared to offline parameter-variant methods while offering more robust improvements than the online parameter-invariant paradigm. The â€™Online Parameter-Variantâ€™ method, which is grounded in model training, focuses on several key metrics: knowledge persistency, flexibility, efficient updating, knowledge timeliness, and superior knowledge quality relative to the previous paradigms.  

> , Vol. 1, No. 1, Article . Publication date: March 2023. Online Training of Large Language Models: Learn while Chatting 5

3 USER INTERFACE: ONLINE TRAINING USING EXTERNAL INTERACTIONS 

We will first introduce the overall design in Sec. 3.1 which consists of three interactions. These three interactions are detailed in Sec. 3.2, Sec. 3.3 and Sec. 3.4. 

3.1 Overall design 

3.1.1 Philosophy. To address these challenges, we introduce a new interactive interface that facilitates user engagement with LLMs through conversational interactions while concurrently enabling fine-tuning through natural language instructions. The proposed system allows users to engage in conversations with a LLMs while providing specific instructions to trigger immediate fine-tuning. Users can seamlessly trigger the training process by employing natural language prompts preceded by " [Learn] ," like "[Learn] I wish you could fetch more news on environmental pollution," within an interface resembling a chat, thereby commencing training grounded on network-sourced information. Upon receiving the triggering signal, our system will comprehend the userâ€™s intended meaning and initiate distinct learning processes accordingly. After the training is completed, the newly enhanced model, enriched with incremental knowledge, will immediately replace the preceding model and seamlessly resume the ongoing conversation with users. 

3.1.2 The three interactions. Online Training using External Interactions introduces three unique learning functionalities that form a comprehensive and versatile toolkit for interactive model train-ing: Instruction-Guided Learning , Document-Driven Learning , and Web Search-Enabled Learning .

Instruction-Guided Learning serves as a soft knowledge source, leveraging conversational interfaces like ChatGPT 2 to facilitate human-like, adaptive responses. This functionality is particu-larly powerful for nuanced or subjective queries where human-like interpretation and flexibility are required. On the other hand, Document-Driven Learning and Web Search-Enabled Learning act as hard knowledge sources. Document-Driven Learning relies on offline sources, allowing for quality-controlled, curated information to be used in model training. This is particularly advanta-geous for tasks that require authoritative or highly reliable information. Web Search-Enabled Learning utilizes online data, offering the advantage of real-time information retrieval. While this allows the model to stay current, it can sometimes introduce bias or less reliable data into the training set. By using self-instruct [ 75 ], instruction backtranslation [ 38 ], and online search augmentation , these functionalities allow for a high degree of customizability and adaptability. They empower users to shape their models according to specific instructions, documents, or real-time web data, thus bridging the gap between static, pre-trained models and dynamic, personalized user needs. To-gether, these functionalities make our framework not only versatile but also user-centric, enabling continuous improvement and adaptability across various application domains. This effectively highlights the complementary nature of the three functionalities in offering different sources and reliability of knowledge, serving to create a balanced, comprehensive approach for online learning with interaction. 

3.1.3 Content Moderation Control. Content moderation is crucial for maintaining the integrity of LLMs. To ensure effective moderation and reduce the risk of generating biased, toxic, or unethical content, the proposed interface utilizes two primary strategies: Prevention and Feedback. First, we employ an external interface specifically designed to monitor and address content moderation issues  

> 2https://chat.openai.com , Vol. 1, No. 1, Article . Publication date: March 2023. 6Juhao Liang, Ziwei Wang, Zhuoheng Ma, Jianquan Li, Zhiyi Zhang, Xiangbo Wu, and Benyou Wang

during training. Second, we integrate a user feedback mechanism, enabling users to contribute to the moderation process through their interactions and observations. For the prevention aspect, all data used for model updates will undergo rigorous scrutiny, filtering out any inappropriate content to ensure that sources, whether local documents or Internet-based, are suitable for LLM moderation. As for the feedback component, a â€™feedbackâ€™ button is available to users, allowing them to report biases or any unsatisfactory elements of the response. When used, the model initiates an updating process, making corrections through a pre-defined mechanism. Interestingly, this feedback mechanism can be viewed as a form of online version reinforcement learning with human feedback (online RLHF) [ 46 ], aimed at aligning LLMs with actual user values and accentuating the personalized aspect of the proposed method. 

3.1.4 Benefits and potential. 

Lifelong Learning . The concept of lifelong learning [ 5] refers to the ability of a system, in this case, a language model, to continuously acquire and integrate new knowledge and skills throughout its existence. Unlike traditional training methods that rely on static datasets, our approach leverages continuous user interactions to adapt and evolve the model over time. This enables lifelong learning, where the model can continuously improve and stay relevant to the userâ€™s changing needs and interests. 

Personalization [ 10 ]. Through our interactive training mode, users have the power to customize the language model to their liking. This customization extends beyond simple prompt-based instructions and allows users to fine-tune the modelâ€™s behavior to suit their unique requirements. This level of personalization results in models that are highly specialized and context-aware, enhancing their utility for specific tasks and domains. 

Accessibility . The heart of our approach lies in allowing users to engage in natural language conversations with the language model. This familiarity with conversational interactions makes it accessible to a wide range of users, regardless of their technical background. Users donâ€™t need to learn complex programming or command syntax; they simply converse as they would with another person. 

User Empowerment . Users are in control of the training process. They can decide when and how to fine-tune the model based on their needs. This sense of empowerment fosters a feeling of ownership over the model, enhancing user engagement and motivation to participate in the training process. Additionally users can track the progress of their customized model over time. They can see how their interactions and commands have shaped the modelâ€™s behavior, providing a sense of achievement and transparency in the training process. 

3.2 Instruction-Guided Learning 

Instruction-Guided Learning constitutes a fundamental component of our interactive language model fine-tuning system, enabling users to impart specific directives to the model regarding information retention and contextual understanding during ongoing conversational interactions. This method is particularly instrumental in customizing the modelâ€™s responses to align with the userâ€™s unique requirements and preferences. Within the Instruction-Guided Learning method, users are granted the capability to issue explicit instructions to the language model. These instructions may pertain to what facts or details the model should remember or any specific information that should be considered during the discourse. Users can convey their instructions in a natural language format, making it an intuitive and user-friendly 

> , Vol. 1, No. 1, Article . Publication date: March 2023.

Online Training of Large Language Models: Learn while Chatting 7The impact of urban development on the natural environment 

depends on planning, sustainability practices, and conservation 

efforts, making it a complex and variable issue with both potential 

for harm and opportunities for mitigation. 

Certainly! I'll perform a web search. This might take a little while. 

[Learn] I hope you read more news about 

environmental pollution. 

Air Pollution Has Decreased Across the US, but New Research 

Finds Health Burdens Remain Unequal Among Racial Groups. 

... 

Model Updating... [100%] 

Finished model updated! I've gathered data from the web search. 

Is there anything specific you'd like to discuss? 

Can you show me some latest news 

about environmental pollution? 

Do you think urban development will destroy the 

natural environment? 

ðŸ˜ƒ 

ðŸ˜ƒ 

ðŸ˜ƒ 

ðŸ˜ƒ 

Hello! How can I assist you today? Hello! How can I assist you today? 

Do you still remember the situation regarding 

Japan's nuclear wastewater in 2023? 

Of course!  The Japanese government's decision to 

discharge nuclear wastewater is controversial... 

Conversation 2 

ðŸ˜ƒ 

ðŸ˜ƒ 

ðŸ˜ƒ 

ðŸ˜ƒ 

Hello! How can I assist you today? Hello! How can I assist you today? 

[Learn] This is a survey and analysis on environmental 

protection compiled by our company, please read it carefully. 

Sure thing! Please drop it in. 

Model Updating... [100%] 

Finished model updated!  I've extracted information from 

the document. 

Can you give me some suggestions on how to add new 

content on environmental protection in the case study 

section of the report? 

Sure, first... 

... 

Conversation 1 

How do you view the situation regarding Japan's nuclear 

wastewater in 2023? 

I'm sorry, my knowledge cutoff date is September 2021, and 

I do not have information about the Japanese nuclear 

wastewater situation in 2023. 

[Learn]  The Japanese government's decision to begin 

releasing treated Fukushima nuclear wastewater into the 

Pacific Ocean starting on August 24, 2023, has garnered 

widespread international attention and controversy... 

Model Updating... [100%] 

Finished model updated!  I already remember the 

information you provided. 

How do you view the situation regarding Japan's 

nuclear wastewater in 2023? 

The Japanese government's decision to discharge treated 

Fukushima nuclear wastewater into the Pacific Ocean 

starting August 24, 2023 has aroused widespread concern 

and controversy in the international community... 

Instruction-Guided Learning 

Document-Driven Learning 

Web Search-Enabled Learning 

Description : Search news about environmental pollution 

Data Augment : Online search augmentation 

Data Preparation 

- Search online 'environmental pollution news' 

- Generate instructions based on the collected materials 

- Instructions selection 

Model Training 

- Model supervised instruction fine-tuning 

Model Deployment 

- Unload the old model; 

- Deploy the updated one. 

Description : Read the document [report.pdf] 

Data Augment : Backtranslation 

Data Preparation 

- Convert the file `[report.pdf]` into text 

- Generate instructions based on the given materials 

- Instruction Selection 

Model Training 

- Model supervised instruction fine-tuning 

Model Deployment 

- Unload the old model; 

- Deploy the updated one. 

Description : Remember the event of nuclear wastewater 

Data Augment : Self-instruction 

Data Preparation 

- Generate instructions based on the given infomation 

- Instructions selection 

Model Training 

- Model supervised instruction fine-tuning 

Model Deployment 

- Unload the old model; 

- Deploy the updated one. 

ðŸ˜ƒ 

ðŸ˜ƒ 

Fig. 1. The figure depicts the manner in which dialogues are conducted between LLM and user within our interactive mode. Notably, users issue distinct directives, each leading to the trigger of three distinct training processes. Furthermore, the figure underscores the modelâ€™s ability to retain knowledge acquired during prior conversational session, even when transitioning across different conversation sessions. 

process. These directives will be transmuted into trainable data via the self-instruct approach [ 75 ], after which we will proceed to iteratively enhance our model using the data thus generated. After receiving user instructions, the model promptly incorporates the provided information into its understanding of the ongoing conversation. This entails the identification and retention of salient details and context specified by the user. The modelâ€™s responses are then guided by the personalized context, resulting in responses that align closely with the userâ€™s directives. As the conversation unfolds, the model continuously adapts its responses based on the instruc-tions and context provided by the user. This iterative adaptation process allows the model to tailor its responses, ensuring that it adheres to the userâ€™s preferences and maintains a coherent and contextually relevant dialogue. Consequently, the user experiences a personalized and highly responsive conversational interface. Instruction-Guided Learning offers users a powerful means to personalize the language modelâ€™s behavior and responses in a conversational setting. By issuing explicit directives, users can shape the modelâ€™s understanding and context, thereby tailoring its responses to their unique requirements. This method enhances the utility of language models across various applications, including personal virtual assistants, domain-specific chatbots, and tailored information retrieval systems, making them versatile tools for a diverse range of user needs. 

, Vol. 1, No. 1, Article . Publication date: March 2023. 8 Juhao Liang, Ziwei Wang, Zhuoheng Ma, Jianquan Li, Zhiyi Zhang, Xiangbo Wu, and Benyou Wang Online  

> Trainin g
> Fig. 2. This figure delineates our comprehensive workflow of chat-based online training. During the interaction between the user and the model, the user issues learning instructions to trigger the learning process. Three different learning methods correspond to three data augmentation techniques with the generated data as input to train new model. Then new model replace the old one seamlessly, allowing the user to continue the conversation.

3.3 Document-Driven Learning 

Document-Driven Learning constitutes a pivotal facet of our interactive language model fine-tuning system, offering users the capability to enrich the modelâ€™s knowledge base with structured and specialized information. This mode is particularly suited for users who seek to imbue the model with domain-specific expertise or train it on authoritative documents, academic texts, or specialized knowledge sources. Users initiate the Document-Driven Learning method by selecting and uploading documents relevant to their specific area of interest or domain. These documents may encompass scholarly articles, technical manuals, legal documents, or any textual resources germane to the subject matter. The system accepts a variety of file formats, including PDFs, text documents, and web links. Upon document submission, the system undertakes a comprehensive preprocessing and trans-formation procedure. Document-derived data produced through the utilization of Instruction Backtranslation [ 38 ] will be meticulously curated for high-quality training purposes. Through iter-ative fine-tuning, the model adapts to the new information derived from the uploaded documents. It learns to contextualize the content, recognize domain-specific terminology, and develop a deeper understanding of the subject matter. Consequently, the modelâ€™s responses become more nuanced and contextually relevant when engaging in discussions related to the uploaded documents or the associated domain. Document-Driven Learning represents a potent mechanism for users to imbue language models with domain-specific knowledge and expertise. By leveraging structured textual resources, users can enhance the modelâ€™s contextual understanding and its capacity to provide informed responses within specialized domains. This method extends the utility of language models across a wide array of professional and academic applications, enabling them to serve as versatile and knowledgeable conversational partners.  

> , Vol. 1, No. 1, Article . Publication date: March 2023. Online Training of Large Language Models: Learn while Chatting 9

3.4 Web Search-Enabled Learning 

The integration of Web Search-Enabled Learning constitutes the third facet within the framework of our interactive language model fine-tuning system, affording users the capability to harness the vast knowledge repository of the internet to augment the modelâ€™s understanding and responsiveness. This method is particularly valuable for users seeking real-time information, staying updated on current events, or training the model on a dynamic and ever-evolving knowledge landscape. Upon receiving user search instructions, the system promptly conducts web searches using well-established search engines and APIs. The retrieved web content, which may include news articles, blog posts, research papers, and other relevant sources, is then subjected to information extraction and summarization processes to distill the key insights and facts. The extracted information from web searches serves as a valuable source of training data for the model [ 75 ]. During the subsequent fine-tuning phase, the model is exposed to the insights obtained from web searches. An inherent advantage of Web Search-Enabled Learning is the modelâ€™s adaptability to real-time information. As the web content evolves, the model continuously adapts to the dynamic knowledge landscape, ensuring that its responses remain up-to-date and accurate in the context of the ongoing conversation. This real-time adaptation is particularly advantageous for users seeking the latest information and insights. The knowledge derived from web searches becomes an integral part of the modelâ€™s memory, enriching its understanding of contemporary topics and factual information. This knowledge integration ensures that the model remains a reliable source of current events, trending topics, and dynamic knowledge domains over time. Web Search-Enabled Learning empowers users to leverage the extensive resources of the internet to enhance the language modelâ€™s knowledge and responsiveness. By instructing the system to retrieve real-time information, users ensure that the model remains current and up-to-date, making it a valuable resource for information retrieval, news updates, and dynamic knowledge domains. This method extends the utility of language models to domains requiring real-time knowledge integration and adaptation, making them versatile tools for a wide array of applications, including news summarization, trend analysis, and current event discussion. 

4 APPLICATION: A CASE STUDY ON TOOL LEARNING 

This section is dedicated to evaluating the effectiveness and efficiency of the proposed novel interaction paradigm, termed Online Training using External Interactions , abbreviated as Online Training (OT) in this section. 

4.1 Problem setting 

In this task, we assume that the userâ€™s objective is to train a LLM to effectively utilize external tools [54, 67]. To achieve this goal, we adopt the tool invocation data format outlined in Sun et al. (2023) [ 67 ], as demonstrated in Appendix A. We assess the modelâ€™s accuracy in invoking the correct plugin and its corresponding inputs when presented with multiple APIs for various questions. As illustrated in Figure 3, we employ two baseline methods: the prompt-based method (ab-breviated as Prompt ) and the full-parameter training method (abbreviated as Full-SFT ). In the 

Prompt method, we use the base model (Llama2-7b-chat [ 72 ]) to generate answers with few-shot prompts [ 76 ] listed in the context. Conversely, the Full-SFT method involves leveraging external annotated training data to train the model, subsequently using the same few-shot prompts as in the 

Prompt method for the test set. And, the proposed approach, online-training (OT), involves generat-ing corresponding training data based on user training instructions, then filtering out low-quality, including toxic or biased, data to train the original model. Subsequently, a prompt-based approach 

> , Vol. 1, No. 1, Article . Publication date: March 2023.

10 Juhao Liang, Ziwei Wang, Zhuoheng Ma, Jianquan Li, Zhiyi Zhang, Xiangbo Wu, and Benyou Wang 

is employed to generate answers for the test set. We select three tools from previous research [ 53 ], randomly choosing 300 data points for the test set and an additional 6k data points for the training dataset for the Full-SFT method Prompt-based Method  

> Online-Training
> Raw Model
> (Llama2-7b-chat)
> Few-shot QA
> Training
> Training Few-shot QA
> Few-shot QA
> test data
> evaluation
> External
> Training Data
> Self-Instruct
> Data
> Annotator
> Full-parameter Training

Fig. 3. Overview of the experimental design 100 500 1000 1500 2000  

> Data Count (Online-training)
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Accuracy (action & input)
> OT*
> Prompt (0 data)
> Full-SFT (6k data)

Fig. 4. The results of the experiment, where the sym-bol * refers to the average of three experiments with random seed. 

4.2 Result Analysis 

The result are shown in Figure 4, where the x-coordinate indicates the data utilized by the online-training methods, and the y-coordinate shows the accuracy of each method using external tools. Accuracy is measured by counting instances of correctness in both the action (correct tool selection) and input (accurate tool parameters generation). The Full-SFT method utilizes 6,000 labeled in-domain data points for model training. In comparison, the performance of the OT method is evaluated using between 100 and 2,000 model-generated data points for training. It is evident that using a single round of model generation 3 can achieve almost double the improvement over the vanilla model in the tool learning task, increasing from nearly 30% to 50%. However, the performance of online-training trained on 100 data points compared to 2000 data points appears similar. This is attributed to data distribution misalignment, which is still considered acceptable. Method Prompt OT (0.1k) SFT (6k) Accuracy 0.35 0.56 0.76 Train (time) / 2ð‘šð‘–ð‘›ð‘  40 ð‘šð‘–ð‘›ð‘  

Inference (time) 2ð‘šð‘–ð‘›ð‘  2ð‘šð‘–ð‘›ð‘  2ð‘šð‘–ð‘›ð‘  

Table 2. Analysis of experiment duration: the time expended during the training process and the inference time for the test set are detailed. 

Table 2 presents the analysis of experiment duration, detailing the time expenditures for both the training and inference processes. It becomes evident that the online-training method effectively amalgamates the benefits of both online parameter-invariant and offline parameter-variant methods, as demonstrated by its reduced training cost and heightened effectiveness on the test set. 

5 DISCUSSION 

This section, drawing upon the experiments and related works, delves into some concerned issues and potential challenges associated with the proposed method. Also, future possibilities in the development of User-LLM interaction paradigms are introduced. 

> 3

Approximately 100 valid data points can be obtained from a single GPT-4 API call. , Vol. 1, No. 1, Article . Publication date: March 2023. Online Training of Large Language Models: Learn while Chatting 11 

5.1 In-context Learning or Fine-Tuning? 

There is always a question regarding language model downstream adaptation: should we opt for in-context learning or model fine-tuning for the continuous learning of trained LLMs? Both approaches have garnered considerable interest from researchers and users alike. As previously discussed, each paradigm has its strengths and weaknesses, depending on the scenario, and they are, in fact, not mutually exclusive. Moreover, increasing research [ 15 , 47 ] is focusing on the relationship between ICL and Fine-Tuning. Dai et al. (2023) [ 15 ] found that ICL behaves similarly to explicit fine-tuning at the prediction level, representation level, and attention behavior level. In light of these findings, we propose a novel interaction paradigm that bridges the gap between these two existing approaches. This method involves injecting knowledge directly into the parameters, rather than solely in the context, thereby enhancing its persistency and robustness. 

Scalability : One of the prominent advantages of the proposed method over ICL is its superior transferability and compositionality. For example, one could prepare specific training data for each learning job and then later decide which training data to be combined for final application. Such a combination could be done without extra inference cost as the increase of learning jobs does not affect the inference cost. Also thanks to the efficient compositionality, our approach could have a better capacity to deal with a larger-scale training, which benefits the scalability. 

Inference Efficiency Our method stands out for its superior inference efficiency when compared to Retrieve and Generate (RAG) or In-Context Learning (ICL) strategies. Both RAG and ICL often result in significantly longer input prompts, which in turn leads to an increase in computational cost - a cost that grows quadratically with the length of the input. Although the training cost of our approach might be higher than that of RAG or ICL, itâ€™s important to note that this is a one-time expense related to the training phase, and remains constant regardless of the number of requests. In contrast, the cost of inference increases linearly with the number of requests, making our method more efficient in the long run. Moreover, unlike approaches that require large-scale databases or additional plugins, our method incorporates knowledge directly into the model through online training, thereby eliminating the need for external dependencies. This not only simplifies the process, but also enhances deployment readiness and operation efficiency. 

5.2 Challenges 

Despite the potential of the online training method, it faces several challenges: 

â€¢ Knowledge Injection and Overfitting: Ovadia et al. (2023) [ 47 ] noted that LLMs often struggle to assimilate new factual information through fine-tuning. A key challenge is effectively injecting necessary knowledge into LLMs within a user-acceptable timeframe to enhance user experience. Rather than relying on a high number of training epochs, which may cause models to overfit by repeatedly training on the same data, our approach increases data diversity. This aligns with user requirements and ensures model generalizability and knowledge acquisition. 

â€¢ Knowledge Persistency: Maintaining the knowledge persistency in LLMs is a crucial aspect of our proposed system. Unlike ICL-type knowledge persistency, which may involve storing information on disk, our parameter-variant methods embed knowledge directly into the LLMsâ€™ parameters. This approach ensures long-lasting knowledge retention, akin to pre-trained knowledge. 

â€¢ Concurrency in LLMs Deployment: The online-training interaction paradigm we propose has similar deployment costs to conventional methods. It can be trained into a specific set of parameters, such as using LoRA, which offers the flexibility to load or unload specific training 

> , Vol. 1, No. 1, Article . Publication date: March 2023.

12 Juhao Liang, Ziwei Wang, Zhuoheng Ma, Jianquan Li, Zhiyi Zhang, Xiangbo Wu, and Benyou Wang 

for users. This ensures privacy and scalability without significantly increasing deployment demands. 

6 CONCLUSION 

In this paper, we introduce a novel interaction paradigm, online parameter-variant, and a new method, online learning using external interactions. This approach focuses on explicit model fine-tuning and instant responses to natural language instructions via a user-friendly interface. As for the future direction, we aim to break down more restrictions on usersâ€™ utilization of LLMs and bring more engaging and beneficial model human-computer interactions to users, leveraging the â€™Online Training using External Interactionsâ€™ paradigm. 

ACKNOWLEDGEMENT 

This work is supported by the Shenzhen Science and Technology Program (JCYJ20220818103001002), Shenzhen Doctoral Startup Funding (RCBS20221008093330065), and Tianyuan Fund for Mathematics of National Natural Science Foundation of China (NSFC) (12326608). 

REFERENCES 

[1] Albers, S. Online algorithms: a survey. Mathematical Programming 97 (2003), 3â€“26. [2] Bahl, L. R., Brown, P. F., de Souza, P. V., and Mercer, R. L. A tree-based statistical language model for natural language speech recognition. IEEE Transactions on Acoustics, Speech, and Signal Processing 37 , 7 (1989), 1001â€“1008. [3] Bengio, Y., and LeCun, Y. Scaling learning algorithms towards AI. In Large Scale Kernel Machines . MIT Press, 2007. [4] Bian, Z., Liu, H., Wang, B., Huang, H., Li, Y., Wang, C., Cui, F., and You, Y. Colossal-ai: A unified deep learning system for large-scale parallel training. arXiv preprint arXiv:2110.14883 (2021). [5] Biesialska, M., Biesialska, K., and Costa-jussÃ  , M. R. Continual lifelong learning in natural language processing: A survey. In Proceedings of the 28th International Conference on Computational Linguistics (2020), International Committee on Computational Linguistics. [6] Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He, H., Leahy, C., McDonell, K., Phang, J., et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745 (2022). [7] Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J. Large language models in machine translation. [8] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877â€“1901. [9] Chase, H. LangChain, Oct. 2022. [10] Chen, J., Liu, Z., Huang, X., Wu, C., Liu, Q., Jiang, G., Pu, Y., Lei, Y., Chen, X., Wang, X., Lian, D., and Chen, E. 

When large language models meet personalization: Perspectives of challenges and opportunities, 2023. [11] Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. Advances in neural information processing systems 30 (2017). [12] Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. 

Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022). [13] Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 (2021). [14] Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. Natural language processing (almost) from scratch. Journal of machine learning research 12 , ARTICLE (2011), 2493â€“2537. [15] Dai, D., Sun, Y., Dong, L., Hao, Y., Ma, S., Sui, Z., and Wei, F. Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers, 2023. [16] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. [17] Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., and Sui, Z. A survey for in-context learning. 

arXiv preprint arXiv:2301.00234 (2022). [18] FairScale authors . Fairscale: A general purpose modular pytorch library for high performance and large scale training. https://github.com/facebookresearch/fairscale, 2021. [19] FastChat authors . Lm-sys: Fastchat (vicuna: An open-source chatbot). https://github.com/lm-sys/FastChat, 2023. [20] Goodfellow, I., Bengio, Y., Courville, A., and Bengio, Y. Deep learning , vol. 1. MIT Press, 2016. , Vol. 1, No. 1, Article . Publication date: March 2023. Online Training of Large Language Models: Learn while Chatting 13 

[21] Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. Retrieval augmented language model pre-training. In 

International conference on machine learning (2020), PMLR, pp. 3929â€“3938. [22] Hao, S., Liu, T., Wang, Z., and Hu, Z. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. arXiv preprint arXiv:2305.11554 (2023). [23] Haystack authors . Haystack. https://github.com/deepset-ai/haystack, 2023. [24] Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 (2020). [25] Hinton, G. E., Osindero, S., and Teh, Y. W. A fast learning algorithm for deep belief nets. Neural Computation 18 

(2006), 1527â€“1554. [26] Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 (2022). [27] Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [28] Hu, L., Liu, Z., Zhao, Z., Hou, L., Nie, L., and Li, J. A survey of knowledge enhanced pre-trained language models. 

IEEE Transactions on Knowledge and Data Engineering (2023). [29] Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., and Han, J. Large language models can self-improve. arXiv preprint arXiv:2210.11610 (2022). [30] Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299 (2022). [31] Jaiswal, A., Babu, A. R., Zadeh, M. Z., Banerjee, D., and Makedon, F. A survey on contrastive self-supervised learning. Technologies 9 , 1 (2020), 2. [32] Jiang, Z., Xu, F. F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J., Yang, Y., Callan, J., and Neubig, G. Active retrieval augmented generation. arXiv preprint arXiv:2305.06983 (2023). [33] Jin, D., Pan, E., Oufattole, N., Weng, W.-H., Fang, H., and Szolovits, P. What disease does this patient have? a large-scale open domain question answering dataset from medical exams, 2020. [34] KÃ¶pf, A., Kilcher, Y., von RÃ¼tte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N. M., Stanley, O., Nagyfi, R., et al. Openassistant conversationsâ€“democratizing large language model alignment. arXiv preprint arXiv:2304.07327 (2023). [35] Langchain authors . LangChain. https://github.com/langchain-ai/langchain, 2023. [36] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., KÃ¼ttler, H., Lewis, M., Yih, W.-t., Rock-tÃ¤schel, T., et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33 (2020), 9459â€“9474. [37] Li, S., Fang, J., Bian, Z., Liu, H., Liu, Y., Huang, H., Wang, B., and You, Y. Colossal-ai: A unified deep learning system for large-scale parallel training. arXiv preprint arXiv:2110.14883 (2021). [38] Li, X., Yu, P., Zhou, C., Schick, T., Zettlemoyer, L., Levy, O., Weston, J., and Lewis, M. Self-alignment with instruction backtranslation. arXiv preprint arXiv:2308.06259 (2023). [39] Li, X., Yu, P., Zhou, C., Schick, T., Zettlemoyer, L., Levy, O., Weston, J., and Lewis, M. Self-alignment with instruction backtranslation, 2023. [40] Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. What makes good in-context examples for gpt-3?

arXiv preprint arXiv:2101.06804 (2021). [41] Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqa, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts, 2023. [42] Liu, Q., Yogatama, D., and Blunsom, P. Relational memory-augmented language models. Transactions of the Association for Computational Linguistics 10 (2022), 555â€“572. [43] Lu, Y., Lu, H., Fu, G., and Liu, Q. Kelm: knowledge enhanced pre-trained language representations with message passing on hierarchical relational graphs. arXiv preprint arXiv:2109.04223 (2021). [44] Malmi, E., Dong, Y., Mallinson, J., Chuklin, A., Adamek, J., Mirylenka, D., Stahlberg, F., Krause, S., Kumar, S., and Severyn, A. Text generation with text-editing models. arXiv preprint arXiv:2206.07043 (2022). [45] Megatron-DeepSpeed authors . Megatron-DeepSpeed. https://github.com/microsoft/Megatron-DeepSpeed, 2023. [46] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 (2022), 27730â€“27744. [47] Ovadia, O., Brief, M., Mishaeli, M., and Elisha, O. Fine-tuning or retrieval? comparing knowledge injection in llms, 2023. [48] Parisi, A., Zhao, Y., and Fiedel, N. Talm: Tool augmented language models. arXiv preprint arXiv:2205.12255 (2022). [49] Park, J. S., Oâ€™Brien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and 

, Vol. 1, No. 1, Article . Publication date: March 2023. 14 Juhao Liang, Ziwei Wang, Zhuoheng Ma, Jianquan Li, Zhiyi Zhang, Xiangbo Wu, and Benyou Wang 

Technology (2023), pp. 1â€“22. [50] Patil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla: Large language model connected with massive apis. 

arXiv preprint arXiv:2305.15334 (2023). [51] Peng, B., Galley, M., He, P., Cheng, H., Xie, Y., Hu, Y., Huang, Q., Liden, L., Yu, Z., Chen, W., et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813 (2023). [52] Peters, M. E., Neumann, M., Logan IV, R. L., Schwartz, R., Joshi, V., Singh, S., and Smith, N. A. Knowledge enhanced contextual word representations. arXiv preprint arXiv:1909.04164 (2019). [53] Qin, Y., Hu, S., Lin, Y., Chen, W., Ding, N., Cui, G., Zeng, Z., Huang, Y., Xiao, C., Han, C., Fung, Y. R., Su, Y., Wang, H., Qian, C., Tian, R., Zhu, K., Liang, S., Shen, X., Xu, B., Zhang, Z., Ye, Y., Li, B., Tang, Z., Yi, J., Zhu, Y., Dai, Z., Yan, L., Cong, X., Lu, Y., Zhao, W., Huang, Y., Yan, J., Han, X., Sun, X., Li, D., Phang, J., Yang, C., Wu, T., Ji, H., Liu, Z., and Sun, M. Tool learning with foundation models, 2023. [54] Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X., Tang, X., Qian, B., et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789 (2023). [55] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. Improving language understanding by generative pre-training. [56] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog 1 , 8 (2019), 9. [57] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21 , 1 (2020), 5485â€“5551. [58] Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (2020), pp. 3505â€“3506. [59] Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207 (2021). [60] Schick, T., Dwivedi-Yu, J., DessÃ¬, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. 

Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761 (2023). [61] Schick, T., Dwivedi-Yu, J., Jiang, Z., Petroni, F., Lewis, P., Izacard, G., You, Q., Nalmpantis, C., Grave, E., and Riedel, S. Peer: A collaborative language model. arXiv preprint arXiv:2208.11663 (2022). [62] Shanahan, M. Talking about large language models. arXiv preprint arXiv:2212.03551 (2022). [63] Shinn, N., Labash, B., and Gopinath, A. Reflexion: an autonomous agent with dynamic memory and self-reflection. 

arXiv preprint arXiv:2303.11366 (2023). [64] Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 (2019). [65] Song, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y. Mass: Masked sequence to sequence pre-training for language generation, 2019. [66] Sorensen, T., Robinson, J., Rytting, C. M., Shaw, A. G., Rogers, K. J., Delorey, A. P., Khalil, M., Fulda, N., and Wingate, D. An information-theoretic approach to prompt engineering without ground truth labels. arXiv preprint arXiv:2203.11364 (2022). [67] Sun, T., Zhang, X., He, Z., Li, P., Cheng, Q., Yan, H., Liu, X., Shao, Y., Tang, Q., Zhao, X., Chen, K., Zheng, Y., Zhou, Z., Li, R., Zhan, J., Zhou, Y., Li, L., Yang, X., Wu, L., Yin, Z., Huang, X., and Qiu, X. Moss: Training conversational language models from synthetic data. [68] Sun, Y., Wang, S., Feng, S., Ding, S., Pang, C., Shang, J., Liu, J., Chen, X., Zhao, Y., Lu, Y., et al. Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. arXiv preprint arXiv:2107.02137 (2021). [69] Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085 (2022). [70] Text Generation Inference authors . Text Generation Inference. https://github.com/huggingface/text-generation-inference, 2023. [71] Tirumala, K., Markosyan, A., Zettlemoyer, L., and Aghajanyan, A. Memorization without overfitting: Analyzing the training dynamics of large language models. Advances in Neural Information Processing Systems 35 (2022), 38274â€“ 38290. [72] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., RoziÃ¨re, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models, 2023. [73] vLLM authors . vllm. https://github.com/vllm-project/vllm, 2023. [74] Wang, B., Ping, W., Xu, P., McAfee, L., Liu, Z., Shoeybi, M., Dong, Y., Kuchaiev, O., Li, B., Xiao, C., et al. Shall we , Vol. 1, No. 1, Article . Publication date: March 2023. Online Training of Large Language Models: Learn while Chatting 15 

pretrain autoregressive language models with retrieval? a comprehensive study. arXiv preprint arXiv:2304.06762 (2023). [75] Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560 (2022). [76] Wang, Y., Yao, Q., Kwok, J. T., and Ni, L. M. Generalizing from a few examples: A survey on few-shot learning. ACM computing surveys (csur) 53 , 3 (2020), 1â€“34. [77] Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021). [78] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35 (2022), 24824â€“24837. [79] Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864 (2023). [80] Yang, Z., Wu, Z., Luo, M., Chiang, W.-L., Bhardwaj, R., Kwon, W., Zhuang, S., Luan, F. S., Mittal, G., Shenker, S., et al. {SkyPilot }: An intercloud broker for sky computing. In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23) (2023), pp. 437â€“455. [81] Yao, Z., Aminabadi, R. Y., Ruwase, O., Rajbhandari, S., Wu, X., Awan, A. A., Rasley, J., Zhang, M., Li, C., Holmes, C., et al. Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales. arXiv preprint arXiv:2308.01320 (2023). [82] Yao, Z., Aminabadi, R. Y., Ruwase, O., Rajbhandari, S., Wu, X., Awan, A. A., Rasley, J., Zhang, M., Li, C., Holmes, C., Zhou, Z., Wyatt, M., Smith, M., Kurilenko, L., Qin, H., Tanaka, M., Che, S., Song, S. L., and He, Y. DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales. arXiv preprint arXiv:2308.01320 (2023). [83] Zeng, G., Han, X., Zhang, Z., Liu, Z., Lin, Y., and Sun, M. Openbmb: Big model systems for large-scale representation learning. In Representation Learning for Natural Language Processing . Springer, 2023, pp. 463â€“489. [84] Zeng, H. Measuring massive multitask chinese understanding. arXiv preprint arXiv:2304.12986 (2023). [85] Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023). [86] Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. [87] Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., Zhang, S., Ghosh, G., Lewis, M., Zettlemoyer, L., and Levy, O. Lima: Less is more for alignment, 2023. [88] Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. 

Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 (2019). 

A EXPERIMENT DETAILS 

For all experiments conducted in this study, we utilized four A100 GPUs, each equipped with 80GB of memory. The learning rate for SFT was set to 2e-06, while for online training, it was established at 2e-5. Following the guidelines in Tirumala et al. (2022) [ 71 ], we set the training batch epoch at 10 for OT and at 2 for SFT. Additionally, the data format for tool invocation is exemplified as follows:  

> 1

Human : Can you provide a weather forecast for Rio de Janeiro , Brazil for the upcoming weekend ?  

> 2

GPT : Thought : I need to use the forecast_weather API to get the weather forecast for Rio de Janeiro , Brazil for the upcoming weekend .  

> 3

Action : weather . forecast_weather  

> 4

Action Input : {" location ": " Rio de Janeiro , Brazil " , " days ": 2} 

, Vol. 1, No. 1, Article . Publication date: March 2023.
