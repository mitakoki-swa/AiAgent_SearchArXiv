Title: 

URL Source: http://arxiv.org/pdf/2201.04480v2

Published Time: Mon, 23 Jan 2023 13:06:07 GMT

Markdown Content:
# Learning to Identify Top Elo Ratings: A Dueling Bandits Approach 

Xue Yan 12 , Yali Du * 3 , Binxin Ru 4, Jun Wang 5, Haifeng Zhang 12 , Xu Chen 6 

> 1

Institute of Automation, Chinese Academy of Sciences, China  

> 2

School of Artificial Intelligence, University of Chinese Academy of Sciences, China  

> 3

King’s College London, UK 4 University of Oxford, UK 5 University College London, UK  

> 6

Beijing Key Laboratory of Big Data Management and Analysis Methods, GSAI, Renmin University of China. 

Abstract 

The Elo rating system is widely adopted to evaluate the skills of (chess) game and sports players. Recently it has been also integrated into machine learning algorithms in evaluating the performance of computerised AI agents. However, an accu-rate estimation of the Elo rating (for the top players) often re-quires many rounds of competitions, which can be expensive to carry out. In this paper, to improve the sample efficiency of the Elo evaluation (for top players), we propose an efficient online match scheduling algorithm. Specifically, we identify and match the top players through a dueling bandits frame-work and tailor the bandit algorithm to the gradient-based up-date of Elo. We show that it reduces the per-step memory and time complexity to constant, compared to the traditional like-lihood maximization approaches requiring O(t) time. Our al-gorithm has a regret guarantee of ˜O(√T ), sublinear in the number of competition rounds and has been extended to the multidimensional Elo ratings for handling intransitive games. We empirically demonstrate that our method achieves supe-rior convergence speed and time efficiency on a variety of gaming tasks. 

Introduction 

In this paper, we investigate the selection of best multi-agent strategies under the Elo rating systems. The evaluation of the competition outcome has received lots of attention, es-pecially in view of the successful usage of reinforcement learning in StarCraft (Vinyals et al. 2019; Han et al. 2019; Du et al. 2019), Game of Go (Silver et al. 2017) and video games (Mnih et al. 2015). The Elo rating system (Elo 1978) is a predominant and valuable algorithm for evaluating and ranking agents. In the widely adopted Bradley-Terry model (Hunter et al. 2004) for Elo, each player is assigned a nu-merical rating which is updated with competition outcomes via online stochastic gradient descent. Further, for dealing with non-transitive relations between interacting agents such as the game of Rock-Paper-Scissors , Balduzzi et al. (2018) proposes multidimensional Elo (mElo), which decomposes a game into transitive and cyclic parts to handle intransitive skills and evaluates different strategies by computing Nash-averaging. 

> *

Corresponding to Yali Du 〈yali.du@kcl.ac.uk 〉.Copyright © 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 

In practical settings when a competition is expensive to conduct, updating Elo rating in a sample efficient way is highly valuable. To achieve such sample efficiency, we need a way to select the most informative pairs for evaluation. Two popular sampling approaches are Round-robin (Ras-mussen and Trick 2008) and Elimination tournament (Groh et al. 2012); The Round-robin (Rasmussen and Trick 2008) is widely used in sport scheduling to balance the total time, venue usage and fairness of tournaments. It would arrange each team to play against all the others in as few as possible days while satisfying some constraints such as each team not playing twice in the same day to promote game fair-ness. By contrast, the Elimination tournament (Groh et al. 2012) only allows the winners at each round to proceed to the next round, so the stronger team will have the chance to play more times. A recent approach, RG-UCB, Rowland et al. (2019) introduces an adaptive sampling scheme to esti-mate the accurate ranking among all agents. RG-UCB con-siders sampling of agent match-ups as a collection of pure exploration bandit problems (Bubeck, Munos, and Stoltz 2011) and requires enough pairwise comparison for estimat-ing each pair of strategies. However, these tournament matching/sampling methods suffer from two major limitations which prohibit their wide usage in the modern large scale evaluations. Firstly, both the Round-robin and the Elimination tournament organise com-petitions following a pre-designed schedule, and the Elimi-nation tournament scheduling may need some prior knowl-edge on the players’ skill. Also each pair of players only compete once in both schemes so the results can be highly noisy. Secondly, the main idea behind the matching schemes of the RG-UCB and the Round-robin is random sampling, which fails to pay more attention to more promising play-ers and/or pairs with higher uncertainty in competition out-come. Thus, they are less sample efficient in identifying the best players. In this work, we propose two sampling algorithms, named MaxIn-Elo and MaxIn-mElo, for the update of Elo and mElo rating systems respectively. Specifically, we maintain a can-didate set with promising players using UCB-based (Up-per Confidence Bound) dueling bandits and then select the pair with the highest uncertainty in competition outcome at each round. On the one hand, our algorithms are adapted to the gradient-based update of Elo rating systems, thus more 

> arXiv:2201.04480v2 [cs.LG] 20 Jan 2022

memory and time efficient compared to a prior work Saha and Gopalan (2020) which relies on maximum likelihood es-timation (MLE). One the other hand, we extend our method to update mElo (multidimensional Elo) ratings to handle in-transitive games, while Saha and Gopalan (2020) is based on a generalized linear model and can only fit to the tran-sitive games. To the best of our knowledge, this is the first work that enables online gradient-based update for dueling bandits, and a theoretical guarantee on the cumulative re-gret is provided.Also compared to a previous dueling bandit method descending through randomly sampled gradients at each time step (Yue and Joachims 2009), our method, by se-lecting the pairs with higher information gains from a set of top candidates, is more sample efficient and are guaranteed to converge at ˜O(√T )1.In summary, our contributions are three-fold: Firstly, we are the first to propose two online active sampling algo-rithms MaxIn-Elo and MaxIn-mElo that select maximum in-formative pairs with dueling bandits to update Elo and mElo ratings. Secondly, we give the regret analysis of our pro-posed algorithms and show that the algorithms converge at 

˜O(√T ). Thirdly, we demonstrate empirically on synthetic and real-world games that our algorithms achieve signifi-cantly lower cumulative regret than all baselines. Notably, our methods outperform MaxInP (Saha, Koren, and Man-sour 2021) which uses maximum likelihood for more accu-rate estimation while with lower time and memory complex-ity. Code of this project is available at https://github.com/ yanxue7/MaxIn-Elo.git. 

Related Work 

Multi-agent evaluation has attracted wide attention in rank-ing of players (Silver et al. 2017; Lai 2015; Arneson, Hay-ward, and Henderson 2010; Gruslys et al. 2018) and in se-lecting stronger strategies in meta games (Muller et al. 2020; Czarnecki et al. 2020). There are many methods used for multi-agent evaluation problem. The Elo rating system is widely used for two-player games such as chess and tennis. It increases (decreases) player’s rating according to player wins (loss) a competition, and updates ratings by online stochastic gradient descent (SGD), which is computation-ally efficient and simple to implement. While the Elo rat-ing system cannot handle intransitive games such as rock-paper-scissors, multidimensional Elo (Melo) (Balduzzi et al. 2018) was introduced. It decomposes the win-loss matrix of an intransitive game into the transitive component and cyclic component baked in Hodge decomposition theory (Jiang et al. 2011). α-rank (Omidshafiei et al. 2019) is an-other popular counterpart in tackling intransitive games; re-cent attempts improve its sample efficiency based on noisy comparisons (Du et al. 2021; Rowland et al. 2019; Omid-shafiei et al. 2019) and scalability by stochastic optimization (Yang et al. 2020). Despite various evaluation algorithms discussed, how to sample agent pairs at each round is of high value to realize these algorithms in large-scale evalu-ation tasks.   

> 1˜Oignores poly-logarithmic factors

We consider dueling bandits for online match scheduling in the evaluation of players. The concept of dueling bandits was firstly proposed in (Yue and Joachims 2009). Compared to traditional bandits algorithms which pull one arm at each round and receive the reward of this arm directly, dueling bandits pull arm-pair at each round and only get a binary comparison result. DBGD (Yue and Joachims 2009) models a convex optimization problem as the dueling bandits prob-lem which aims to find the best point in a convex space, and DBGD uses a random gradient as the direction of ex-ploration for selecting the next arm-pair. Yue et al. (2012) formulates the best player identification as a dueling bandits problem with noisy comparison results and an underlying winning probability matrix, and proposes two algorithms as well as their corresponding regret bounds. These algorithms identify best arms based on the observed binary feedback however do not learn the player’s skills (ratings), which is helpful in predicting future competition outcomes. Sz¨ or´ enyi et al. (2015) regards the ranking of M alternatives (e.g. hu-man players or agents) as a dueling bandits problem. They introduce the confidence interval of rating or winning prob-ability into the dueling bandits problem, and design algo-rithms to identify the close-to-optimal item or to obtain the close-to-optimal whole ranking respectively. Saha, Koren, and Mansour (2021) studies the adversarial setting, in which the winning probability is non-stationary because players’ skill may change over time. And they measure arms’ abil-ities by estimating Borda score, however Borda score does not possess predictive power of future competition results and the algorithm of (Saha, Koren, and Mansour 2021) esti-mates Borda score only by simply calculating the frequency of wins. Heckel et al. (2019) gives an active ranking algo-rithm that can solve the top-k player identification problem and find the entire sequential ranking among all players. While existing algorithms could rely on Borda score to update the rankings or design specific sort algorithms to obtain the ranking of items, they are not suitable for the Elo rating systems that adopt stochastic gradient descent to update ratings. Eearlier attempt (Ding, Hsieh, and Sharp-nack 2021) proposes SGD-TS for contextual bandit prob-lem, which learns parameters in the generalized linear model through online SGD instead and employs Thompson Sam-pling (TS) (Thompson 1933; Agrawal and Goyal 2012, 2013) to encourage exploration in arm-pulling. Compared to UCB-GLM (Li, Lu, and Zhou 2017) that adopt maximum likelihood estimators, SGD-TS achieves a similar theoretical cumulative regret bound, but lower time and memory com-plexity.However, no prior work has studied the SGD update in dueling bandits setting. In this work, we will tame dueling bandits for the on-line match scheduling in the Elo rating system that adopts stochastic gradient descent. Saha and Gopalan (2020) pro-poses the algorithm that selects Maximum-Informative-Pair (MaxInP) for K-armed contextual dueling bandits. This al-gorithm utilizes the MLE to estimate parameter ˆθ and uses UCB (Auer, Cesa-Bianchi, and Fischer 2002) estimator to narrow down the set of candidate pairs from which the pair of arms with the maximum uncertainty is selected at each round. Our algorithms adopt a similar design as MaxInP Saha and Gopalan (2020) in calculating the uncertainty of a pair. However, we use an online batch SGD instead of MLE to update Elo rating, which is more time and memory effi-cient. 

Methodology 

Background 

Suppose there are n players, the Elo rating system (Elo 1978) assigns a rating rx, x ∈ [n] to each player represent-ing its skill. Let r∗ denote the true ratings of n players. Our aim is to identify the best player among all n players: 

x∗ = arg max  

> x∈[n]

r∗ 

> x

(1) Denote P as the true winning probability matrix, pxy as the underlying groundtruth probability of x beating y. Based on the Bradley-Terry model (Hunter et al. 2004), the predicted probablility of player x winning y is 

ˆpxy = σ(rx − ry ). (2) 

σ(x) is a sigmoid function with σ(x) = 11+ e−x . Elo ratings are updated by maximizing the likelihood of win-loss pre-dictions which corresponds to minimizing the loss: 

`Elo (pxy , ˆpxy ) = −pxy log ˆ pxy − (1 − pxy ) log (1 − ˆpxy ) .

(3) At time t player x compete with player y with outcome otxy :

otxy = 1 if x wins and otxy = 0 otherwise. We can use otxy to compute the gradient of Eq. (3) and update Elo by gradient descent: 

rt+1  

> x

← rtx − η · ∇ rx `Elo 

(otxy , ˆptxy 

) = rtx + η · (otxy − ˆptxy 

) .

(4) Let T denote the total number of rounds, at each round 

t ∈ [T ], we adopt a system that will pull a pair of players 

(xt, y t) ∈ [n] × [n] and get comparison result ot(xt, y t) ∼

Bern (pxy ). The cumulative regret of T rounds is defined as 

R(T ) = 

> T

∑

> t=1

[r∗ 

> x∗

− 12 (r∗ 

> xt

+ r∗ 

> yt

)] . (5) The definition is consistent with (Saha and Gopalan 2020). It measures the reward difference between the best arm and the two selected arms at each round. 

MaxIn-Elo Algorithm 

We use the notations below in the followed presentations. 

• n: the number of players. 

• τ : the batch size. 

• r: a vector of n players’ ratings. 

• r∗: the true ratings of n players. 

• ˆrt: estimator by MLE with t round comparisons. 

• ˜rj : the SGD estimator at batch j.

• ¯r = ∑jq=1 ˜rq : the average of previous SGD iterations. 

• ‖ x‖ = √x>x: the standard `2 norm. 

• ei: the i-th unit base vector, i.e., the i-dimension equals 

1 and all other components equal 0.Algorithm 1: MaxIn-Elo: Dueling bandits with online SGD for top player identification. 

Input: batch size τ , maximum number of rounds T , N

players’ strategies, parameters α, γ .

Output: output r

1: Randomly choose a pair to compare and record as 

xt, y t, o t for t ∈ [τ ]

2: Vτ +1 = ∑τt=1 (ext − eyt )( ext − eyt )T

3: Calculate the maximum-likelihood estimator ˆrτ by solving 

∇r

∑τt=1 `Elo (ot, ˆp(xt, y t)) = 0 

4: Maintain convex set C = {r : ‖r − ˆrτ ‖ ≤ 2}

5: for t = τ + 1 , τ + 2 , . . . , T do 

6: if t%τ = 1 then 

7: j ← b (t − 1) /τ c and ηj = 1

> αj

8: Calculate gradient ∇r lj,τ (˜ rj−1) through Eq. (4) 

9: Update ratings ˜rj through Eq. (8) 

10: Compute ¯r = 1

> j

∑jq=1 ˜rq

11: end if 

12: Define a candidate optimal set S = {x | ¯rx − ¯ry +

γ ‖ex − ey ‖V −1

> t

> 0, ∀y ∈ [n]/{x}} 

13: Select a pair as: 

(xt, y t) = arg max (x,y )∈S ‖ex − ey ‖V −1

> t

.

14: Let players (xt, y t) compete and observe ot(xt, y t)

15: Compute Vt+1 = Vt + ( ext − eyt )( ext − eyt )T

16: end for 

• Vt: the history matrix recording previous t pulling in-formation defined by Vt = ∑t−1 

> i=1

(exi − eyi )( exi − eyi )>.

• ‖ x‖V : a special `2-norm associated with matrix V de-fined by ‖x‖V = √x>V x .

• B a neighborhood of r∗ with B = {r| ‖ r − r∗‖ ≤ 3}.

• C : a neighborhood of ˆrτ with C = {r| ‖ r − ˆrτ ‖ ≤ 2}.

• ∏ 

> C

(.): the projection operation defined by: 

∏

> C

(r) = ˆ rτ + 2 ∗ (r − ˆrτ )min {2, ‖r − ˆrτ ‖} (6) 

Algorithm Overview The main idea of our MaxIn-Elo al-gorithm is to maintain a candidate set of promising items via UCB and select the most informative pairs out of the set to evaluate at each round. Firstly, the ratings are initialized by maximizing likelihood of Eq. (3) on a batch of randomly sampled pairs with batch size τ . The solution is denoted as 

ˆrτ and ˜r0 = ˆ r. Then starting from round t = τ + 1 , we update ˜rj every τ rounds by solving the following objective function 

lj,τ (r) = 

> jτ

∑ 

> t=( j−1) τ+1

`Elo (ot, ˆp(xt, y t)) . (7) The stochastic gradient update of ˜rj reads 

˜rj ← ∏

> C

(˜ rj−1 − ηj ∇r lj,τ (˜ rj−1)) . (8) First, the strong convexity of the objective function is re-quired for fast convergence, and if we select a suitable τ

through Eq. (14), the aggregated objective function lj,τ (r) is a α-strong convex function when r ∈ B . Second, to ensure 

˜rj ∈ B , ˜rj is projected into the convex set C (also discussed in the proof of Lemma 2). For each update, a batch of pairs are selected that lead to maximal information gain. The UCB score of a pair is defined by: 

h(xt, y t) = ¯ rxt − ¯ryt + γ ‖ext − eyt ‖V −1

> t

, (9) with the balance parameter γ. The specific V −1 

> t

norm 

γ ‖ext − eyt ‖V −1

> t

measures the uncertainty between two arms. The UCB estimator balances the exploitation and ex-ploration through combining ratings estimation ¯r and the un-certainty term. At each round t, we obtain a set of optimal player candi-dates S with positive UCB scores: 

S = {x|h(x, y ) > 0, ∀ y ∈ [n]/{x}} . (10) From the candidate set S, we then pull a pair of arms with highest uncertainty by 

(xt, y t) = arg max  

> (x,y )∈S×S

‖ex − ey ‖V −1

> t

(11) to induce sufficient exploration. A detailed algorithm of our MaxIn-Elo is shown in Algorithm 1. Compared to (Saha and Gopalan 2020) which uses MLE at each iteration, MaxIn-Elo uses SGD to update the Elo rating r as traditional Elo does. Thus, our method is more efficient in both computation and time, and simple to imple-ment. Because the objective function lj,τ is the aggregation of a batch with τ steps for the α-strong convexity, the online SGD on this objective function is actually the mini-batch gradient descent on τ single steps. The mini-batch update provides diversity and makes our algorithm more smooth and steady. Thus, the selection of the batch size τ is impor-tant to achieve the fast convergence speed and smoothness during learning. While RG-UCB randomly selects a pair to evaluate, our MaxIn-Elo selects the maximum informative pair and trade-off exploration and exploitation. To our best knowledge, this is the first algorithm that allows stochastic gradient descent update in dueling bandits settings. See Ta-ble1 for a comparison on time and memory. 

MaxIn-mElo Algorithm 

To enable the rating system to handle the intransitive skills, we extend the online sampling algorithm to multidimen-sional Elo ratings (mElo) (Balduzzi et al. 2018). Baking in the Hodge decomposition theory (Jiang et al. 2011), mElo proposed to decompose the antisymmetric logits matrix of win-loss probabilities into a transitive component, i.e. gradi-ent flow of rating vector, and a cyclic component to capture the intransitive relations. By learning a 2k-dimensional vec-tor cx and a rating rx per player, the win-loss prediction for mElo 2k is defined as: 

ˆpxy = σ (rx − ry + c> 

> x

· Ω2k×2k · cy

) . (12) Table 1: Comparison of regret, time complexity and mem-ory with other algorithms. Our MaxIn-Elo and the MaxInP achieve the lowest regret bound ˜O(√T ), but our MaxIn-Elo has lower time and memory complexity than the MaxInP. Algorithms Regret Time Complexity Memory DBGD O(T 2/3) O(T ) O(n)

RG-UCB No O(T ) O(n)

Random No O(T ) O(n)

MaxInP ˜O(√T ) O(nT 2 + n2T ) O(nT )

MaxIn-Elo ˜O(√T ) O(n2T ) O(n2)

where Ω2k×2k = ∑ki=1 (e2i−1e> 

> 2i

− e2ie>

> 2i−1

).The UCB estimate of a pair (xt, y t) for mElo then be-comes: 

h(xt, y t) = ¯ rxt − ¯ryt + ¯ cTx Ω¯ cy + γ ‖ext − eyt ‖V −1

> t

. (13) Notice that compared to Elo ratings with k = 0 (Eq. (2)), mElo ratings assign a feature vector per player to approxi-mated intransitive interactions. We present the details for the mElo ratings and Algorithm 2 for MaxIn-mElo in Appendix. 

Regret analysis 

We give the cumulative regret bound of MaxIn-Elo, as for as we know, this is the first work that combines the online gra-dient update with dueling bandits and gives the cumulative regret of dueling bandits while being updated with SGD. We make a mild assumption on the link function σ.

Assumption 1. Define cη = inf {‖ r−r∗‖≤ η} σ′ (rx − ry ),where (x, y ) ∈ [n] × [n], and we assume c3 > 0.

This assumption is similar to that in (Ding, Hsieh, and Sharpnack 2021). Our main results rely on the following concentration events and the proofs of which are deferred to Appendix. 

Lemma 1. Suppose we sample a sequence of arm pairs 

{(x1, y 1), (x2, y 2), . . . , (xt, y t)} through Algorithm 1 up to round t, and assume the selected batch size τ satisfy that 

λmin (Vτ +1 ) ≥ 1, where λmin (Vτ +1 ) means the minimum eigenvalue of the matrix (Vτ +1 ), Then ∀t > 0, 

> τ+t

∑ 

> i=τ+1

‖(exi − eyi )‖V −1

> i

<

√

2nt log 

( 2τ + tn

)

.

Lemma 1 gives the bound of the sum of selected pair’s uncertainty from round τ + 1 to t. And this lemma will be adopted to derive the cumulative regret bound. In the follow-ing Lemma 2, we show that when the batch size τ is chosen as Eq. (14), we have the concentration property of the aver-aged SGD estimator ¯r.

Lemma 2. Assume that there exists a positive constant λf

such that λmin 

(E[( ext − eyt )( ext − eyt )T ]) ≥ λf holds at each round t > τ , where (xt, y t) is sampled through Algo-rithm 1. Let the batch size τ satisfies 

τ1 = 2 

( C1

√n + C2

√2 log Tλmin (B)

)2

+ 16( n + 2 log T )

c21λmin (B) ,τ2 = 2 

( C1

√n + C2

√2 log Tλf

)2

+ 4αc3λf

,τ = dmax {τ1, τ 2}e ,

(14) 

where B = E(x,y )iid  

> ∼[n]×[n]

[(ex − ey )( ex − ey )T ]. Define 

g1(t) and g2(j),

g1(t) = 12c1

√

n

2 log 

(

1 + 2tn

)

+ 2 log T , (15) 

g2(j) = τα

√1 + log j. (16) 

For a constant α ≥ c3, there exists two positive constants 

C1, C2 such that if the batch size τ is chosen as Eq. (14) ,then we have that at each round t > τ corresponding to batch j = b t−1 

> τ

c, event E1(t) holds with probability at least 

1 − 5  

> T2

, where E1(t) = {∀ (x, y ) : ∣∣(ex − ey )T (¯ rj − r∗)∣∣ ≤

g1(jτ )‖ex − ey ‖V −1 

> jτ +1

+ g2(j) √2√j }.

The following Lemma 3 shows how to select a suitable balanced parameter γ of UCB score that ensures the best player is always in the candidate set. 

Lemma 3. Define the constant C =

√

2nT log ( T +τn

).At each round t > τ , let UCB balanced parameter γ =2g1(t) and assume ∆ > g 1(T )C, if α satisfies that α ≥√2τ √1+log j  

> (∆ −g1(T)C)√j

, then we have x∗ ∈ S holds with proba-bility at least 1 − 5  

> T2

, where j = b t−1 

> τ

c, ∆ is the differ-ence between ratings of optimal player x∗ and sub-optimal player x′. Recall x∗ = arg max x∈[n] r∗

> x

, and define x′ =arg max x∈[n]/x ∗ r∗

> x

, ∆ = r∗ 

> x∗

− r∗ 

> x′

.

Lemma 3 shows that if we properly select UCB bal-anced parameter γ and parameter α which describes objec-tive function lj,τ as a α-strongly convex, then it is promised that the best player x∗ is in candidates set S with high prob-ability. This property is helpful for the top-1 identification because the candidate set S will become tighter with the time, and x∗ always in S, thus candidate set S only con-tains x∗ eventually. Together we are ready to present our main results in Theorem 1. 

Theorem 1. We run our Algorithm 1 to get a se-quence of arm-pair, and let the learning rate parameter 

α ≥ max {c3,    

> √2τ√1+log j
> (∆ −g1(T)C)√j

} with assumption that ∆ >g1(T )C, the balanced parameter γ = 2 g1(t), there exists two positive parameter C1, C 2 such that if the batch size τ

is chosen as Eq. (14) , then we have the cumulative regret satisfies that:              

> R(T)≤τ∗∆max + (2 + τ)g1(T)
> √
> 2nT log( 2τ+Tn) + 4 g2(J)√τ T ,

with probability at least 1 − 10  

> T

, where J = b Tτ c, ∆max =max i r∗ 

> i

− min i r∗ 

> i

, g1(T ), g 2(J) is defined in Eq. (15) and 

C is a constant defined as C =

√

2nT log ( T +τn

).

Note that τ ∼ O(max {n, log T }) (Eq. (14)), g1(T ) ∼

O(√n log T ), g2(J) ∼ O(√log T ). Combining the above analysis, we have R(T ) ∼ O(n log T √T ) (or ˜O(√T ).This regret upper bound is equivalent to that in (Saha and Gopalan 2020) which employs MLE estimators. However, our algorithm improves the efficiency in terms of memory and time. The memory cost is constant with respect to T

while MaxInP’s memory cost is linear in the time horizon 

T . The time complexity of our MaxIn-Elo is O(n2T ), while MaxInP’s time complexity is O(nT 2 + n2T ). See Table 1 for a detailed comparison. Detailed proofs are referred to Appendix. 

Experiments 

We consider the following two batteries of experiments to evaluate the performance of our algorithms in the scenarios of transitive and intransitive real world meta-games. Abla-tion studies of parameter γ, dimension of mElo and the batch size τ can be found in Appendix. 

Baselines 

Random : The pairwise matching scheme of the classical Round-robin (Rasmussen and Trick 2008) tournament is based on random sampling. We construct a simple baseline that randomly select a pair from all n ∗ (n − 1) /2 pairs with replacement. After sampling a pair, we use the Elo/mElo model to update the ratings. 

RG-UCB (Rowland et al. 2019): This algorithm adopts a pure exploration sampling scheme, which uniformly sam-ples a pair from the set containing pairs that need to be es-timated. And the stopping condition C(δ) controls the total number of comparisons of each pair, where δ is a hyper pa-rameter deciding the confidence level of estimated competi-tive results. 

DBGD (Yue and Joachims 2009): This dueling bandits algorithm is popular in ranking tasks when only pair-wise binary feedback is available. It maintains one winning arm at each round, and randomly synthesizes a gradient to ob-tain the opponent arm in the contextual bandit setting. In our feature free setting, this is equivalent to randomly selecting a player as the opponent. 

α-IG (Rashid, Zhang, and Ciosek 2021): This is an active sampling algorithm used for estimating the α-rank (Omid-shafiei et al. 2019). This algorithm selects a pair with largest information gain at each round. In the transitive case, the top player has an α-rank score equal to 1. Due to the high com-putation cost at each round ( computing α-rank for 80000 times in a 4 × 4 game), we only compare with it in a 4 × 4

transitive game: the ‘2 Good, 2Bad’ game given by α-IG. 

MaxInP (Saha and Gopalan 2020): This algorithm is for the generalized linear contextual dueling bandits problem, in which arms are represented as feature vectors. And it uses MLE to estimate model parameters θ relying on all historical pairwise comparisons at each round t. This algorithm cal-culates a candidate set containing advanced arms and pulls an arm-pair with the largest uncertainty. In order to fit their model, each player is described as a one-hot vector, and the estimated parameters θ correspond to players’ ratings in our setting. 

MaxIn-Elo : Our first algorithm adopts dueling bandits to adaptively sample pairs for Elo rating update in Eq. (2). The aim of our MaxIn-Elo is to identify the advanced players gradually, and to minimize the cumulative regret described in Eq. (5) simultaneously. 

MaxIn-mElo : Our second algorithm tames the intransi-tive scenarios. Different to MaxIn-Elo, there is an extra vec-tor c to capture intransitive relationship in competition out-come prediction. The dimension of c is set to 8 in experi-ments. For the MaxIn-mElo algorithm, we hope to identify players with superior mElo ratings and to minimize cumula-tive regret on mElo ratings. 

Experiments setting 

Real world games We do our experiments on twelve real-games released by Czarnecki et al. (2020), most of which are implemented on the OpenSpiel framework (Lanctot et al. 2019). The six games used for evaluating Elo are Triangular game, Transitive game, Elo game, and three noisy variants of Elo games. The first three are transitive games; the three variants of Elo game are Elo games with additive Gaussian noises. The six intransitive games used for evaluating mElo are Kuhn-poker, AlphaStar, tic tac toe, hex, Blotto and 5,3-Blotto game. The intransitivity of games can be revealed by sink strongly connected components (SSCCs) (Omidshafiei et al. 2019), which is a set of strategies that cannot be defeated by external strategies and all internal strategies become a circle, such as Rock, Paper, Scissors. The statistics of these games is shown in Table 2 in Appendix. 

Metrics Except the cumulative regret defined in Eq. (5), we introduce three other metrics for Reciprocal Rank (RR), Normalized Discounted Cumulative Gain (NDCG), and Hit Raio (HR). RR is used for the results on generating top-1 players. NDCG and HR report discrete performance for top-1 performance and are thus used in top-k results. Reciprocal Rank (RR) (Donmez, Svore, and Burges 2009) give the reciprocal of predicted ranking of the best player x∗.Define 

RR = 1

R(x∗) ,

where R(x) returns the ranking of player x relying on cur-rently predicted ratings ¯r. Larger RR corresponds to better performance on the top-1 player identification. Hit Ratio @K (He et al. 2015) is defined as the ratio of the predicted top-k that belongs to the true top-k. Since hit ratio does not consider the positions of correct predictions, we also adopts NDCG (Donmez, Svore, and Burges 2009) which assigns higher importance to results at top ranks. Normalized Discounted Cumulative Gain (NDCG) is widely used in the evaluation of rankings and the NDCG@k mea-sures the importance of predicted top-k players. It is given by NDCG @K = 1

NKK∑

> i=1

2l(di) − 1log( i + 1) ,0.0 0.2 0.4 0.6 0.8 1.0 Rounds t 1e3        

> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> 1.0 RR
> 4 × 4 game
> 0.0 0.2 0.4 0.6 0.8 1.0 Rounds t1e3
> 0
> 1
> 2
> 3
> 4
> 5
> 6
> 7Regret
> 1e3
> Random RG-UCB DBGD MaxInP -IG
> MaxIn-Elo

Figure 1: Results on 4 × 4 game (2 Good 2 Bad). where Nk is a normalizer to ensure that the perfect ranking would result in NDCG @K = 1 . d i denote the index of pre-dicted i-th player, and l(x) ∈ { 0, 1} is the relevance level about top-k identification, we set l(x) = 1 if player x in true top-k otherwise 0. 

Parameters setting For Random, DBGD, and RG-UCB baseline, we perform a grid search for the initial step size 

η in the range {0.01 , 0.05 , 0.1, 0.5, 1, 5, 10 }. For RG-UCB, stopping confidence δ = 0.2. For MaxInP, we tune the UCB balanced parameter γ ∈ { 0.2, 0.4, 0.6, ..., 2.0}. For MaxIn-Elo and MaxIn-mElo, we tune the initialized learn-ing rate η ∈ { 0.01 , 0.05 , 0.1, 0.5, 1, 5, 10 }, and the learning rate at batch j is set as ηj . And the UCB balanced parame-ter γ ∈ { 0.2, 0.4, 0.6, ..., 2.0}. The batch size τ of MaxInP, MaxIn-Elo and MaxIn-mElo is set to 0.7∗n. When baselines ues mElo model to calculate ratings, we set the dimension of the extra vector c as 8. We use the parameters that report the best performance for α-IG. We repeat experiments 5 times with different random seeds and plot the averaged perfor-mance with standard deviations. All experiments were run in a single x86 64 GNU/Linux machine with 256 AMD EPYC 7742 64-Core Processor and 2 A100 PCIe 40GB GPU. We use sklearn(0.24.2) to solve the MLE. 

Results 

Figure 1, 2, 3 show the results of top-1 identification on 13 games. To ensure a fair comparison between all baselines, we perform a grid search to select parameters with the best RR performance for each random seed. If the winning prob-ability matrix can be fitted into the Elo model, then we cal-culate the true ratings through Eq. (2), otherwise we use the mElo ratings as the true ratings through Eq. (12). 

Evaluation of MaxIn-Elo Figure 1 shows the results on a 4 × 4 transitive game. MaxIn-Elo has the highest con-vergence rate on both RR and cumulative regret metrics, and MaxIn-Elo has the lowest cumulative regret close to 

0. As shown in Figure 2, MaxIn-Elo significantly outper-forms all other baselines on five games and achieves similar performance on Triangular game. Regarding the RR metric, MaxIn-Elo can converges to 1 on four games. Even on Tran-sitive game and Elo game + noise= 0.1, RR scores as up to 

0.6 and 0.8 respectively, which indicates that the rank of the top player no more than 2. Thus we think MaxIn-Elo has the ability to effectively identify the top player . On the Elo game, Elo game + noise= 0.01 , and Elo game + noise= 0.01 ,Random RG-UCB DBGD MaxInP MaxIn-Elo 0.5 1.0 Rounds t 1e3                                         

> 0.2
> 0.4
> 0.6
> 0.8
> 1.0 RR
> Elo game
> 0.0 0.5 1.0 Rounds t1e3
> 0
> 2
> 4
> 6Regret
> 1e3 0.5 1.0 Rounds t1e3
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0 RR
> Elo game + noise=0.01
> 0.0 0.5 1.0 Rounds t1e3
> 0
> 1
> 2
> 3
> 4Regret
> 1e3 0.5 1.0 Rounds t1e3
> 0
> 2
> 4
> 6
> 8RR
> 1e 1Transitive game
> 0.0 0.5 1.0 Rounds t1e3
> 0.0
> 0.5
> 1.0 Regret
> 1e3 0.5 1.0 Rounds t1e3
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0 RR
> Elo game + noise=0.05
> 0.0 0.5 1.0 Rounds t1e3
> 0
> 1
> 2
> 3Regret
> 1e3 0.5 1.0 Rounds t1e3
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 RR
> Triangular game
> 0.0 0.5 1.0 Rounds t1e3
> 0
> 2
> 4Regret
> 1e3 0.5 1.0 Rounds t1e3
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 RR
> Elo game + noise=0.1
> 0.0 0.5 1.0 Rounds t1e3
> 0.0
> 0.5
> 1.0
> 1.5 Regret
> 1e3

Figure 2: Results of Elo on transitive games. the cumulative regret are closed to convergence at around 

500 rounds. When the cumulative regret meets convergence, the candidate optimal set S only contains the top player, and no regret increasing. Different from the other 5 stochastic games, Triangular game is a deterministic game with all winning probabilities are equal to 1 or 0, thus it is easy to evaluate. For DBGD baseline, it maintains the current best player and randomly selects an opponent, so it could find the best player more quickly, but has a large cumulative regret because of ran-domly selected opponents. 

Evaluation of MaxIn-mElo Figure 3 shows the results of baselines on six real-world intransitive games. MaxInP is based on the Elo model for it is a special generalized linear model only with rating parameter r without cyclic vector parameter c, but all other baselines are based on the mElo model. As the Figure 3 shows, MaxIn-mElo has the lowest cumulative regret and the highest RR on all six games. With regard to the RR, MaxIn-mElo can be up to 1 on all games except for Blotto. One possible reason why MaxIn-mElo cannot be up to 1 on Blotto may be that its size of top SSCC is very large. The other reason is that we use the low-rank ap-proximation of the probability matrix’s rotation on the mElo model. Although we misidentified the top-1 player, we are still better than all other baselines. 

Results of Top-k player identification Figure 4 gives the results of top-k predictions on transitive games. MaxIn-Elo and MaxInP both have a parameter γ used to balance ex-ploration and exploitation, larger γ can lead to a larger can-didate set then lead to better top-k performance. We keep other parameters fixed and run experiments with different 

γ ∈ { 0.2, 0.4, 0.6, ..., 2.0}, and we report the performance of MaxInP and MaxIn-Elo under the best γ. Figure 4 shows that MaxIn-Elo has the best performance of the top-1 iden-tification on all games, and it achieves the comparable per-formance of top-k identification on most games. Results of Random RG-UCB DBGD MaxInP MaxIn-mElo 1 2Rounds t 1e3                                            

> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 RR
> Kuhn-poker
> 012Rounds t1e3
> 0.0
> 0.5
> 1.0
> 1.5 Regret
> 1e3 0.5 1.0 1.5 Rounds t1e3
> 0.25
> 0.50
> 0.75
> 1.00 RR
> AlphaStar
> 012Rounds t1e3
> 0
> 2
> 4
> 6Regret
> 1e3 0.5 1.0 1.5 Rounds t1e3
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 RR
> tic_tac_toe
> 012Rounds t1e3
> 0
> 2
> 4
> 6Regret
> 1e3 0.5 1.0 1.5 Rounds t1e3
> 0.25
> 0.50
> 0.75
> 1.00 RR
> hex(board_size=3)
> 012Rounds t1e3
> 0
> 2
> 4
> 6
> 8Regret
> 1e3 0.5 1.0 1.5 Rounds t1e3
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8 RR
> Blotto
> 012Rounds t1e3
> 0.0
> 0.5
> 1.0
> 1.5 Regret
> 1e3 0.5 1.0 Rounds t1e3
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0 RR
> 5,3-Blotto
> 0.0 0.5 1.0 Rounds t1e3
> 0.0
> 0.5
> 1.0 Regret
> 1e3

Figure 3: Results of mElo on intransitive games. Random RG-UCB DBGD MaxInP MaxIn-Elo 1 5 10 Top K                                   

> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 Hit Ratio@ K
> Elo game
> 1510 Top K
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 NDCG@ K
> = 2
> = 0.8 1510 Top K
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 Hit Ratio@ K
> Elo game + noise=0.01
> 1510 Top K
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 NDCG@ K
> = 0.6
> = 1 1510 Top K
> 0
> 2
> 4
> 6Hit Ratio@ K
> 1e 1Transitive game
> 1510 Top K
> 0
> 2
> 4
> 6NDCG@ K
> 1e 1
> = 0.1
> = 0.4 1510 Top K
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 Hit Ratio@ K
> Elo game + noise=0.05
> 1510 Top K
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 NDCG@ K
> = 0.6
> = 1 1510 Top K
> 0.25
> 0.50
> 0.75
> 1.00 Hit Ratio@ K
> Triangular game
> 1510 Top K
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0 NDCG@ K
> = 0.6
> = 1 1510 Top K
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 Hit Ratio@ K
> Elo game + noise=0.1
> 1510 Top K
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 NDCG@ K
> = 2
> = 0.8

Figure 4: Results of Top-k player identification on transitive games. γ in red and purple indicates that reports best perfor-mance for MaxInP and MaxIn-Elo respectively. different γ can be found in Appendix. 

Discussions 

This work studied the problem of multi-agent evaluation with Elo ratings. We have adopted an online match schedul-ing framework to improve the sample efficiency of the Elo rating system and its extension mElo for the intransitive set-tings. Both empirical and theoretical results justify that our algorithms can achieve higher sample efficiency and lower regret on most of the tasks. We consider two limitations of this work. Firstly, the match outcome prediction in our algorithm is based on only ratings without considering features that describe players. Future work may consider adding features into the match prediction. Secondly, our algorithm focuses more on iden-tifying the best player without being tailored for identifying top-k players. Future work can consider active sampling that achieves better results on both top-1 and top-k cases. 

Ethical Statement 

This work proposes algorithms for online match schedul-ing that improve the efficiency in identifying top players in competitive games such as chess. While empirical studies in this work, which are based on AI agents, have demonstrated the superior gain of using our proposed methods, there is a caveat that our algorithms assume that the all players’ skill levels remain unchanged throughout the repeated competi-tion rounds. This assumption likely does not hold for human players whose playing strengths will be affected by energy consumption due to frequent matches. Therefore, extra cau-tion needs to be taken when deploying our methods to sched-ule real-world competitions involving human players and an interesting research extension would be to model such per-formance strength changes explicitly in designing the match scheduling algorithms. 

References 

Agrawal, S.; and Goyal, N. 2012. Analysis of thompson sampling for the multi-armed bandit problem. In Conference on learning theory , 39–1. Agrawal, S.; and Goyal, N. 2013. Thompson sampling for contextual bandits with linear payoffs. In International Con-ference on Machine Learning (ICML) , 127–135. Arneson, B.; Hayward, R. B.; and Henderson, P. 2010. Monte Carlo tree search in Hex. IEEE Transactions on Com-putational Intelligence and AI in Games , 2(4): 251–258. Auer, P.; Cesa-Bianchi, N.; and Fischer, P. 2002. Finite-time analysis of the multiarmed bandit problem. Machine learn-ing , 47(2): 235–256. Balduzzi, D.; Tuyls, K.; Perolat, J.; and Graepel, T. 2018. Re-evaluating evaluation. In Advances in Neural Informa-tion Processing Systems (NeurIPS) , 3268–3279. Bubeck, S.; Munos, R.; and Stoltz, G. 2011. Pure explo-ration in finitely-armed and continuous-armed bandits. The-oretical Computer Science , 412(19): 1832–1852. Czarnecki, W. M.; Gidel, G.; Tracey, B.; Tuyls, K.; Omid-shafiei, S.; Balduzzi, D.; and Jaderberg, M. 2020. Real World Games Look Like Spinning Tops. Advances in Neu-ral Information Processing Systems (NeurIPS) , 33. Ding, Q.; Hsieh, C.-J.; and Sharpnack, J. 2021. An efficient algorithm for generalized linear bandit: Online stochastic gradient descent and thompson sampling. In International Conference on Artificial Intelligence and Statistics (AIS-TATS) , 1585–1593. Donmez, P.; Svore, K. M.; and Burges, C. J. 2009. On the local optimality of lambdarank. In Proceedings of the 32nd international ACM SIGIR conference on Research and de-velopment in information retrieval , 460–467. Du, Y.; Han, L.; Fang, M.; Dai, T.; Liu, J.; and Tao, D. 2019. LIIR: learning individual intrinsic reward in multi-agent reinforcement learning. In Proceedings of the 33rd International Conference on Neural Information Processing Systems (NeurIPS) , 4403–4414. Du, Y.; Yan, X.; Chen, X.; Wang, J.; and Zhang, H. 2021. Es-timating α -Rank from A Few Entries with Low Rank Ma-trix Completion. In International Conference on Machine Learning (ICML) , 2870–2879. PMLR. Elo, A. E. 1978. The rating of chessplayers, past and present . Arco Pub. Groh, C.; Moldovanu, B.; Sela, A.; and Sunde, U. 2012. Op-timal seedings in elimination tournaments. Economic The-ory , 49(1): 59–80. Gruslys, A.; Dabney, W.; Azar, M. G.; Piot, B.; Bellemare, M.; and Munos, R. 2018. The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning. In International Conference on Learning Representations (ICLR) .Han, L.; Sun, P.; Du, Y.; Xiong, J.; Wang, Q.; Sun, X.; Liu, H.; and Zhang, T. 2019. Grid-wise control for multi-agent reinforcement learning in video game ai. In Inter-national Conference on Machine Learning (ICML) , 2576– 2585. PMLR. He, X.; Chen, T.; Kan, M.-Y.; and Chen, X. 2015. Tri-rank: Review-aware explainable recommendation by mod-eling aspects. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management (CIKM) , 1661–1670. Heckel, R.; Shah, N. B.; Ramchandran, K.; Wainwright, M. J.; et al. 2019. Active ranking from pairwise compar-isons and when parametric assumptions do not help. The Annals of Statistics , 47(6): 3099–3126. Hunter, D. R.; et al. 2004. MM algorithms for generalized Bradley-Terry models. The Annals of Statistics , 32(1): 384– 406. Jiang, X.; Lim, L.-H.; Yao, Y.; and Ye, Y. 2011. Statisti-cal ranking and combinatorial Hodge theory. Mathematical Programming , 127(1): 203–244. Lai, M. 2015. Giraffe: Using deep reinforcement learning to play chess. arXiv preprint arXiv:1509.01549 .Lanctot, M.; Lockhart, E.; Lespiau, J.-B.; Zambaldi, V.; Upadhyay, S.; P´ erolat, J.; Srinivasan, S.; Timbers, F.; Tuyls, K.; Omidshafiei, S.; et al. 2019. OpenSpiel: A frame-work for reinforcement learning in games. arXiv preprint arXiv:1908.09453 .Li, L.; Lu, Y.; and Zhou, D. 2017. Provably optimal algo-rithms for generalized linear contextual bandits. In Inter-national Conference on Machine Learning (ICML) , 2071– 2080. Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.; and Ostrovski, G. 2015. Human-level control through deep reinforcement learning. Nature , 518(7540): 529. Muller, P.; Omidshafiei, S.; Rowland, M.; Tuyls, K.; Pero-lat, J.; Liu, S.; Hennes, D.; Marris, L.; Lanctot, M.; Hughes, E.; et al. 2020. A Generalized Training Approach for Mul-tiagent Learning. In International Conference on Learning Representations (ICLR) , 1–35. Omidshafiei, S.; Papadimitriou, C.; Piliouras, G.; Tuyls, K.; Rowland, M.; Lespiau, J.-B.; Czarnecki, W. M.; Lanctot, M.; Perolat, J.; and Munos, R. 2019. α-rank: Multi-agent evalu-ation by evolution. Scientific reports , 9(1): 1–29. Rashid, T.; Zhang, C.; and Ciosek, K. 2021. Estimating α-Rank by Maximizing Information Gain. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) , vol-ume 35, 5673–5681. Rasmussen, R. V.; and Trick, M. A. 2008. Round robin scheduling–a survey. European Journal of Operational Re-search , 188(3): 617–636. Rowland, M.; Omidshafiei, S.; Tuyls, K.; Perolat, J.; Valko, M.; Piliouras, G.; and Munos, R. 2019. Multiagent evalua-tion under incomplete information. In Advances in Neural Information Processing Systems (NeurIPS) , 12291–12303. Saha, A.; and Gopalan, A. 2020. Regret Minimization in Stochastic Contextual Dueling Bandits. arXiv preprint arXiv:2002.08583 .Saha, A.; Koren, T.; and Mansour, Y. 2021. Adversarial dueling bandits. In International Conference on Machine Learning (ICML) , 9235–9244. Silver, D.; Schrittwieser, J.; Simonyan, K.; Antonoglou, I.; Huang, A.; Guez, A.; Hubert, T.; Baker, L.; Lai, M.; and Bolton, A. 2017. Mastering the game of Go without human knowledge. Nature , 550(7676): 354. Sz¨ or´ enyi, B.; Busa-Fekete, R.; Paul, A.; and H¨ ullermeier, E. 2015. Online rank elicitation for plackett-luce: A du-eling bandits approach. In Proceedings of the 19th inter-national Conference on Neural Information Processing sys-tems (NeurIPS) .Thompson, W. R. 1933. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika , 25(3/4): 285–294. Vinyals, O.; Babuschkin, I.; Czarnecki, W. M.; Mathieu, M.; Dudzik, A.; Chung, J.; Choi, D. H.; Powell, R.; Ewalds, T.; Georgiev, P.; et al. 2019. Grandmaster level in Star-Craft II using multi-agent reinforcement learning. Nature ,575(7782): 350–354. Yang, Y.; Tutunov, R.; Sakulwongtana, P.; and Ammar, H. B. 2020. αα-Rank: Practically Scaling α-Rank through Stochastic Optimisation. In Proceedings of the 19th Inter-national Conference on Autonomous Agents and MultiAgent Systems (AAMAS) , 1575–1583. Yue, Y.; Broder, J.; Kleinberg, R.; and Joachims, T. 2012. The k-armed dueling bandits problem. Journal of Computer and System Sciences , 78(5): 1538–1556. Yue, Y.; and Joachims, T. 2009. Interactively optimizing information retrieval systems as a dueling bandits problem. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML) , 1201–1208. Additional Details on Algorithms 

Additional background on multi-dimensional Elo ratings (mElo) 

The winning probability matrix P can be converted into an antisymmetric matrix A = σ−1(P ) with Aij = log Pij  

> 1−Pij

.From the Hodge decomposition theory (Jiang et al. 2011), the antisymmetric matrix A decomposes as A = grad (r)+ rot (A),where rot (A)ij = 1

> n

∑nk=1 Aij + Ajk − Aik , capturing the intransitive relations, grad (r) = r · 1> − 1 · r>, indicating the combination gradient of divergence vector r = 1 

> n

A · 1. According to the Schur decomposition, mElo extends the Elo model by learning an low-rank( 2k) approximation of rotation matrix rot (A), the cyclic component of A. Consider 

A = grad (r) + rot (A) ≈ grad (r) + Cn×2kΩ2k×2kC>

> n×2k

,

where Ω2k×2k = ∑ki=1 (e2i−1e> 

> 2i

− e2ie>

> 2i−1

). By learning a 2k-dimensional vector cx and a rating rx per player, the win-loss prediction for mElo 2k is defined as: 

ˆpxy = σ (rx − ry + c> 

> x

· Ω2k×2k · cy

) . (17) Notice that the Elo model uses k = 0 .Based on the probability estimation function in Eq. (17), the update functions equivalent to Eq. (4) on mElo is: 

rt+1  

> x

= rtx + η · (otxy − ˆptxy 

) , (18) 

ct+1  

> x

(2 i − 1) = ctx(2 i − 1) + η · (otxy − ˆptxy 

) cty (2 i), (19) 

ct+1  

> x

(2 i) = ctx(2 i) − η · (otxy − ˆptxy 

) cty (2 i − 1) . (20) Comparing to the Elo ratings, mElo 2k adopts a different win-loss prediction function by introducing a vector cx, but still follows the gradient descent update. 

Algorithm for MaxIn-mElo 

Algorithm 2 gives the details of MaxIn-mElo. Our MaxIn-mElo algorithm can be obtained by modifying the loss function, gradient and UCB score function in our MaxIn-Elo algorithm. In order to solve the intransitive games, our MaxIn-mElo employs the mElo model, and considers both the transitive and cyclic components to calculate the UCB score. Algorithm 2: MaxIn-mElo: Dueling bandits with online SGD for mElo. 

Input: T, N, τ, α, γ 

Output: output θ

1: Randomly choose a pair to compare and record as xt, y t, o t for t ∈ [τ ]

2: Vτ +1 = ∑τt=1 (ext − eyt )( ext − eyt )T

3: Initialize ˆrτ by MLE as Line 3 in Alg. 1 and c.

4: Maintain convex set C = {r : ‖r − ˆrτ ‖ ≤ 2}

5: for t = τ + 1 , τ + 2 , . . . , T do 

6: if t%τ = 1 then 

7: j ← b (t − 1) /τ c and ηj = 1

> αj

8: Calculate gradient ∇r lj,τ (˜ rj−1) according to Eq. (4) 

9: Update gradient ˜rj ← ∏ 

> C

(˜ rj−1 − ηj ∇r lj,τ (˜ rj−1)) 

10: Compute ¯r = 1

> j

∑jq=1 ˜rq

11: Calculate gradient ∇C lj,τ ( ˜Cj−1)

12: Update gradient ˜Cj ← ˜Cj−1 − ηj ∇C lj,τ ( ˜Cj−1)

13: end if 

14: Define a candidate optimal set S = {x | ¯rx − ¯ry + ¯ cTx Ω¯cy + γ ‖ex − ey ‖V −1

> t

> 0, ∀y ∈ [n]/{x}} 

15: Select a pair (xt, y t) = arg max (x,y )∈S ‖ex − ey ‖V −1

> t

.

16: Pull arms (xt, y t) and observe ot(xt, y t)

17: Compute Vt+1 = Vt + ( ext − eyt )( ext − eyt )T

18: end for 

Additional Experiments 

Table 2 gives the details of the games used in experiments. The sizes of the top SSCC of six transitive games are equal to 1, which indicates that there exists a top player and its winning probabilities against other players are greater than 0.5. For the intransitive games, the top SSCC size of AlphaStar is equal to 1, but there also exist cycles among players outside of the top Table 2: Statistics of twelve real world games. |Top SSCC | indicates the size of the SSCC with best performance. Game # Policies |Top SSCC | Transitivity Transitive game 100 1 Yes Triangular game 100 1 Yes Elo game 100 1 Yes Elo game + noise=0.01 100 1 Yes Elo game + noise=0.05 100 1 Yes Elo game + noise=0.1 100 1 Yes Kuhn-poker 64 64 No AlphaStar 100 1 No tic tac toe 100 2 No hex(board size=3) 100 2 No Blotto 100 99 No 5,3-Blotto 21 18 No =0.4 =0.6 =0.8 =1 =1.2 1 5 10 Top K                                   

> 0.2
> 0.4
> 0.6
> 0.8
> 1.0 Hit Ratio@ K
> Elo game
> 1510 Top K
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0 NDCG@ K1510 Top K
> 0
> 2
> 4
> 6Hit Ratio@ K
> 1e 1Transitive game
> 1510 Top K
> 0
> 2
> 4
> 6NDCG@ K
> 1e 11510 Top K
> 0.0
> 0.5
> 1.0 Hit Ratio@ K
> Triangular game
> 1510 Top K
> 0.0
> 0.5
> 1.0 NDCG@ K1510 Top K
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 Hit Ratio@ K
> Elo game + noise=0.01
> 1510 Top K
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 NDCG@ K1510 Top K
> 0.25
> 0.50
> 0.75
> 1.00 Hit Ratio@ K
> Elo game + noise=0.05
> 1510 Top K
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0 NDCG@ K1510 Top K
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 Hit Ratio@ K
> Elo game + noise=0.1
> 1510 Top K
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 NDCG@ K

Figure 5: Additional results of different γ of our MaxIn-Elo under top-k performance. The best top-1 curves are in dashed lines. The best top-k curves are in bold. C=0 C=2 C=4 C=8 C=16 1 2Rounds t 1e3 

0.00 

0.25 

0.50 

0.75 

1.00 RR 

Kuhn-poker 

0 1 2Rounds t 1e3 

0

2

4

6

8Regret 

1e2 1 2Rounds t 1e3 

0.00 

0.25 

0.50 

0.75 

1.00 RR 

Blotto 

0 1 2Rounds t 1e3 

0.0 

0.5 

1.0 Regret 

> 1e3

Figure 6: Results of different parameters of C of MaxIn-mElo. SSCC. The top SSCC is the smallest set of players such that the winning probabilities of outside players against inside players are less than 0.5. =1               

> =0.1n
> =0.3n
> =0.5n
> =0.7n
> =0.9n
> =1.0n
> =2.0n
> =4.0n
> =8.0n 12Rounds t1e3
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 RR
> Elo game
> 012Rounds t1e3
> 0
> 2
> 4
> 6Regret
> 1e3 12Rounds t1e3
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00 RR
> Kuhn-poker
> 012Rounds t1e3
> 0
> 1
> 2Regret
> 1e3

Figure 7: Results of different batch size τ of MaxIn-Elo on Elo game and MaxIn-mElo on Kuhn-poker. Figure 5 shows the comparison between different γ of MaxIn-Elo under top-k identification. γ trade off exploration and exploitation in selecting arm-pairs. Smaller γ leads to tighter candidate sets then leads to better top-1 performance, but too small γ may result in that the top player is excluded from the candidate set. On Elo game, its three variants and Triangular game, MaxIn-Elo has the best performance when γ = 0 .6. The performance of top-1 gradually drops with γ increasing. But it misidentifies the best player when γ = 0 .4. By contrast, larger γ leads to larger candidate set then lead to better top-k

performance, but too large γ results in much useless exploration on top players identification. In general, larger γ has better top-k performance, but γ = 1 .2 is worse than γ = 1 because of too large candidate set and too much exploration. On Elo game, MaxIn-Elo has best top-1 performance on γ = 0 .6, best top-k performance on γ = 1 , and γ = 0 .8 balances the top-1 and top-k

identification. On Transitive game, MaxIn-Elo has best top-1 and top-k performance when γ = 0 .4.Figure 6 gives results MaxIn-mElo with different dimensions of c. Overall, C = 8 returns best performance. The performance of top-1 identification and cumulative of MaxIn-mElo gradually increases as the dimension C increases, but the performance drops when C = 16 . In general, larger C leads to better approximation of the intransitive skills, but this also requires more samples for training. Figure 7 shows results of MaxIn-Elo on Elo game and of MaxIn-mElo on Kuhn-poker with different batch size τ . When the batch size τ = 1 , both MaxIn-Elo and MaxIn-mElo have bad performance on top-1 player identification and large cumulative reward, which may because of that the objective function is not strongly convex and hard to convergence under this setting. On the other hand, when selecting too large τ , such as 4.0n, 8.0n, the number of updates is less than that of small τ with the same sampling rounds. The MaxIn-Elo on the transitive Elo game gets satisfactory top-1 identification and regret performance at around τ = 0 .5n 1.0n, and the MaxIn-Elo on the intransitive Kuhn-poker obtains best top identification and regret performance at around τ = 0 .7n.Theories and Proofs 

We provide detailed proofs below. 

Proofs of Lemmas 

Proposition 1 ((Saha and Gopalan 2020) ) . We select a batch of feature vectors X1, X 2, . . . , X τ from a distribution v and 

‖X‖ ≤ 2. Let Vτ +1 = ∑τt=1 (XtX> 

> t

), Σ = EXiid   

> ∼v

[XX >]. There exist two positive constant C1, C 2 such that if the batch size 

τ is appropriately selected as τ ≥ 2

( C1

> √n+C2

√log(1 /δ )

> λmin (Σ)

)2

+ 4Cλmin (Σ) , then λmin (Vτ +1 ) ≥ C holds with probability at least 

1 − 1 

> δ

.

The Proposition 1 provides guidance about the selection of batch size τ and is a corollary of Lemma 12 in (Saha and Gopalan 2020). In Algorithm 1, the first batch of pairs are randomly selected for initializing the maximum likelihood estimator, the rest batches of pairs are selected with maximum uncertainty based on UCB estimation. The batch size τ can be suitably to achieve low MLE error and the α-strongly convex condition. 

Lemma 1. Suppose we sample a sequence of arm pairs {(x1, y 1), (x2, y 2), . . . , (xt, y t)} through Algorithm 1 up to round t, and assume the selected batch size τ satisfy that λmin (Vτ +1 ) ≥ 1, where λmin (Vτ +1 ) means the minimum eigenvalue of the matrix (Vτ +1 ), Then ∀t > 0, τ +t∑ 

> i=τ+1

‖(exi − eyi )‖V −1

> i

<

√

2nt log 

( 2τ + tn

)

.

Proof of Lemma 1. Our setting is a special case of generalized linear bandits(GLM)(Li, Lu, and Zhou 2017), and we can directly get our Lemma 1 by substitute ex − ey into Lemma 2 of (Li, Lu, and Zhou 2017). According to (Li, Lu, and Zhou 2017), we have  

> τ+t

∑ 

> i=τ+1

‖exi − eyi ‖2 

> V−1
> i

≤ 2 log det Vt+1 

det Vτ +1 

≤ 2d log 

( tr ( Vτ +1 ) + td

)

− 2 log det Vτ +1 . (21) The trace of Vτ +1 is tr (Vτ +1 ) = ∑τi=1 tr ((exi − eyi )( exi − eyi )>) = ∑τi=1 ‖exi − eyi ‖2 = 2 τ , and the determinant satisfies det (Vτ +1 ) = ∏di=1 λi ≥ λd 

> min

(Vτ +1 ) ≥ 1 for we assume λmin (Vτ +1 ) ≥ 1, where {λi} are the eigenvalues of Vτ +1 .Applying the Cauchy-Schwarz inequality, we have  

> τ+t

∑ 

> i=τ+1

‖(exi − eyi )‖V −1

> i

≤

√√√√t

> t

∑ 

> i=τ+1

‖(exi − eyi )‖2 

> V−1
> i

≤

√

2td log 

( t + 2 τd

)

(22) Thus the proof of Lemma 1 is complete. 

Lemma 2. Assume that there exists a positive constant λf such that λmin 

(E[( ext − eyt )( ext − eyt )T ]) ≥ λf holds at each round t > τ , where (xt, y t) is sampled through Algorithm 1. Let the batch size τ satisfies 

τ1 = 2 

( C1

√n + C2

√2 log Tλmin (B)

)2

+ 16( n + 2 log T )

c21λmin (B) ,τ2 = 2 

( C1

√n + C2

√2 log Tλf

)2

+ 4αc3λf

,τ = dmax {τ1, τ 2}e ,

(14) 

where B = E(x,y )iid  

> ∼[n]×[n]

[(ex − ey )( ex − ey )T ]. Define g1(t) and g2(j),

g1(t) = 12c1

√

n

2 log 

(

1 + 2tn

)

+ 2 log T , (15) 

g2(j) = τα

√1 + log j. (16) 

For a constant α ≥ c3, there exists two positive constants C1, C2 such that if the batch size τ is chosen as Eq. (14) , then we have that at each round t > τ corresponding to batch j = b t−1 

> τ

c, event E1(t) holds with probability at least 1 − 5  

> T2

, where 

E1(t) = {∀ (x, y ) : ∣∣(ex − ey )T (¯ rj − r∗)∣∣ ≤ g1(jτ )‖ex − ey ‖V −1 

> jτ +1

+ g2(j) √2√j }.Proof of Lemma 2 . Relying on the Lemma 1 in (Ding, Hsieh, and Sharpnack 2021) and Proposition 1, we set the batch size 

τ ≥ τ1 in Eq. (14), thus we have ‖ˆrjτ − r∗‖ ≤ 1 with probability at least 1 − 1  

> T2

for any j ≥ 1. So we have ‖ˆrjτ − ˆrτ ‖ ≤ 2 with probability at least 1 − 2  

> T2

. In Algorithm 1, we project the SGD estimation ˜rj into the convex hall C = {r : ‖r − ˆrτ ‖ ≤ 2}, thus we have ˜rj ∈ { r : ‖r − r∗‖ ≤ 3} with probability at least 1 − 1  

> T2

. And from Proposition 1, if the batch size τ ≥ τ2 defined in Eq. (14), then λmin (∑jτ t=( j−1) τ +1 (ext − eyt )( ext − eyt )>) ≥ αc3 holds with probability at least 1 − 1  

> T2

. Thus with probability at least 1 − 1  

> T2

, the objective function lj,τ (r) in Eq. (7) is an α-strongly convex function when r ∈ { r : ‖r − r∗‖ ≤ 3}.Relying on the above analysis and according to Lemma 2 in (Ding, Hsieh, and Sharpnack 2021), we have that if the batch size τ is chosen as Eq. (14), then the convergence rate of the α-strongly convex objective function lj,τ (r) satisfies that 

‖¯rj − ˆrjτ ‖ ≤ τα

√

1 + log jj (23) for all j ≥ 1 with probability at least 1 − 3  

> T2

.From Proposition 1 and τ ≥ τ1 in Eq. (14), we have λmin (Vτ +1 ) ≥ 4( n+2 log T )

> c21

≥ 1 with probability at least 1 − 1  

> T2

because 

c1 is upper bounded by 14 . Then according to the Lemma 3 in (Li, Lu, and Zhou 2017), we have that the event 

E = {‖ ˆrt − r∗‖Vt ≤ 12c1

√

n

2 log 

(

1 + 2tn

)

+ 2 log T , t ≥ τ } (24) holds with probability at least 1 − 2  

> T2

. Combining the Eq. (23) and Eq. (24) and applying the union bound, we can get that for all t ≥ τ the event E1(t) holds with probability at least 1 − 5  

> T2

.

|(ex − ey )T (¯ rj − r∗)| ≤| (ex − ey )T (¯ rj − ˆrjτ )| + |(ex − ey )T (ˆ rjτ − r∗)|

> (1)

≤ √2 τα

√

1 + log jj + |(ex − ey )T (ˆ rjτ − r∗)|

=g2(j)

√2

√j + |(ex − ey )T (ˆ rjτ − r∗)|

> (2)

≤ g2(j)

√2

√j + ‖ˆrjτ − r∗‖Vjτ +1 ‖ex − ey ‖V −1 

> jτ +1
> (3)

≤ g2(j)

√2

√j + g1(jτ )‖ex − ey ‖V −1 

> jτ +1

(25) Inequality (1) holds because the Eq. (23); inequality (2) is due to the Cauchy-Schwartz inequality, where Vjτ +1 is a positive definite matrix with probability at least 1 − 1  

> T2

. (3) holds from the Eq. (24). 

Lemma 3. Define the constant C =

√

2nT log ( T +τn

). At each round t > τ , let UCB balanced parameter γ = 2 g1(t) and assume ∆ > g 1(T )C, if α satisfies that α ≥    

> √2τ√1+log j
> (∆ −g1(T)C)√j

, then we have x∗ ∈ S holds with probability at least 1 − 5  

> T2

, where 

j = b t−1 

> τ

c, ∆ is the difference between ratings of optimal player x∗ and sub-optimal player x′. Recall x∗ = arg max x∈[n] r∗

> x

,and define x′ = arg max x∈[n]/x ∗ r∗

> x

, ∆ = r∗ 

> x∗

− r∗ 

> x′

.Proof of Lemma 3. Following the proof of Lemma 2, for any j ≥ 1 we have that: 

|(ex − ey )T (¯ rj − r∗)| ≤| (ex − ey )T (¯ rj − ˆrjτ )| + |(ex − ey )T (ˆ rjτ − r∗)| + |(ex − ey )T (r∗ − ˆrt−1)| + |(ex − ey )T (ˆ rt−1 − r∗)|≤√2 τα

√

1 + log jj + |(ex − ey )T (ˆ rjτ − r∗)| + 2 |(ex − ey )T (ˆ rt−1 − r∗)|

> (1)

≤ g2(j)

√2

√j + ‖ˆrjτ − r∗‖Vjτ +1 ‖ex − ey ‖V −1 

> jτ +1

+ ‖ˆrt − r∗‖Vt ‖ex − ey ‖V −1

> t
> (2)

≤ g2(j)

√2

√j + g1(jτ )‖ex − ey ‖V −1 

> jτ +1

+ 2 g1(t)‖ex − ey ‖V −1

> t
> (3)

≤ g2(j)

√2

√j + g1(jτ )

√

2njτ log 

( (j + 1) τ + 1 

n

)

+ 2 g1(t)‖ex − ey ‖V −1

> t

≤g2(j)

√2

√j + g1(T )

√

2nT log 

( T + τn

)

+ 2 g1(t)‖ex − ey ‖V −1

> t

(26) Inequality (1) is due to the Cauchy-Schwartz inequality, where Vjτ +1 and Vt is a positive definite matrix with probability at least 1 − 1  

> T2

. (2) holds because the Eq. (24). (3) holds from the Lemma 1. Define the constant 

C =

√

2nT log ( T +τn

), the inequality ∣∣(ex − ey )T (¯ rj − r∗)∣∣ ≤ g2(j) √2√j + g1(T )

√

2nT log ( T +τn

) + 2 g1(t)‖ex − ey ‖V −1

> t

holds with high probability as above derivation. This is equivalent to the following: 

(ey − ex∗ )(¯ rj − r∗) ≤ g2(j)

√2

√j + g1(T )C + 2 g1(t)‖ex − ey ‖V −1

> t

⇒(ex∗ − ey )(¯ rj − r∗) + 2 g1(t)‖ex∗ − ey ‖V −1

> t

+ g2(j)

√ 2

j + g1(T )C ≥ 0

⇒¯rj (x∗) − ¯rj (y) − r∗(x∗) + r∗(y) + 2 g1(t)‖ex∗ − ey ‖V −1

> t

+ g2(j)

√ 2

j + g1(T )C ≥ 0

⇒¯rj (x∗) − ¯rj (y) + g1(t)‖ex∗ − ey ‖V −1

> t

≥ r∗(x∗) − r∗(y) − g2(j)

√ 2

j − g1(T )C

⇒¯rj (x∗) − ¯rj (y) + g1(t)‖ex∗ − ey ‖V −1

> t

≥ r∗(x∗) − r∗(y) − τα

√1 + log j

√ 2

j − g1(T )C

> (1)

⇒h(x∗, y ) ≥ r∗(x∗) − r∗(y) − τα

√1 + log j

√ 2

j − g1(j)C

> (2)

⇒h(x∗, y ) ≥ ∆ − τα

√1 + log j

√ 2

j − g1(j)C (3) 

≥ 0

⇒h(x∗, y ) ≥ 0.

(27) Inequality (1) holds for γ = 2 g1(t) and the definition of h(x, y ) in Eq. (9); (2) follows from ∆ is the difference between optimal and sub-optimal arms, so ∀y ∈ [n]/x ∗, ∆ ≤ r∗(x∗) − r∗(y). (3) holds when α ≥   

> √2τ√1+log j
> (∆ −g1(j)C)√j

and ∆ > g 1(j)C.

Proofs of Theorem 1 

Proof of theorem 1. We use bt to denote the regret at round t and recall that 

bt = r∗ 

> x∗

− 12 (r∗ 

> xt

+ r∗ 

> yt

) = (ex∗ − ext )T r∗ + ( ex∗ − eyt )T r∗

2 ,and the index of batch is j = b(t − 1) /τ c. Then we have that when t > τ :

2bt = ( ex∗ − ext )T r∗ + ( ex∗ − eyt )T r∗

= ( ex∗ − ext )T ¯rj + ( ex∗ − ext )T (r∗ − ¯rj )+ ( ex∗ − eyt )T ¯rj + ( ex∗ − eyt )T (r∗ − ¯rj )

> (1)

≤ γ ‖ex∗ − ext ‖V −1

> t

+ ( ex∗ − ext )T (r∗ − ¯rj )+ γ ‖ex∗ − eyt ‖V −1

> t

+ ( ex∗ − eyt )T (r∗ − ¯rj )

> (2)

≤ γ ‖ex∗ − ext ‖V −1

> t

+ g1(jτ )‖ex∗ − ext ‖V −1 

> jτ +1

+ g2(j) 2

√j

+ γ ‖ex∗ − eyt ‖V −1

> t

+ g1(jτ )‖ex∗ − eyt ‖V −1 

> jτ +1

+ g2(j) 2

√j

> (3)

≤ γ ‖ext − eyt ‖V −1

> t

+ g1(jτ )‖ext − eyt ‖V −1 

> jτ +1

+ g2(j) 2

√j

+ γ ‖ext − eyt ‖V −1

> t

+ g1(jτ )‖ext − eyt ‖V −1 

> jτ +1

+ g2(j) 2

√j

(28) Inequality (1) holds for xt, y t belong to candidate set S, inequality (2) holds with probability at least 1 − 5  

> T2

following from Lemma 2. Inequality (3) holds according to Lemma 3 where we have x∗ ∈ S with probability at least 1 − 5  

> T2

and our arm-pair selection strategy. By above analysis, we have Eq. (28) holds with probability at least 1 − 10   

> T2

at each round t. Next, we give the results on cumulative regret bound: 

R(T ) = 

> τ

∑

> t=1

bt +

> T

∑ 

> t=τ+1

bt

> (1)

≤ τ ∗ ∆max +

> T

∑ 

> t=τ+1

bt

> (2)

≤ τ ∗ ∆max +

> T

∑ 

> t=τ+1

12

[γ ‖ext − eyt ‖V −1

> t

+ g1(jτ )‖ext − eyt ‖V −1 

> jτ +1

+ g2(j) 2

√j

+ γ ‖ext − eyt ‖V −1

> t

+ g1(jτ )‖ext − eyt ‖V −1 

> jτ +1

+ g2(j) 2

√j

]

> (3)

≤ τ ∗ ∆max +

> T

∑ 

> t=τ+1

γ ‖ext − eyt ‖V −1

> t

+ τ

> T

∑ 

> t=τ+1

g1(t − 1) ‖ext − eyt ‖V −1

> t

+ 2 g2(j)

> T

∑ 

> t=τ+1

1

√j

> (4)

≤ τ ∗ ∆max + 2 g1(T )

√

2nT log( τ + Tn ) + τ g 1(T )

√

2nT log 

( τ + Tn

)

+ 2 g2(J)

> T

∑ 

> t=τ+1

1

√j

> (5)

≤ τ ∗ ∆max + (2 + τ )g1(T )

√

2nT log( τ + Tn ) + 4 g2(J)√τ T 

(29) Inequality (1) holds because ∆max = max i r∗ 

> i

− min i r∗ 

> i

. As the Eq. (29), (2) holds with probability at least 1 − 10  

> T

by the union bound. Consider that there are τ exploration tracks with length T and we only use the nodes at jτ , thus (3) holds due to 

∑Tt=τ +1 g1(jτ )‖ext −eyt ‖V −1 

> jτ +1

≤ ∑Tt=τ +1 g1(t−1) ‖ext −eyt ‖V −1

> t

, where j = b t−1 

> τ

c. As the analysis in the proof of Lemma 2, λmin (Vτ +1 ) ≥ 1 holds with probability at least 1 − 1  

> T2

, then we have ∑ti=τ +1 ‖(exi − eyi )‖V −1

> i

≤

√

2nt log ( 2τ +tn

) by the Lemma 1. Also because the MaxIn-Elo sets γ = 2 g1(t), J = b Tτ c, thus (4) holds. (5) holds because ∑Tt=τ +1 1√j ≤ 2√τ T see Eq. (21) in the Appendix of (Ding, Hsieh, and Sharpnack 2021) . Thus the proof of Theorem 1 is completed.
