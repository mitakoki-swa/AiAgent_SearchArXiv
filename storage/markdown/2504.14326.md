Title: 

URL Source: http://arxiv.org/pdf/2504.14326v1

Published Time: Tue, 22 Apr 2025 00:37:02 GMT

Markdown Content:
> 1

# Diffusion-based Dynamic Contract for Federated AI Agent Construction in Mobile Metaverses 

Jinbo Wen, Jiawen Kang, Yang Zhang, Yue Zhong, Dusit Niyato, Fellow, IEEE , Jie Xu, Fellow, IEEE ,Jianhang Tang, Member, IEEE , and Chau Yuen, Fellow, IEEE 

Abstract —Mobile metaverses have attracted significant atten-tion from both academia and industry, which are envisioned as the next-generation Internet, providing users with immersive and ubiquitous metaverse services through mobile devices. Driven by Large Language Models (LLMs) and Vision-Language Models (VLMs), Artificial Intelligence (AI) agents hold the potential to empower the creation, maintenance, and evolution of mobile metaverses. Currently, AI agents are primarily constructed using cloud-based LLMs and VLMs. However, several challenges hin-der their effective implementation, including high service latency and potential sensitive data leakage during perception and processing. In this paper, we develop an edge-cloud collaboration-based federated AI agent construction framework in mobile metaverses. Specifically, Edge Servers (ESs), acting as agent infrastructures, collaboratively create agent modules in a dis-tributed manner. The cloud server then integrates these modules into AI agents and deploys them at the edge, thereby enabling low-latency AI agent services for users. Considering that ESs may exhibit dynamic levels of willingness to participate in federated AI agent construction, we design a two-period dynamic contract model to continuously motivate ESs to participate in agent module creation, effectively addressing the dynamic information asymmetry between the cloud server and the ESs. Furthermore, we propose an Enhanced Diffusion Model-based Soft Actor-Critic (EDMSAC) algorithm to efficiently generate optimal dynamic contracts, in which dynamic structured pruning is applied to DM-based actor networks to enhance denoising efficiency and policy learning performance. Extensive simulations demonstrate the effectiveness and superiority of the EDMSAC algorithm and the proposed contract model. 

Index Terms —Mobile metaverses, AI agent construction, dy-namic contract theory, enhanced diffusion models, deep rein-forcement learning. 

I. I NTRODUCTION 

With the rapid evolution of edge computing, metaverse, and Generative Artificial Intelligence (GenAI) technologies, mobile metaverses are emerging as a transformative digital 

J. Wen and Y. Zhang are with the College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, China (emails: jinbo1608@nuaa.edu.cn; yangzhang@nuaa.edu.cn). J. Kang and Y. Zhong are with the School of Automation, Guang-dong University of Technology, China (emails: kavinkang@gdut.edu.cn; 2112404106@mail2.gdut.edu.cn). D. Niyato is with the College of Computing and Data Science, Nanyang Technological University, Singapore (email: dniyato@ntu.edu.sg). J. Xu is with the School of Science and Engineering (SSE), the Shenzhen Future Network of Intelligence Institute (FNii-Shenzhen), and the Guangdong Provincial Key Laboratory of Future Networks of Intelligence, The Chinese University of Hong Kong (Shenzhen), China (email: xujie@cuhk.edu.cn). J. Tang is with the State Key Laboratory of Public Big Data, Guizhou University, China (email: jhtang@gzu.edu.cn). C. Yuen is with the School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore (email: chau.yuen@ntu.edu.sg). 

ecosystem [1]. By integrating extended reality and advanced communication technologies, they establish interconnected vir-tual spaces accessible through mobile devices [2]. Positioned as a cornerstone of the next-generation Internet, mobile meta-verses have attracted significant interest due to their robust levels of connectivity, enabling users to interact in real time with immersive virtual spaces. Furthermore, mobile meta-verses hold the potential to revolutionize various fields, such as healthcare, education, and entertainment [2]. However, the development of mobile metaverses confronts several technical challenges. Firstly, conventional content creation methods of-ten involve time-consuming and resource-intensive processes, leading to static virtual content that fails to dynamically adapt to real-time user interactions or environmental variations [3]. Secondly, the inherent limitations of mobile input modalities, such as touchscreen gestures and voice commands, would inevitably degrade the fluidity of immersive interactions. GenAI is driving the evolution of the metaverse due to its incredible ability to generate novel content [4]. As an advanced paradigm of GenAI, AI agents are expected to play a pivotal role in the creation, maintenance, and evolution of mobile metaverses. Empowered by Large Language Models (LLMs) and Vision-Language Models (VLMs) [5], AI agents are artificial entities comprising four core modules: perception, memory, decision-making, and action. With these modules, AI agents are capable of perceiving surrounding environments, processing and storing information, formulating strategies, and executing tasks to achieve specific objectives [6]. Within mobile metaverses, AI agents can improve immersion by dynamically generating and adapting virtual content, as well as creating realistic avatars and environments using Digital Twin (DT) and three-dimensional rendering technologies [5]. Moreover, they can deliver personalized and interactive user experiences by acting as intelligent and personalized assistants, guiding users through virtual environments, and tailoring in-teractions based on individual preferences. Currently, AI agent construction often relies on cloud-based LLMs and VLMs, which require the transmission of perceived data to remote servers for processing before returning the results [5]. Although AI agents hold significant potential in mobile metaverses, several critical challenges penalize their effective implementation: 1) The cloud-based AI agent archi-tecture introduces inherent service latency, negatively impact-ing the user experience. For example, delays in processing and rendering real-time location data within augmented reality navigation systems may lead to user disorientation, operational inefficiencies, or even safety hazards in scenarios requiring 

> arXiv:2504.14326v1 [cs.NI] 19 Apr 2025 2

precise spatial awareness [7]. 2) During peak usage periods, cloud servers may experience performance bottlenecks due to simultaneous service requests from large user bases [8]. Such bottlenecks result in response delays and potential service unavailability under high-load conditions, undermining the seamless interaction demanded by immersive metaverse envi-ronments. 3) The transmission of perceived data, which often contains sensitive personal information, remains vulnerable to interception during transmission and susceptible to breaches if stored centrally in cloud repositories [9]. This exposes users to risks such as unauthorized tracking, data misuse, and identity theft, particularly in mobile metaverses where data flows across heterogeneous networks. Motivated by the feasibility of federated model construc-tion [10]–[14], we propose a novel edge-cloud collaboration-based federated AI agent construction framework, where Edge Servers (ESs) serve as agent infrastructures responsible for constructing agent modules, while the cloud server integrates these constructed modules into AI agents and eventually deploys them at the edge. Unlike traditional federated learning, which deals with the same task across participating nodes [15], the agent modules in the federated AI agent construction framework can be heterogeneous, with each module perform-ing distinct functions. Nonetheless, in the federated AI agent construction framework, ESs exhibit different dynamic levels of Willingness to Participate (WTP) over multiple periods due to various factors, such as the high energy consumption of agent module construction and the prioritization of other com-puting tasks. Moreover, the WTP levels of ESs are typically unknown to the cloud server, leading to the intention of some ESs to free-ride on the efforts of others [16]. To this end, we design a dynamic contract-based incentive mechanism to incentivize ESs to participate sustainably in federated AI agent construction, addressing the challenge of dynamic information asymmetry . Particularly, we propose an enhanced diffusion-based Deep Reinforcement Learning (DRL) algorithm to iden-tify optimal dynamic contracts. Our main contributions are summarized as follows:  

> •

Federated AI Agent Construction Framework: We propose an edge-cloud collaboration-based federated AI agent construction framework in mobile metaverses. In the proposed framework, ESs serve as agent infrastruc-tures, leveraging their computing, storage, and communi-cation resources to create agent modules in a distributed manner. The cloud server then integrates these created agent modules into complete AI agents and deploys them at the edge, thereby providing low-latency AI agent services for users in mobile metaverses.  

> •

Dynamic Contract-based Incentive Mechanism: Con-sidering the dynamic WTP levels of ESs in federated AI agent construction, we design a two-period dynamic con-tract model to motivate ESs to continuously participate in agent module creation. The self-disclosure property of the proposed model ensures that each ES selects the contract best suited to its WTP level, thereby addressing the problem of dynamic information asymmetry. Notably, the two-period dynamic contract design maintains relatively low computational complexity and thus can be recursively extended to multiple periods.  

> •

Diffusion Model for Dynamic Contract Design: We adopt an Enhanced Diffusion Model-based Soft Actor-Critic (EDMSAC) algorithm to identify optimal dynamic contracts. Specifically, the DM-based actor networks pos-sess both analytical and generative capabilities, which can generate contract samples through an iterative de-noising process. To enhance its denoising efficiency, we incorporate dynamic structured pruning techniques into the DM-based actor networks, enabling a more efficient and scalable exploration of the optimal dynamic contract policy. To the best of our knowledge, this is the first work to leverage DRL algorithms, especially diffusion-based DRL, for optimal dynamic contract design . 

> •

Extensive Simulations for Performance Evaluation: 

We utilize LLMs called Qwen-VL 1 to construct an AI agent capable of operating mobile phones to perform simple tasks. Specifically, we leverage Qwen-VL-Plus to create a perception module and Qwen-VL-Max to create decision-making, reflection, and planning modules. This construction process can be extended to the proposed framework. We then compare the EDMSAC algorithm with two DM-based DRL algorithms and three traditional DRL benchmark algorithms. Simulation results demon-strate the effectiveness of the proposed algorithm. The remainder of the paper is organized as follows: Section II reviews the related work. Section III introduces the edge-cloud collaboration-based federated AI agent construction framework. In Section IV, we propose the two-period dynamic contract model to motivate ESs to participate continuously in federated AI agent construction. Section V presents the EDMSAC algorithm for generating optimal dynamic contracts. In Section VI, we provide extensive simulation results to demonstrate the feasibility and effectiveness of the proposed framework and algorithm. Section VII concludes the paper. II. R ELATED WORK 

A. Federated Model Construction 

Federated Learning (FL) is a distributed Machine Learning (ML) framework that enables collaborative model training while preserving data privacy through secure aggregation, making it particularly suitable for applications with stringent privacy requirements [17]. Recent research has highlighted the effectiveness and potential of FL in constructing DT and ML models, particularly in enhancing privacy protection and optimizing resource utilization [10]–[14]. For instance, the authors in [11] combined FL with DT technology and introduced a trust mechanism to optimize the learning process. Similarly, the authors in [12] leveraged FL to optimize DT-enabled industrial Internet of Things systems. In [13], the authors proposed an FL architecture based on over-the-air computing [18], enabling the efficient and secure development of DT models. The authors in [14] proposed a generalized federated Reinforcement Learning (RL) framework that incor-porates meta-learning techniques, which enables the fusion of 

> 1https://github.com/QwenLM/Qwen-VL?tab=readme-ov-file 3

RL models trained across multiple intelligent devices into a unified and generalizable model. Combined with a device-edge-cloud collaborative comput-ing architecture, FL can facilitate efficient model training by enabling small-scale learning on devices while leveraging the cloud for large-scale model updates. This decentralized approach minimizes network dependency, significantly en-hancing the responsiveness and operational stability of AI agents and making it a promising solution for AI agent model construction. Inspired by the above works, we aim to develop a framework for federated AI agent construction. 

B. Contract Theory-based Applications 

Contract theory is an economic tool that examines coop-eration in settings with incomplete information and designs incentives for agents to participate in tasks involving asym-metric information [19]. In the context of FL, the authors in [20] proposed an incentive contract tailored for scenarios with strong information asymmetry, leveraging an iteration algorithm to determine optimal contract elements. In [21], the authors utilized contract theory to motivate privately owned vehicles to contribute their onboard computing resources. Tra-ditional contracts are generally one-dimensional, accounting for a static type of information asymmetry between the two parties involved [22]. Therefore, there is a growing interest in multi-dimensional and dynamic contracts that account for multiple asymmetries and evolving conditions. As for the applications of multi-dimensional contracts [23], [24], the authors in [23] developed a multi-dimensional contract-theoretic framework to optimize the design of rewards of data in mobile networks under asymmetric information. In [24], the authors constructed a multi-dimensional con-tract theory-based framework, incorporating Prospect Theory (PT) and Generative DM (GDM)-based DRL algorithms to optimize the dynamic migration of embodied AI twins in vehicular embodied AI networks. As for the applications of dynamic contracts [16], [22], the authors in [16] proposed a two-period contract-based incentive mechanism to motivate users to participate in FL training under dynamic information asymmetry. In [22], the authors proposed a dynamic contract-based incentive mechanism to motivate vehicles to share their threat data for LLM fine-tuning. Inspired by the above works, we propose a dynamic contract-based incentive mechanism to incentivize ESs to participate in AI agent construction. 

C. Deep Reinforcement Learning for Contract Design 

DRL algorithms have been extensively used to identify optimization strategies, including optimal static contract de-sign [25]–[27]. For instance, the authors in [25] employed Proximal Policy Optimization (PPO) algorithms to identify optimal contracts, incentivizing mobile devices to share data in mobile AI-Generated Content (AIGC) networks. In [26], the authors utilized the twin-delayed deep deterministic policy gradient algorithm to determine the incentive reward policy. Nevertheless, traditional DRL algorithms often struggle to di-rectly identify optimal contracts due to the complex Individual Rationality (IR) and Incentive Compatibility (IC) constraints. Fortunately, GDMs excel at capturing high-dimensional in-formation in dynamic and complicated network optimization scenarios [28]. Several studies have explored the use of GDMs for optimal static contract design [19], [24], [29]. For example, the authors in [19] proposed a DM-based DRL algorithm to generate optimal contracts under PT, addressing the problem of information asymmetry in edge AIGC service provision. However, existing works still rely on heuristic algorithms for optimal dynamic contract design [16], [22], which may not be practical in real scenarios. To fill this gap, we propose the EDMSAC algorithm to identify optimal dynamic contracts. III. E DGE -C LOUD COLLABORATION -BASED FEDERATED 

AI A GENT CONSTRUCTION FRAMEWORK 

In this section, we first introduce the proposed framework for constructing AI agents through edge-cloud collaboration. Then, we present the model quality and energy consumption for federated AI agent construction. Finally, we formulate the utilities of ESs and the cloud server, respectively. 

A. Framework Design 

For federated AI agent construction, we consider a central cloud server and a set of geographically distributed ESs, denoted by N = {1, . . . , n, . . . , N }, as shown in Fig. 1. During the whole AI agent construction period, AI agents can be split into multiple agent modules, such as perception, memory, and action, each responsible for specific tasks by processing the featured data perceived by ESs [5]. Before the beginning of agent module construction, the cloud server transmits the initial configuration parameters for the agent modules to the ESs (Step 1⃝). The ESs, serving as agent infrastructures with sufficient resources (e.g., computing, storage, and bandwidth), leverage local LLMs and AI models (e.g., icon detection models) to construct the agent modules in a distributed manner (Step 2⃝) [30]. Upon completing the construction, the ESs upload the constructed agent modules to the cloud server (Step 3⃝). Finally, the cloud server creates the complete AI agents by integrating the uploaded agent modules (Step 4⃝). Once constructed, the created AI agents can be deployed at the edge, allowing users to connect to them through the IP address of the current local area network with the ES [31]. 

B. Model Quality of Agent Modules 

Without loss of generality, we employ an L-Lipschitz con-tinuous and ε-strongly convex loss function for the model training performed by ESs [32]. We denote Qn as the model quality of agent modules created by ES n. Qn is calculated from the number of training rounds Tn that ES n determines to take for creating agent modules, which is given by [32] 

Qn = 1 − 2 −Tn (2 −Lδ )δε  

> 2

, (1) where δ ∈ (0 , 2 

> L

), L, and ε are pre-defined hyperparameters. Given that the second derivative of Qn in (1) is calculated as  

> ∂2Qn
> ∂(Tn)2

= − (2 −Lδ )δε  

> 2

ln 2 22 −Tn (2 −Lδ )δε  

> 2

< 0, Qn is a convex function with respect to Tn, indicating that there exists the optimal number of training rounds for ESs to obtain the highest model quality of agent modules. 4User Layer 

> Edge Server
> Agent module
> Cloud Server

Edge Layer     

> ②Construct agent modules
> ④Integrate created agent modules
> Provide AI agent services (e.g., personal assistants)
> Perception
> Embedding Models
> Modality encoder
> Word embedding layer
> …
> Inputs …
> Action
> Text Tools
> Call API
> Embodiment
> LLMs
> VLMs
> …

Fig. 1: The proposed edge-cloud collaboration-based federated AI agent construction framework. Note that the interface of personal assistants is from the OmAgent app [31] downloaded by our Android phone. 

C. Energy Consumption of Edge Servers 

We represent the total energy consumption of ES n for federated AI agent construction as ET ot n , which consists of the energy consumption of perceiving feature data EP er n , creating agent modules ECre n , and uploading created agent modules to the cloud server EU pl n . Specifically, the energy consumption of perceiving feature data EP er n is given by [33] 

EP er n = ςnΥP er n DF ea n f 2

> n

, (2) where ςn is a constant related to the hardware architecture of ES n, ΥP er n (cycles /bit ) is the number of Central Processing Unit (CPU) cycles required for perceiving feature data, DF ea n

(bits ) is the size of feature data perceived by ES n, and fn

(Hz ) is the CPU speed of ES n.After perceiving feature data, ES n constructs agent mod-ules under iterative model training to improve the model quality of agent modules, and its energy consumption of creating agent modules ECre n is given by [15] 

ECre n = ρnTnΥCre n f 2

> n

, (3) where ρn represents the effective switched capacitance of ES 

n and ΥCre n (cycles /bit ) is the number of CPU cycles required for creating one bit of agent modules on ES n.Then, each ES n uploads its constructed agent modules to the cloud server for integration, and the corresponding energy consumption EU pl n is given by [15] 

EU pl n = PnDAgent n

rn,c 

, (4) where Pn (dBm ) is the transmit power of ES n, DAgent n (bits )is the model size of agent modules constructed by ES n, and 

rn,c (Mbps ) represents the transmission rate from ES n to the cloud server through the fiber link. Therefore, the total energy consumption of ES n for feder-ated AI agent construction ET ot n is expressed as 

ET ot n = ( ςnΥP er n DF ea n + ρnTnΥCre n )f 2 

> n

+ PnDAgent n

rn,c 

. (5) 

D. Utilities of Edge Servers and Cloud Server 

For ES n, we define the level of WTP in federated AI agent construction as θn [16], which reflects the perception, communication, and computing capabilities of ES n [27], [29]. The utility of ES n is formulated based on the reward 

Rn received from the cloud server and the total energy consumption ET ot n for federated AI agent construction [16], [27], which is given by 

un = θnRn − cT n − E, (6) where c = σρ nΥCre n f 2 

> n

and E = σ(ςnΥP er n DF ea n f 2 

> n

+ 

> PnDAgent n
> rn,c

). Here, σ is the unit cost of energy consumption. Similarly, the utility of the cloud server is formulated based on the total profit obtained from all ESs and the cost of energy consumption of global AI agent integration EC , as given by 

UCS =

> N

X

> n=1

(αQ n − Rn) − σE C , (7) where α is a scaling factor that affects the overall magnitude of economic benefits [22], and EC is influenced by several factors [32], including global AI agent integration latency, both CPU and graphics processing unit resources on the cloud, and the average number of instructions in integrating agent modules. Due to high energy consumption, certain ESs may not always be willing to participate in federated AI agent construc-tion. Thus, ESs have different levels of WTP for federated AI agent construction [16]. The WTP levels of ESs are generally unknown to the cloud server and may change over multiple training rounds [16], which results in dynamic information asymmetry between the cloud server and ESs. Hence, it is necessary to develop a dynamic incentive scheme for ESs to participate in federated AI agent construction sustainably. IV. D YNAMIC CONTRACT DESIGN 

In this section, we propose a two-period dynamic contract model to motivate ESs to continuously participate in federated AI agent construction under dynamic information asymmetry. Note that the proposed dynamic contract model possesses the 5

inherent correlation between two periods within the whole agent construction time and can be easily extended to accom-modate multiple periods recursively [16], [22]. 

A. Two-Period Dynamic Contract Formulation 

Due to information asymmetry, the cloud server only knows the historical distribution of WTP levels of ESs through previous participation records rather than the precise WTP level of each ES [16], [19]. Thus, the cloud server can use data mining tools to divide ESs into a set Θ = {θk|θk ∈

N+, k ∈ K = {1, . . . , K, K ≤ N }} of K types based on the historical distribution of WTP levels [16], [19]. Considering the certain correlations in WTP across periods [16], [22], in the first period, the k-th type of ESs can be denoted as θ1

> k

, k ∈ K ,and in the second period, the k-th type of ESs is denoted as 

θ2 

> j

(θ1

> k

), j ∈ K , which is an expectation correlated to θ1

> k

. Note that these two types are drawn independently from Θ. Without loss of generality, the types in the first and second periods are sorted in non-decreasing order as θ11 ≤ · · · ≤ θ1 

> k

≤ · · · ≤ θ1

> K

and θ21 (θ1

> k

) ≤ · · · ≤ θ2 

> j

(θ1

> k

) ≤ · · · ≤ θ2 

> K

(θ1

> k

), respectively. We define ptk as the probability that ESs belong to type θtk during period t, where PKk=1 ptk = 1 , t ∈ { 1, 2}.To incentivize ESs to participate in federated AI agent construction, the cloud server issues a two-period dynamic contract Ω( θ1

> k

, θ 2 

> j

(θ1

> k

)) at the beginning of the first period before the agent module creation begins. The contract item consists of WTP along with the corresponding reward. In the first period, the contract item designed for type-k ESs is represented as {T 1 

> k

(θ1

> k

), R 1

> k

(θ1

> k

)}, and in the second period, the contract item designed for the same type-k ESs is represented as {T 2 

> j

(θ2 

> j

(θ1

> k

)) , R 2 

> j

(θ2 

> j

(θ1

> k

)) }, k, j ∈ K . Thus, the formulation of the two-period dynamic contract Ω is expressed as 

Ω = {(T 1 

> k

(θ1

> k

), R 1

> k

(θ1

> k

)) , (T 2 

> j

(θ2 

> j

(θ1

> k

)) , R 2 

> j

(θ2 

> j

(θ1

> k

))) }. (8) For conciseness, we denote T 2 

> j

(θ1

> k

) and R2 

> j

(θ1

> k

) as T 2 

> j

(θ2 

> j

(θ1

> k

)) 

and R2 

> j

(θ2 

> j

(θ1

> k

)) , respectively. Based on (6), the utility of ESs with type θ1 

> k

in period 1 is given by 

u1

> k

(T 1 

> k

) = θ1

> k

R1

> k

(θ1

> k

) − cT 1 

> k

(θ1

> k

) − E. (9) Considering that the WTP level θ1 

> k

of type-k ESs in period 1 would affect the utility in period 2 [22], the expected utility of type-k ESs in period 2 is given by 

Uk = u1

> k

(T 1 

> k

) + β

> K

X

> j=1

p2 

> j

(T 1 

> k

)u2 

> j

(T 2 

> j

(T 1 

> k

)) 

| {z }

> Expected discounted utility

, (10) where β ∈ (0 , 1] represents a discount factor for the utility in period 2. The second term in (10) represents the expected discounted utility across all K types that users are charac-terized with certain probabilities in period 2, and a higher β

indicates a greater valuation of the future utility relative to the current utility [16], [22]. Since WTP levels across two periods are positively correlated [16], we have p2 

> j

(T 1 

> k

)̸ = p2 

> j

(T 1 

> k′

) and 

PKj=1 p2 

> j

(T 1 

> k

)u2 

> j

(T 2 

> j

(T 1 

> k

)) ̸ = PKj=1 p2 

> j

(T 1 

> k′

)u2 

> j

(T 2 

> j

(T 1 

> k′

)) , k ̸ =

k′, with proofs presented in Lemma 7. …

> Edge Servers
> (Contract confirmation)
> Stage I:
> Stage II:
> Stage III:
> Stage I:
> Stage IV:
> Stage V:
> Release a dynamic contract with multiple contract items
> Sign suitable contract items
> Upload the constructed agent modules in period 1
> Offer rewards in period 1
> Upload the constructed agent modules in period 2
> Offer rewards in period 2
> Period 1  Period 2
> Cloud Server
> (Contract design)

Fig. 2: The workflow of the proposed two-period dynamic con-tract for agent module construction. To minimize disruptions to the construction progress, the proposed contract remains valid across both periods without requiring redefinition and retransmission to the ESs in period 2. Based on (7), the expected utility of the cloud server in period t ∈ { 1, 2} is derived as [16], [22] 

U tCS =

" KX

> k=1

ptkN



α



1 − 2 −T tk (2 −Lδ )δε 

> 2



− Rtk

#

− σE C .

(11) Therefore, the total profit of the cloud server over two periods is expressed as 

U (Ω) = U 1 

> CS

+ βU 2 

> CS

. (12) The dynamic contract Ω is directly applied across two periods. According to [22], the long-term contract cycle is divided into five stages, as shown in Fig. 2.  

> •

Stage I. Contract confirmation: At the beginning of federated AI agent construction, the cloud server releases the two-period dynamic contract Ω with a set of contract items to ESs. Upon receiving the contract, each ES evaluates its WTP and selects a suitable contract item.  

> •

Stage II. Agent module creation of period 1: After ESs accept selected contract items, the first period of agent module creation begins. ESs first collect feature data from the surrounding environment through inherent wireless sensors. Based on the collected feature data and the initial values informed by the cloud, ESs create agent modules and upload them to the cloud server for integration. 6 

> •

Stage III. Contract realization of period 1: At the end of period 1, the cloud server integrates the created agent modules. If the integration is successful, the cloud server will offer the rewards R1

> k

(θ1

> k

) specified in the contract to ESs. Otherwise, the ESs will not obtain the rewards.  

> •

Stage IV. Agent module creation of period 2: After completing the agent module creation of period 1, ESs can learn their second-period types, and the subsequent process of agent module creation is similar to Stage II.  

> •

Stage V. Contract realization of period 2: The contract realization of period 2 is similar to Stage III. 

B. Second-Period Contract Design 

Since the second-period contract is independent [16], we first use the standard static contract approach [19] to solve the second-period contract. For a feasible contract, the second-period contract should simultaneously satisfy both the IR and IC constraints [16], [22]. 

Definition 1. (IR Constraints in Period 2): Each type-k ES in period 2 can obtain a non-negative utility by selecting the contract item suitable for its second-period type i, i.e., 

u2 

> i

(T 2 

> i

(T 1 

> k

)) = θ2 

> i

R2 

> i

(θ1

> k

) − cT 2 

> i

(θ1

> k

) − E ≥ 0, ∀i ∈ K . (13) 

Definition 2. (IC Constraints in Period 2): Each type-k ES in period 2 can achieve the maximum utility by selecting the contract item designed for its second-period type i rather than any other contract item, i.e., 

θ2 

> i

R2 

> i

− cT 2 

> i

− E ≥ θ2 

> i

R2 

> j

− cT 2 

> j

− E, ∀i̸ = j ∈ K . (14) IR constraints in (13) motivate ESs to participate in feder-ated AI agent construction by guaranteeing that their utilities are non-negative if they select suitable contract items. IC constraints in (14) encourage ESs to reveal their actual type, avoiding the risk that ESs misrepresent their actual types to cheat higher rewards. In the following, we derive the necessary conditions of the second-period contract. 

Lemma 1. For any feasible contract, R2 

> i

(θ1

> k

) ≥ R2 

> j

(θ1

> k

) if and only if T 2 

> i

(θ1

> k

) ≥ T 2 

> j

(θ1

> k

), where i̸ = j.Proof. We first prove that if T 2 

> i

(θ1

> k

) ≥ T 2 

> j

(θ1

> k

), then R2 

> i

(θ1

> k

) ≥

R2 

> j

(θ1

> k

). Based on the IC constraints in (14), we have 

θ2 

> i

R2 

> i

(θ1

> k

) − cT 2 

> i

(θ1

> k

) ≥ θ2 

> i

R2 

> j

(θ1

> k

) − cT 2 

> j

(θ1

> k

). (15) We transform the above inequality into 

θ2 

> i

[R2 

> i

(θ1

> k

) − R2 

> j

(θ1

> k

)] ≥ c[T 2 

> i

(θ1

> k

) − T 2 

> j

(θ1

> k

)] . (16) Since T 2 

> i

(θ1

> k

) ≥ T 2 

> j

(θ1

> k

), we have θ2 

> i

[R2 

> i

(θ1

> k

) − R2 

> j

(θ1

> k

)] ≥

0, then R2 

> i

(θ1

> k

) ≥ R2 

> j

(θ1

> k

). Similarly, given that R2 

> i

(θ1

> k

) ≥

R2 

> j

(θ1

> k

), we have T 2 

> i

(θ1

> k

) ≥ T 2 

> j

(θ1

> k

). Thus, the proof of Lemma 1 is completed. 

Lemma 2. For any feasible contract, if θ2 

> i

(θ1

> k

) ≥ θ2 

> j

(θ1

> k

), then 

R2 

> i

(θ1

> k

) ≥ R2 

> j

(θ1

> k

).Proof. Based on the IC constraints in (14), we have 

θ2 

> i

R2 

> i

(θ1

> k

) − cT 2 

> i

(θ1

> k

) ≥ θ2 

> i

R2 

> j

(θ1

> k

) − cT 2 

> j

(θ1

> k

), (17) 

θ2 

> j

R2 

> j

(θ1

> k

) − cT 2 

> j

(θ1

> k

) ≥ θ2 

> j

R2 

> i

(θ1

> k

) − cT 2 

> i

(θ1

> k

). (18) By combining (17) and (18), we have 

(θ2 

> i

− θ2 

> j

)( R2 

> i

(θ1

> k

) − R2 

> j

(θ1

> k

)) ≥ 0. (19) Since θ2 

> i

(θ1

> k

) ≥ θ2 

> j

(θ1

> k

), we have R2 

> i

(θ1

> k

) ≥ R2 

> j

(θ1

> k

). Thus, the proof of Lemma 2 is completed. Lemma 1 ensures that ESs can receive higher rewards with more training rounds for improving the model quality of agent modules. Lemma 2 indicates that ESs with higher WTP can obtain higher rewards. Based on Lemmas 1 and 2, we can obtain the monotonicity property of the second-period contract as follows: 

Lemma 3. (Monotonicity Property): For any feasible contract, if θ2 

> i

(θ1

> k

) ≥ θ2 

> j

(θ1

> k

), then T 2 

> i

(θ1

> k

) ≥ T 2 

> j

(θ1

> k

).

Since it is exceedingly challenging to make the contract solution under K number of IR constraints and K(K − 1) 

number of IC constraints, we reduce the numbers of IR and IC constraints to derive a tractable set of constraints to solve the second-period contract. 

Lemma 4. (Reduce IR Constraints in Period 2): The K

number of IR constraints in period 2 can be reduced to a single constraint, i.e., 

θ21 R21(θ1

> k

) − cT 21 (θ1

> k

) − E = 0 . (20) 

Proof. Given the IC constraints in (14) and the ascending order of types of ESs, we have 

θ2 

> i

R2 

> i

(θ1

> k

) − cT 2 

> i

(θ1

> k

) − E ≥ θ2 

> i

R21(θ1

> k

) − cT 21 (θ1

> k

) − E

≥ θ21 R21(θ1

> k

) − cT 21 (θ1

> k

) − E ≥ 0. (21) From (21), we can observe that if the type-1 user with the lowest WTP has a non-negative utility, it implies that the other IR constraints automatically hold. Additionally, for 

θ21 R21(θ1

> k

) − cT 21 (θ1

> k

) − E ≥ 0, the cloud server would reduce the reward R21(θ1

> k

) as much as possible until θ21 R21(θ1

> k

) −

cT 21 (θ1

> k

) − E = 0 [19], thus maximizing its utility. Thus, the proof of Lemma 4 is completed. 

Lemma 5. The IC constraints in period 2 can be reduced into Local Downward Incentive Compatibility (LDIC) (22) and Local Upward Incentive Compatibility (LUIC) (23), i.e., 

θ2 

> i

R2 

> i

(θ1

> k

)−cT 2 

> i

(θ1

> k

)−E ≥ θ2 

> i

R2

> i−1

(θ1

> k

)−cT 2

> i−1

(θ1

> k

)−E. (22) 

θ2 

> i

R2 

> i

(θ1

> k

)−cT 2 

> i

(θ1

> k

)−E ≥ θ2 

> i

R2

> i+1

(θ1

> k

)−cT 2

> i+1

(θ1

> k

)−E. (23) 

Proof. For the proof of LDIC, we can first obtain the following equations based on the IC constraints in (14): 

θ2 

> i

R2 

> i

(θ1

> k

) − cT 2 

> i

(θ1

> k

) ≥ θ2 

> i

R2

> i−1

(θ1

> k

) − cT 2

> i−1

(θ1

> k

). (24) 

θ2

> i−1

R2

> i−1

(θ1

> k

) − cT 2

> i−1

(θ1

> k

) ≥ θ2

> i−1

R2

> i−2

(θ1

> k

) − cT 2

> i−2

(θ1

> k

),

(25) where (25) can be transformed into 

θ2

> i−1

(R2

> i−1

(θ1

> k

) − R2

> i−2

(θ1

> k

)) ≥ c(T 2

> i−1

(θ1

> k

) − T 2

> i−2

(θ1

> k

)) . (26) Based on the ascending order of types, we can further obtain 

θ2 

> i

(R2

> i−1

(θ1

> k

) − R2

> i−2

(θ1

> k

)) ≥ c(T 2

> i−1

(θ1

> k

) − T 2

> i−2

(θ1

> k

)) , (27) 7

which can be transformed into 

θ2 

> i

R2

> i−1

(θ1

> k

) − cT 2

> i−1

(θ1

> k

) ≥ θ2 

> i

R2

> i−2

(θ1

> k

) − cT 2

> i−2

(θ1

> k

). (28) By repeating the above process for type j ∈ {i − 2, i −

3, . . . , 1}, we can obtain the global DIC as follows: 

θ2 

> i

R2 

> i

(θ1

> k

) − cT 2 

> i

(θ1

> k

) ≥θ2 

> i

R2

> i−1

(θ1

> k

) − cT 2

> i−1

(θ1

> k

) ≥· · · ≥ θ2 

> i

R21(θ1

> k

) − cT 21 (θ1

> k

). (29) A similar process can be taken to prove that if the LUIC constraints hold, the global UIC constraints also hold, i.e., 

θ2 

> i

R2 

> i

(θ1

> k

) − cT 2 

> i

(θ1

> k

) ≥θ2 

> i

R2

> i+1

(θ1

> k

) − cT 2

> i+1

(θ1

> k

) ≥· · · ≥ θ2 

> i

R2 

> K

(θ1

> k

) − cT 2 

> K

(θ1

> k

). (30) Thus, the proof of Lemma 5 is completed. 

Lemma 6. (Reduce IC Constraints in Period 2): The K(K−1) 

number of IC constraints in period 2 can be reduced to (K−1) 

number of IC constraints, i.e., 

θ2 

> i

R2 

> i

− cT 2 

> i

− E = θ2 

> i

R2 

> i−1

− cT 2 

> i−1

− E, ∀i ≥ 2. (31) 

Proof. To prove Lemma 6, we can use the contradiction method to demonstrate that if θ2 

> i

R2 

> i

(θ1

> k

) − cT 2 

> i

(θ1

> k

) − E =

θ2 

> i

R2

> i−1

(θ1

> k

) − cT 2

> i−1

(θ1

> k

) − E, ∀i ∈ {2, . . . , K } and the monotonicity hold, the LUIC constraints hold. If θ2 

> i

≥ θ2

> i−1

, then R2 

> i

(θ1

> k

) ≥ R2

> i−1

(θ1

> k

), and we have 

θ2 

> i

(R2 

> i

(θ1

> k

) − R2

> i−1

(θ1

> k

)) ≥ θ2

> i−1

(R2 

> i

(θ1

> k

) − R2

> i−1

(θ1

> k

)) , (32) which (32) can be further transformed into 

c(T 2 

> i

(θ1

> k

) − T 2

> i−1

(θ1

> k

)) ≥ θ2

> i−1

(R2 

> i

(θ1

> k

) − R2

> i−1

(θ1

> k

)) , (33) and we have 

θ2

> i−1

R2

> i−1

(θ1

> k

) − cT 2

> i−1

(θ1

> k

) ≥ θ2

> i−1

R2 

> i

(θ1

> k

) − cT 2 

> i

(θ1

> k

). (34) Therefore, the LUIC constraints remain valid. The proof of Lemma 6 is completed. Based on Lemmas 4 and 6, the K number of IR constraints (13) and K(K − 1) number of IC constraints (14) can be reduced to one IR constraint (20) and (K − 1) number of IC constraints (31). We derive the sufficient and necessary conditions of the feasible contract in period 2 as follows: 

Theorem 1. (Sufficient and Necessary Conditions in Period 2): A feasible contract in period 2 must satisfy the following sufficient and necessary conditions: 

1) 0 ≤ R21(θ1

> k

) ≤ · · · ≤ R2

> k

(θ1

> k

) ≤ · · · ≤ R2 

> K

(θ1

> k

).

2) 0 ≤ T 21 (θ1

> k

) ≤ · · · ≤ T 2 

> k

(θ1

> k

) ≤ · · · ≤ T 2 

> K

(θ1

> k

).

3) θ21 R21(θ1

> k

) − cT 21 (θ1

> k

) − E = 0 .

4) θ2 

> i

R2 

> i

− cT 2 

> i

− E = θ2 

> i

R2 

> i−1

− cT 2 

> i−1

− E, ∀i ≥ 2.

In Theorem 1, the first two conditions indicate the mono-tonicity property of the second-period contract. The last two conditions are the simplified IR and IC constraints in period 2, respectively. Based on (20) and (31), the optimal reward in period 2 can be derived by using the iterative method. 

Theorem 2. (Optimal Rewards in Period 2): The optimal reward (R2 

> i

)⋆ in period 2 is given by 

(R2 

> i

)⋆ =



cT 21 (θ1

> k

) + Eθ21

, i = 1 ,cT 21 (θ1

> k

) + Eθ21

+ c

> i

X

> j=2

T 2 

> j

(θ1

> k

) − T 2

> j−1

(θ1

> k

)

θ2

> j

, i ̸ = 1 .

(35) 

Proof. Please refer to Appendix A. 

C. First-Period Contract Design 

For the feasible contract design in period 1, the IR con-straints are similar to that in Definition 1, which is given by 

u1

> k

(T 1 

> k

) = θ1

> k

R1 

> k

− cT 1 

> k

− E ≥ 0, ∀k ∈ K . (36) Since ESs may misreport their WTP levels in period 1 to gain higher utilities in period 2 [22], instead of considering just the one-period IC constraints, the cloud server should consider the Intertemporal IC (IIC) constraints to incentivize ESs to reflect their actual types [22], and the IIC constraints are expressed as 

u1

> k

(T 1 

> k

) + β

> K

X

> j=1

p2 

> j

(T 1 

> k

)u2 

> j

(T 2 

> j

(T 1 

> k

)) ≥

u1

> k

(T 1 

> k′

) + β

> K

X

> j=1

p2 

> j

(T 1 

> k

)u2 

> j

(T 2 

> j

(T 1 

> k′

)) , ∀k̸ = k′ ∈ K .

(37) From the IIC constraints in (37), we know that ESs will obtain higher utilities by truthfully declaring their WTP levels. When the first-period and second-period types are mutually independent, we have PKj=1 p2 

> j

(T 1 

> k

)u2 

> j

(T 2 

> j

(T 1 

> k

)) = 

PKj=1 p2 

> j

(T 1 

> k

)u2 

> j

(T 2 

> j

(T 1 

> k′

)) , where PKj=1 p2 

> j

(T 1 

> k

)u2 

> j

(T 2 

> j

(T 1 

> k′

)) 

represents the discounted expected utility of the type-k ESs that declare their types are θ1 

> k′

in period 1, and the IIC constraints can be simplified as the IC constraints of the stan-dard static contract. In the following, we derive the necessary conditions of the feasible contract in period 1. 

Lemma 7. For any feasible contract in period 1, if θ1 

> k

> θ 1 

> k′

,then T 1 

> k

> T 1 

> k′

, and T 2 

> j

(T 1 

> k

) > T 2 

> j

(T 1 

> k′

), where k̸ = k′.Proof. Please refer to Appendix B. 

Lemma 8. For any feasible contract in period 1, T 1 

> k

> T 1 

> k′

if and only if R1 

> k

> R 1 

> k′

, where k̸ = k′.Proof. Please refer to Appendix C. Lemma 7 indicates that the ESs with higher WTP levels will perform more training rounds for agent module creation in both period 1 and period 2, even if they have the same WTP level in period 2. Lemma 8 represents the monotonicity of first-period rewards and training rounds, indicating that the ESs with higher WTP levels will receive higher rewards from the cloud server in period 1. In the following, we reduce the IR and IIC constraints in period 1, which is similar to period 2. 8

Lemma 9. (Reduce IR Constraints in Period 1): Similar to (20), the K number of IR constraints in period 1 can also be reduced to a single constraint, i.e., 

θ11 R11 − cT 11 − E = 0 . (38) 

Lemma 10. (Reduce IIC Constraints in Period 1): Similar to (31), the K(K − 1) number of IIC constraints can be reduced into the (K − 1) number of local downward IIC constraints, which is given by 

u1

> k

(T 1 

> k

) + β

> K

X

> j=1

p2 

> j

(T 1 

> k

)u2 

> j

(T 2 

> j

(T 1 

> k

)) = u1

> k

(T 1

> k−1

) + β

> K

X

> j=1

p2 

> j

(T 1 

> k

)u2 

> j

(T 2 

> j

(T 1

> k−1

)) .

(39) The proofs of Lemmas 9 and 10 are similar to Lemmas 4 and 6, respectively. We present the sufficient and necessary conditions of the feasible contract in period 1 in the following. 

Theorem 3. (Sufficient and Necessary Conditions in Period 1): A feasible contract in period 1 must satisfy the following sufficient and necessary conditions: 

1) 0 ≤ R11 ≤ · · · ≤ R1 

> k

≤ · · · ≤ R1 

> K

,

2) 0 ≤ T 11 ≤ · · · ≤ T 1 

> k

≤ · · · ≤ T 1 

> K

,

3) θ11 R11 − cT 11 − E = 0 ,

4) u1

> k

(T 1 

> k

) + β XKj=1 p2 

> j

(T 1 

> k

)u2 

> j

(T 2 

> j

(T 1 

> k

)) = u1

> k

(T 1

> k−1

) + β XKj=1 p2 

> j

(T 1 

> k

)u2 

> j

(T 2 

> j

(T 1

> k−1

)) .

In Theorem 3, the first two conditions are the monotonicity property of the first-period contract. The last two conditions present the simplified IR and IIC constraints in period 1, respectively. Based on (38) and (39), the optimal reward in period 1 can also be derived using the iterative method. 

Theorem 4. (Optimal Rewards in Period 1): The optimal reward (R1 

> i

)⋆ in period 1 is given by 

(R1 

> i

)⋆ =



cT 11 + Eθ11

, i = 1 ,cT 11 + Eθ11

+

> i

X

> k=2

"

(T 1 

> k

− T 1

> k−1

) cθ1

> k

+ βθ1

> kK

X

> j=2

p2 

> j

(T 1 

> k

)Φ j

#

, i ̸ = 1 ,

(40) 

where Φj = Pj−1

> l=1

h

θ2 

> j

c(T 2 

> l

(θ1

> k−1

) − T 2 

> l

(θ1

> k

))   1

> θ2
> l

− 1

> θ2
> l+1

i

.Proof. The proof of the optimal reward (40) in period 1 is similar to the proof of the optimal reward (35) in period 2. 

D. Optimal Two-Period Contract 

To solve the optimal two-period contract, we substitute the derived optimal rewards (40) and (35) into the expected utility of the cloud server (11), and the expected utility of the cloud server in period 2 is expressed as 

U 2 

> CS

(T 2 

> j

(θ1

> k

)) = 

" KX

> j=1

p2 

> j

(T 1 

> k

)N (αQ 2 

> j

− R2 

> j

)

#

− σE C , (41) where Q2 

> j

= 1 − 2 −T 2   

> j(θ1
> k)(2 −Lδ )δε
> 2

.Similarly, the expected utility of the cloud server in period 1 is expressed as 

U 1 

> CS

(T 1 

> k

) = 

" KX

> k=1

p1

> k

(T 1 

> k

)N (αQ 1 

> k

− R1

> k

)

#

− σE C , (42) where Q1 

> k

= 1 − 2 −T 1  

> k(2 −Lδ )δε
> 2

.Therefore, based on (12), the dynamic contract optimization problem is formulated as 

max      

> {T1
> k,T 2
> j(θ1
> k)}

U 1 

> CS

(T 1 

> k

) + βU 2 

> CS

(T 2 

> j

(θ1

> k

)) s.t. (35) and (40) .

(43) 

Remark 1. The number of variables in the dynamic contract optimization problem (43) has been reduced from 4K to 2K.Currently, researchers rely on heuristic algorithms to solve this problem [16], [22]. However, in practical scenarios, the parameters of ESs and the cloud server, such as communi-cation channel conditions, fluctuate dynamically. As a result, heuristic algorithms must be frequently re-designed and re-implemented, leading to substantial computing resource con-sumption. Due to their adaptability to dynamic environments, DRL algorithms have been successfully applied for optimal static contract design [24], [27]. Notably, researchers have adopted GDMs to identify optimal static contracts, achieving superior performance compared with traditional DRL algo-rithms [19], [29]. Building on this foundation, we propose the EDMSAC algorithm that leverages GDMs to generate op-timal dynamic contracts, effectively adapting to the changing environment during federated AI agent construction. 

V. E NHANCED DIFFUSION MODEL -BASED SOFT 

ACTOR -C RITIC ALGORITHMS FOR OPTIMAL DYNAMIC 

CONTRACT DESIGN 

In this section, we first model the optimization problem (43) as a Markov Decision Process (MDP). Subsequently, we introduce the Denoising Diffusion Probabilistic Model (DDPM). Finally, we present the enhanced diffusion policy for optimal dynamic contract design, as shown in Fig. 3. 

A. Markov Decision Process Formulation 

MDP serves as a mathematical framework for modeling decision-making problems. It is represented as a five-element tuple consisting of a state space, an action space, a reward function, a discount factor determining the weight of the future reward relative to the current reward, and state transition prob-abilities. In the following, we explicitly define the state space, action space, and reward function of the MDP framework. 9... 

> ...
> Input Layer
> Output Layer
> Hidden Layers Dynamic Pruning

…

…

Forward Process 

Reverse Process        

> MLP
> (𝑺𝑺 ,𝑀𝑀 ,�𝑨𝑨 𝑀𝑀 )(𝑺𝑺 , 1, �𝑨𝑨 1)
> MLP
> Gaussian Noise
> Optimal Contract
> �𝑨𝑨 0�𝑨𝑨 𝑀𝑀 �𝑨𝑨 𝑀𝑀−1 �𝑨𝑨 1
> Noise Noise

Diffusion Model-based Actor Network 

Action 

𝑨𝑨 0

> Edge Servers

State 

𝑺𝑺 

…

> Cloud Server

Environment 

Replay Buffer 

… 

> Recent Past
> 𝑨𝑨 0
> Map

Record 

Optimizer  

> Policy Optimizer
> Value Function Optimizer

Sample 

Update policy 

Critic Networks  ...    

> ...
> Critic 𝑄𝑄 𝝊𝝊 1Critic 𝑄𝑄 𝝊𝝊 2
> ...
> ...

Double Critic 

Evaluate policy 

Update double critic networks                            

> ...
> ...
> Input Layer
> Output Layer
> Hidden Layers Dynamic Pruning
> 𝜃𝜃 11⋯𝜃𝜃 𝑚𝑚 1⋮⋱⋮
> 𝜃𝜃 1𝑛𝑛 ⋯𝜃𝜃 𝑚𝑚 𝑛𝑛
> 𝜃𝜃 11⋯𝜃𝜃 𝑚𝑚 1⋮⋱⋮
> 𝜃𝜃 1𝑛𝑛 ⋯𝜃𝜃 𝑚𝑚 𝑛𝑛
> 𝜃𝜃 11⋯𝜃𝜃 𝑚𝑚 1⋮⋱⋮
> 𝜃𝜃 1𝑛𝑛 ⋯𝜃𝜃 𝑚𝑚 𝑛𝑛
> 𝜃𝜃 11⋯𝜃𝜃 𝑚𝑚 1⋮⋱⋮
> 𝜃𝜃 1𝑛𝑛 ⋯𝜃𝜃 𝑚𝑚 𝑛𝑛
> The 𝑧𝑧 + 1 -th training round
> The 𝑧𝑧 -th training round

Soft Update 

Update target critic networks 

Update the target actor network 

Target Actor 

…

…   

> Denoise
> ...
> ...
> Critic 𝑄𝑄 �𝝊𝝊 1Critic 𝑄𝑄 �𝝊𝝊 2
> ...
> ...

Target Critic  𝑄𝑄 𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡 

Fig. 3: The architecture of the proposed EDMSAC algorithm for dynamic contract design. The EDMSAC algorithm can generate optimal two-period contracts through the denoising process. To enhance denoising efficiency, we innovatively apply dynamic structured pruning techniques to the MLP of DM-based actor networks. Note that the unimportant neurons in the actor networks are simply masked, not removed outright. 

1) State space: At each step z ∈ { 1, 2, . . . , Z }, the state space from the environment is defined as 

S ≜ {N, K, σ, P n, r n,c , E C , p 1

k, p 2

j (T 1

k ), θ 1

k, θ 2

j }, ∀j, k ∈ K ,

(44) where N and K are constants, while the other terms vary with each training round. Notably, the size of the state space S is 

(6 + 3 K + K2).

2) Action space: According to the optimization problem (43), the action space in the z-th training round is defined as 

A ≜ {T 1

k , T 2

j (θ1

k)}, ∀j, k ∈ K , (45) where A is mapped from the denoising action ˜A.

3) Reward function: After executing the action A based on the current state S, the DRL agent receives an immediate reward r(S, A). To enhance the stability of its learning process, we define a penalty-based reward function as 

r(S, A) = U 1

CS (T 1

k ) + βU 2

CS (T 2

j (θ1

k)) − λ

2KX

i=1 

( ˜A[i]) 2

| {z }

Action penalty 

, (46) where λ is a pre-defined parameter that controls the level of the action penalty. 

B. Denoising Diffusion Probabilistic Models 

DDPMs are powerful generative models that generate data based on forward and reverse processes [34]. In the forward process, given initial data x0 ∼ q(x0), the Gaussian noise is gradually added to the data x0 ∼ q(x0) across M steps with a pre-defined variance schedule ϵ [35], described as 

q(xm|xm−1) := N (xm; √1 − ϵmxm−1, ϵ mI), (47) where ϵm = 1 − e− ϵmin M − 2m−12M 2 (ϵmax −ϵmin ) and I is the identity matrix. By using the Markov chain property, the joint distribution of x1, . . . , xM conditioned on x0 is given by 

q(x1: M |x0) = 

MY

m=1 

q(xm|xm−1). (48) In the reverse process, the DDPM denoises the noisy data 

xM by learning parameterized transitions gθ (xm−1|xm) [35], which is expressed as 

gθ (xm−1|xm) = N (xm−1; μθ (xm, m ), Σθ (xm, m )) , (49) where μθ and Σθ are parameterized by deep neural networks with model parameters θ. Hence, the trajectory from xM to 

x0 under the condition that QMm=1 (1 − ϵm) ≈ 0 is given by 

gθ (x0: M ) = N (xM ; 0, I)

MY

m=1 

gθ (xm−1|xm). (50) The training objective of DDPMs is to maximize the vari-ational lower bound defined as Eq(x0: M )

 ln gθ (x0: M )

q(x1: M |x0)

 [34]. After completing the training of DDPMs, the original data x0

can be recovered from the noisy data xM .

C. Enhanced Diffusion Policy for Dynamic Contract Design 

We define the dynamic contract design policy as πθ ( ˜A|S)

with parameters θ. The goal of πθ ( ˜A|S) is to generate optimal dynamic contracts by maximizing the expected cumulative reward based on (46), which is given by [36] 

E

" ZX

z=0 

γz (r(Sz , Az ) + κH(πθ (·| Sz ))) 

#

, (51) where γ is the discount factor for future rewards, H(πθ (·| Sz )) 

represents the entropy of the policy πθ (Sz ), and κ is the tem-perature coefficient that controls the strength of the entropy. 10 

Algorithm 1: EDMSAC for Dynamic Contract Design  

> 1

Initialize policy network πθ , critic networks Qυ1 , Q υ2 ,target networks ˆπ ˆθ , Q ˆυ1 , Q ˆυ2 , relay buffer D, pruning rate ϱ, and mask matrix W . 

> 2

## Training  

> 3

for the training step v = 1 to V do  

> 4

for the number of transitions z = 1 to Z do  

> 5

### Generating contracts  

> 6

Observe state Sz and initialize a random normal distribution ˜AM ∼ N (0, I). 

> 7

Map ˜A0 to A0 and perform A0, observe the next state Sz+1 , and receive reward rz . 

> 8

Store record (Sz , A0, Sz+1 , r z ) into D. 

> 9

Sample a random mini-batch of transitions Bz

with the size B from D. 

> 10

### Dynamic pruning  

> 11

Calculate neuron importance of θ and ˆθ by using (57), respectively.  

> 12

Use the mask matrix W to mask output neurons based on the pruning rate ϱ by (58).  

> 13

### Parameter updates  

> 14

Update the policy πθ using Bz by (59).  

> 15

Update Qυ1 , Q υ2 using Bz by (56).  

> 16

Update target networks ˆπ ˆθ , Q ˆυ1 , Q ˆυ2 by (60).  

> 17

end  

> 18

end  

> 19

Reconstruct the compact policy networks.  

> 20

## Inference  

> 21

Input a state S. 

> 22

Generate ˜A0 based on the target policy ˆπ ˆθ by (54).  

> 23

return A0 = Ω ⋆.According to (50), the dynamic contract design policy 

πθ ( ˜A|S) can be obtained through the reverse process of a conditional DM, expressed as [34] 

πθ ( ˜A|S) = N ( ˜AM ; 0, I)

> M

Y

> m=1

gθ ( ˜Am−1| ˜Am, S), (52) where the end sample of the reverse chain is ˜A0. According to [19], [34], gθ ( ˜Am−1| ˜Am, S) is modeled as a Gaussian distribution N ( ˜Am−1; μθ ( ˜Am, S, m ), Σθ ( ˜Am, S, m )) based on (49), with the covariance matrix Σθ ( ˜Am, S, m ) = ζmI and the mean μθ ( ˜Am, S, m ) constructed as 

μθ ( ˜Am, S, m ) = 1

√χm



˜Am − ζm

√1 − ¯χm

ξθ ( ˜Am, S, m )



,

(53) where ζm ∈ (0 , 1) is a hyperparameter for model training, 

χm = 1 − ζm, and ¯χm = Qmi=0 χi [29], [34]. ξθ ( ˜Am, S, m )

is the Multi-Layer Perceptron (MLP) of DM-based actor networks, as shown in Fig. 3. We first sample ˜AM ∼ N (0, I),and then a series of samples ˜Am, m = M, . . . , 1 can be obtained through the reverse diffusion chain parameterized by 

θ, which is given by 

˜Am−1| ˜Am = ˜Am

√χm

− ζm

pχm(1 − ¯χm) ξθ ( ˜Am, S, m )+ pζmξ, 

(54) where ξ ∼ N (0, I). Notably, when m = 1 , we have ξ = 0 

[34], [35]. For policy improvement, we introduce the Q-function [34]. Specifically, we create two critic networks 

Qυ1 , Q υ2 , target critic networks Qˆυ1 , Q ˆυ2 , and a target policy 

ˆπ ˆθ . The final policy-learning objective is to minimize the actor loss, which is given by 

L(θ) = −ES∼D , ˜A0∼πθ [Qυ (S, ˜A0) + κ log πθ ( ˜A0|S)] , (55) where Qυ (S, ˜A0) = min {Qυ1 (S, ˜A0), Q υ2 (S, ˜A0)} and D

is the relay buffer. To update the Q-function, we minimize the temporal difference error, expressed as 

E(Sz ,Az ,Sz+1 ,r z )∼B z

h X 

> i=1 ,2

(r(Sz , Az ) + γz (1 − dz+1 )(Qˆυ (Sz+1 ) − κ log ˆ π ˆθ (Sz+1 )) − Qυi (Sz , Az )) 2i

,

(56) where Bz is a mini-batch of transitions sampled from D and 

dz+1 ∈ { 0, 1} is a terminated flag, with dz+1 = 1 indicating that the training episode has ended. To enhance the denoising efficiency, we employ dynamic structured pruning techniques to linear layers of ξθ (·, ·, ·).Considering ξθ (·, ·, ·) with H fully connected layers, we denote the parameters in the h-th fully connected layer as 

θ(h) ∈ Rx×y , h ∈ { 1, . . . , H }. We first calculate the impor-tance of each row in θ(h), which is expressed as 

|| θ(h) 

> i

|| 2 =

vuut

> y

X

> j=1

θ2 

> i,j

, i = 1 , 2, . . . , x. (57) Then, we introduce a mask matrix W (h) ∈ Rx×y to prune fully connected layers with low importance based on the pre-defined pruning rate. The elements of W (h) ∈ Rx×y are binary, where W (h) 

> x

= 0 indicates that the output neuron o(h)

> x

is pruned, while o(h) 

> x

remains otherwise. Hence, the pruned parameters in the h-th fully connected layer are given by 

θ′(h) = θ(h) ⊙ W (h), (58) where ⊙ represents the Hadamard product [37]. The pruned policy parameters θ′ can be updated by performing gradient descent algorithms such as Adam, as given by 

θ′ 

> v+1

← θ′ 

> v

− η∇θ′ 

> v

L(θ), (59) where θ′ 

> v

are the pruned policy parameters in the v-th training step and η ∈ (0 , 1] is the learning rate of the actor. Similarly, the parameters of the target policy are also dynamically pruned. During gradient descent, the parameters of the target networks remain frozen and then are updated through a soft update mechanism [34], [36], which is given by 

ˆθ′ 

> v+1

← τ θ′ 

> v

+ (1 − τ ) ˆθ′ 

> v

,

ˆυi,v +1 ← τ υi,v + (1 − τ ) ˆ υi,v , for i = {1, 2}, (60) where υi,v gives the critic parameters in the v-th training step and τ ∈ (0 , 1] is the update rate of the target networks. 11 Prompt 2: Open the Bilibili app, select a video at random and like it Prompt 1: Help me log in to Bilibili account  

> 1
> 2
> 34
> 5
> 67
> 8
> 1
> 2
> 3
> 4
> 5
> 678
> 9
> 10

Fig. 4: Two case studies of mobile phone operations using an AI agent. We call the Qwen-VL-Plus and Qwen-VL-Max models through the Qwen-VL API to construct the mobile AI agent, utilizing Android Debug Bridge (ADB) to operate a mobile phone with the Android OS system, where ADB can simulate all operations of the AI agent [30]. 10/10 7/9 6/6 

> Decision Reflection Planning
> 0246810
> Rounds
> Accuracy Inaccuracy

(a) Case 1. 3/8 6/7 2/3  

> Decision Reflection Planning
> 0123456789
> Rounds
> Accuracy Inaccuracy

(b) Case 2. 

Fig. 5: Performance evaluation of mobile AI agents. Algorithm 1 illustrates the procedure of the EDMSAC algo-rithm, where its computational complexity primarily consists of three components: In action sampling, the computational overhead arises from environment interaction and the reverse process, with a complexity of O(V Z (1 + M |θ|)) [36]. In dynamic pruning, the computational overhead is due to the calculations of neuron importance and masking, leading to a complexity of O(PHh=1 (x(h)y(h) + x(h) log x(h))) [37]. In parameter updates, the computational complexity stems from updating network parameters, given by O(V (B + 1)( |θ| +

|υ|)) [36]. Therefore, the computational complexity of the EDMSAC algorithm is approximately O(V Z (1 + M |θ|) + PHh=1 (x(h)y(h) + x(h) log x(h)) + V (B + 1)( |θ| + |υ|)) .VI. S IMULATION RESULTS 

In this section, we first evaluate the performance of mobile AI agents, where we leverage LLMs called Qwen-VL to construct an AI agent capable of operating a mobile phone to execute various tasks. We then compare the proposed EDMSAC algorithm with several DRL algorithms in optimal dynamic contract design. Finally, we validate the effectiveness of the proposed two-period dynamic contract scheme. TABLE I: Simulation Parameters 

Parameters Values 

Number of ESs ( N ) {3, 6, 12 , 18 }

Pre-defined hyperparameters ( δ, L, ε, λ ) {0.02 , 8, 2, 0.01 }

Constant related to the hardware architecture of ES n (ςn) 10 −23 

Number of CPU cycles required for perceiving feature data ( ΥP er n ) 100 cycles /bit 

CPU speed of ES n (fn) 64 MHz 

Size of feature data perceived by ES n

(DF ea n ) 1 MB 

Effective switched capacitance of ES n (ρn) 10 −16 

Number of CPU cycles required for creating one bit of agent modules ( ΥCre n ) 120 cycles /bit 

Transmit power of ES n (Pn) [20 , 33] dBm 

Model size of agent modules constructed by ES n (DAgent n ) 10 MB 

Transmission rate from ES n to the cloud server through the fiber link ( rn,c ) [1 , 3] Mbps 

Unit cost of energy consumption ( σ) [0 .5, 1] 

Scaling factor that affects the overall magnitude of economic benefits ( α) {200 , 250 }

Energy consumption of global AI agent integration ( EC ) [20 , 25] Joules 

Discount factor ( β) {0, 0.5, 1}

A. Experimental Setup Experiment Settings: We consider ESs with homogeneous hardware capabilities and classify them into two types: high WTP and low WTP [19], [29]. Specifically, θt 

> 1

and θt 

> 2

are randomly sampled within [15 , 20] and [20 , 25] , respectively. 

p1 

> k

and p2 

> k

are randomly generated following the Dirichlet distribution [4]. The setting of simulation parameters are summarized in Table I [15], [16], [22], [32], [33]. For the algorithm design, DM-based actor networks use the Variance Proportional (VP) noise schedule strategy, and the actor and critic networks are trained by the Adam optimizer, with a weight decay rate of 0.0001 for regularizing model weights [36]. In addition, the discount factor γ is set to 0.99 , and τ is 12 0 0.5 1 1.5 2 2.5 3 3.5 4     

> Training Steps 10 5
> -3 -2 -1 012345
> Test Rewards
> 10 4
> EDMSAC PPO SAC DDPG
> 33.2 3.4 3.6 3.8 10 5
> 3.5 44.5 510 4

(a) EDMSAC vs PPO, SAC, and DDPG 0 0.5 1 1.5 2 2.5 3 3.5 4  

> Training Steps 10 5
> 33.5 44.5 5
> Test Rewards
> 10 4
> EDMSAC DMSAC DMDDPG

(b) EDMSAC vs DMSAC and DMDDPG Average test rewards Total profit of cloud server  

> Performance of Generated Dynamic Contracts
> -150 -100 -50 050 100 150 200 250
> Values
> EDMSAC PPO DMSAC DMDDPG SAC DDPG

(c) Performance results 

Fig. 6: Performance comparisons of the EDMSAC algorithm with several DRL algorithms in optimal dynamic contract design. 0 1 2 3 4   

> Training Steps 10 5
> 2.5 33.5 44.5 55.5 6
> Test Rewards
> 10 4
> Pruning rate = 0.7 Pruning rate = 0.5 Pruning rate = 0.3
> 33.5 410 5
> 4.7 4.8 4.9 55.1 10 4
> 215.36 243.68 254.68
> 0.3 0.5 0.7  050 100 150 200 250 300
> Total Profit of the Cloud Server

(a) Pruning rates. 0 1 2 3 4    

> Training Steps 10 5
> 33.5 44.5 55.5 6
> Test Rewards
> 10 4
> Denoising step = 3 Denoising step = 6 Denoising step = 12
> 33.5 410 5
> 4.8 510 4
> 255.06 243.68 226.5
> 3612  050 100 150 200 250 300
> Total Profit of the Cloud Server

(b) Denoising steps. 0 1 2 3 4   

> Training Steps 10 5
> 2.5 33.5 44.5 55.5 6
> Test Rewards
> 10 4
> LR2: Actor lr = 2e -7 ; Critic lr = 2e -6
> LR1: Actor lr = 1e -4 ; Critic lr = 1e -4
> 203.79 243.68
> LR1 LR2 050 100 150 200 250
> Total Profit of the Cloud Server

(c) Learning rates. 

Fig. 7: Performance of the EDMSAC algorithm in optimal dynamic contract design under different parameter configurations. set to 0.005 to soft update the target networks. Notably, our experiments use PyTorch with CUDA 12.0 on an NVIDIA GeForce RTX 3080 Laptop GPU. 

Scheme Details: To evaluate the performance of the pro-posed scheme, we compare it with other typical schemes: 

• Two-period static contract scheme [22]: This scheme involves executing the static contract twice, and we set 

β = 0 to simplify it to a one-stage static contract. 

• Two-period random contract scheme [19]: This scheme randomly generates two-period contracts for ESs. When 

β̸ = 0 , this scheme operates dynamically across two periods. Conversely, when β = 0 , this scheme is static. 

B. Performance Evaluation of Mobile AI Agents 

As illustrated in Fig. 4, we present two case studies in which a mobile AI agent operates a real smartphone to perform tasks. For the perception module, we leverage Qwen-VL-Plus to create it, and we use the ConvNextViT-document model and the GroundingDINO model as text and icon recognition tools, respectively [30]. For the decision-making, reflection, and planning modules, we leverage Qwen-VL-Max to create them. To evaluate the performance of the constructed AI agent, we adopt several evaluation metrics: Decision Accuracy (DA), which measures the accuracy of the decision-making module; Reflection Accuracy (RA), which assesses the accuracy of the reflection module; and Planning Accuracy (PA), which evaluates the accuracy of the planning module. As shown in Fig. 5, the performance of the AI agent in executing simple tasks, such as Case 1, is superior to that in handling more complex tasks, like Case 2, as evidenced by higher values of DA, RA, and PA. Notably, when executing simple tasks, the AI agent may perform additional rounds to verify task completion, leading to increased service latency. 

C. Performance Evaluation of the EDMSAC Algorithm 

In Fig. 6, we present a performance comparison between the proposed EDMSAC algorithm and several representative DRL algorithms in optimal dynamic contract design, including DMSAC [36], DM-based Deep Deterministic Policy Gradient (DMDDPG) [28], PPO, SAC, and DDPG. From Figs. 6(a) and 6(b), we observe that the proposed EDMSAC algorithm outperforms other DRL algorithms in optimal dynamic con-tract design because DRL algorithms struggle in different suboptimal solution spaces, especially PPO algorithms. The superior performance of the proposed EDMSAC algorithm can be primarily attributed to two key aspects. First, the EDMSAC algorithm has a strong capability to capture complex environ-mental information because of DMs [28], which helps avoid the identification of suboptimal dynamic contracts. Moreover, its fine-grained policy tuning mitigates the impact of environ-mental randomness, enhancing decision robustness. Second, through structural pruning techniques, the unimportant neurons 13 30.15 28.73 42.93 51.90 33.74 45.28 51.89 73.63  

> Period 1 Period 2
> 010 20 30 40 50 60 70 80 90
> Contract Values
> Type-1 contract item: T i1
> Type-1 contract item: R i1
> Type-2 contract item: T i2
> Type-2 contract item: R i2

(a) State 1. 30.23 27.52 43.65 42.22 48.03 44.75 86.20 60.98   

> Period 1 Period 2
> 010 20 30 40 50 60 70 80 90 100 110
> Contract Values
> Type-1 contract item: T i1
> Type-1 contract item: R i1
> Type-2 contract item: T i2
> Type-2 contract item: R i2

(b) State 2. 25.61 36.26 46.04 81.58 50.65 74.54 124.46 133.86   

> Period 1 Period 2
> 020 40 60 80 100 120 140
> Contract Values
> Type-1 contract item: T i1
> Type-1 contract item: R i1
> Type-2 contract item: T i2
> Type-2 contract item: R i2

(c) State 3. 

Fig. 8: Two-period dynamic contracts generated by the EDMSAC algorithm under three network states. 0 50 100 150 200 250 300 350 400 

> Training Epochs
> 1.5 22.5 33.5 44.5 5
> Test Rewards
> 10 4
> Our scheme Two-period random dynamic contract Two-period static contract Two-period random static contract

(a) Test reward comparison between the proposed scheme and three baseline contract schemes. 3 6 12 18 Number of ESs 

> 0100 200 300 400 500 600 700 800 900
> Total Profit of the Cloud Server
> Our scheme Two-period random dynamic contract Two-period static contract Two-period random static contract

(b) Relationship between the total profit of the cloud server and the number of ESs. 4 6 8 10 12 14 16 18     

> Number of ESs
> 0500 1000 1500 2000 2500
> Total Profit of the Cloud Server
> = 200; = 0.5 = 250; = 0.5 = 200; = 1.0 = 250; = 1.0

(c) Impacts of α and β on the total profit of the cloud server under different numbers of ESs. 

Fig. 9: Performance evaluation of the proposed two-period dynamic contract scheme. in the DM-based actor networks are masked rather than permanently removed, thereby preserving the network struc-ture while reducing computational redundancy. Therefore, the EDMSAC algorithm can achieve higher denoising efficiency, enabling the generation of more optimal and stable dynamic contracts. From Fig. 6(c), we also observe that the EDMSAC algorithm achieves a higher total profit of the cloud server compared with other DRL algorithms. Overall, the above analysis demonstrates the effectiveness and superiority of the proposed EDMSAC algorithm in optimizing dynamic contract design, which enhances the system performance of federated AI agent construction under dynamic information asymmetry. As shown in Fig. 7, we present the performance of the EDMSAC algorithm in optimal dynamic contract design with different parameter configurations. Fig. 7(a) illustrates the performance of the EDMSAC algorithm under different pruning rates, i.e., 30% , 50% , and 70% . We observe that the EDMSAC algorithm demonstrates superior performance under higher pruning rates, leading to faster convergence and higher test rewards. The reason is that a higher pruning rate indicates that more redundant and unimportant neurons in the DM-based actor networks are structurally masked. As a result, the pruned DM-based actor networks exhibit better generalization capabilities and are more effective at learning essential features in optimal dynamic contract design. Fig. 7(b) presents the performance of the EDMSAC algorithm under different denoising steps, i.e., 3, 6, and 12 . We observe that the EDMSAC algorithm achieves faster convergence and yields higher total profits for the cloud server under a smaller denoising step. The reason is that a smaller denoising step can achieve higher computational efficiency of the DM-based actor networks. Fig. 7(c) illustrates the performance of the EDMSAC algorithm under two groups of different learning rates. The first group is the actor networks with a learning rate of 2 × 10 −7 and the critic networks with a learning rate of 2 × 10 −6. The second group is the actor networks and critic networks with the same learning rate of 1 × 10 −4. We observe that the EDMSAC algorithm has better performance under the first group of learning rates, indicating that the pruned actor networks need to learn an efficient policy in optimal dynamic contract design under small learning rates. 

D. Performance Evaluation of the Dynamic Contract Scheme 

In Fig. 8, we illustrate the two-period dynamic contracts generated by the EDMSAC algorithm under three network states, with a pruning rate of 0.5 and a denoising step of 

6. Additionally, the learning rates of the actor and critic networks are set to 2 × 10 −7 and 2 × 10 −6, respectively. Attributed to the exploration and generalization capabilities of DMs, the proposed EDMSAC algorithm demonstrates the ability to generate optimal dynamic contracts across varying network states, indicating that our algorithm possesses strong 14 

adaptability to the dynamic environments inherent in federated AI agent construction, thereby ensuring robust system perfor-mance in mobile metaverses. We observe that in period 2, ESs with higher types perform more training rounds T k 

> 2

for agent module creation and consequently receive higher rewards Rk 

> 2

,which validates Lemmas 1 and 2, respectively. In period 1, ESs with higher types also perform more training rounds T ki for agent module creation and receive higher rewards Rki , which demonstrates Lemmas 7 and 8, respectively. In Fig. 9, we compare the proposed two-period dynamic contract scheme with three baseline contract schemes. Fig. 9(a) presents the test reward comparison between the proposed scheme and three baseline contract schemes. Our scheme consistently achieves higher test rewards compared with the other three contract schemes, indicating that the proposed two-period dynamic contract scheme possesses superior per-formance. In addition, the test rewards obtained by dynamic contract schemes consistently surpass those of static contract schemes, highlighting the advantages of incorporating adapt-ability in contract design. The reason is that, unlike static contract schemes, dynamic contract schemes take into account the profit of the cloud server in the second period. As a result, the total profit of the cloud server achieved under the dynamic contract scheme is higher than that of the static contract scheme. Fig. 9(b) illustrates the relationship between the total profit of the cloud server and varying numbers of ESs under four contract schemes. No matter under which contract scheme, as the number of ESs increases, the total profit of the cloud server also increases. Moreover, compared with the three baseline contract schemes, the proposed two-period dynamic contract scheme can achieve the highest total profit for the cloud server. This is primarily because the proposed scheme is capable of designing suitable contracts for ESs based on their respective types rather than generating contracts randomly, which enables more effective resource utilization for federated AI agent construction and contributes to higher total profits for the cloud server. Fig. 9(c) presents the impacts of the scaling factor α and the discount factor β on the total profit of the cloud server under different numbers of ESs. When β remains constant, the total profit of the cloud server increases with the rise of α. The reason is that a larger α reflects a greater overall magnitude of economic benefits allocated to the cloud server, thereby enhancing its total profit. In addition, when α

remains constant, the total profit of the cloud server increases as β increases. This is primarily because the cloud server can obtain higher profits in the second period of federated AI agent construction, which also enhances its total profit. VII. C ONCLUSION 

In this paper, we have studied federated AI agent construc-tion in mobile metaverses. Specifically, we have proposed an edge-cloud collaboration-based federated AI agent con-struction framework in mobile metaverses. In this framework, ESs act as agent infrastructures to create agent modules in a distributed manner, and then the cloud server integrates them into complete AI agents and deploys the constructed AI agents at the edge, thereby providing AI agent services for users in mobile metaverses. Considering the dynamic information asymmetry in federated AI agent construction, we have designed a two-period dynamic contract model to incentivize ESs to continuously participate in federated AI agent construction. Furthermore, to effectively identify optimal dynamic contracts, we have proposed the EDMSAC algorithm, which dynamically prunes redundant neurons in the DM-based actor networks, thereby enhancing denoising efficiency for dynamic contract sampling. Extensive simulations have demonstrated the effectiveness and robustness of the proposed scheme and the EDMSAC algorithm. For future work, we aim to develop a mobile AI agent paradigm built upon the proposed framework, which will empower AI agents with enhanced adaptability and decision-making capabilities in real-world mobile environments. APPENDIX APROOF FOR THEOREM 2We use the contradiction method to demonstrate the unique-ness of the optimal reward in period 2. We assume that there is another optimal reward ˆR2 

> i

that facilitates the greater profit of the cloud server. Since the utility of the cloud server is inversely proportional to the total reward, we can obtain PKi=1 ˆR2 

> i

< PKi=1 (R2 

> i

)⋆, indicating that there is at least one type θ2 

> i

satisfying ˆR2 

> i

< (R2 

> i

)⋆, i ∈ K .We consider ˆR2 

> i

< (R2 

> i

)⋆, where 1 ≤ i < K . Based on the LDIC constraints, we have 

ˆR2 

> i

≥ ˆR2 

> i−1

+ c(T 2 

> i

(θ1

> k

) − T 2

> i−1

(θ1

> k

)) 

θ2

> i

. (A-1) Based on Lemma 6, we have 

(R2 

> i

)⋆ − (R2

> i−1

)⋆ = c(T 2 

> i

(θ1

> k

) − T 2

> i−1

(θ1

> k

)) 

θ2

> i

. (A-2) By substituting (A-2) into (A-1), we have 

ˆR2 

> i

− (R2 

> i

)⋆ ≥ ˆR2 

> i−1

− (R2

> i−1

)⋆. (A-3) Since ˆR2 

> i

< (R2 

> i

)⋆, we have ˆR2 

> i−1

< (R2

> i−1

)⋆. By recursion, we can eventually obtain ˆR21 < (R21)⋆ = cT 21 (θ1 

> k)+ Eθ21

, which violates the IR constraints. Therefore, there does not exist the reward ˆR2 

> i

that facilitates the greater profit of the cloud server. The proof of Theorem 2 is completed. APPENDIX BPROOF FOR LEMMA 7By substituting the optimal reward (R2 

> i

)⋆ in period 2 into the IIC constraints, we have 

u1

> k

(T 1 

> k

) + β

> K

X

> j=2

p2 

> j

(T 1 

> k

)

×

" θ2 

> j

Eθ21

+

> j−1

X

> l=1

θ2 

> j

c

 T 2 

> l

(θ1

> k

)

θ2

> l

− T 2 

> l

(θ1

> k

)

θ2

> l+1

#

≥ u1

> k

(T 1 

> k′

) + β

> K

X

> j=2

p2 

> j

(T 1 

> k

)

×

" θ2 

> j

Eθ21

+

> j−1

X

> l=1

θ2 

> j

c

 T 2 

> l

(θ1 

> k′

)

θ2

> l

− T 2 

> l

(θ1 

> k′

)

θ2

> l+1

#

.

(B-1) 15 

For ease of notation, we write 

a = θ2 

> j

Eθ21

+

> j−1

X

> l=1

θ2 

> j

c

 T 2 

> l

(θ1

> k

)

θ2

> l

− T 2 

> l

(θ1

> k

)

θ2

> l+1



, (B-2) 

b = θ2 

> j

Eθ21

+

> j−1

X

> l=1

θ2 

> j

c

 T 2 

> l

(θ1 

> k′

)

θ2

> l

− T 2 

> l

(θ1 

> k′

)

θ2

> l+1



. (B-3) Repeating the above procedure can also obtain the IIC constraint for type-k′ ESs like (B-1). Then, by combining the two IIC constraints, we have 

u1

> k

(T 1 

> k

) − u1

> k

(T 1 

> k′

) + u1 

> k′

(T 1 

> k′

) − u1 

> k′

(T 1 

> k

)+ β

> K

X

> j=2

[p2 

> j

(T 1 

> k

) − p2 

> j

(T 1 

> k′

)]( a − b) ≥ 0. (B-4) To better analyze whether (B-4) is true, we separate (B-4) into three parts, which are given by 

u1

> k

(T 1 

> k

) − u1

> k

(T 1 

> k′

) + u1 

> k′

(T 1 

> k′

) − u1 

> k′

(T 1 

> k

)= ( θ1 

> k

− θ1 

> k′

)( R1 

> k

− R1 

> k′

), (B-5) 

p2 

> j

(T 1 

> k

) − p2 

> j

(T 1 

> k′

), (B-6) and 

a − b = θ2 

> j

c

> j−1

X

> l=1

" 1

θ2

> l

− 1

θ2

> l+1



(T 2 

> l

(θ1

> k

) − T 2 

> l

(θ1 

> k′

)) 

#

. (B-7) Since the types of period 1 and period 2 are positively correlated [16], [22], we have p2 

> j

(T 1 

> k

) > p 2 

> j

(T 1 

> k′

), i.e., (B-6) is positive. To make (B-4) hold, (B-5) and (B-7) must be positive. Following Lemma 7, (B-5) will be positive if T 1 

> k

> T 1 

> k′

, and (B-7) will be positive if T 2 

> j

(T 1 

> k

) > T 2 

> j

(T 1 

> k′

). Thus, the proof of Lemma 7 is completed. APPENDIX CPROOF FOR LEMMA 8Based on the IIC constraints for the ESs with type θ1

> k

, we can obtain 

u1

> k

(T 1 

> k

)+ β

> K

X

> j=1

p2 

> j

(T 1 

> k

)[ u2 

> j

(T 2 

> j

(T 1 

> k

)) −u2 

> j

(T 2 

> j

(T 1 

> k′

))] ≥ u1

> k

(T 1 

> k′

).

(C-1) Since β and p2 

> j

(T 1 

> k

) are small, (C-1) can be simplified into 

u1

> k

(T 1 

> k

) ≥ u1

> k

(T 1 

> k′

), (C-2) which can be transformed into 

θ1

> k

(R1 

> k

− R1 

> k′

) ≥ c(T 1 

> k

− T 1 

> k′

). (C-3) We first prove the sufficiency of Lemma 8, namely if T 1 

> k

>T 1 

> k′

, then R1 

> k

> R 1 

> k′

. From (C-3), we can know that R1 

> k

> R 1

> k′

since T 1 

> k

> T 1 

> k′

. Thus, the sufficiency is proved. We then prove the necessity of Lemma 8, namely if R1 

> k

>R1 

> k′

, then T 1 

> k

> T 1 

> k′

. Based on the IIC constraint for type-k′

ESs, we can obtain 

u1 

> k′

(T 1 

> k′

) ≥ u1 

> k′

(T 1 

> k

), (C-4) which can also be transformed into 

θ1 

> k′

(R1 

> k′

− R1

> k

) ≥ c(T 1 

> k′

− T 1 

> k

). (C-5) Given R1 

> k′

> R 1

> k

, the left-hand side of (C-5) is negative. Thus, we have T 1 

> k

− T 1 

> k′

. The proof of Lemma 8 is completed. REFERENCES [1] X. Ren, H. Du, C. Qiu, T. Luo, Z. Liu, X. Wang, and D. Niyato, “Dual-level resource provisioning and heterogeneous auction for mobile metaverse,” IEEE Transactions on Mobile Computing , vol. 23, no. 11, pp. 10 329–10 343, 2024. [2] M. Xu, W. C. Ng, W. Y. B. Lim, J. Kang, Z. Xiong, D. Niyato, Q. Yang, X. Shen, and C. Miao, “A full dive into realizing the edge-enabled metaverse: Visions, enabling technologies, and challenges,” 

IEEE Communications Surveys & Tutorials , vol. 25, no. 1, pp. 656– 700, 2023. [3] J. Liu, M. Xiao, J. Wen, J. Kang, R. Zhang, T. Zhang, D. Niyato, W. Zhang, and Y. Liu, “Optimizing resource allocation for multi-modal semantic communication in mobile AIGC networks: A diffusion-based game approach,” IEEE Transactions on Cognitive Communications and Networking , pp. 1–1, 2025. [4] J. Wen, J. Nie, J. Kang, D. Niyato, H. Du, Y. Zhang, and M. Guizani, “From generative AI to generative Internet of Things: Fundamentals, framework, and outlooks,” IEEE Internet of Things Magazine , vol. 7, no. 3, pp. 30–37, 2024. [5] Z. Durante, Q. Huang, N. Wake, R. Gong, J. S. Park, B. Sarkar, R. Taori, Y. Noda, D. Terzopoulos, Y. Choi, K. Ikeuchi, H. Vo, L. Fei-Fei, and J. Gao, “Agent AI: Surveying the horizons of multimodal interaction,” 2024. [Online]. Available: https://arxiv.org/abs/2401.03568 [6] C. Zhang, K. Yang, S. Hu, Z. Wang, G. Li, Y. Sun, C. Zhang, Z. Zhang, A. Liu, S.-C. Zhu, X. Chang, J. Zhang, F. Yin, Y. Liang, and Y. Yang, “Proagent: Building proactive cooperative agents with large language models,” Proceedings of the AAAI Conference on Artificial Intelligence , vol. 38, no. 16, pp. 17 591–17 599, Mar. 2024. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/view/29710 [7] Y. Siriwardhana, P. Porambage, M. Liyanage, and M. Ylianttila, “A survey on mobile augmented reality with 5G mobile edge computing: Architectures, applications, and technical aspects,” IEEE Communica-tions Surveys & Tutorials , vol. 23, no. 2, pp. 1160–1192, 2021. [8] K. Peng, Y. Hu, H. Ding, H. Chen, L. Wang, C. Cai, and M. Hu, “Large-scale service mesh orchestration with probabilistic routing in cloud data centers,” IEEE Transactions on Services Computing , vol. 18, no. 2, pp. 868–882, 2025. [9] V.-L. Nguyen, P.-C. Lin, B.-C. Cheng, R.-H. Hwang, and Y.-D. Lin, “Security and privacy for 6G: A survey on prospective technologies and challenges,” IEEE Communications Surveys & Tutorials , vol. 23, no. 4, pp. 2384–2428, 2021. [10] J. Kang, J. Wen, D. Ye, B. Lai, T. Wu, Z. Xiong, J. Nie, D. Niyato, Y. Zhang, and S. Xie, “Blockchain-empowered federated learning for healthcare metaverses: User-centric incentive mechanism with optimal data freshness,” IEEE Transactions on Cognitive Communications and Networking , vol. 10, no. 1, pp. 348–362, 2024. [11] W. Sun, S. Lei, L. Wang, Z. Liu, and Y. Zhang, “Adaptive federated learning and digital twin for industrial Internet of Things,” IEEE Transactions on Industrial Informatics , vol. 17, no. 8, pp. 5605–5614, 2021. [12] W. Yang, W. Xiang, Y. Yang, and P. Cheng, “Optimizing federated learning with deep reinforcement learning for digital twin empowered industrial IoT,” IEEE Transactions on Industrial Informatics , vol. 19, no. 2, pp. 1884–1893, 2023. [13] B. Jiang, J. Du, C. Jiang, Z. Han, A. Alhammadi, and M. Debbah, “Over-the-air federated learning in digital twins empowered UAV swarms,” 

IEEE Transactions on Wireless Communications , vol. 23, no. 11, pp. 17 619–17 634, 2024. [14] P. Consul, I. Budhiraja, D. Garg, N. Kumar, R. Singh, and A. S. Almogren, “A hybrid task offloading and resource allocation approach for digital twin-empowered UAV-assisted MEC network using federated reinforcement learning for future wireless network,” IEEE Transactions on Consumer Electronics , vol. 70, no. 1, pp. 3120–3130, 2024. [15] Z. Yang, M. Chen, W. Saad, C. S. Hong, and M. Shikh-Bahaei, “Energy efficient federated learning over wireless communication networks,” 

IEEE Transactions on Wireless Communications , vol. 20, no. 3, pp. 1935–1949, 2021. [16] W. Y. B. Lim, S. Garg, Z. Xiong, D. Niyato, C. Leung, C. Miao, and M. Guizani, “Dynamic contract design for federated learning in smart healthcare applications,” IEEE Internet of Things Journal , vol. 8, no. 23, pp. 16 853–16 862, 2021. [17] W. Shen, M. Ye, W. Yu, and P. C. Yuen, “Build yourself before collaboration: Vertical federated learning with limited aligned samples,” 

IEEE Transactions on Mobile Computing , pp. 1–14, 2025. 16 

[18] G. Zhu, J. Xu, K. Huang, and S. Cui, “Over-the-air computing for wire-less data aggregation in massive IoT,” IEEE Wireless Communications ,vol. 28, no. 4, pp. 57–65, 2021. [19] J. Wen, J. Nie, Y. Zhong, C. Yi, X. Li, J. Jin, Y. Zhang, and D. Niyato, “Diffusion-model-based incentive mechanism with prospect theory for edge AIGC services in 6G IoT,” IEEE Internet of Things Journal ,vol. 11, no. 21, pp. 34 187–34 201, 2024. [20] S. Wang, W. Xia, H. Zhao, Y. Ni, C. Zhu, and H. Zhu, “Incentiviz-ing federated learning with contract theory under strong information asymmetry,” in 2024 IEEE Wireless Communications and Networking Conference (WCNC) , 2024, pp. 1–6. [21] S. M. A. Kazmi, T. N. Dang, I. Yaqoob, A. Manzoor, R. Hussain, A. Khan, C. S. Hong, and K. Salah, “A novel contract theory-based incentive mechanism for cooperative task-offloading in electrical vehicu-lar networks,” IEEE Transactions on Intelligent Transportation Systems ,vol. 23, no. 7, pp. 8380–8395, 2022. [22] C. He, Y. Wang, J. Hu, T. H. Luan, Y. Bi, and Z. Su, “Collabo-rative vehicular threat sharing: A long-term contract-based incentive mechanism with privacy preservation,” IEEE Transactions on Intelligent Transportation Systems , vol. 25, no. 12, pp. 21 528–21 544, 2024. [23] Z. Xiong, J. Kang, D. Niyato, P. Wang, H. V. Poor, and S. Xie, “A multi-dimensional contract approach for data rewarding in mobile networks,” 

IEEE Transactions on Wireless Communications , vol. 19, no. 9, pp. 5779–5793, 2020. [24] Y. Zhong, J. Kang, J. Wen, D. Ye, J. Nie, D. Niyato, X. Gao, and S. Xie, “Generative diffusion-based contract design for efficient AI twin migration in vehicular embodied AI networks,” IEEE Transactions on Mobile Computing , vol. 24, no. 5, pp. 4573–4588, 2025. [25] J. Wen, Y. Zhang, Y. Chen, W. Zhong, X. Huang, L. Liu, and D. Niyato, “Learning-based big data sharing incentive in mobile AIGC networks,” 2024. [Online]. Available: https://arxiv.org/abs/2407.10980 [26] N. Zhao, Y. Pei, Y.-C. Liang, and D. Niyato, “A deep reinforcement learning-based contract incentive mechanism for mobile crowdsourcing networks,” IEEE Transactions on Vehicular Technology , vol. 73, no. 3, pp. 4511–4516, 2024. [27] I. Lotfi, D. Niyato, S. Sun, D. I. Kim, and X. Shen, “Semantic information marketing in the metaverse: A learning-based contract theory framework,” IEEE Journal on Selected Areas in Communications ,vol. 42, no. 3, pp. 710–723, 2024. [28] H. Du, R. Zhang, Y. Liu, J. Wang, Y. Lin, Z. Li, D. Niyato, J. Kang, Z. Xiong, S. Cui, B. Ai, H. Zhou, and D. I. Kim, “Enhancing deep reinforcement learning: A tutorial on generative diffusion models in network optimization,” IEEE Communications Surveys & Tutorials ,vol. 26, no. 4, pp. 2611–2646, 2024. [29] J. Wen, J. Kang, D. Niyato, Y. Zhang, and S. Mao, “Sustainable diffusion-based incentive mechanism for generative AI-driven digital twins in industrial cyber-physical systems,” IEEE Transactions on In-dustrial Cyber-Physical Systems , vol. 3, pp. 139–149, 2025. [30] J. Wang, H. Xu, H. Jia, X. Zhang, M. Yan, W. Shen, J. Zhang, F. Huang, and J. Sang, “Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration,” 2024. [Online]. Available: https://arxiv.org/abs/2406.01014 [31] L. Zhang, T. Zhao, H. Ying, Y. Ma, and K. Lee, “Omagent: A multi-modal agent framework for complex video understanding with task divide-and-conquer,” 2024. [Online]. Available: https: //arxiv.org/abs/2406.16620 [32] Z. Xu, D. Li, W. Liang, W. Xu, Q. Xia, P. Zhou, O. F. Rana, and H. Li, “Energy or accuracy? Near-optimal user selection and aggregator placement for federated learning in MEC,” IEEE Transactions on Mobile Computing , vol. 23, no. 3, pp. 2470–2485, 2024. [33] C. Ding, J.-B. Wang, H. Zhang, M. Lin, and G. Y. Li, “Joint MIMO pre-coding and computation resource allocation for dual-function radar and communication systems with mobile edge computing,” IEEE Journal on Selected Areas in Communications , vol. 40, no. 7, pp. 2085–2102, 2022. [34] Z. Wang, J. J. Hunt, and M. Zhou, “Diffusion policies as an expressive policy class for offline reinforcement learning,” 2023. [Online]. Available: https://arxiv.org/abs/2208.06193 [35] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” in Advances in Neural Information Processing Systems ,H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 6840–6851. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/ 2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf [36] H. Du, Z. Li, D. Niyato, J. Kang, Z. Xiong, H. Huang, and S. Mao, “Diffusion-based reinforcement learning for edge-enabled AI-generated content services,” IEEE Transactions on Mobile Computing , vol. 23, no. 9, pp. 8902–8918, 2024. [37] J. Kang, Y. Zhong, M. Xu, J. Nie, J. Wen, H. Du, D. Ye, X. Huang, D. Niyato, and S. Xie, “Tiny multiagent DRL for twins migration in UAV metaverses: A multileader multifollower Stackelberg game approach,” 

IEEE Internet of Things Journal , vol. 11, no. 12, pp. 21 021–21 036, 2024.
