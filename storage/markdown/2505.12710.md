Title: Confidence-Regulated Generative Diffusion Models for Reliable AI Agent Migration in Vehicular Metaverses

URL Source: http://arxiv.org/pdf/2505.12710v1

Published Time: Sat, 14 Jun 2025 00:06:48 GMT

Markdown Content:
> arXiv:2505.12710v1 [cs.LG] 19 May 2025
> 1

# Confidence-Regulated Generative Diffusion Models for Reliable AI Agent Migration in Vehicular Metaverses 

Yingkai Kang, Jiawen Kang, Jinbo Wen, Tao Zhang, Zhaohui Yang, Dusit Niyato, Fellow, IEEE , and Yan Zhang, Fellow, IEEE 

Abstract —Vehicular metaverses are an emerging paradigm that merges intelligent transportation systems with virtual spaces, leveraging advanced digital twin and Artificial Intelligence (AI) technologies to seamlessly integrate vehicles, users, and digi-tal environments. In this paradigm, vehicular AI agents are endowed with environment perception, decision-making, and action execution capabilities, enabling real-time processing and analysis of multi-modal data to provide users with customized interactive services. Since vehicular AI agents require substantial resources for real-time decision-making, given vehicle mobility and network dynamics conditions, the AI agents are deployed in RoadSide Units (RSUs) with sufficient resources and dynamically migrated among them. However, AI agent migration requires frequent data exchanges, which may expose vehicular metaverses to potential cyber attacks. To this end, we propose a reliable vehicular AI agent migration framework, achieving reliable dynamic migration and efficient resource scheduling through cooperation between vehicles and RSUs. Additionally, we design a trust evaluation model based on the theory of planned behavior to dynamically quantify the reputation of RSUs, thereby better accommodating the personalized trust preferences of users. We then model the vehicular AI agent migration process as apartially observable markov decision process and develop aConfidence-regulated Generative Diffusion Model (CGDM) to efficiently generate AI agent migration decisions. Numerical results demonstrate that the CGDM algorithm significantly outperforms baseline methods in reducing system latency and enhancing robustness against cyber attacks. 

Index Terms —Vehicular metaverses, vehicular AI agents, ser-vice migration, reputations, generative diffusion models, deep reinforcement learning. 

I. I NTRODUCTION 

Metaverses are expected to build a unified network that seamlessly integrates physical and virtual spaces [1]. The rapid 

Y. Kang and J. Kang are with the School of Automation, Guang-dong University of Technology, Guangzhou 510006, China (e-mails: 3122000883@mail2.gdut.edu.cn; kavinkang@gdut.edu.cn). J. Wen is with the College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China (e-mail: jinbo1608@nuaa.edu.cn). T. Zhang is with the School of Cyberspace Science and Technology, Beijing Jiaotong University, Beijing 100044, China (e-mail: taozh@bjtu.edu.cn). Z. Zhang is with the College of Information Science and Electronic Engineering, Zhejiang Provincial Key Lab of information processing, com-munication and networking, Zhejiang University, Hangzhou 310007, China (e-mail: yang zhaohui@zju.edu.cn). D. Niyato is with the College of Computing and Data Science, Nanyang Technological University, Singapore (e-mail: dniyato@ntu.edu.sg). Y. Zhang is with the Department of Informatics, University of Oslo, Norway, and also with the Simula Research Laboratory, Norway (e-mail: yanzhang@ieee.org). 

Corresponding author: Jiawen Kang. 

development of key technologies, including Extended Reality (XR), Digital Twin (DT), and generative Artificial Intelligence (AI), has provided a robust technical foundation for enabling immersive real-time interactions [2]. Vehicular metaverses are emerging as a new paradigm that integrates Intelligent Transportation Systems (ITSs) with metaverses [3]. By incor-porating technologies such as Augmented Virtual (AR) head-up displays, DT systems, and personalized content recom-mendations, the vehicular metaverse expands real-time virtual interactions of vehicles in the physical environment, providing users with immersive experiences that go beyond traditional ITSs. In vehicular metaverses, AI agents are regarded as key enablers [4], [5]. Vehicles are mapped to vehicular AI agents, comprising perception, brain, and action modules. These AI agents effectively integrate and reason about multi-modal data through Large Language Models (LLMs) and Visual Language Models (VLMs) [6], [7], providing users with immersive meta-verse services. Specifically, vehicular AI agents continuously collect multi-modal data from in-vehicle and external envi-ronments through onboard sensors and perception modules. The collected data is converted by the perception module into a format that LLMs and VLMs can understand and then is conveyed to the brain module, where comprehensive decisions are generated through reasoning, planning, and memory [6]. Subsequently, the action module executes these decisions, thereby providing users with immersive services (e.g., AR navigation and 3D virtual meeting rooms) [6]. Due to the requirement for vehicular AI agents to collect multi-modal data and perform reasoning in real-time, the resulting computation-intensive tasks require substantial com-puting resources. The local computing power of vehicles is in-sufficient to meet these real-time processing requirements [1]. Consequently, deploying AI agents on RoadSide Units (RSUs) with large-scale computing resources has been proposed as a promising solution [5], [8]. Given that the vehicular network environment is constantly changing as vehicles move at high speeds while the load and channel conditions of RSUs are highly dynamic [9], AI agents need to be migrated among RSUs in real time to maintain vehicular service continuity and low latency. Moreover, each RSU typically needs to handle multiple vehicle requests concurrently. Therefore, it is essential to optimize resource allocation and task scheduling for AI agent migration. During the AI agent migration process, frequent data ex-changes between vehicles and RSUs make vehicular meta-2

verses highly susceptible to cyber attacks. For example, attack-ers may launch Distributed Denial of Service (DDoS) attacks against specific RSUs, thereby depleting their communication and computing resources [10]. This can disrupt normal data transmission and task scheduling at the targeted RSUs, ulti-mately exposing vehicular AI agent migration to significant latency or even failure. The Moving Target Defense (MTD) strategy is proposed to reduce the probability of successful attacks. By randomizing network topology and reconfiguring addresses, this defense forces the attacker to repeat reconnais-sance, thereby shortening the attack window and raising the attack cost [11]. However, the MTD strategy cannot cover all attack windows, and RSUs remain exposed to security risks [12]. To identify compromised RSUs, trust mechanisms have received significant attention [13]–[16], which quantify the credibility of RSUs through trust evaluation models. The widely used subjective logic model dynamically updates RSU reputation values based on user feedback but fails to fully consider interference from malicious evaluators [16]. Some recent studies have proposed multi-level trust evaluation strate-gies that integrate multi-source trust evidence to enhance the reliability of trust evaluations [13], [14]. Although these meth-ods consider both objective metrics and subjective feedback, they are still insufficient in capturing user preferences and environment differences. To address the above challenges, we propose a reliable ve-hicular AI agent migration framework in vehicular metaverses. Specifically, we design a trust evaluation model based on the Theory of Planned Behavior (TPB) to dynamically compute the reputation values of RSUs. During the vehicular AI agent migration, both vehicles and RSUs jointly formulate decisions based on vehicle mobility, task requirements, and the reputa-tion values of RSUs, ensuring low-latency and high-reliability services in a dynamic network. Given that the AI agent migration problem is NP-hard [8], traditional optimization algorithms (e.g., heuristic algorithms) are insufficient to meet the real-time requirements in migration scenarios due to their limited efficiency. Fortunately, Generative Diffusion Models (GDMs) for Deep Reinforcement Learning (DRL) have shown great potential in network optimization [17]–[19]. However, most diffusion policies in DRL face policy update instability due to the high variance of policy gradients [20]. To overcome this, we innovatively propose a Confidence-regulated GDM (CGDM) algorithm that enhances the final convergence per-formance while ensuring stable policy updates. Our main contributions are summarized as follows:  

> •

We design a highly reliable AI agent migration frame-work in vehicular metaverses, ensuring the low-latency and high-reliability requirements of AI agent migration. In this framework, we propose a TPB-based trust evalu-ation model to ensure the security of vehicular AI agent migration while meeting diverse user trust preferences. Moreover, we introduce bilateral collaboration between vehicles and RSUs in AI agent migration for the first time, which has not been considered in previous work.  

> •

In the TPB-based trust evaluation model, by quantifying user attitudes, subjective norms, and perceived behavioral control, we can model the multi-dimensional behavioral intentions underlying the selection of RSUs, thereby providing a basis for AI agents to formulate reliable and personalized AI agent migration decisions in complex vehicular network environments.  

> •

To achieve high reliability and low latency in vehic-ular AI agent migration, we formulate the task as a minimum-latency optimization problem subject to re-source and security constraints. We model this prob-lem as a Partially Observable Markov Decision Process (POMDP) and propose the CGDM algorithm to effi-ciently generate optimal AI agent migration decisions.  

> •

In the CGDM algorithm, we introduce a denoising con-sistency term in the actor objective function to constrain the smoothness of the generated action distribution. we use it as a confidence measure to adaptively balance Q-value feedback with denoising loss during policy updates. Numerical results show that the CGDM algorithm signif-icantly outperforms baseline methods in terms of both stability and performance. The rest of the paper is organized as follows. Section II sum-marizes the related work. Section III introduces the proposed framework and formulates it as an optimization problem. In Section IV, we model the migration process as a POMDP and introduce the CGDM algorithm. Section V presents numerical results, and Section VI concludes this paper. II. R ELATED WORK 

A. Vehicular Metaverses 

The metaverse is an immersive ecosystem that integrates 3D virtual spaces and the real world [21], providing new application scenarios and exploration directions for various fields, such as ITSs, emerging the concept of vehicular metaverses [22]. The vehicular metaverse is not limited to a platform for virtual entertainment. Leveraging AI agents empowered by LLMs and VLMs, the vehicular metaverse is expected to advance ITSs toward a new paradigm of au-tonomous decision-making and real-time interaction. These AI agents are defined as digital entities that integrate perception, brain, and action modules [6], enabling them to perceive their environment, make decisions, and execute actions to achieve predefined goals. In [23], vehicles are mapped to AI agents, synchronizing their physical and digital counterparts in real time. Consequently, vehicles evolve from mere transportation tools into intelligent decision-making entities capable of self-learning and interaction with environments. However, deploy-ing AI agents to the vehicular metaverse also faces challenges in real-time data processing, dynamic environment adaptation, and privacy security. In [24], a multi-modal semantic percep-tion framework was introduced that significantly reduced data transmission and latency in vehicular networks by extracting skeletal features and semantic information from images. Fur-thermore, in [25], the focus was on the security of AI agent migration, where the authors modeled network attacks against RSUs and proposed an online AI agent migration framework based on DRL and trust evaluation mechanisms, effectively defending against DDoS and malicious RSU attacks to achieve high-security AI agent migration in the vehicular metaverse. 3

B. Service Migration 

Given that advanced AI systems (e.g., DTs or AI agents) are hosted on RSUs, when a vehicle leaves the coverage area of its current RSU, its AI systems need to be migrated to the next adjacent RSU to maintain continuous service and low latency. Existing works optimize service migration by proposing various evaluation metrics and methods, such as migration task freshness and latency [8], [26], and balance migration latency with resource overhead by adopting methods such as contract models, dynamic resource scheduling, game theory, and DRL [27]–[29]. In [9], the authors proposed a pre-migration strategy in which, before the vehicle leaves the coverage area of its current RSU, some or all of its service instances are replicated to a neighboring RSU. By predicting vehicle trajectories using long short-term memory and optimizing the pre-migration strategy with multi-agent DRL, the service migration latency was effectively reduced. However, existing works focus on making decisions from the perspective of vehicles or RSUs, ignoring the importance of bilateral collaboration in service migration. Furthermore, frequent service migration in the vehicular metaverse brings potential security threats. In [30], the author proposed a defense scheme based on dual pseudonyms and synchronous replacement for privacy attacks during the DT migration process, and combined blockchain and inventory theory to optimize pseudonym management, which signifi-cantly improved the effectiveness of privacy protection. To ensure large-scale and highly reliable service migration in the vehicular metaverse, the authors in [16] proposed a trust evaluation mechanism based on the subjective logic model, which effectively identified malicious RSUs. Although exist-ing works have quantified the trustworthiness of RSUs from multiple perspectives, they overlook the individual differences in trust preference toward RSUs among vehicular users. 

C. Diffusion Models for Deep Reinforcement Learning 

Recently, diffusion models have received significant atten-tion in the field of DRL. In the domain of offline RL, a policy optimization method based on the diffusion model has shown obvious advantages. The diffusion-based Q-Learning (QL) algorithm proposed by the authors in [31] effectively alleviates the issue of policy distribution drift from the training data by integrating diffusion models with QL, achieving leading per-formance on the D4RL benchmark tasks. The diffusion Actor-Critic (AC) algorithm proposed in [32] further optimizes the integration between diffusion models and the AC architecture, making the policy optimization more efficient and stable. In the domain of online RL, the application of diffusion models has also gradually attracted attention. The DIffusion POlicy (DIPO) algorithm proposed by the authors in [33] first employed diffusion models for online policy optimization. By integrating action gradient updates with the enhanced representation capabilities of diffusion models for action distri-butions, DIPO outperforms traditional Gaussian-based policy methods. Subsequently, the authors in [34] proposed the Q-weighted variational policy optimization algorithm. By de-signing a diffusion loss function based on Q-value weighting, the diffusion model can effectively integrate value information during online policy optimization and achieve better decision-making performance. In [35], the authors integrated diffusion policies with the AC architecture and addressed the difficulty of computing policy entropy through an entropy-regulation mechanism. However, the existing methods that combine dif-fusion models with online RL generally face the problem of unstable policy updates, which still needs to be solved. III. R ELIABLE VEHICULAR AI A GENT MIGRATION 

FRAMEWORK 

In this section, we first introduce how to migrate AI agents in the vehicular metaverse. Then, we propose a TPB-based trust evaluation model to address security threats during the migration process. To ensure low-latency and highly-reliable AI agent migration, we formulate this task as an optimization problem. The key notations in the paper are listed in Table I. 

A. Vehicular AI Agent Migration Model 

We consider a vehicular metaverse consisting of multiple vehicles and multiple RSUs. As shown in Fig. 1, each vehicle deploys its AI agent to an RSU and continuously uploads environmental data and user requests to the AI agent during driving. The AI agent processes these multi-modal data in real time to generate personalized vehicular metaverse services, which are then downloaded and executed by the vehicle. To ensure an immersive experience, the data required to construct the AI agent is pre-migrated to the next target RSU (i.e., the pre-migration process). The vehicle set is denoted as V = {1, . . . , v, . . . , V }, the RSU set is denoted as Z = {1, . . . , s, . . . , S }, and time is divided into discrete time slots T = {1, . . . , t, . . . , T } [36]. Initially, we analyze the latency incurred when an AI agent is deployed on an RSU. Due to the continuous movement of vehicles, the Euclidean distance between the vehicle and the RSU changes dynamically. Specifically, the position of vehicle 

v at time slot t is denoted as (xv (t), y v (t)) , and the fixed location of RSU s is denoted as (xs, y s). Thus, the Euclidean distance between vehicle v and RSU s can be calculated as Pv,s (t) = p[xv (t) − xs]2 + [ yv (t) − ys]2. Furthermore, the wireless communication channel between vehicles and RSUs is dynamically changing with distance. Considering uniform characteristics for both uplink and downlink wireless transmission channels [8], the path-loss channel gain between vehicle v and RSU s can be calculated as 

gv,s (t) = Γ 

 c

4πf P v,s (t)

2

, (1) where Γ denotes the channel gain coefficient, c is the speed of light, and f is the carrier frequency. When a vehicle enters the coverage area of an RSU, if it lacks a copy of the corresponding AI agent, the vehicle needs to upload the data required to construct its AI agent (e.g., model parameters and status information). To mitigate interference from other vehicles transmitting data, we employ orthogonal frequency division multiple access to allocate orthogonal subcarriers to different vehicles [37]. According to the Shannon-Hartley 4Vehicular AI Agent Migration Process     

> Upload the AI agent data Deploy the AI agent Upload multi-modal data Process computing tasks Execute the MTD strategy Download task results Pre-migrate the AI agent data
> Vehicle RSU MTD strategy Deployment progress Attacker Multi-moal data AI agent data Vehicle trajectory Wireless link Physical link Launch attack Communi-cation link AI agent
> Depoly ing MTD
> Depoly ing
> MTD
> 1
> 2
> 4
> 3
> 5
> 6
> 7
> AI Agent AI Agent AI Agent AI Agent
> 1
> 2
> 4
> 3
> 5
> 6
> 7
> 3

Fig. 1: A reliable vehicular AI agent migration framework in vehicular metaverses. When a vehicle enters the coverage area of an RSU that does not hold a replica of its AI agent, the vehicle uploads the data required to construct the AI agent. Then, the RSU deploys the AI agent and allocates the computing resources. While driving, the vehicle continuously uploads multi-modal data. After performing environment perception, decision-making, and action execution, the AI agent returns the resulting metaverse services to the vehicle. To ensure an immersive experience for users as vehicles move in real time, replicas of the AI agent are proactively pre-migrated to the next appropriate RSU. Moreover, the RSU can actively implement MTD strategies that dynamically reconfigure network addresses and topology, resulting in a brief link reestablishment. 

formula [38], the uplink transmission rate from vehicle v to RSU s can be calculated as 

Rup v,s (t) = Bup log 2



1 + hv gv,s (t)

N 2

> s



, (2) where Bup is the uplink bandwidth, hv is the transmission power of vehicle v, and N 2 

> s

is the additive Gaussian white noise at RSU s. Thus, the latency for uploading the data Dcon v

required to construct the AI agent can be calculated as 

T uc v,s (t) = Dcon v (t)

Rup v,s (t) . (3) Upon receiving the data required to construct the AI agent, the RSU s performs the deployment operations. These include loading the corresponding model, synchronizing the state, and allocating computing resources to ensure that the AI agent can promptly respond when a user initiates a service request. The latency caused by deploying the AI agent for vehicle v can be calculated as 

T dep v,s (t) = dsDcon v (t)

αv,s (t)Cs

, (4) where αv,s (t) ∈ (0 , 1) is the ratio of computing resources allocated by RSU s to the AI agent corresponding to vehicle v,

ds and Cs denote the number of CPU cycles required by RSU 

s to process one unit of data and its CPU speed, respectively. To realize immersive vehicular metaverse services, vehicle v

continuously collects environmental information (e.g., real-time images and radar data) and user interaction requirements (e.g., AR navigation requests) during driving, and uploads these multi-modal data to the AI agent deployed on RSU s.The resulting uplink latency can be calculated as 

T um v,s (t) = Draw v (t)

Rup v,s (t) , (5) where Draw v (t) represents the size of raw multi-modal data collected by the vehicle v at time slot t. After the AI agent receives the uploaded multi-modal data, it first performs data preprocessing, feature extraction, and environment perception through the perception module. Then the brain module gener-ates decisions based on these processed data, e.g., optimizing the navigation route or producing virtual interactive content. Subsequently, the action module executes tasks according to 5

these decisions, e.g., computing the optimal driving route or rendering AR virtual scenes. The computing load incurred by the AI agent during environment perception, decision-making, and action execution is represented as Dcom v , and its processing latency can be calculated as 

T pro v,s (t) = dsDcom v (t)

αv,s (t)Cs

. (6) After the AI agent completes its computing task, the gener-ated results, such as navigation instructions or rendered frames, are transmitted back to the vehicle through the wireless link between the RSU and the vehicle, thereby enabling users to ex-perience the metaverse services through XR devices. Similar to the uplink rate, the downlink rate between the vehicle v and the RSU s is calculated as Rdown v,s (t) = Bdown log 2[1 + pv gv,s (t) 

> N2
> s

].Thus, the latency of the vehicle v downloading the result data 

Dres v can be calculated as 

T down v,s (t) = Dres v (t)

Rdown v,s (t) . (7) Due to the limited coverage area of a single RSU, before vehicle v leaves the coverage area of the current RSU s,the data required to construct the AI agent should be pre-migrated to the next appropriate RSU sp, thus avoiding the long latency caused by repeatedly uploading large volumes of data to recreate a AI agent replica. When the vehicle enters the coverage of RSU sp, RSU sp can directly deploy the AI agent for vehicle v, ensuring service continuity. The latency caused by the pre-migration process can be calculated as 

T mig s,s p (t) = Dcon v (t)

Bs,s p

, (8) where Bs,s p is the physical link bandwidth between RSU s

and RSU sp. Considering potential security threats to RSUs, such as attackers launching DDoS attacks to interrupt data exchanges between RSUs and vehicles [39], an RSU may opt to implement MTD strategies [40]. These strategies include dynamically updating the network topology, reconfiguring network addresses, or executing secure handshakes and key updates, all designed to actively disrupt the ability of attackers to identify and track the network environment, thereby reduc-ing the security risk. However, when implementing the MTD strategy, RSU s need to redeploy its internal configurations and reestablish communication links [10], which causes a brief latency in the communication between vehicles and RSUs, consequently introducing an additional latency T ext v,s in the vehicular AI agent migration. In summary, to provide users with immersive metaverse services, vehicle v first uploads the data required to construct its AI agent to RSU s and completes the deployment, which incurs uplink latency T uc v,s (t) and deployment latency T dep v,s (t).Then, vehicle v continuously transmits the collected multi-modal data, and the AI agent performs data preprocessing, fea-ture extraction, and decision generation, as well as executing corresponding tasks, resulting in uplink latency T um v,s (t) and processing latency T pro v,s (t). Simultaneously, to ensure service continuity, the AI agent proactively pre-migrates a replica of itself to the next target RSU sp, resulting in migration                                                          

> TABLE I: Key Notations in the Paper
> Parameters Descriptions
> Au,s The attitude factor of user uto RSU s
> Bu,s Reputation value of RSU sfor user uBs,s pPhysical link bandwidth between RSU sand RSU sp
> CsCPU speed of RSU s
> MsMigration efficiency of RSU s
> Pu,s The perceived behavioral control factor of user uto RSU s
> RsData transmission reliability of RSU sRup v,s , R down v,s Uplink and downlink transmission rates between vehicle vand RSU s, respectively
> Su,s The subjective norm factor of user uto RSU sTuc v,s , T dep v,s Latency caused by uploading and deploying AI agent, respectively
> Tum v,s Latency caused by uploading multi-modal data
> Tpro v,s Latency caused by processing computing tasks
> Tdown v,s Latency caused by downloading task results
> Tmig v,s Latency caused by pre-migrating AI agent
> Text v,s Latency caused by RSU sexecuting MTD strategies
> Ttot vTotal latency for the vehicular AI agent migration
> U,ESet of vehicular metaverse users and their interaction evaluations, respectively
> V,S,TSet of vehicles, RSUs and time slots, respectively
> ϑu,s The probability that user ugives a positive evaluation to RSU s

latency T mig s,s p . After the computing task is completed, the results generated by the AI agent are transmitted back to vehicle v through the downlink, resulting in a downlink latency 

T down v,s (t). Additionally, RSU s can choose to execute MTD strategy to enhance security, which introduces an extra latency 

T down v,s (t). Therefore, the total latency for the vehicular AI agent migration can be calculated as 

T tot v (t) = (1 − εs)( T uc v,s (t) + T dep v,s (t)) + T um v,s (t) + T down v,s (t)+ max 

n

T pro v,s (t), T mig s,s p (t)

o

+ βs(t)T ext v,s (t),

(9) where εs ∈ { 0, 1} indicates whether RSU s has a copy of the AI agent corresponding to vehicle v. Specifically, εs = 1 

if the RSU s has the AI agent copy, and εs = 0 otherwise. Additionally, βs(t) ∈ { 0, 1} indicates whether RSU s executes the MTD strategy at time slot t, where βs(t) = 1 indicates execution of the MTD strategy, and βs(t) = 0 otherwise. 

B. TPB-based Trust Evaluation Model 

To address the security threats caused by cyber attacks during vehicular AI agent migration, RSUs can proactively implement MTD strategies to reduce the possibility of cyber attacks. However, relying solely on proactive defenses at the RSU side is insufficient to eliminate all potential threats. Once an RSU fails to execute the MTD strategy in time and becomes compromised, vehicles that continue to migrate their AI agents to that RSU will face significant security risks. Therefore, it is necessary for vehicles to conduct trust evaluations of RSUs to identify those that have been compromised or have potential risks in real time. Considering that existing trust evaluation models fail to account for differences in user preferences regarding security 6

risks and service quality [41], and lack a comprehensive characterization of multi-source trust variability, we propose a TPB-based trust evaluation model. This model not only objectively quantifies RSU security but also considers the differences in trust preferences among vehicle occupants (i.e., vehicular metaverse users), thereby providing highly reliable and personalized trust evaluation results for vehicular AI agent migration decisions. According to the TPB [42], the intention of an individual to perform a certain behavior is mainly determined by the attitude A, the subjective norm S, and the perceived behavioral control P. Specifically, A reflects the positive or negative evaluation of the behavior itself, S reflects the social pressure and support perceived during the decision-making process, and 

P reflects the perception of the ease or difficulty of performing the behavior. We define the vehicular metaverse user set as 

U = {1, . . . , u, . . . , U }, which corresponds one-to-one to the vehicle set V = {1, . . . , v, . . . , V }. Moreover, we define the reputation value Bu,s of RSU s, indicating the behavioral intention of user u to migrate its AI agent to RSU s.

1) Attitude A

The attitude factor is the subjective preference of users towards an RSU, measured by their historical interaction evaluation. Let E = enu,s |u ∈ U , s ∈ Z, n = 1 , . . . , n u,s 

represent the interaction evaluation set between users and RSUs, where nu,s is the total number of interactions between user u and RSU s, and enu,s ∈ { 0, 1} represents the n-th evaluation to RSU s by user u. Here, enu,s = 1 indicates a positive evaluation, whereas enu,s = 0 indicates a negative evaluation. Since these evaluations are binary, they can be modeled using a Bernoulli distribution with parameters ϑu,s ,i.e., enu,s ∼ Ber( ϑu,s ), where ϑu,s denotes the probability that user u gives a positive evaluation. To estimate ϑu,s 

with limited evaluation data, let the Beta distribution as the prior [43], which can be expressed as 

ϑu,s ∼ Beta( αA, β A), (10) where αA, β A > 0 are adjustable prior hyperparameters of ϑu,s . Owing to the conjugate relationship between the Bernoulli and Beta distributions, after obtaining the interactive evaluation set Eu,s between user u and RSU s, the posterior distribution of ϑu,s remains a Beta distribution, which can be expressed as 

ϑu,s |E u,s ∼ Beta( αA + pu,s , β A + qu,s ), (11) where pu,s = Pnu,s  

> n=1

enu,s and qu,s = nu,s − pu,s are the total numbers of positive and negative evaluations by user u for RSU s, respectively. Taking the expectation of the posterior distribution of ϑu,s as the subjective favorability of user u to RSU s, the attitude factor can be calculated as 

Au,s = E[ϑu,s |E u,s ] = αA + pu,s 

αA + βA + pu,s + qu,s 

. (12) 

2) Subjective Norm S

The subjective norm factor describes the influence of the ex-ternal environment on user behavior, reflecting the guiding role of group consensus on user trust judgment. It is measured by the overall evaluation of other users on RSU s. Similar to the quantification of attitude factor, we also use a Beta distribution as the prior distribution for the probability ϑ¬u,s that other users give positive evaluations, i.e., ϑ¬u,s ∼ Beta( αS , β S ),where αS , β S > 0 are prior hyperparameters of ϑ¬u,s . After obtaining the interaction evaluation set E¬u,s = {enu′,s |u′ ∈

(U\{ u}), n = 1 , . . . , n u′,s } by other users for RSU s, and leveraging the conjugacy between Beta and Bernoulli distri-bution, the posterior distribution of ϑ¬u,s is given by 

ϑ¬u,s |E ¬u,s ∼ Beta( αS + p¬u,s , β S + q¬u,s ), (13) where p¬u,s = P

> u′∈(U\{ u})

Pnu′,s  

> n=1

enu′,s and q¬u,s =P 

> u′∈(U\{ u})

nu′,s −p¬u,s are the total number of positive and negative evaluations on RSU s by other users, respectively. Therefore, the subjective norm factor is quantified as the expectation of the posterior distribution of ϑ¬u,s . By updating 

p¬u,s and q¬u,s to reflect the trust changes of other users on RSU s, the subjective norm factor can be calculated as 

Su,s = E[ϑ¬u,s |E ¬u,s ] = αS + p¬u,s 

αS + βS + p¬u,s + q¬u,s 

. (14) 

3) Perceived Behavioral Control P

The perceived behavioral control factor describes the per-ception of users regarding whether RSUs can complete the AI agent migration tasks smoothly and quickly. This factor is quantified by evaluating the reliability and efficiency of RSUs during vehicular AI agent migration. To evaluate the data transmission reliability of RSU s, we use the packet statistics recorded in the beacon messages periodically broadcast by RSU s and vehicles during data transmission. Specifically, let Dsand Df represent the numbers of packets successfully and unsuccessfully received by RSU s, reflecting its ability to obtain multi-modal raw data from vehicles. Let Fs and Ff

represent the numbers of packets successfully and unsuccess-fully forwarded by RSU s, indicating its capacity to deliver processed results to vehicles. Based on the above beacon messages, the packet delivery rate RDs and the forwarding rate RFs of RSU s can be calculated as 

RDs = Ds − Df

Ds + Df

, RFs = Fs − Ff

Fs + Ff

. (15) Since the data exchange between the RSU and vehicles is bidirectional, both the uplink and downlink transmission processes must be considered. Therefore, the overall data transmission reliability of RSU s is defined as 

Rs = [ λRDs + (1 − λ)RFs ], (16) where λ ∈ (0 , 1) is the weight coefficient that adjusts the relative importance of RDs and RFs . Next, to evaluate the effi-ciency of RSU s during the vehicular AI agent migration, we consider the set of total migration latency Ls = {T tot j (t)|j ∈Vs(t), t ∈ (T ′ − τ, T ′)} associated with RSU s, where the set of vehicles whose AI agents are deployed on RSU s at time slot t is denoted by Vs(t), T ′ is the current time, and τ is the length of the time window. The average total migration latency at RSU s in this time window can be calculated as 

T tot s = 1

|L s|

X  

> t∈(T′−τ,T ′)

X

> j∈V s(t)

T tot j (t) . (17) 7

To reflect the personalized requirements of different users for migration efficiency, we introduce the maximum latency that user u can accept, i.e., tolerable latency T max u , and define the migration efficiency of RSU s as 

Ms = max 

(

0, 1 − T tot 

> s

T max 

> u

)

. (18) When the average total migration latency T tot s is signifi-cantly lower than the tolerable latency T max u , Ms approaches 

1, indicating that the migration efficiency of RSU s fully satisfies the latency requirement of user u. As the average migration latency T tot s gradually approaches the tolerable latency T max u , the satisfaction of user u with the migration efficiency of RSU s decreases. Once the average migration latency T tot s exceeds the tolerable latency T max u , Ms drops to 0, indicating that the migration efficiency of RSU s is unacceptable for user u. Finally, by taking a weighted sum of data transmission reliability Rs and migration efficiency 

Ms, the perceived behavioral control factor is given by 

Pu,s = σRs + (1 − σ)Ms, (19) where σ ∈ (0 , 1) is used to adjust the relative importance between Rs and Ms.

4) Behavioral Intention as Reputation Value 

After obtaining the attitude factor Au,s , the subjective norm factor Su,s , and the perceived behavior control factor Pu,s , the behavioral intention of user u to migrate the AI agent to RSU 

s can be quantified according to TPB, i.e., the reputation value of RSU s for user u, which is calculated as 

Bu,s = ζAAu,s + ζS Su,s + ζP Pu,s , (20) where ζA, ζ S , ζ P ∈ (0 , 1) satisfy ζA + ζS + ζP = 1 . These weights can be allocated based on the relative importance of 

Au,s , Su,s , and Pu,s .Since the state of RSU s is dynamically changing, a single trust evaluation is often insufficient to reflect its long-term stability. Therefore, the reputation value Bu,s need to be updated smoothly to integrate both historical evaluations and the latest data. Let Bnear u,s denote the instantaneous reputation value obtained by evaluating the latest data, and let Bold u,s 

denote the reputation value from the previous time slot. Then, the updated reputation value at the current moment can be calculated as 

Bnew u,s = ξBnear u,s + (1 − ξ)Bold u,s , (21) where ξ ∈ (0 , 1) controls the update rate. 

C. Problem Formulation 

To achieve low latency and high reliability of vehicular AI agent migration while optimizing the decision variable A

under the constraints of limited RSU resources and security requirements, the optimization problem is formulated as 

min 

> A
> T

X

> t=1
> V

X

> v=1

T tot v (t) (22a) 

s.t.

> |V s(t)|

X

> j=1

αj,s (t) ≤ 1, ∀s ∈ Z, ∀t ∈ T , (22b) 

Ls(t) ≤ Lmax s , ∀s ∈ Z, ∀t ∈ T , (22c) 

Bv,s p (t) ≥ B thre , ∀v ∈ V, ∀t ∈ T . (22d) Constraint (22b) limits the computing resources allocated by each RSU to not exceed its total computing resources. Con-straint (22c) ensures that the load Ls(t) of each RSU is below its maximum load capacity Lmax s . Finally, constraint (22d) requires that the reputation value of the target RSU sp selected for the AI agent in the pre-migration process should exceed a safety threshold Bthre , thereby ensuring reliable vehicular AI agent migration. IV. C ONFIDENCE -R EGULATED GENERATIVE DIFFUSION 

MODELS FOR VEHICULAR AI A GENT MIGRATION 

In this section, we model vehicular AI agent migration as a POMDP and propose a CGDM algorithm that combines DRL and diffusion models to generate optimal decisions. 

A. POMDP Modeling 

For vehicular AI agent migration, the objective is to min-imize the total latency in (22a). Given that the network environment is highly dynamic [18], with migration decisions intricately coupled to RSU load conditions and potential security risks arising from cyber attacks [40], we model the vehicular AI agent migration process as a POMDP as follows: 

1) Observation Space 

The observation space O consists of the environmental information that vehicles and RSUs can obtain in each time slot, which is defined as O(t) = {PV (t), K V (t), L Z(t), B(t)}.Here, PV (t) = {(xv , y v )|v ∈ V} is the position set of all vehicles, reflecting the communication distances and link qualities between vehicles and RSUs. KV (t) = {Kv (t) | v ∈V, K v (t) ∈ { 1, 2, . . . , S }} is the index set of the RSUs on which AI agents are deployed. These indexes are determined by the pre-migration decision of vehicles in the previous time slot and are used to help RSUs to allocate computing resources. LZ(t) = {Ls(t)|s ∈ Z} is the load set of all RSUs, which reflects the current computing burden on each RSU. 

B(t) = {B u,s (t)|u ∈ U , s ∈ Z} is the reputation value set of each RSU for different users, which helps vehicles in pre-migrating their AI agents to reliable RSUs. 

2) Action Space 

The action space is defined as the decision set that the DRL agent makes for all vehicles and RSUs at each time slot, denoted as A = {AV , A Z}. According to Section III, the decisions regarding the selection of a pre-migration RSU for vehicles and the decision on whether an RSU executes the MTD strategy are discrete, while the decision for RSU resource allocation is continuous, resulting in a hybrid action 8

space. To avoid the complexities and instability in policy gradient calculation caused by the hybrid action space [44], we express all decisions as continuous actions and map them to discrete or continuous decisions when executing the decisions. Specifically, the DRL agent makes a pre-migration decision 

AV (t) = {av (t)|v ∈ V} for all vehicles at time slot t, where the action vector av (t) = [av, 1(t), a v, 2(t), . . . , a v,S (t)] ∈

[0 , 1] S of vehicle v is converted into a probability distribution of each RSU as a pre-migration target through a softmax operation, and the RSU with the highest probability is se-lected as the migration target when executing the decision. Concurrently, the DRL agent makes resource allocation and defense strategies AZ(t) = {(αs(t), β s(t)) |s ∈ Z} for all RSUs, where αs(t) = {αj,s (t) | j ∈ V s(t), α j,s (t) ∈ (0 , 1) }

is the computing resource allocation ratio of RSU s to the vehicle set Vs(t) it is currently serving, satisfying the total resource constraint (22b). Moreover, βs(t) ∈ [0 , 1] indicates the probability that RSU s executes the MTD strategy. When executing the decision, βs(t) is a threshold such that if 

βs(t) > 0.5, it is mapped to 1, indicating the execution of the MTD strategy, otherwise, βs(t) is mapped to 0.

3) Transition Function 

The transition function characterizes the probability distri-bution of transitioning from state O(t) to O(t + 1) after taking action A(t). Due to the highly dynamic and uncertain environ-ment, it is difficult to formulate this function mathematically. Specifically, the positions of vehicles are determined by their own motion pattern and change continuously in each time slot, thus influencing the communication link quality between vehicles and RSUs. The load state of RSUs is affected by the data volume uploaded by vehicles, resource allocation strategies, and network transmission conditions. Concurrently, the reputation values of RSUs are dynamically updated based on their performance in data transmission and task processing, as well as user feedback. Additionally, potential cyber attacks may impact the migration process of AI agents, further in-creasing the complexity of state transitions. All these factors collectively determine how the state transitions over time. 

4) Reward Function 

The reward represents the immediate return that the DRL agent obtains by executing action A(t) in state O(t). To minimize the total latency of vehicular AI agent migration, the immediate reward at time slot t is defined as 

R (O (t) , A (t)) = −

> V

X

> v=1

T tot v (t). (23) 

B. Confidence-Regulated Generative Diffusion Models for Generating Optimal Decisions 

AI agent migration problems typically exhibit highly dy-namic and nonlinear characteristics, resulting in a complex, multi-modal decision distribution [17]. However, most DRL algorithms rely on unimodal distributions (e.g., Gaussian dis-tributions) for policy representation [45], which inadequately capture the subtle differences among multiple potential optimal decisions in the environment and are prone to converge to sub-optimal solutions. In contrast, diffusion policies for DRL can capture the multi-modal structure of the decision distribution through a denoising process [31], effectively representing the diversity and complexity of policy distributions. Therefore, we propose the CGDM algorithm for generating optimal AI agent migration decisions. In the following, we first introduce the forward and reverse processes of the diffusion model and then describe the algo-rithmic architecture of CGDM. 

1) Forward Process 

The forward process in the diffusion model refers to the process of gradually adding Gaussian noise to the original data, which is usually modeled as a Markov chain of length 

T . At each time step k = 1 , 2, . . . , K , Gaussian noise is added to the data distribution xk−1 from the previous time step to obtain the data distribution xk. The conditional probability distribution of a single-step transfer is defined as a normal distribution with a mean of √1 − βkxk−1 and a variance of 

βk, expressed as [46] 

q(xk | xk−1) = N



xk; p1 − βkxk−1, β kI



, (24) where βk controls the strength of the noise added at the time step k and is determined by a variational scheduler. Since the forward process satisfies the Markov property, the state of each step depends only on the state of the previous time step. Therefore, the joint probability distribution for transitioning from the original data distribution x0 to the final data distribution xK can be expressed as [46] 

q(x1: K | x0) = 

> K

Y

> k=1

q(xk | xk−1). (25) Based on (25), we can further derive a direct expression for the data distribution at any time step k relative to the original data distribution x0. By defining αk = 1 − βk and the cumulative product ¯αk = Qki=1 αi, we obtain [46] 

xk = √¯αkx0 + √1 − ¯αkϵ, (26) where ϵ ∼ N (0 , I). When K is large enough, xK almost loses the original data distribution structure and is close to an isotropic standard normal distribution N (0 , I). This property facilitates the derivation and formulation of the reverse process in the diffusion model. 

2) Reverse Process 

The objective of the reverse process is to gradually recover samples that approximate the original data distribution from pure noise. Specifically, under the condition of the environ-mental state O, a noise xK ∼ N (0 , I) is first sampled from the standard normal distribution. Subsequently, noise is gradually removed through the reverse process until the final decision 

x0 is recovered. The reverse process can be modeled as a Markov chain comprising K conditional probabilities, with joint distribution expressed as 

pθ (x0: K ) = p(xK )

> K

Y

> k=1

pθ (xk−1 | xk), (27) where the reverse conditional probability pθ (xk−1 | xk) is not directly available but can be approximated by a parameterized model that learns its mean and variance, expressed as 

pθ (xk−1 | xk) = N



xk−1; μθ (xk, k, O), ˜βkI



, (28) 9Critic 1 Critic 2  

> GDM-based Network Critic 1 Critic 2

Double Critic Target Actor Target Critic Vehicular AI Agent Migration Environment Replay Buffer     

> ... ... ... ... ...
> Soft Update

Actor 

> Critic Loss Policy Loss Update Update
> Policy Optimizer
> Q function Optimizer

Optimizer     

> Execute Action Next State Reward
> Vehicle 1 Vehicle V RSU 1 RSU S

... ...       

> Sample a batch of experiences
> Actual Noise Predicted Noise
> , Action State
> Gaussian Noise Optimal Decision  GDM-based Network
> Sampling Use (34) to obtain Denoising Consistency Loss
> Fig. 2: The overall architecture of the CGDM algorithm. The actor network πθgenerates action by performing Kdenoising steps on Gaussian noise. The resulting state–action pairs are then evaluated by the double critic networks Qϕto guide policy optimization. A denoising consistency loss is computed to measure the confidence to adaptively adjust the optimization objective of the actor. All interaction samples are stored in the experience replay buffer for updates of both the actor and critic networks. To ensure stable training, the target networks are softly updated at each iteration.

where the variance ˜βk = 1− ¯αk−1  

> 1−¯αk

βk is set as a fixed time-dependent constant according to the Denoising Diffusion Probabilistic Model (DDPM) [46]. The mean μθ (xk, k, O) is learned through a neural network. By applying the Bayesian formula, the posterior distribution can be obtained as 

q(xk−1 | xk, x 0) = N



xk−1; ˜ μ(xk, x 0, k ), ˜βtI



, (29) where the posterior mean ˜μ(xk, x 0, k ) is computed by 

˜μ(xk, x 0, k ) = 

√¯αk−1βk

1 − ¯αk

x0 +

√αk(1 − ¯αk−1)1 − ¯αk

xk. (30) Since the decision x0 is unknown during inference, we employ a denoising network ϵθ (xk, t, O) to predict the noise contained in xk. By substituting (26) into (30) to obtain an approximate posterior mean, given by 

μθ (xk, k, O) = 1

√αk



xk − βk tanh ( ϵθ (xk, k, O)) 

√1 − ¯αk



, (31) where ϵθ (xk, k, O) is the output of the denoising network, which estimates the magnitude of the Gaussian noise added to xk based on the current environment state O, the noisy decision xk, and the time step k. The hyperbolic tangent activation function is used to limit the output amplitude, thereby avoiding instability in the decision recovery process caused by excessive prediction noise. To enable the gradient of the loss function to back-propagate to the network parameters θ, we apply the reparameteriza-tion trick to sample xk−1 from the conditional distribution 

pθ (xk−1 | xk) as follows [18] 

xk−1 = μθ (xk, k, O) + 

q

˜βkϵ. (32) By sequentially applying (32) from k = K down to k = 1 ,the reverse process can transform pure noise into the optimal AI agent migration decision x0.

3) Algorithm Architecture 

The framework of CGDM includes an actor network πθ , a double critic network Qϕ, a target actor network ˆπˆθ , a target critic network ˆQ ˆϕ, an environment, and an experience replay buffer D, as shown in Fig. 2. In CGDM, the actor πθ generates decisions through the reverse process. Specifically, πθ takes the state O and the noisy action AK as input, and obtains the original action A after gradual denoising. The reverse denoising mechanism ensures the expressiveness of the multi-modal decision structure and avoids the traditional reliance on unimodal distributions in policy networks. However, if the maximization of the value function Qϕ is directly used as the optimization goal, the actor may overfit the local high-value area given by the value function, resulting in the degradation of the multi-modal expression of policy [45]. Moreover, the actor could easily deviate from the prior distribution in high-dimensional action spaces, resulting in unstable policy updates. To this end, we propose an adaptive confidence mechanism based on a diffusion model-driven denoising consistency loss, which dynamically adjusts the optimization objective of the actor. 10 

The denoising consistency loss for the actor at a random time step k ∈ [1 , K ] is computed as 

LBC (θ; O, A) = Ek∼U{ 1,K }∥ϵθ (Ak, k, O) − ϵ∥2, (33) where ϵ ∼ N (0 , I) is the actual noise added, ϵθ is the predicted noise by the enoising network ϵθ , and Ak is the noisy action computed from the original action A according to (26). This loss measures the reliability of the denoising process. Thus, we define the confidence of actor as 

wa(O, A) = exp( −κ LBC (θ; O, A)) , (34) where κ > 0 is a hyperparameter that controls the sensitivity to denoising consistency loss LBC .To maximize the action Q-value while maintaining consis-tency between the noise injection and denoising processes, the confidence in (34) is used to adaptively regulate the weight of the Q-value. When LBC is large, it indicates that the actor has an insufficient grasp of the action prior, causing the confidence 

wa to approach 0. Directly using the Q-value of such actions to update the policy would likely lead to over-exploration and distribution collapse [47]. Therefore, the confidence is used to attenuate the influence of the Q-value on the policy network, while the denoising consistency term enhances the reliability of the denoising process, thereby ensuring policy stability. Conversely, when LBC is small, the confidence wa

approaches 1, and the actor should fully leverage Q-value information to guide updates of the policy network toward generating higher-value actions. The optimization objective of the actor is expressed as 

max  

> θ

EO∼D 

wa

 O, π θ (O)Qϕ

 O, π θ (O) 

− ρE(O,A)∼D [LBC (θ; O, A)] , (35) where the coefficient ρ controls the weight of the behavior clone term. We use the Adam optimizer to optimize the objective function in (35), and the gradient of the actor network parameter θ can be calculated as 

∇θ La(θ) = EO∼D 

h

−∇ θ

 wa(O, π θ (O)) Qϕ(O, π θ (O)) i

+ E(O,A)∼D 

h

∇θ LBC (θ; O, A)

i

.

(36) Subsequently, the actor network parameters are iteratively optimized by gradient descent, and the update speed is con-trolled by the learning rate ηa, which is given by 

θ′ ← θ − ηa · ∇ θ La(θ). (37) To mitigate the overestimation of the value function, we adopt a double critic structure and use Temporal Difference (TD) to optimize the value network parameters ϕ = {ϕ1, ϕ 2}.First, a batch of experiences (O, A, R, O′) is randomly sam-pled from the replay buffer, and the target actor outputs the next action A′ = ˆ πˆθ (O′) based on the next state O′. The TD target ˆy can be expressed as 

ˆy = R + γ min 

n ˆQ ˆϕ1 (O′, A′), ˆQ ˆϕ2 (O′, A′)

o

, (38) 

Algorithm 1: Confidence-Regulated GDMs  

> 1:

Initialize actor network πθ , double critic networks Qϕ,target networks ˆπˆθ ← πθ , ˆQ ˆϕ ← Qϕ, the environment, and an experience replay buffer D; 

> 2:

for each training epoch e = 1 , 2, . . . , E do  

> 3:

for each trajectory m = 1 , 2, . . . , M do  

> 4:

Obtain the current observation O from the environment;  

> 5:

Sample initial noise xT ∼ N (0 , I); 

> 6:

for denoising step k = K down to 1 do  

> 7:

Infer the noise ϵθ (xk, k, O) added to xk using the actor network;  

> 8:

Compute the mean of the reverse distribution 

pθ (xk−1 | xk) using (31);  

> 9:

Sample xk−1 from the reverse process using the reparameterization trick (32);  

> 10:

end for  

> 11:

Process the final output x0 to obtain the action A; 

> 12:

Execute the action A in the environment to receive reward R and next observation O′; 

> 13:

Record the transition (O, A, R, O′) in the replay buffer D; 

> 14:

end for  

> 15:

Sample a batch of transitions from D; 

> 16:

Compute the denoising consistency loss LBC (θ; O, A)

and get the confidence wa(O, A) of actor, as defined in (33) and (34);  

> 17:

Update the actor parameters θ using (37);  

> 18:

Update the critic parameters ϕ by minimizing the loss in (39);  

> 19:

Soft-update target network parameters ˆθ, ˆϕ using (40);  

> 20:

end for  

> 21:

return the optimized actor network πθ ;where γ ∈ (0 , 1) controls the degree to which future rewards are discounted. The value network parameters are optimized by minimizing the mean squared error, which is expressed as 

min  

> ϕ1,ϕ2

E(O,A,R,O′)∼D [ X

> i=1 ,2

(ˆ y − Qϕi (O, A)) 2]. (39) To ensure training stability, the target network parameters of both the actor and critic are updated through a soft update mechanism, which are given by 

ˆθ′ ← τ θ + (1 − τ ) ˆθ, 

ˆϕ′ ← τ ϕ + (1 − τ ) ˆϕ, (40) where τ ∈ (0 , 1) is the soft update coefficient used to prevent drastic fluctuations in the parameters during training. In summary, the CGDM algorithm adaptively regulates the optimization objective in (35) through the confidence mechanism. It actively leverages value information to opti-mize the policy when confidence is high and reinforces the structural priors of the denoising process when confidence is low. Consequently, it enables stable exploration and facilitates near-optimal solutions in high-dimensional action spaces with-11 

out expert data. The pseudocode of the CGDM algorithm is presented in Algorithm 1. 

4) Complexity Analysis 

The computational complexity of the CGDM algorithm consists of two main parts. The first part is the trajectory collection. The algorithm runs for E iterations, and during each iteration, the agent interacts with the environment to collect M trajectories, each with an interaction cost of F ,and every decision is obtained by a reverse denoising process comprising K steps performed by the actor network. The computational cost of each denoising step is linearly related to the size of the network parameters |θ|, resulting in a trajectory collection complexity of O(EM (F + K|θ|)) [18]. The second part is the network parameter update. In each iteration, a batch of N samples is randomly sampled from the replay buffer to update the actor and double critic net-works, with computational complexities of O(EN |θ|) and 

O(EN |ϕ|) [8], respectively. Additionally, the soft updates of target networks incur a complexity of O(E(|θ| + |ϕ|)) .Therefore, the overall computational complexity of the CGDM algorithm is O(E[M (F + K|θ|) + ( N + 1)( |θ| + |ϕ|)]) .V. N UMERICAL RESULTS 

In this section, we comprehensively evaluate the perfor-mance of the CGDM algorithm. We first present the exper-imental configuration, and then we conduct an ablation study on the key components of the CGDM algorithm. We further compare CGDM with several baseline algorithms to analyze its convergence. Finally, we evaluate the performance of CGDM and baseline algorithms under various scenario configurations. 

A. Experimental Settings 

In the simulation experiment, we uniformly deploy four RSUs, which are interconnected through physical links. The vehicles move along their respective driving trajectories, and each vehicle deploys and dynamically migrates the corre-sponding AI agent to adapt to real-time changes in the vehicle positions and network environment. Vehicles establish wireless communication links with the RSUs within their coverage areas to upload multi-modal data and receive computed results in real time. Additionally, attackers are introduced into the environment and launch intermittent DDoS attacks on specific RSUs, causing temporary service disruptions. The key param-eters of the experiment are shown in Table II [8], [9], [18]. 

B. Ablation Study of CGDM Algorithm 

In Fig. 3, we show the convergence performance of the CGDM algorithm when specific components are removed, aiming to explore the role of each component within CGDM. Specifically, we removed the confidence mechanism (i.e., 

κ = 0 ) and the denoising consistency term (i.e., ρ = 0 ), corresponding to the test reward curves labeled CGDM w.o. Con and CGDM w.o. DC, respectively. We also include the Generative Diffusion Model (GDM) for comparison, which also leverages a diffusion model as a policy and has shown excellent performance in network optimization tasks [17].                                  

> TABLE II: Key Parameter Setting
> Parameters Descriptions Values
> VNumber of vehicles 8
> SNumber of RSUs 4
> CZComputing capability range of all RSUs [100 ,300 ] MHz
> Dcon
> VSize range of multi-modal data by all vehicles [100 ,600 ] MB
> BV,SWireless bandwidth between vehicles and RSUs [100 ,300 ] Mbps
> ζA, ζ S, ζ PWeights of each factor of the reputation value
> 0.33
> ξThe update speed of reputation values
> 0.7
> ETotal number of episodes 200
> KDenoising steps 5
> DMaximum capacity of the replay buffer
> 1×10 6
> MBatch size 256
> ηaLearning rate of the actor networks
> 1×10 −4
> ηcLearning rate of the critic networks
> 1×10 −3
> τWeight of soft update 0.005
> γDiscount factor 0.95

From the test reward curves, we observe that when removing the confidence mechanism, the CGDM w.o. Con scheme still achieves relatively fast convergence and stability compared with the GDM-based DRL algorithm. This occurs due to the persistent regularization effect from the denoising consistency term, which constrains the denoising process and prevents the policy from deviating excessively from the diffusion prior when pursuing high-Q-value actions. However, in the absence of confidence regulation, the final solution remains subop-timal. Conversely, removing the denoising consistency term significantly reduces both convergence speed and decision quality. Without the regularization provided by the denoising consistency term, the policy optimization overly prioritizes maximizing action Q-values. The confidence mechanism alone merely scales the Q-value without effectively preventing the policy from falling into suboptimal solutions. In contrast, the CGDM algorithm combines the denoising consistency term with an adaptive confidence mechanism. This combination en-hances policy adherence to the diffusion prior when confidence is low and leverages Q-value guidance when confidence is high. Therefore, the CGDM algorithm ensures stable training process and achieves the best overall performance. Moreover, we analyze the impact of the denoising step 

K on the performance of CGDM in Fig. 4, selecting K ∈{1, 2, 3, 4, 5, 6, 8, 10 , 15 } and normalizing both the maximum test reward and training time. Although training time grows linearly with increasing K, consistent with theoretical com-plexity, performance does not improve monotonically. The maximum reward rapidly increases up to K = 5 and subse-quently declines, with a significant performance drop observed after K = 10 . This is because excessive denoising steps lead to gradient explosion and degrade policy stability [48]. Con-sequently, to balance computational efficiency with optimal performance, we select the turning point of the test reward curve, i.e., denoising step K = 5 , as the default setting for subsequent evaluations. 12 0 50 100 150 200 250     

> Steps
> 650
> 600
> 550
> 500
> 450
> 400
> 350
> 300
> Test Rewards
> ×10 3
> CGDM
> GDM
> CGDM w.o. DC
> CGDM w.o. Con
> Fig. 3: Comparison between CGDM with and without confidence mechanism or denoising consistency term. 0510 15
> Denoising Steps
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Normalized Test Rewards
> Normalized Test Rewards
> Normalized Time 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Normalized Time
> Fig. 4: Comparison of normalized test rewards and training time with different denoising steps.

C. Convergence Analysis 

As shown in Fig. 5, we compare the training convergence performance of our proposed CGDM algorithm with the GDM algorithm, Proximal Policy Optimization (PPO) algorithm, Soft Actor-Critic (SAC) algorithm, Twin Delayed Deep Deter-ministic policy gradient (TD3) algorithm, Deep Deterministic policy gradient (DDPG) algorithm, and a Random policy al-gorithm. From the overall performance, the CGDM algorithm shows superior convergence speed and policy quality, with performance improvements of 5.7 %, 12.3 %, 10.2 %, 34.6 %,39.3 %, and 44.8 % compared with GDM, PPO, SAC, TD3, DDPG, and the Random algorithm, respectively. We can observe that the GDM algorithm exhibits slow performance improvements during the training steps between approximately 5 × 10 4 and 20 × 10 4. This slowdown arises because the optimization objective of GDM solely focuses on maximizing the action Q-value without the constraint provided by denoising consistency loss during the denoising process of the diffusion model. Consequently, the policy tends to become trapped in local regions with relatively high Q-values. Only after approximately 20 × 10 4 training steps does the policy manage to explore beyond these local regions. In con-trast, our proposed CGDM algorithm incorporates an adaptive confidence mechanism based on denoising consistency loss, enabling it to autonomously adjust its optimization objective, thereby quickly and stably converging to a global optimum. Furthermore, CGDM significantly outperforms classical 0 50 100 150 200 250 300 350 400      

> Steps
> 650
> 600
> 550
> 500
> 450
> 400
> 350
> 300
> Test Rewards
> ×10 3
> CGDM
> GDM
> PPO
> SAC
> TD3
> DDPG
> Random
> Fig. 5: Comparison of test reward curves of different algorithms. 100 200 300 400 500 600
> Multi-modal Data Size (MB)
> 200
> 400
> 600
> 800
> 1000
> 1200
> 1400
> 1600
> Latency (s)
> CGDM
> GDM
> SAC
> PPO
> TD3
> DDPG
> Random
> Fig. 6: Total system latency under different sizes of raw multi-modal data uploaded by vehicles.

DRL algorithms. Although the PPO algorithm demonstrates high training stability, its policy distribution is limited to a unimodal Gaussian form, making it challenging to cap-ture the multi-modal characteristics inherent to vehicular AI agent migration decisions in high-dimensional action spaces. Similarly, the SAC algorithm, despite promoting exploration by maximizing policy entropy, also employs a unimodal Gaussian distribution to represent policies, inevitably resulting in convergence to suboptimal solutions. Deterministic policy gradient methods such as TD3 and DDPG exhibit inadequate exploration in decision space, causing them to easily converge to poor-performing local regions in high-dimensional action scenarios. Compared with the above algorithms, CGDM ef-fectively captures the multi-modal characteristics of policy distribution through the denoising mechanism of diffusion models, significantly enhancing decision quality and stability in complex environments. 

D. Performance Evaluation 

To demonstrate the robustness of the CGDM algorithm, we analyze its performance under various system parameters. In Fig. 6, we illustrate the change in the total system latency as the size of the multimodal data uploaded by vehicles increases from 100 MB to 600 MB. The results indicate that the CGDM algorithm consistently achieves the lowest system latency as the data size grows. Compared with GDM, SAC, PPO, TD3, DDPG, and Random algorithms, CGDM 13 100 125 150 175 200 225 250 275 300         

> Wireless Bandwidth (Mbps)
> 400
> 600
> 800
> 1000
> 1200
> Latency (s)
> CGDM
> GDM
> SAC
> PPO
> TD3
> DDPG
> Random
> Fig. 7: Total system latency under different wireless bandwidths. 100 125 150 175 200 225 250 275 300
> Computing Capability (MHz)
> 300
> 400
> 500
> 600
> 700
> 800
> Latency (s)
> CGDM
> GDM
> SAC
> PPO
> TD3
> DDPG
> Random
> Fig. 8: Total system latency under different computing capabilities of RSUs.

reduces latency by 7.6 %, 16.3 %, 19.9 %, 35.6 %, 43.8 %, and 48.7 %, respectively. When data volumes are small, perfor-mance differences among algorithms are similar. However, as data volume increases, performance gaps become more pronounced. This occurs because larger data sizes significantly raise the upload and processing latency between vehicles and RSUs. Owing to its diffusion-based decision approach, the CGDM algorithm effectively optimizes RSU resource alloca-tion, thereby reducing the total system latency and ensuring an immersive user experience. In Fig. 7, we show the optimization effects of different algorithms as wireless bandwidth increases from 100 Mbps to 

300 Mbps. Under limited bandwidth conditions, CGDM effi-ciently allocates RSU’s computing resources, reducing overall system latency. As bandwidth increases, wireless transmission latencies decrease rapidly. At this stage, total system latency is dominated by task processing latency at RSUs. When all algorithms are constrained by the same computing resource of RSUs, the latency gap among them tends to decrease. Specifically, CGDM reduces system latency by 9.0 %, 14.8 %,21.5 %, 38.7 %, 44.9 %, and 49.5 % compared to GDM, SAC, PPO, TD3, DDPG, and Random algorithms, respectively. In Fig. 8, we present the system performance as the comput-ing capacity of the RSU increases from 100 MHz to 300 MHz. With the increase in computing capacity, the performance gap among the algorithms gradually widens. However, when the computing capacity is further enhanced beyond 250 MHz, the differences narrow again. According to the system model, 0.2 0.4 0.6 0.8 1.0       

> Attack Frequency
> 300
> 350
> 400
> 450
> 500
> 550
> Latency (s)  CGDM
> GDM
> SAC
> PPO
> TD3
> DDPG
> Random
> Fig. 9: Total system latency under different attack frequencies. 246810
> Steps
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Reputation Values
> 3.75 4.00 4.25
> 0.65
> 0.70
> 0.75
> 0.80
> User 1
> User 2
> User 3
> User 4
> User 5
> User 6
> User 7
> User 8
> Fig. 10: Reputation values of RSUs for different users in one episode.

the computing capacity directly influences data processing latency. With moderate computing resources, the CGDM algo-rithm can allocate resources efficiently, avoiding both redun-dant and insufficient resource allocation among RSUs, thus showcasing significant performance advantages. When each RSU has surplus computing capacity, the performance gaps among the algorithms decrease. Numerical results demonstrate that CGDM reduces system latency by 8.9 %, 12.3 %, 14.5 %,21.7 %, 31.1 %, and 33.4 % compared with GDM, SAC, PPO, TD3, DDPG, and Random algorithms, respectively. In Fig. 9, we show the impact of varying attack frequencies on the system latency. The CGDM algorithm consistently demonstrates the lowest latency across different attack fre-quencies, achieving latency reductions of 6.3 %, 9.8 %, 11.2 %,32.4 %, 37.8 %, and 43.6 % compared with GDM, SAC, PPO, TD3, DDPG, and Random algorithms, respectively. At lower attack frequencies, performance differences among algorithms are relatively minor. However, as the attack frequency ex-ceeds 0.5, the CGDM algorithm demonstrates a significant advantage, reflecting its stronger capability to resist attacks and control latency. This is because CGDM can accurately identify the RSU under attack based on changes in RSU reputation values, and dynamically optimize the migration decision, thereby effectively avoiding the performance degra-dation caused by high-frequency attacks. Moreover, as the attack intensity reaches saturation, the latency curves of each algorithm tend to be flat, limiting further impact on system latency from increasing attack frequencies. 14 

Additionally, to evaluate the effectiveness of the TPB-based trust evaluation model, we record the reputation value changes of RSUs towards different users within one episode, as shown in Fig. 10. We set the initial reputation value of RSU for all users to 0.7. After several steps of interaction, the reputation values exhibit obvious personalized differences, reflecting the ability of the TPB-based evaluation model to effectively capture user trust preferences. When the attack occurs, the reputation value of RSUs for each user quickly drops below the safety threshold, indicating that the TPB-based trust evaluation model can accurately and timely identify RSUs under attack, thereby effectively ensuring the security of vehicular AI agent migration. VI. C ONCLUSION 

In this paper, we have studied the real-time migration of AI agents in the vehicular metaverse. To address potential security threats and trust biases during migration, we have proposed a TPB-based trust evaluation model, which objectively quanti-fies the reputation of RSUs while fully considering personal-ized user trust preferences. Additionally, we have formulated the vehicular AI agent migration optimization problem as a POMDP and introduced a CGDM algorithm to solve it. The proposed CGDM algorithm leverages diffusion mod-els to effectively capture multi-modal decision distributions and employs a denoising consistency term and an adaptive confidence mechanism to ensure stable and efficient policy updates. Numerical results have demonstrated that our method achieves superior convergence speed and robustness compared to multiple baseline algorithms. For future work, we plan to extend the CGDM algorithm to multi-agent collaborative scenarios to generate decisions more efficiently. REFERENCES [1] K. Li, Y. Cui, W. Li, T. Lv, X. Yuan, S. Li, W. Ni, M. Simsek, and F. Dressler, “When Internet of Things meets metaverse: Convergence of physical and cyber worlds,” IEEE Internet of Things Journal , vol. 10, no. 5, pp. 4148–4173, 2022. [2] M. Xu, W. C. Ng, W. Y. B. Lim, J. Kang, Z. Xiong, D. Niyato, Q. Yang, X. Shen, and C. Miao, “A full dive into realizing the edge-enabled metaverse: Visions, enabling technologies, and challenges,” 

IEEE Communications Surveys & Tutorials , vol. 25, no. 1, pp. 656– 700, 2022. [3] M. Xu, D. Niyato, H. Zhang, J. Kang, Z. Xiong, S. Mao, and Z. Han, “Generative AI-empowered effective physical-virtual synchronization in the vehicular metaverse,” in 2023 IEEE International Conference on Metaverse Computing, Networking and Applications (MetaCom) . IEEE, 2023, pp. 607–611. [4] V. Chamola, G. Bansal, T. K. Das, V. Hassija, S. Sai, J. Wang, S. Zeadally, A. Hussain, F. R. Yu, M. Guizani et al. , “Beyond reality: The pivotal role of generative AI in the metaverse,” IEEE Internet of Things Magazine , vol. 7, no. 4, pp. 126–135, 2024. [5] M. Xu, D. Niyato, J. Chen, H. Zhang, J. Kang, Z. Xiong, S. Mao, and Z. Han, “Generative AI-empowered simulation for autonomous driving in vehicular mixed reality metaverses,” IEEE Journal of Selected Topics in Signal Processing , vol. 17, no. 5, pp. 1064–1079, 2023. [6] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou et al. , “The rise and potential of Large Language Model based agents: A Survey,” Science China Information Sciences , vol. 68, no. 2, p. 121101, 2025. [7] W. Hong, W. Wang, Q. Lv, J. Xu, W. Yu, J. Ji, Y. Wang, Z. Wang, Y. Dong, M. Ding et al. , “Cogagent: A visual language model for GUI agents,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2024, pp. 14 281–14 290. [8] J. Kang, J. Chen, M. Xu, Z. Xiong, Y. Jiao, L. Han, D. Niyato, Y. Tong, and S. Xie, “UAV-assisted dynamic avatar task migration for vehicular metaverse services: A multi-agent deep reinforcement learning approach,” IEEE/CAA Journal of Automatica Sinica , vol. 11, no. 2, pp. 430–445, 2024. [9] J. Chen, J. Kang, M. Xu, Z. Xiong, D. Niyato, C. Chen, A. Jamalipour, and S. Xie, “Multi-agent deep reinforcement learning for dynamic avatar migration in AIoT-enabled vehicular metaverses with trajectory prediction,” IEEE Internet of Things Journal , vol. 11, no. 1, pp. 70–83, 2023. [10] T. Zhang, C. Xu, Y. Lian, H. Tian, J. Kang, X. Kuang, and D. Niyato, “When moving target defense meets attack prediction in digital twins: A convolutional and hierarchical reinforcement learning approach,” IEEE Journal on Selected Areas in Communications , vol. 41, no. 10, pp. 3293– 3305, 2023. [11] Y. Zhou, G. Cheng, Y. Zhao, Z. Chen, and S. Jiang, “Toward Proactive and Efficient DDoS Mitigation in IIoT Systems: A Moving Target De-fense Approach,” IEEE Transactions on Industrial Informatics , vol. 18, no. 4, pp. 2734–2744, 2022. [12] S. Sengupta, A. Chowdhary, A. Sabur, A. Alshamrani, D. Huang, and S. Kambhampati, “A Survey of Moving Target Defenses for Network Security,” IEEE Communications Surveys & Tutorials , vol. 22, no. 3, pp. 1909–1941, 2020. [13] M. Huang, Z. Li, F. Xiao, S. Long, and A. Liu, “Trust mechanism-based multi-tier computing system for service-oriented edge-cloud networks,” 

IEEE Transactions on Dependable and Secure Computing , vol. 21, no. 4, pp. 1639–1651, 2024. [14] Y. Kang, J. Wen, J. Kang, T. Zhang, H. Du, D. Niyato, R. Yu, and S. Xie, “Hybrid-generative diffusion models for attack-oriented twin migration in vehicular metaverses,” arXiv preprint arXiv:2407.11036 , 2024. [15] L. Ismail, M. Qaraqe, A. Ghrayeb, and D. Niyato, “Enhancing trust and security in the vehicular metaverse: A reputation-based mechanism for participants with moral hazard,” in 2024 IEEE Wireless Communications and Networking Conference (WCNC) , 2024, pp. 1–6. [16] Y. Zhong, J. Wen, J. Zhang, J. Kang, Y. Jiang, Y. Zhang, Y. Cheng, and Y. Tong, “Blockchain-assisted twin migration for vehicular metaverses: A game theory approach,” Transactions on Emerging Telecommunica-tions Technologies , vol. 34, no. 12, p. e4856, 2023. [17] H. Du, R. Zhang, Y. Liu, J. Wang, Y. Lin, Z. Li, D. Niyato, J. Kang, Z. Xiong, S. Cui, B. Ai, H. Zhou, and D. I. Kim, “Enhancing deep reinforcement learning: A tutorial on generative diffusion models in network optimization,” IEEE Communications Surveys & Tutorials ,vol. 26, no. 4, pp. 2611–2646, 2024. [18] H. Du, Z. Li, D. Niyato, J. Kang, Z. Xiong, H. Huang, and S. Mao, “Diffusion-based reinforcement learning for edge-enabled AI-generated content services,” IEEE Transactions on Mobile Computing , vol. 23, no. 9, pp. 8902–8918, 2024. [19] H. Du, R. Zhang, D. Niyato, J. Kang, Z. Xiong, D. I. Kim, X. Shen, and H. V. Poor, “Exploring collaborative distributed diffusion-based AI-generated content (AIGC) in wireless networks,” IEEE Network , vol. 38, no. 3, pp. 178–186, 2023. [20] M. Psenka, A. Escontrela, P. Abbeel, and Y. Ma, “Learning a Diffusion Model Policy from Rewards via Q-Score Matching,” in Proceedings of the 41st International Conference on Machine Learning , 2024, pp. 41 163–41 183. [21] H. Sami, A. Hammoud, M. Arafeh, M. Wazzeh, S. Arisdakessian, M. Chahoud, O. Wehbi, M. Ajaj, A. Mourad, H. Otrok, O. Abdel Wa-hab, R. Mizouni, J. Bentahar, C. Talhi, Z. Dziong, E. Damiani, and M. Guizani, “The metaverse: Survey, trends, novel pipeline ecosystem & future directions,” IEEE Communications Surveys & Tutorials , vol. 26, no. 4, pp. 2914–2960, 2024. [22] Y. K. Dwivedi, L. Hughes, A. M. Baabdullah, S. Ribeiro-Navarrete, M. Giannakis, M. M. Al-Debei, D. Dennehy, B. Metri, D. Buhalis, C. M. Cheung et al. , “Metaverse beyond the hype: Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy,” International Journal of Information Management , vol. 66, p. 102542, 2022. [23] H. Ma, Y. Liu, Q. Jiang, B. Y. He, X. Liao, and J. Ma, “Mobility AI Agents and Networks,” IEEE Transactions on Intelligent Vehicles , vol. 9, no. 7, pp. 5124–5129, 2024. [24] M. M. Soliman, E. Ahmed, A. Darwish, and A. E. Hassanien, “Arti-ficial intelligence powered Metaverse: analysis, challenges and future perspectives,” Artificial Intelligence Review , vol. 57, no. 2, p. 36, 2024. [25] X. Wen, J. Wen, M. Xiao, J. Kang, T. Zhang, X. Li, C. Chen, and D. Niyato, “Defending Against Network Attacks for Secure AI Agent Migration in Vehicular Metaverses,” arXiv preprint arXiv:2412.20154 ,2024. 15 

[26] J. Wen, J. Kang, Z. Xiong, Y. Zhang, H. Du, Y. Jiao, and D. Niyato, “Task freshness-aware incentive mechanism for vehicle twin migration in vehicular metaverses,” in 2023 IEEE International Conference on Metaverse Computing, Networking and Applications (MetaCom) . IEEE, 2023, pp. 481–487. [27] Y. Zhong, J. Kang, J. Wen, D. Ye, J. Nie, D. Niyato, X. Gao, and S. Xie, “Generative Diffusion-Based Contract Design for Efficient AI Twin Migration in Vehicular Embodied AI Networks,” IEEE Transactions on Mobile Computing , vol. 24, no. 5, pp. 4573–4588, 2025. [28] N. H. Chu, D. T. Hoang, D. N. Nguyen, K. T. Phan, E. Dutkiewicz, D. Niyato, and T. Shu, “Metaslicing: A novel resource allocation framework for metaverse,” IEEE Transactions on Mobile Computing ,vol. 23, no. 5, pp. 4145–4162, 2023. [29] J. Kang, J. Zhang, H. Yang, D. Ye, and M. S. Hossain, “When Metaverses Meet Vehicle Road Cooperation: Multiagent DRL-Based Stackelberg Game for Vehicular Twins Migration,” IEEE Internet of Things Journal , vol. 11, no. 22, pp. 35 928–35 941, 2024. [30] X. Luo, J. Wen, J. Kang, J. Nie, Z. Xiong, Y. Zhang, Z. Yang, and S. Xie, “Privacy attacks and defenses for digital twin migrations in vehicular metaverses,” IEEE Network , vol. 37, no. 6, pp. 82–91, 2023. [31] Z. Wang, J. J. Hunt, and M. Zhou, “Diffusion policies as an expres-sive policy class for offline reinforcement learning,” arXiv preprint arXiv:2208.06193 , 2022. [32] L. Fang, R. Liu, J. Zhang, W. Wang, and B.-Y. Jing, “Diffusion Actor-Critic: Formulating Constrained Policy Iteration as Diffusion Noise Regression for Offline Reinforcement Learning,” in The Thirteenth International Conference on Learning Representations , 2025. [33] L. Yang, Z. Huang, F. Lei, Y. Zhong, Y. Yang, C. Fang, S. Wen, B. Zhou, and Z. Lin, “Policy Representation via diffusion probability model for reinforcement learning,” arXiv preprint arXiv:2305.13122 , 2023. [34] S. Ding, K. Hu, Z. Zhang, K. Ren, W. Zhang, J. Yu, J. Wang, and Y. Shi, “Diffusion-based Reinforcement Learning via Q-weighted Variational Policy Optimization,” in The Thirty-eighth Annual Conference on Neural Information Processing Systems , 2024. [35] Y. Wang, L. Wang, Y. Jiang, W. Zou, T. Liu, X. Song, W. Wang, L. Xiao, J. Wu, J. Duan et al. , “Diffusion Actor-Critic with entropy regulator,” in 

The Thirty-eighth Annual Conference on Neural Information Processing Systems , 2024. [36] S. He, K. Shi, C. Liu, B. Guo, J. Chen, and Z. Shi, “Collaborative sensing in Internet of Things: A comprehensive survey,” IEEE Communications Surveys & Tutorials , vol. 24, no. 3, pp. 1435–1474, 2022. [37] F. Tang, Y. Zhou, and N. Kato, “Deep reinforcement learning for dy-namic uplink/downlink resource allocation in high mobility 5G HetNet,” 

IEEE Journal on Selected Areas in Communications , vol. 38, no. 12, pp. 2773–2782, 2020. [38] C. E. Shannon, “Communication in the presence of noise,” Proceedings of the IRE , vol. 37, no. 1, pp. 10–21, 1949. [39] Q. He, C. Wang, G. Cui, B. Li, R. Zhou, Q. Zhou, Y. Xiang, H. Jin, and Y. Yang, “A game-theoretical approach for mitigating edge DDoS attack,” IEEE Transactions on Dependable and Secure Computing ,vol. 19, no. 4, pp. 2333–2348, 2021. [40] T. Zhang, C. Xu, P. Zou, H. Tian, X. Kuang, S. Yang, L. Zhong, and D. Niyato, “How to mitigate DDoS intelligently in SD-IoV: A moving target defense approach,” IEEE Transactions on Industrial Informatics ,vol. 19, no. 1, pp. 1097–1106, 2022. [41] J. John and K. John Singh, “Trust value evaluation of cloud service providers using fuzzy inference based analytical process,” Scientific Reports , vol. 14, no. 1, p. 18028, 2024. [42] I. Ajzen, “The theory of planned behavior,” Organizational Behavior and Human Decision Processes , vol. 50, no. 2, pp. 179–211, 1991. [43] H. El Sayed, S. Zeadally, and D. Puthal, “Design and evaluation of a novel hierarchical trust assessment approach for vehicular networks,” 

Vehicular Communications , vol. 24, p. 100227, 2020. [44] C. Eisenach, H. Yang, J. Liu, and H. Liu, “Marginal policy gradients: A unified family of estimators for bounded action spaces with applica-tions,” arXiv preprint arXiv:1806.05134 , 2018. [45] Z. Huang, L. Liang, Z. Ling, X. Li, C. Gan, and H. Su, “Reparameterized policy learning for multimodal trajectory optimization,” in International Conference on Machine Learning . PMLR, 2023, pp. 13 957–13 975. [46] J. Ho, A. Jain, and P. Abbeel, “Denoising Diffusion Probabilistic Models,” Advances in Neural Information Processing Systems , vol. 33, pp. 6840–6851, 2020. [47] S. Li, R. Krohn, T. Chen, A. Ajay, P. Agrawal, and G. Chalvatzaki, “Learning multimodal behaviors from scratch with diffusion policy gradient,” Advances in Neural Information Processing Systems , vol. 37, pp. 38 456–38 479, 2024. [48] C. Wang, M. Uehara, Y. He, A. Wang, A. Lal, T. Jaakkola, S. Levine, A. Regev, Hanchen, and T. Biancalani, “Fine-Tuning Discrete Diffusion Models via Reward Optimization with Applications to DNA and Pro-tein Design,” in The Thirteenth International Conference on Learning Representations , 2025.
