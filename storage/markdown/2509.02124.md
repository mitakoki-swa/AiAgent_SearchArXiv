Title: FlexNGIA 2.0: Redesigning the Internet with Agentic AI - Protocols, Services, and Traffic Engineering Designed, Deployed, and Managed by AI

URL Source: http://arxiv.org/pdf/2509.02124v1

Published Time: Wed, 03 Sep 2025 02:46:37 GMT

Markdown Content:
# FlexNGIA 2.0: Redesigning the Internet with Agentic AI 

## Protocols, Services, and Traffic Engineering Designed, Deployed, and Managed by AI 

Mohamed Faten Zhani 1,2, Younes Korbi 1,2 and Yamen Mkadem 1,21FlexNGIA, Tunisia 

> 2

ISITCom, University of Sousse, Tunisia 

{mfzhani, ykorbi, ymkadem }@FlexNGIA.net 

Abstract The escalating demands of immersive communications, alongside advances in network softwarization and AI-driven cognition and generative reasoning, create a pivotal opportunity to rethink and reshape the future Internet. In this context, we introduce in this paper, FlexNGIA 2.0, an Agentic AI-driven Internet architecture that leverages LLM-based AI agents to autonomously orchestrate, configure, and evolve the network. These agents can, at runtime, perceive, reason, coordinate among themselves to dynamically design, implement, deploy, and adapt communication protocols, Service Function Chains (SFCs), network functions, resource allocation strategies, congestion control, and traffic engineering schemes, thereby ensuring optimal performance, reliability, and efficiency under evolving conditions. The paper first outlines the overall architecture of FlexNGIA 2.0 and its constituent LLM-Based AI agents. For each agent, we detail its design, implementation, inputs and outputs, prompt structures, interactions with tools and other agents, followed by preliminary proof-of-concept experiments demonstrating its operation and potential. The results clearly highlight the ability of these LLM-based AI agents to automate the design, the implementation, the deployment, and the performance evaluation of transport protocols, service function chains, network functions, congestion control schemes, and resource allocation strategies. Building on these capabilities, FlexNGIA 2.0 paves the way for a new class of Agentic AI-Driven networks, where fully cognitive, self-evolving AI agents can autonomously design, implement, adapt and optimize the network’s protocols, algorithms, and behaviors to efficiently operate across complex, dynamic, and heterogeneous environments. To bring this vision to reality, we also identify in this paper key research challenges and future directions toward achieving fully autonomous, adaptive, and agentic AI-driven networks. 

Keywords— Internet Architecture, Communication Protocols, Network and Service Management, Agentic AI, AI agent, Congestion Control, Resource Allocation, Software-Defined Networking, Network Function Virtualization, FlexNGIA 

CONTENTS 

I Introduction 2

II FlexNGIA 1.0 Overview 3

III FlexNGIA 2.0 Architecture 4III-A FlexNGIA 1.0 to 2.0: A Shift Toward Agentic AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4III-B LLM-based AI Agent: Building Blocks & Design Principles . . . . . . . . . . . . . . . . . . . . . . . . 4III-C FlexNGIA 2.0 - Agentic AI Architecture for the Future Internet . . . . . . . . . . . . . . . . . . . . . . 6

IV FlexNGIA 2.0 AI Agents: Design, Proof of Concept, and Preliminary Experimental Results 8IV-A Information Fusion Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8IV-B AI-Driven SFC & Protocol Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8IV-C AI-Driven Congestion Control Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 IV-D AI-Driven Resource Allocation Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 

V Key Research Directions 17 V-A Building Powerful and Reliable LLMs for Network Agentic AI: Design, Customization, and Evaluation 17 V-B LLM Prompting for the Design of Intelligent Network Protocols and Management Algorithms . . . . . 17 V-C Rethinking the way Network Services, Protocols and, Algorithms are Designed . . . . . . . . . . . . . 17 V-D AI Agent Design and Coordination Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 V-E Designing New Breeds of Network Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 

VI Conclusion 18 

References 18 

> arXiv:2509.02124v1 [cs.NI] 2 Sep 2025

I. I NTRODUCTION 

Immersive communications, including real-time virtual and augmented reality, holography, haptics, represent the forthcoming pinnacle of digital demand. Yet the stringent performance requirements of these applications, such as ultra-low latency, high reliability, and massive bandwidth, remain far beyond the capabilities of today’s Internet. Fortunately, recent advances in network softwarization and programmability, combined with breakthroughs in AI-driven cognitive, Large Language Models (LLMs), and generative reasoning, could provide an unprecedented opportunity to fundamentally rethink and reshape the architecture of the Internet to make it able to meet the ambitious performance, reliability, and responsiveness demanded by these next-generation applications. Before exploring this opportunity, it is important to examine the core limitations of today’s Internet architecture. They can be summarized as follows. 

• Best-effort Service Model: Today’s Internet relies on a best-effort delivery model, where Internet Service Providers offer basic transit services without any performance guarantees. This model is inadequate for emerging applications that demand deterministic performance. 

• Rigid Communication Protocols: The Internet’s communication stack relies on rigid, monolithic protocols. For instance, transport protocols (e.g., TCP [1], QUIC [2], SCTP [3]) offer a fixed bundle of services (e.g., reliability, in-order delivery, congestion control) without the flexibility to customize them based on application needs. They are also based on a two-endpoint model, which clashes with modern distributed applications connecting multiple endpoints, resulting in flows being managed independently in the network, even when they are interdependent or even belong to the same application. Another fundamental limitation is the lack of cross-layer collaboration. For instance, the transport and network layers operate in silos with limited visibility into each other’s state and goals. This isolated design prevents holistic optimization across all flows and leads to suboptimal or conflicting behaviors. 

• Lack of Customizable Network Functions and Services: 

The current core network of the Internet primarily focuses on data delivery and lacks the inherent capability to dynamically integrate new network functions and services to support application-specific requirements. Ideally, the network would allow the instantiation of customizable functions and services, such as real-time content analysis and processing, content-based routing, advanced packet caching and forwarding, and AI/ML-driven services. 

• Congestion Control - A Key Bottleneck for Performance: 

Congestion control (CC), i.e., dynamically adjusting the sending rate, plays a critical role in determining overall application performance. Numerous CC algorithms have been proposed in the literature, each tailored to specific environments, runtime conditions and application requirements (e.g., wired vs. wireless networks, WAN vs. LAN, high-bandwidth links) [4], [5], [6], [7], [8], [9]. This implies that a CC scheme tailored for specific network conditions may suffer significant performance degradation when those conditions change. In today’s Internet, however, CC algorithms are typically fixed at deployment and remain static regardless of the network conditions and application requirements [10], [11], [12], [13], [13], and this lack of adaptability constitutes a major performance bottleneck. To address these limitations, a novel fully-Flex ible 

Next-Generation Internet Architecture (FlexNGIA 1.0) was recently proposed [14], [15]. FlexNGIA overcomes the Internet’s rigid protocols, lack of guarantees, and limited in-network intelligence by enabling per-application Service Function Chains (SFCs) with guaranteed performance. An SFC is an ordered sequence of customized network functions (NFs) orchestrated through tailored communication protocols. The originality of FlexNGIA lies in its ability to compose SFCs with NFs that can operate across any layer of the OSI model, while being governed by fully customizable protocols adapted to the specific requirements of each application. Most of these customizations in FlexNGIA 1.0, such as designing SFCs, network functions, communication protocols, congestion control and traffic engineering schemes, is performed manually by developers or network engineers. Once deployed, these components cannot be modified at runtime, limiting the network’s ability to adapt to changing conditions. In addition, the FlexNGIA 1.0 management framework, like its traditional counterparts [16], [17], [18], [19], [20], is composed of modular, self-contained functional components for resource allocation, failure management, and monitoring. These modules operate in a largely static, rule-based manner, relying on pre-programmed algorithms that can only be tuned by adjusting parameters, without the ability to modify the underlying logic or behavior. In this paper, we build on the FlexNGIA 1.0 foundation to propose FlexNGIA 2.0, an agentic AI architecture for the next-generation Internet, in which LLM-based AI agents can autonomously design, implement, adapt, and manage SFCs, network functions, communication protocols, congestion control and traffic engineering schemes, and resource allocation algorithms, all at runtime. These agents are equipped with machine learning, reasoning, and natural language understanding [21], [22]. They can collaborate, learn, and adapt, continuously evolving at runtime the underlying logic, behavior, and algorithms that drive communication protocols, network functions, service management, and traffic engineering schemes. FlexNGIA 2.0 redefines the network architecture by embedding cognitive intelligence and reasoning, enabling AI agents to redesign the network’s protocols, logic and algorithms on the fly. This game-changing capability delivers the flexibility, intelligence, and responsiveness needed to adapt in real time to different network environments, and application demands. In this context, the contributions of this paper could be summarized as follows:  

> •

We introduce the FlexNGIA 2.0 architecture and the role of its key composing LLM-based AI agents.  

> •

We present a detailed design for each agent, including its internal architecture, implementation, inputs and outputs, prompt structures, and interactions with other agents and tools, highlighting their potential to autonomously support complex network tasks such as protocol design, service function chaining, congestion control, and resource allocation.  

> •

We implemented a prototype of the key agents 1, and we demonstrate through experiments their efficiency. These experiments highlight the potential of AI Agents powered by advanced LLM models with reasoning and problem-solving capabilities (e.g., GPT-5 Thinking [23], DeepSeek-R1-Distill-Llama-70B [24]) to autonomously design, customize, implement and manage network protocols and services. Specifically, we show their efficiency in (1) analyzing application requirements, designing and implementing a complete custom end-to-end SFC, including all its composing network functions along with a tailored transport protocol; 

(2) dynamically deciding among different CC schemes, and even design and implement new schemes at runtime to better match the application’s specific requirements and network conditions; and (3) guiding the resource allocation strategy to satisfy applications’ performance and ensure the operator’s operational, sustainability, and economic objectives.  

> •

We finally highlight current limitations, identify key research challenges, and outline future directions toward evolving FlexNGIA 2.0 and building a fully autonomous, Agentic AI-driven Internet. In the remainder of this paper, we begin by providing an overview of FlexNGIA 1.0 (Section II). We then introduce FlexNGIA 2.0, an agentic AI-based Internet Architecture fully managed by AI agents (Section III). Next, we detail the design of its key internal AI agents, demonstrating their operation through a proof of concept and experimental results (Section IV). Following this, we outline key research directions to evolve FlexNGIA 2.0’s agentic AI design (Section V). The paper concludes with a summary discussion (Section VI). II. F LEX NGIA 1.0 O VERVIEW 

The FlexNGIA architecture is motivated by the need to overcome the aforementioned fundamental limitations of today’s Internet, particularly its rigid protocol stacks, lack of service guarantees, and limited in-network intelligence. Its key features can be summarized as follows: 

• Service Function Chain per application: Rather than relying on traditional best-effort data delivery, the FlexNGIA architecture leverages in-network computing to deploy 

> 1The code of the prototype, the developed agents, experiments is available upon request. Please use FlexNGIA Contact Page [15]

a dedicated Service Function Chain for each application. This SFC consists of a tailored set of network functions that steer traffic from sources to destinations while meeting the specific needs of the application (see, the example of SFC 1 associated with application 1 in Fig. 1). Within this model, applications’ developers can define reliability and performance requirements (e.g., throughput, packet loss, end-to-end delay, and jitter) and specify customized network functions and communication protocols that best support these requirements. This SFC-based per-application approach enables fine-grained control, performance optimization, and customized network services. 

• Advanced network functions: In FlexNGIA, service function chaining supports the dynamic composition of a wide range of network functions, selected and ordered to best serve the needs of each application. These functions can operate at any layer of the network stack and are not limited to traditional routing or forwarding tasks. For instance, it is possible to design application-aware network functions that include advanced capabilities such as context- and application-aware traffic analysis, processing and engineering, AI/ML-driven decision-making, real-time data analysis and processing, or media-specific operations like video/audio compression, cropping, and rendering. Network functions could also operate at lower layers. For instance, transport and traffic engineering network functions can manage packet scheduling, grouping, caching, and retransmission to optimize delivery under varying network conditions. Another type of network functions could focus on monitoring and measurement to enable in-network continuous collection, filtering, and AI-based analysis of monitoring data in order to support adaptive management of the network, services and applications. 

• Communication Protocol Customization: Rather than relying solely on legacy protocols like TCP or QUIC, FlexNGIA enables the design and deployment of communication protocols specifically adapted to each SFC and tailored to the unique requirements of its associated application. These protocols can operate at individual OSI layers or in cross-layer designs to enable inter-layer coordination. Additionally, they can incorporate interactions and collaboration between endpoints and the network functions of the SFC in order to enhance overall performance and reliability. They can also support multi-endpoint communication models, moving beyond the two-endpoint model of TCP and QUIC. For each designed protocol, a custom packet header is defined, enabling the inclusion of metadata and even control commands to be executed directly in the data plane and network functions. By enabling rapid design and deployment of customized protocols and services, FlexNGIA moves beyond rigid layering and outdated Internet principles, and it opens the network infrastructure to innovation and deep customization, empowering developers to optimize communication for diverse and evolving application requirements. 

• FlexNGIA management framework: The FlexNGIA S1             

> 0
> 1
> 2
> 6
> 7
> 59
> 12
> 8
> 10
> S0
> S2
> D1
> Application 1-SFC 1
> Physical Infrastructure
> NF 12
> NF 11
> NF 13
> Mapping
> 3
> 4
> 13
> 11
> Resource Management Framework
> Monitoring
> Module
> Application
> Control
> Module
> Monitoring
> Data
> Failure Management
> Module
> AA
> AA
> AA
> AA
> …… ..
> …… ..
> AA
> AA
> AA
> Resource Allocation
> Module
> NB :For simplicity ,the figure shows only the mapping of the chain SFC 1
> associated to Application 1
> S6
> S5
> S7
> DN
> Application n-SFC n
> NF n2
> NF n1
> NF n3
> Monitoring
> Data
> AA
> AA
> AA
> AA
> Application
> Control
> Module
> Sensors /
> objects
> Signaling Module
> Commands
> Monitoring
> Data
> Point of Presence
> Physical Link
> Virtual Link
> D
> STraffic Source
> Traffic Destination
> Mapped Instance
> AA Application Assistant
> Service Function Chain SFC
> Sensors /objects
> AA

Fig. 1: FlexNGIA 1.0 management framework. proposes a management framework comprising several core modules (Fig. 1). The Signaling module initiates the SFC creation upon application requests. The Application Control module manages each application’s SFC, estimating traffic and resources while adapting the chain based on continuous monitoring data. The Resource Allocation module provisions resources to meet operator goals like performance, costs, and energy efficiency [25], [26], [27], [28], [29]. The Failure Management module detects and predicts failures to ensure reliability and availability [30], [31]. Finally, the Monitoring module gathers data on physical and virtual components. FlexNGIA 1.0 addresses fundamental Internet issues but depends on static modules and manual intervention for key tasks, limiting its flexibility, intelligence, and responsiveness to complex, real-time challenges. To overcome these shortcomings, we introduce, in the following, FlexNGIA 2.0, an agentic AI framework that enables a fully intelligent, autonomous, and adaptive network management. III. F LEX NGIA 2.0 A RCHITECTURE 

This section presents FlexNGIA 2.0, first discussing the shift from static components to agentic AI, then describing the building blocks of LLM-based agents, and finally introducing the complete FlexNGIA 2.0 architecture. 

A. FlexNGIA 1.0 to 2.0: A Shift Toward Agentic AI 

FlexNGIA 1.0 is composed of modular, self-contained functional components that perform predefined tasks using static algorithms. Moreover, many critical responsibilities in FlexNGIA 1.0, such as designing service functions, developing communication protocols, implementing CC schemes, and building network functions, are handled manually by developers and network engineer. This places the burden of customization and adaptability on manual engineering rather than on the system itself. Once these components are designed and deployed, they are typically fixed and cannot be modified at runtime, significantly limiting the system’s ability to respond to different network conditions and application requirements. In contrast, FlexNGIA 2.0 adopts an agentic AI architecture, where autonomous AI agents are equipped with techniques like machine learning, reasoning, and natural language understanding, and are able to reason, set goals, choose strategies, plan to achieve them, learn, collaborate and adapt in real time to dynamic network conditions and evolving application requirements. These agents can also access and deploy specialized tools, such as measurement utilities, analysis modules, or protocol components, to perceive and act upon their environment. They maintain internal memory to store relevant context, including historical observations, prior decisions, and learned behaviors, enabling long-term reasoning and informed decision-making. Through machine learning, they can recognize patterns and optimize strategies over time. Natural language understanding allows them to interpret human-provided guidance or documentation when available, while reasoning capabilities support goal-driven planning and coordination with other agents. This combination of tools, memory, learning, and interaction enables these agents to operate in a decentralized yet cooperative manner, continuously improving their ability to manage complex, dynamic networking scenarios. In short, FlexNGIA 2.0 AI agents can design and manage at runtime service function chains, network functions, communication protocols, congestion control schemes, and resource allocation algorithms. By interpreting semantic, contextual, and real-time information, they dynamically adapt application protocols, network functions, and service chains, enabling more flexible, intelligent, and scalable networks. This evolution represents a fundamental shift from rigid, deterministic modules, protocols, and algorithms toward a cognitive system designed to autonomously evolve and meet the evolving network conditions and application demands. Table I provides a comparative overview highlighting key differences between existing literature, FlexNGIA 1.0, and the advanced capabilities introduced in FlexNGIA 2.0. 

B. LLM-based AI Agent: Building Blocks & Design Principles 

LLMs have become powerful systems for generating and processing human-like text, driving advances in natural language generation, code synthesis, and beyond [32], [33]. However, LLMs lack the capacity to interact with the external world, perform actions, or autonomously select and use tools to acquire new information. Bridging this gap requires agentic behavior [34], which integrates environmental perception, reasoning, memory, and tool-driven action to enable adaptive, goal-directed operation. Building on this concept, LLM-based agents are designed to extend LLMs with these agentic capabilities, thereby enabling autonomous, context-aware decision-making and interaction with the environment. TABLE I: Comparison of Existing Literature, FlexNGIA 1.0, and FlexNGIA 2.0                                                                

> Aspect Existing Literature FlexNGIA 1.0 FlexNGIA 2.0 Architecture Type Static, protocol-centric Modular, rule-based Agentic AI, cognitive and adaptive
> Functional Design and Control Predefined, hard-coded Modular, developer-defined Runtime-adapted by autonomous agents
> Service Function Chaining (SFC) limited chain (packet forwarding, IDS, Firewall) Designed and configured by network engineers Dynamically designed and adapted at runtime by AI agents
> Network Function Design Rigid Designed and configured by network engineers AI agents generate and evolve functions at runtime
> Communication Protocols Design Legacy Protocols (e.g., TCP) Developer-designed AI agents generate and evolve protocols at runtime
> Runtime Adaptation for the SFC, Network Functions & Communication Protocols
> Limited or none Not supported Fully supported via agent-based decision-making augmented by LLMs
> Customization Manual, time-consuming developer-led Fully automated, AI-driven
> Context/Semantic Awareness Absent or minimal Minimal Agents interpret semantic, contextual, and runtime information
> Learning & Intelligence Not integrated Absent Built-in learning, reasoning, and adaptation
> Scalability & Flexibility Poor in dynamic environments Limited by static design High - agents scale and adapt the chain and the protocols
> Responsiveness to Change Manual reconfiguration needed Requires redeployment Real-time response to dynamic conditions
> Human Involvement High (design, tuning, updates) High (especially for protocols/Network Functions/SFC) Minimal - system adapts autonomously

As shown in Fig. 2, at the heart of an LLM-based agent lie the following fundamental building blocks [35]:  

> •

Brain: This component consists of an LLM that serves as the central ”brain” responsible for interpreting user requests thanks to its ability to process and understand natural language input, reasoning about tasks, and generating appropriate responses.  

> •

Memory: A persistent store of past actions, interactions, and context to inform future reasoning, planning, and decision-making.  

> •

Planning: This component is responsible for breaking down complex tasks into manageable subtasks, determining a logical step-by-step execution plan, evaluating options, and performing reflective reasoning. The planning module generates these detailed plans by prompting an LLM, either the same one used by the brain or a separate LLM instance.  

> •

Tools: This component serves as an interface that allows the agent to interact with external tools and APIs, such as databases, network measurement utilities, resource management systems, enabling it to gather information, execute actions, and complete tasks [36]. External  Tools and  System s  

> (e.g., APIs, data sources, actuators, programs )

Tools 

Memory 

LLM -Based Agent 

LLM 

Planning 

System 

prompt 

System 

prompt 

Tools 

Memory 

LLM -Based Agent 

LLM 

Planning 

System 

prompt 

> Tools
> Memory
> LLM -Based Agent
> LLM
> Planning
> System
> prompt
> Tools
> Memory
> LLM -Based Agent
> LLM
> Planning
> System
> prompt
> Tools
> Memory
> LLM -Based Agent
> LLM
> Planning
> System
> prompt
> Tools
> Memory
> LLM -Based Agent
> LLM
> Planning
> System
> prompt
> Tools
> Memory
> LLM -Based Agent
> LLM
> Planning
> System
> prompt
> Tools
> Memory
> LLM -Based Agent
> LLM
> Planning
> System
> prompt
> Output/Actions
> User Prompt/Input/
> Instructions/Feedback
> Output/Actions
> User Prompt/Input/
> Instructions/Feedback

Fig. 2: Building Blocks of an LLM-Based Agent and Its Interaction with External Tools. Among the above components, the planning module is crucial as it establishes the strategy and the steps that direct the agent toward achieving its intended objective. A key part of this planning involves prompt engineering: designing and refining the instructions, known as prompts , provided to the agent. In the following, we present a more detailed on this critical aspect of LLM-based AI model design. 

Prompt engineering: A prompt is a textual instruction or input provided to the agent, while prompt engineering refers to the process of designing, structuring, and refining that prompt to optimize the agent’s reasoning and output. Well-crafted prompts are critical, as they directly influence the agent’s performance, the quality and relevance of its results, and the accuracy with which it achieves the intended objectives. The prompt given to an LLM-based agent may include, but is not limited to: the agent’s role (to provide context), a set of goals, a planning formulation technique, the desired output format, and the specific task or instructions. A planning formulation technique refers to the style or structure of prompts used to guide an LLM in breaking down, organizing, and creating a coherent plan to achieve a goal. Notable techniques include Chain of Thought, which guides the agent to reason through problems in logical, sequential steps [37]; Tree of Thought (ToT), an extension of Chain of Thought prompting that explores multiple coherent intermediate “thought” steps to solve problems [38]; Hierarchical planning focuses on decomposing tasks into subtasks arranged in a layered hierarchy, enabling abstraction and refinement [39], while decision-tree planning models decisions and possible outcomes as a tree of decision points, evaluating different paths to select the optimal plan [38]. AI agent prompts can be categorized into two primary types:  

> •

System Prompt: It establishes the agent’s fundamental identity, role, behavior and operational constraints. It typically remains consistent across interactions and ensures that the AI maintains a coherent persona, tone, Application                           

> metadata
> Environment
> Analysis Agent
> QoE Measurement
> Agent
> Multi -layer QoS
> Measurement Agent
> Information Fusion Agent
> User Equipment
> Tools
> Memory
> App Analysis Agent
> LLM Requirement
> Inference Agent
> Mobility Mobility
> Network Functions Network Functions Network Functions
> Resource Management Framework
> LLM
> Tools
> Memory
> LLM
> Tools
> Memory
> Information Fusion Agent
> Planning
> Planning
> Tools
> Memory
> SFC & Protocol
> Agent
> LLM
> Planning
> Tools
> Memory
> SFC & Protocol
> Agent
> LLM
> Planning
> Tools
> Memory
> Congestion Control Agent
> LLM
> Planning
> Tools
> Memory
> Congestion Control Agent
> LLM
> Planning
> Tools
> Memory
> Resource Allocation
> Agent
> LLM
> Planning
> Tools
> Memory
> Resource Allocation
> Agent
> LLM
> Planning
> Tools
> Memory
> Monitoring
> & Observability Agent
> LLM
> Planning
> Tools
> Memory
> Monitoring
> & Observability Agent
> LLM
> Planning
> Tools
> Memory
> Failure Management
> Agent
> LLM
> Planning
> Tools
> Memory
> Failure Management
> Agent
> LLM
> Planning
> Point of Presence
> Physical Link
> Virtual Link
> D
> STraffic Source
> Traffic Destination
> Mapped Function
> UE User Equipment
> Service Function Chain SFC
> Internal Agent
> Internal/External Agent
> Point of Presence
> Physical Link
> Virtual Link
> D
> STraffic Source
> Traffic Destination
> Mapped Function
> UE User Equipment
> Service Function Chain SFC
> Internal Agent
> Internal/External Agent
> Mapping Mapping
> 0
> 1
> 2
> 6
> 7
> 59
> 12
> 8
> 10
> Physical Infrastructure
> 3
> 4
> 13
> 11 UE
> UE
> UE
> UE 0
> 1
> 2
> 6
> 7
> 59
> 12
> 8
> 10
> Physical Infrastructure
> 3
> 4
> 13
> 11 UE
> UE
> UE
> UE
> S1
> S0
> S2
> D1
> Application 1-SFC 1
> NF 12 NF 11 NF 13
> UE
> UE
> UE
> UE
> S1
> S0
> S2
> D1
> Application 1-SFC 1
> NF 12 NF 11 NF 13
> UE
> UE
> UE
> UE
> Application
> Location Location Flow
> Analyzer
> Flow
> Analyzer

Fig. 3: FlexNGIA 2.0 - Agentic AI-based Management Framework. and style. It is defined by the agent’s developers as part of its initial design and configuration. It can also include instructions for using tools, storing memory, and applying a planning formulation technique to enforce step-by-step reasoning toward accomplishing a goal or task. 

• User Prompt/input: Contains task-specific instructions, queries, or inputs provided by the end users (if the agent is interacting with humans), other agents or external systems. It varies dynamically depending on the current task or interaction and directs the agent to perform specific actions or generate particular outputs. For each request or interaction, the LLM receives the system prompt, user prompt/input, and pertinent information retrieved from the memory module (e.g., past interactions, decisions), all of which are combined and processed jointly to produce the model’s response. As such, the model always recalls previous interactions and stored overarching context (via the memory), its role and constraints (via the system prompt) while addressing the current input (via the user prompt/input). In this paper, the word “prompt” will refer to the system prompt, which includes placeholders or references to incorporate inputs. Fig. 14 is an example of a well-structured system prompt that clearly states the goal, outlines the step-by-step reasoning process to accomplish it (using chain-of-thought reasoning), and contains placeholders (indicated by red text within curly brackets) to retrieve inputs. It is also worth noting that agents can communicate and coordinate with each other using protocols like Agent2Agent (A2A) [40] and Agent Communication Protocol (ACP) [41]. Model Context Protocol (MCP) [42] could be used to provide agents with standardized access to external tools and data sources. 

C. FlexNGIA 2.0 - Agentic AI Architecture for the Future Internet 

FlexNGIA 2.0 introduces an Agentic AI-Driven network architecture that embeds autonomous, intelligent LLM-based agents capable of reasoning, planning, and adapting in real time. The framework leverages LLM-based agents to design, manage, and optimize network functions, protocols, and services dynamically, addressing changing network conditions and application requirements. The main components of the proposed architecture are illustrated in Fig. 3 and described as follows: 

• App Analysis Agent: Operating primarily in offline mode, this agent analyzes the application and determines their functional requirements, user satisfaction criteria, and performance expectations, based on the data available about the application (e.g., documentation, user feedback). Based on this analysis, the agent has two key tasks. The first is to identifies the multi-layer Quality of Service (QoS) metrics that are most relevant to the specific application. These metrics span several layers of the protocol stack. At the network layer, this includes parameters such as latency tolerance, jitter, bandwidth, and packet loss. At the transport layer, relevant indicators include round-trip time, retransmission rates, congestion window behavior, and timeouts. At the application layer, metrics may cover response time, throughput, error rates, and user engagement measures. The agent also identifies the relevant Quality of Experience (QoE) metrics that should be monitored like responsiveness, video/audio quality, or user engagement indicators. The second task of this agent is to build a Target QoS Profile that outlines the target values of the key performance metrics like the maximum allowable latency, jitter, required bandwidth, and tolerable packet loss. Once the App Analysis Agent identifies the key relevant metrics and the Target QoS Profile, they are shared with other agents to guide their operation. 

• Environment Analysis Agent: This agent is tasked with monitoring and analyzing the software stack, hardware components, runtime state, and contextual environmental parameters of the User Equipment (UE). The software stack includes information about the deployed OS, protocols, CC scheme, and running applications or services. The hardware components refer to UE’s physical resources like the CPU, GPU, memory, battery, and onboard sensors. The runtime state reflects the UE’s current operational and networking conditions, including resource utilization, power mode, number of active processes, and network and flow statistics. Finally, the contextual environmental parameters encompass external factors such as network signal strength, user mobility, GPS location, and the presence of nearby wireless devices or interference. Together, these dimensions provide a comprehensive, real-time view of the UE’s status, serving as a critical input for other agents to design protocols and network functions that are dynamically adapted to the current UE’s environment. 

• Multi-layer QoS Measurement Agent: This Agent is responsible for continuously monitoring the performance of the application by collecting and analyzing QoS metrics across multiple protocol layers. Rather than operating with a fixed monitoring strategy, the agent dynamically adapts its monitoring scope and methods based on the Target QoS Profile received from the App Analysis Agent. This allows the QoS Measurement Agent to focus on the most impactful indicators for each flow, reducing unnecessary overhead. As an autonomous agent, this agent is capable of deploying and activating the appropriate measurement tools at runtime. 

• QoE Measurement Agent: The QoE Measurement Agent evaluates QoE by measuring user-perceived quality, which often reflects factors beyond traditional network metrics. It collects the specific QoE metrics defined and provided by the App Analysis Agent, enabling targeted and relevant monitoring tailored to each application. Leveraging advanced AI-based tools, such as Vision-Language Model (VLM), Image Quality Assessment (IQA) and Video Quality Assessment (VQA) models, the agent predicts perceived quality automatically and in real time, rather than relying solely on delayed or sparse human feedback. 

• Requirement Inference Agent: This module plays a crucial role in FlexNGIA 2.0 architecture by continuously analyzing at runtime the application, user interactions, and contextual data to dynamically infer and update the precise QoS and performance requirements for each active flow. By leveraging real-time monitoring and intelligent reasoning, this agent translates raw measurements and semantic context into actionable requirement profiles that reflect the current needs of the application and its users. These inferred requirements are then shared with other agents, enabling them to adapt and optimize their operation to meet the evolving application requirements. 

• Information Fusion Agent: The Information Fusion Agent (IFA) plays a central role in aggregating and synthesizing the data collected by all the aforementioned agents to create a comprehensive report, called an IFA report ( Fig. 3). It contains a coherent, comprehensive view of system performance, sate, and user experience. It includes key measurements, along with contextual correlations and analysis, producing a semantically meaningful dataset that other agents can use to guide their operations. 

• SFC & Protocol Agent Given a particular application and its requirements (as described in the IFA Report), this agent is responsible for designing and developing the SFC, its composing network functions and its associated communication protocols. More precisely, it can build the chain leveraging existing network functions or by creating and implementing new ones tailored to the specific requirements of the application. Additionally, it could select one of the legacy protocols (e.g., TCP, QUIC) for the SFC, or decide to design and implement a new communication protocols adapted to the application, specifying how packets are processed and forwarded as they traverse the sequence of network functions, and considering different factors like the SFC’s network functions and the type of the application (e.g., point-to-point vs. multipoint). This ensures that the SFC is perfectly tailored to each application. 

• Congestion Control Agent: The CC Agent dynamically adjusts the congestion control behavior to align with the Target QoS Profile provided by the App Analysis Agent. This agent uses the information provided in the IFA report like the current environment state, QoS and QoE metrics, specific application requirements, and historical decision data. Based on these data, it can decide to tune the parameters of the current CC scheme, switch to an alternative CC scheme, or even design and deploy a new CC scheme tailored to the current network conditions and application requirements. 

• Resource Allocation Agent: The Resource Allocation (RA) Agent is responsible for dynamically managing and optimizing the allocation strategy of computational, bandwidth, storage, and energy resources across the physical infrastructure. Its primary goal is to ensure that the resource allocation algorithm satisfies SFC-specific requirements and QoS constraints while also meeting operational, sustainability, and economic objectives. These objectives may include improving energy efficiency, maximizing the utilization of green and renewable resources, increasing the number of successfully mapped SFCs, minimizing operational costs, and maximizing revenues. Furthermore, the Resource Allocation Agent tightly coordinates with the SFC & Protocol Agent, ensuring that protocol selection and service chain deployment remain feasible given the current resource constraints. Thus, resource allocation decisions are not made in isolation but are tightly coupled with protocol selection and SFC composition. 

• Monitoring & Observability Agent: The Monitoring & Observability Agent is responsible for continuously collecting, analyzing, and interpreting telemetry and performance data across the network and service function chains. Its role is to maintain real-time awareness of network health, resource utilization, and service quality, enabling proactive detection of anomalies, congestion, or potential failures. It provides critical information to other agents to ensure their smooth operation and to guide their decisions. 

• Failure Management Agent: The Failure Management Agent is responsible for detecting, diagnosing, and mitigating failures across the service function chains, network function and the physical infrastructure. IV. F LEX NGIA 2.0 AI A GENTS : D ESIGN , P ROOF OF 

CONCEPT , AND PRELIMINARY EXPERIMENTAL RESULTS 

This Section presents the core design principles of the FlexNGIA 2.0 AI agents, each supported by initial proof-of-concept experiments. These experiments provide preliminary insights into the efficiency of the AI Agents, laying the groundwork for further optimization and validation. 

A. Information Fusion Agent 

The Information Fusion Agent acts as a centralized intelligence synthesizer. It is powered by an LLM that aggregates heterogeneous telemetry data collected IFA Repor t (Template) 

# Observed  QoS  Metrics  (Multi -layer QoS Measurement Agent )

- CWND:  (μ=[ X],  σ=[σ])  ⬈, range=[Y –Z] 

- RTT:  (μ=[ X] ms,  σ=[σ])  ⬊, range=[Y –Z] ms 

- Throughput:  (μ=[ X] Mbps,  σ=[σ])  ⬌, range=[Y –Z] Mbps 

- Loss Ratio:  (μ=[ X] %,  σ=[σ])  ⬈, range=[Y –Z] % 

μ= mean,  σ= standard deviation, 

⬈=increasing, 

⬊=decreasing, 

⬌=stable 

- [Other fields defined by the Agent] 

# User -Perceived Quality  (QoE Measurement Agent )

- AI -Predicted QoE Score:  [0.0  – 1.0] 

- Startup Delay Perceived:  [Acceptable / Noticeable] 

- Current Resolution:  [e.g., 720p, 1080p, Auto] 

- Buffering Events:  [e.g., Smooth, Pixelated] 

- Visual Quality:  [e.g., Clear, Choppy] 

- [Other fields defined by the Agent] 

# Runtime Requirements  (Requirement Inference Agent )

- Key Priorities Inferred:  [e.g., Consistent visual quality, Low buffering frequency] 

- QoS:  [Increase throughput, packet loss could now be tolerated up to 10%, further reduce latency] 

- Behavioral Logic:  [e.g., User prefers stable quality over resolution changes] 

- Strategic Guidance:  [e.g., Recommend maintaining bitrate, reduce retransmission] 

- [Other fields defined by the Agent] 

# Application Profile  (App Analysis Agent )

- Application Type:  [e.g., Video Streaming] 

- Relevant QoS Metrics:  [e.g.,  Latency, Throughput, Packet Loss Ratio, jitter] 

- Relevant QoE Metrics:  [e.g, Startup delay, Buffering ratio, Video resolution/bitrate, Frame 

freezing/stalling events, Playback smoothness, page load time] 

- Requirement Description:  [e.g., real -time audio, video, text, loss tolerable,  ….] 

- Target QoS Profile: 

- Latency:  ≤ [target_latency] ms 

- Throughput:  ≥ [target_throughput] Mbps 

- Packet Loss: < [loss_threshold] % 

- Startup Delay:  ≤ [startup_delay_target] seconds 

- [Other fields defined by the Agent] 

# Device &  Environment  (Environment Analysis Agent )

- Network Interface:  [e.g., Wi -Fi, 5G] 

- Current Congestion Control Algorithm:  [e.g., Reno, BBR, LLM_CC] 

- RF Environment: 

- Signal Strength (RSSI):  [–XX dBm] 

- Channel Utilization: [XX%] 

- Frequency Band: [2.4 / 5 / 6 GHz] 

- Wi -Fi Details: 

- SSID: [e.g., CoffeeShopWiFi, CampusNet, GuestNetwork] 

- BSSID : [e.g, 00:1A:2B:3C:4D:5] 

- PHY Mode: [802.11n / ac / ax] 

- Historical performance and statistics: [e.g., High congestion] 

- Mobility Context: 

- Movement Status: [e.g., Stationary, Mobile, Vehicular , speed ]

- GPS Coordinates: [Latitude, Longitude] 

- Location Type:  [e.g, Urban, Indoor, Outdoor] 

- Competing Flows : [Number of Flows, Consumed Bandwidth] 

- OS: [Android, Windows] 

- [Other fields defined by the Agent] 

# Summary  Report : Application, Environment, QoS, QoE, Requiements 

Fig. 4: Template of the IFA report generated by the Information Fusion Agent. from other agents (Fig. 3) to generate a single, unified report (called IFA Report ). This report provides a coherent snapshot of the system that serves as input for downstream agents. To ensure a consistent structure for the IFA report, we designed a report template (Fig. 4) that provides to the IFA’s LLM the key sections and metrics to be included in the report. Of course, the LLM may identify and add other metrics that it deems relevant. As shown in the figure, the template is organized into sections. The Application Profile section describes the application’s type (e.g., eXtended Reality, file transfer), QoS and QoE metrics, performance requirements, and the target QoS. The Device and Environment section captures details about the environment where the application is running, such as network technology, radio frequency conditions, mobility status, device type, operating system, and competing flows. The Observed QoS Metrics section include key QoS measurements. The User-Perceived Quality section reflects inferred or measured QoE. Finally, the Runtime Requirements section outlines current requirements, priorities, and strategic guidance for runtime adaptation. This structured report serves as the primary input for other agents, providing essential context on the state of the performance, network, application, and device. 

B. AI-Driven SFC & Protocol Agent 

The SFC & Protocol Agent is in charge of the design, deployment, and runtime adjustment of an end-to-end SFC to support the application including its composing NFs and communication protocol. As shown in Fig. 5, this agent can dynamically design the SFC associated with a given application. It determines which network functions should be included in the chain, either by selecting from the Network Function Catalog, a repository of pre-implemented and ready-to-use functions, or by designing and implementing new functions as needed. In parallel, the agent selects the most suitable communication protocol from legacy protocols (e.g., UDP, TCP, QUIC) or designs a custom one, Possible Decisions:      

> –(if a new SFC) Design the SFC and select/design
> its associated network functions and protocol
> –(if existing SFC) check the need to adapt the SFC
> and its associated network functions and protocol
> IFA
> Report Tools
> Memory
> Resource Allocation Agent
> LLM
> Tools
> Memory
> SFC & Protocol Agent
> LLM
> Network Function Catalog
> NF 11
> NF 13
> LLM
> Tools
> Memory
> Information Fusion Agent
> Coordination
> Deployment
> NF 12 NF 12
> Planning Planning Planning
> UE
> S0
> UE
> S0
> Server
> D1
> Server
> D1
> CustomTransportProt.c CustomTransportProt.c NF12.rs NF12.rs NF12.p4 NF12.p4 NF11.c NF11.c CustomTransportProt.c CustomTransportProt.c
> RA Report

Fig. 5: Operation of the SFC & Protocol Agent. then fine-tunes its parameters according to current network conditions and application requirements. In the following, we present the configuration and system prompt of the LLM associated with this agent, along with experimental results that demonstrate its operation. 

• Prompt Structure for the the SFC & Protocol Agent: 

The SFC & Protocol Agent is guided by a structured system prompt that instructs it to design the optimal SFC and communication protocol for a given application, using inputs such as the IFA report, the NF catalog, and the RA Report containing potential paths to host the SFC’s network functions. As shown in Fig. 6, the prompt defines the agent’s goal: to analyze application requirements, select or design and implement appropriate NFs and transport protocols, and a deployment-ready SFC. It also lists actions, from reusing catalog NFs with standard protocols to combining catalog or new NFs with standard or custom protocols. The agent follows a 9-step reasoning process (Fig. 6) that starts with analyzing the application and its requirements, SFC & Protocol Agent System  Prompt 

# Given an IFA report and NF catalog, take a decision by applying the 9 -step 

reasoning process below to design and deploy the optimal Service Function Chain 

(SFC) and communication protocol for the application. 

# Inputs expected:  IFA Report,  Resource Allocation  Report 

# NF Catalog:  {NF catalog will be added here} 

# Goal:  Analyze the application requirements, select appropriate network functions (NFs) and 

Transport  protocol, then generate an SFC deployment with consistent protocol header 

definitions. 

Possible actions: 

(a) Use NFs From catalog + standard protocol (e.g., TCP, UDP, QUIC, SCTP). 

(b) Use NFs From catalog + custom in -chain protocol. 

(c) Use NFs From catalog and/or design new NFs + standard protocol. 

(d) Use NFs From catalog and/or design new NFs + custom in -chain protocol. 

# 9 -step reasoning process: 

Step 1: Identify the application type, communication model (e.g., end -to -end, multipoint), and 

performance/reliability goals from the IFA report. 

Step 2: Extract QoS/QoE requirements, traffic patterns, and protocol constraints. 

Step 3: Check the NF catalog and available protocol options. 

Step 4: Determine whether existing NFs and protocols meet the requirements. 

Step 5: Identify functional gaps in NF or protocol behavior (e.g., missing retransmission, lack of 

sequencing). 

Step 6: Decide whether to use existing, customized, or new NFs and protocols (select a, b, c, or 

d). 

Step 7: Predict how the chosen combination will affect reliability, throughput, latency, and 

compatibility. 

Step 8: Create  the  SFC Composition  (i.e.,  specify  for each network function, its  order, name, role, 

physical  location) . The SFC should be deployed in one of the paths provide d in the Resource 

Allocation Report .

Step 9: Generate the full implementation bundle with consistent protocol headers  (all NF code, 

client.c  for the chain source , server .c  for the chain destination , Makefile for each file ).  If a new 

protocol should be used, consider a custom header on top of UDP identified by a magic field. 

# Output format: 

{

"decision":  "(a), (b), (c), or (d)", 

"explanation" : "...", 

"steps" : { 

"Step 1": "...", 

"Step 2": "...", 

... 

"Step 9": "..." 

}, 

"SFC  Composition ": [ 

{

"Network Function  Order":  order in the chain 

"Network Function Name": " …. ", 

"Role / Description": " ... ", 

"Input Format": "UDP + ProtocolHeader", 

"Output Format": "UDP + ProtocolHeader", 

"Physical Node ": "[s1|s2|s3| …]" 

"Platform ": " Program in c, program in Rust, P4, container, virtual machine …"

}

], 

"Communication Protocol Description" : "Protocol name, purpose,  header  struct, field usage, 

and end -to -end behavior , programming langage ", 

"Code" : { 

"network_functions" : { 

"NF_<name>.c": "Fully commented  c code with debug prints" 

"Makefile ": " Makefile for the code "

}, 

"clien t.c" : "...", 

"server .c" : "...", 

"Makefile ": "...", 

}

}

Fig. 6: System prompt of the SFC & Protocol Agent. +------------------+--------------+------------+-----------------+

| Magic Field (32b)| Version (8b) | Flags (8b) | Header Len (16b)| 

+------------------+--------------+------------+-----------------+

| Seq Num (32b) | Timestamp (64b) |

+----------------------------------------------------------------+

| Original UDP Payload/application data (variable) ... |

+----------------------------------------------------------------+

• Magic Field (32 bits): e.g., 0xABCD1234, unique identifier for your protocol. 

• Version (8 bits): allows evolution of the header format. 

• Flags (8 bits): control bits for optional features (e.g., reliability, encryption). 

• Header Length (16 bits): so the parser knows where the original UDP payload starts. 

• Sequence Number (32 bits): useful for ordering, loss detection. 

• Timestamp (64 bits): for latency measurements and synchronization. 

• Payload: the actual application data. 

Fig. 7: Example of a custom header generated by the SFC & Protocol Agent then checks whether available NFs and protocols fit to these requirements. It then decides to use them or design and implement new ones, and predicts the impact of the chosen solution on performance metrics. Finally, it creates the SFC and generates a complete implementation code bundle including the NFs, client/server programs, and Makefiles. 

• Protocol Generation: When the SFC & Protocol Agent decides to generate a new custom transport protocol rather than using legacy protocols (e.g., UDP, TCP, QUIC, SCTP), we opted to create it on top of UDP to ensure compatibility with today’s networks. The custom protocol is realized by embedding a custom header within the UDP payload. This is performed by inserting a magic field, i.e., a fixed-value sequence (e.g., 0xABCD1234), at the start of the UDP payload as proposed in [43]. By checking this field, intermediate NFs can recognize and parse the custom header (see Fig. 7). If the magic field is missing, then the packet is treated as a traditional UDP packet with no special header. From an implementation standpoint, the new header can be inserted into the UDP payload either by a kernel-level module on the UE or directly by the application itself. 

• Coordination with the Resource Allocation Agent: 

The SFC & Protocol Agent works in close coordination with the Resource Allocation Agent to ensure that the SFC Tools     

> Memory
> SFC & Protocol Agent
> LLM
> Planning
> Tools
> Memory
> SFC & Protocol Agent
> LLM
> Planning
> Tools
> Memory
> Resource Allocation Agent
> LLM
> Planning
> Tools
> Memory
> Resource Allocation Agent
> LLM
> Planning
> SFC_Requirements
> RA_Report
> {
> "sfc_id": "sfc -0",
> "round": 1,
> "source": "NodeA",
> "destination": "NodeB",
> "bandwidth_mbps": 2,
> "latency_ms": 40,
> "chosen_path_id": "p1",
> "nf_requirements": [
> {"nf_id": "NF1", "node": "S1", "platform": "C"},
> {"nf_id": "NF2", "node": "S2", "platform": "P4"}
> ]
> }
> {
> "sfc_id": "sfc -0",
> "round": 1,
> "status": "allocation_confirmed",
> "chosen_path_id": "p1"
> }
> {
> "sfc_id": "sfc -0",
> "round": 0,
> "source": "NodeA",
> "destination": "NodeB",
> "bandwidth_mbps": 2,
> "latency_ms": 40
> }
> {
> "sfc_id": "sfc -0",
> "round": 0,
> "available_paths": [
> {"path_id": "p1", "latency_ms": 30, "nodes": [
> {"id": "S1", "cpu": 1, "mem_gb": 1},
> {"id": "S2", "cpu": 1, "mem_gb": 1}]},
> {"path_id": "p2", "latency_ms": 40, "nodes": [
> {"id": "S4", "cpu": 1, "mem_gb": 1},
> {"id": "S6", "cpu": 1, "mem_gb": 2},
> {"id": "S7", "cpu": 2, "mem_gb": 1}]}
> ]
> }
> SFC_Requirements
> RA_Report
> 1
> 2
> 3
> 4

Fig. 8: Coordination Messages Between the SFC & Protocol Agent and the Resource Allocation Agent. NF 1: 

Name:  Packet Forwarder 

Role:  Forwards IP packets to the next hop without modification 

InputFormat:  IP Packets,  OutputFormat:  IP Packets 

Platforms:  C, Containers, P4 

Requirements:  1 vCPU, 256 MB RAM, No GPU 

NF 2: 

Name:  Transport Assistant 

Role:  Caches packets, detects losses, and retransmits missing segments to 

ensure reliable delivery 

InputFormat:  IP Packets using TCP,  OutputFormat:  IP Packets using TCP 

Platforms:  C, Containers, Python 

Requirements:  2 vCPUs, 1 GB RAM, No GPU 

NF 3: 

Name:  QoS Enforcer 

Role:  Applies scheduling and rate -limiting rules based on QoS tags 

InputFormat:  Tagged Packets with QoS Labels,  OutputFormat:  Prioritized/ 

Policed Packets 

Platforms:  C, Containers, P4 

Requirements:  2 vCPUs, 1 GB RAM, No GPU 

NF 4: 

Name:  Inline Encryption Agent 

Role:  Applies encryption to sensitive flows in real time without affecting 

delay -critical traffic 

InputFormat:  IP Packets,  OutputFormat:  Encrypted IP Packets 

Platforms:  C, Containers, Python, GPU -enabled systems 

Requirements:  3 vCPUs, 2 GB RAM, Optional GPU 

Network Function catalog 

NF 1: 

Name:  Packet Forwarder 

Role:  Forwards IP packets to the next hop without modification 

InputFormat:  IP Packets,  OutputFormat:  IP Packets 

Platforms:  C, Containers, P4 

Requirements:  1 vCPU, 256 MB RAM, No GPU 

NF 2: 

Name:  Transport Assistant 

Role:  Caches packets, detects losses, and retransmits missing segments to 

ensure reliable delivery 

InputFormat:  IP Packets using TCP,  OutputFormat:  IP Packets using TCP 

Platforms:  C, Containers, Python 

Requirements:  2 vCPUs, 1 GB RAM, No GPU 

NF 3: 

Name:  QoS Enforcer 

Role:  Applies scheduling and rate -limiting rules based on QoS tags 

InputFormat:  Tagged Packets with QoS Labels,  OutputFormat:  Prioritized/ 

Policed Packets 

Platforms:  C, Containers, P4 

Requirements:  2 vCPUs, 1 GB RAM, No GPU 

NF 4: 

Name:  Inline Encryption Agent 

Role:  Applies encryption to sensitive flows in real time without affecting 

delay -critical traffic 

InputFormat:  IP Packets,  OutputFormat:  Encrypted IP Packets 

Platforms:  C, Containers, Python, GPU -enabled systems 

Requirements:  3 vCPUs, 2 GB RAM, Optional GPU 

Network Function catalog (a) Network Function Catalog. Tools 

Memory 

SFC & Protocol Agent 

LLM 

Planning 

System 

prompt 

Tools 

Memory 

SFC & Protocol Agent 

LLM 

Planning 

System 

prompt 

Tools 

Memory 

SFC & Protocol Agent 

LLM 

Planning 

System 

prompt 

Decision:  (d) 

Analysis & explanation: 

- Step 1:  Application type: ONLY a subset of packets 

require reliability. Communication model: single 

source to single destination. Goals:  (i)  end -to -end 

reliability for marked packets,  (ii)  do NOT retransmit 

unmarked packets,  (iii)  sustain a  stable 1 Mbps  data 

rate from the source through the chain. 

- Step 2:  QoS/QoE: 1 Mbps steady throughput; 

retransmissions MUST apply ONLY to packets marked 

as reliability -required. Protocol constraints: end -to -

end reliability semantics. 

- Step 3: NF catalog options  relevant:  (1)  Packet 

Forwarder,  (2)  Transport Assistant  (TA)  (TCP by 

default),  (3)  QoS Enforcer for rate -limiting  (4)  Inline 

Encryption Agent for encryption.  Protocol options: 

TCP/UDP/QUIC/SCTP or a custom in -chain protocol. 

- Step 4:  TCP and QUIC enforce end -to -end reliability 

and congestion control, which retransmit all packets 

and modulate the sending rate, preventing the 

required stable 1 Mbps forwarding. QUIC also 

encrypts transport headers, blocking NF visibility and 

hop -by -hop ACKs. SCTP provides reliable, in -order 

delivery. UDP alone provides no reliability, requiring 

a custom in -chain protocol to support selective 

reliability with hop -by -hop semantics. 

- Step 5:  Functional gaps:  (i)  Need per -packet 

reliability signaling and behavior (selective E2E 

retransmission).  (ii)  Need hop -by -hop loss hardening 

without converting non -reliable packets into reliable 

ones.  (iii)  Need explicit 1 Mbps rate conformance. TA 

by default operates on TCP; we need it over a custom 

header and ONLY for REL -marked packets. 

- Step 6:  Chosen action (d):  Use catalog NFs with a 

custom in -chain UDP -based protocol.  Adapt the 

Transport Assistant  to operate over the custom 

header. Non -REL packets are simply forwarded.  QoS 

Enforcer  will police/pace to 1 Mbps.  Packet 

Forwarder  handle directionally -correct hop routing. 

- Step 7:  Impact: Reliability  ↑ for REL -marked packets 

via both end -to -end ARQ (client  dest) and hop -by -

hop local repair (TA). Latency: minimal increase due to 

pacing at 1 Mbps and possible RETX for REL packets 

only. Throughput:  locked to 1 Mbps by QoS Enforcer; 

non -REL packets experience no retrans overhead. 

Compatibility: If magic missing, packets will not be 

parsed. 

- Step 8:  Path choice:  Use abundant -resource path: 

client  → S1  → S2  → S3  → S4  → destination .

Placement: S1=QoS Enforcer, S2=Transport Assistant, 

S3=Packet Forwarder, S4=Packet Forwarder. 

Code: 

- Network functions: 

{protocol.h}, 

{NF_QoS_Enforcer.c} 

{NF_TransportAssistant.c} 

{NF_Forwarder.c} 

- UE:  {client.c}, {server.c} 

- {script.sh}, {Makefile} 

- Step  9: (1) Selective reliability via REL flag; only REL 

packets cached/ACKed. (2) End -to -end ACKs for REL; client 

retransmits until ACK; TA releases on E2E ACK. (3) Non -REL 

are best -effort, never retransmitted. (4) Client + QoS 

Enforcer pace at 1 Mbps to maintain stable rate. 

SFC: 

- Network Function Order: 1 

Network Function Name:  QoS Enforcer 

Role / Description:  Token -bucket pacing/policing at 

exactly 1 Mbps for data packets; forwards ACKs upstream 

immediately. 

Input Format:  CustomHdr -wrapped UDP 

Output Format:  Same as input; paced for data 

Physical Node:  S1 (10.0.0.1) 

Platform:  C

- Network Function Order: 2 

Network Function Name:  Transport Assistant (adapted) 

Role / Description:  Validates header, sends immediate 

ACK upstream, caches, forwards downstream, retransmits 

on timeout. Non -REL: pure forward. 

Input Format:  CustomHdr UDP frames 

Output Format:  CustomHdr UDP frames 

Physical Node:  S2 (10.0.0.2) 

Platform:  C

- Network Function Order: 3, 4 

Network Function Name:  Packet Forwarder 

Role / Description:  Directional hop forwarder: data  →

next hop; ACKs  → previous hop. 

Input Format:  CustomHdr UDP frames 

Output Format:  CustomHdr UDP frames 

Physical Nodes:  S3 (10.0.0.3), S4 (10.0.0.4) 

Platform:  C

Communication Protocol Description: 

A custom UDP protocol on port 8889 with a header 

(magic, flags REL/ACK/RETX, seq, timestamp) enables 

selective per -packet reliability. REL packets get hop -by -

hop ACK/caching and end -to -end ACK/retransmission, 

while non -REL packets are simply forwarded without 

retransmit 

Network Function 

Catalog 

RA Report  (Excerpt) 

# Service Function Chain Deployment Context 

- The chain connects the source (client) to the 

destination (server). 

- Two possible deployment paths are available: 

- Path 1:  client  → s1  → s2  → s3  → s4  →

destination 

- Resource availability: abundant at all nodes. 

- Execution platform: C code on every node. 

- Path 2:  client  → s1  → s2  → s6  → s5  → s4  →

destination 

- Resource availability: limited at all nodes. 

- Execution platform: C code on every node. 

RA Report  (Excerpt) 

# Service Function Chain Deployment Context 

- The chain connects the source (client) to the 

destination (server). 

- Two possible deployment paths are available: 

- Path 1:  client  → s1  → s2  → s3  → s4  →

destination 

- Resource availability: abundant at all nodes. 

- Execution platform: C code on every node. 

- Path 2:  client  → s1  → s2  → s6  → s5  → s4  →

destination 

- Resource availability: limited at all nodes. 

- Execution platform: C code on every node. 

IFA Report  (Excerpt) 

# Application Profile  (App Analysis Agent )

- Application Type:  Custom data transfer 

(selectively reliable communication) 

- Target QoS Profile: 

- Throughput:  ≥ 1 Mbps  and stable 

- Retransmission Policy:  Selective per -packet 

reliability, retransmit only important packets 

- Target QoS Profile:  The application requires 

selective end -to -end reliability: only reliability -

critical packets are retransmitted, while others 

are not, to avoid wasting resources. Traffic is sent 

at a stable 1 Mbps and  must be forwarded at the 

same rate. 

- Minimize f low completion time 

# Device & Environment  (Environment Analysis 

Agent )

- Network Interface:  Wi -Fi 

- RF Environment: 

- Signal Strength (RSSI):  -75 dBm 

-Channel Quality:  Lossy (Non -negligible packet 

loss due to noise and interference) 

- Frequency Band:  2.4 GHz 

- Wi -Fi Details: 

- PHY Mode: 802.11g  (b) Input and output of the SFC & Protocol Agent. 

Fig. 9: Inputs and Outputs of the SFC & Protocol Agent in our experiments. composition, network functions, and protocol configuration are aligned with available computational, bandwidth, and energy resources. This tight integration enables joint optimization, where protocol selection, SFC design, and resource allocation reinforce each other, maximizing QoS guarantees. Fig. 8 illustrates an example of a basic coordination protocol between the two agents. In this example, the SFC & Protocol agent provides the basic requirements of the SFC (e.g., source, destination, required latency and bandwidth) to the RA agent, which in turn supplies it with the paths and resources available in the physical infrastructure between the SFC’s source and destination. This information is taken into account by the the SFC & Protocol agent to carefully craft an adapted SFC. 

• SFC & Protocol Agent in Action – Experimental Proof of Concept: To demonstrate the operation of the SFC & Protocol Agent, we implemented a prototype powered by the LLM GPT-5 Thinking to leverage its advanced reasoning capabilities [44], [23]. Fig.9 illustrates the inputs to the SFC & Protocol Agent (i.e., the IFA and RA reports, NF catalog) used in our experiment and the corresponding output. Fig. 9a shows the NF catalog, which includes four NFs. Each function is defined by its name, role, input/output packet formats, platform, and resource requirements. The catalog spans from basic to advanced functions, addressing reliability, QoS, and security. Fig. 9b shows the IFA report that describes the application’s requirements: it requires selective reliability, i.e., important packets must be retransmitted upon loss, while non-critical packets are delivered on a best-effort basis. In addition, the application demands a stable throughput of at least 1 Mbps and flow completion time should be minimized. The figure also shows the RA report, provided by the Resource Allocation Agent, which contains information about the paths that could potentially host the application’s SFC. As shown, the report indicates two possible paths, detailing the characteristics of each path, its constituent nodes, and the available resources. As shown in the output (Fig. 9b), the LLM agent performed multi-step reasoning, it identified the application’s requirements (step 1 and 2), determined the required network functions (step 3), and decided, with justification for not using legacy protocols (step 4), identified functional gaps (step 5), recognized the need for custom transport protocol and functions (step 6), predicted the performance (step 7), selected the path to embed the SFC, crafted the SFC with its composing NFs, and generated a complete C code bundle for all components (steps 8 and 9). As it can be seen in the output, the agent selected action (d) :

use of NFs from the catalog and/or design new NFs combined with a custom in-chain protocol . The output traces the agent’s step-by-step reasoning, which led it to (1) design LLM-Proto, a lightweight UDP-based protocol with a custom header carrying reliability flags, sequence numbers, and timestamps, and to (2) incorporate two existing NFs into the chain ( QoS Enforcer , Packet Forwarder ); and (3) adapt an existing NF, the Transport Assistant to implement selective reliability. The output provides a detailed description of the SFC and the communication protocol designed by the agent, and includes the generated, ready-to-deploy bundle of C code for the new NFs, the client and the server, a Makefile for compilation, and a shell script for executing the complete SFC in Mininet. Fig. 10 shows the generated SFC and its mapping onto the physical infrastructure, all operating using the generated protocol, LLM-Proto. S0  D1 UE  UE              

> Mapping Mapping
> Physical Infrastructure
> Service Function Chain generated by the SFC & Protocol Agent
> (operates over LLM -Proto)
> 01UE UE
> Packet Forwarder Packet Forwarder
> S3 S3 S2 S2
> QoS Enforcer
> S1 S1
> Point of Presence
> Physical Link
> D
> STraffic Source
> Traffic Destination
> UE User Equipment UE User Equipment
> Virtual Link Virtual Link
> Applied protocol Applied protocol
> Point of Presence
> Physical Link
> D
> STraffic Source
> Traffic Destination
> UE User Equipment
> Virtual Link
> Applied protocol
> S4 S4
> Transport Assistant Transport Assistant

Fig. 10: Description of the generated SFC and custom protocol LLM-Proto. Now that the LLM agent has designed the SFC and provided the implementation in C of the NFs, and the associated custom protocol, we need to evaluate their validity, and effectiveness, and to assess their advantages compared to existing protocols (e.g., TCP and UDP). To this end, we ran experiments with the 

mininet-wifi emulator [45], emulating the network shown in Fig. 11. The application runs on the source UE, sending data to the destination, while other hosts generate background traffic. We assume the application generates 10,000 messages of 1400 bytes each at the rate of 1 Mbps with 50% of these messages requiring reliability. To mimic a realistic lossy environment, the the source UE is connected through a Wi-Fi interface operating in IEEE 802.11g mode on channel 6, under interference conditions similar to those reported in [4]. Three scenarios were compared: (1) no SFC deployed between the source and destination, with TCP as the transport protocol; (2) no SFC with UDP as the transport protocol; and (3) using the SFC (including its NFs) and the custom communication protocol (LLM-Proto) generated by the SFC & Protocol Agent. Our first observation is that the C code generated by 

GPT-5 Thinking ran successfully without any errors, and each NF correctly implemented its functional requirements as defined by the agent. We ran experiments to evaluate performance metrics across the studied scenarios as shown in Fig. 12: percentage of reliable/non-reliable packets received by the destination, flow completion time (i.e., time to transfer the data), and sending/receiving rates at the source/destination. S1  S2  S3  S4       

> S6
> S8 S7
> S5
> (80, 25) (70, 15) (50, 20) (50, 5) (50, 20)
> (Bandwidth Mbps, delay in ms)
> UE : User Equipment
> (50, 10)
> (50, 5) (50, 5)
> (50, 10)
> (70, 10)
> (60, 5)
> (50, 5)
> (60, 12)
> UE
> UE
> Destination
> UE
> Destination
> UE
> AP AP Source
> UE
> Source
> UE
> AP Source
> UE

Fig. 11: Experimental network topology. 

(a) Packets received (b) Flow completion time 

(c) Sending/Receiving rate (Mbps) 

Fig. 12: Performance of the SFC and Protocol generated by the SFC & Protocol Agent (LLM-Proto) compared to legacy protocols UDP and TCP. Fig.12a shows that, as expected, TCP delivers both reliable and non-reliable packets with 100% success, but at the cost of a high flow completion time (Fig. 12b) and unstable sending/receiving rates far below 1Mbps (Fig. 12c), violating the throughput and latency requirements. This behavior is due to TCP’s CC scheme, Reno, which leads to highly variable throughput and to the mandatory retransmission of all packets, increasing the flow completion time. In contrast, UDP delivers packets unreliably ( ≈80%), but achieves low completion time and stable rates near 1Mbps, meeting timeliness requirements but not reliability. Finally, using the SFC and LLM-Proto generated by the SFC & Protocol Agent, all objectives are met: reliable packets received with 100% success, best-effort for non-reliable packets ( ≈79%), lower flow completion time, and stable sending/receiving rates close to 1Mbps (Fig. 12b and Fig. 12c), thereby providing selective reliability while meeting throughput and minimizing flow completion time. In summary, the SFC & Protocol Agent successfully analyzed the application, decided not to use legacy protocols, and generated a fully functional SFC, with custom NFs and transport protocol, tailored to the application’s specific performance, reliability, and behavioral requirements, and outperforming legacy protocols in terms of all requirements. 

C. AI-Driven Congestion Control Agent 

Congestion control, the process of adjusting the sending rate at the source, has been a persistent challenge since the inception of the Internet. Numerous CC algorithms, such as TCP Reno [5], CUBIC [6], BBR [7], and Westwood+ [8], have been proposed to adapt the congestion control behavior to different factors like network conditions (e.g., wireless vs. wired, WAN vs. data centers). Recent studies [4] have also confirmed that the performance of a CC scheme is highly sensitive to the environment in which it operates. As a result, no single CC algorithm performs optimally across all environments, and there is currently no straightforward mechanism to dynamically select or switch between CC schemes based on real-time conditions. In practice, most operating systems adopt a fixed default CC scheme regardless of the environment. For instance, Linux has used CUBIC as the default CC scheme [10], while Windows uses CTCP and DCTCP [11] or CUBIC depending on the version [12], [13]. Newer versions of macOS and iOS default to BBR or CUBIC [13]. These observations highlights two key limitations: (1) the underutilization of existing CC algorithms designed for diverse conditions, and (2) the lack of dynamic mechanisms to adapt CC behavior to real-time network and performance requirements. To address these limitations, FlexNGIA 2.0 introduces an AI-driven Congestion Control Agent capable of analyzing real-time conditions (e.g., execution environment, network status) and dynamically adjusting the parameters of the current CC scheme or switching between different schemes at runtime. Beyond that, it can also design and deploy custom CC scheme on the fly, tailored to the prevailing network conditions. In the following, we present more details about the operation of the CC agent, the prompt for the associated LLM, and experimental results showcasing the operation of the CC agent. 

• Operation of the CC Agent: Fig. 13 illustrates the operation of the CC Agent and its interactions with other agents within the FlexNGIA 2.0 framework. As shown, the Information Fusion Agent provides its detailed IFA report to the CC Agent, which analyzes it to determine the most appropriate decision: (1) maintain the current CC scheme without any changes, (2) retain it and adjust its parameters, and (3) design, implement and deploy a new CC mechanism adapted to the prevailing network conditions and environment. Fig. 13 also shows the various CC modules already deployed in the kernel of the UE’s operating system (e.g., Reno, Westwood+, BBRv2, CUBIC, and Vegas). These modules are commonly available by default in most Linux distributions and the CC Agent can decide to use one them as needed. Additionally, the figure shows custom CC modules (llm cc v( i)) that were dynamically designed and loaded into the kernel at runtime by the CC Agent. The figure also depicts QoS measurement                  

> module Reno Reno BBRv2 BBRv2 CUBIC CUBIC Vegas Vegas llm_cc_v1 llm_cc_v1
> Kernel space
> TCP CC modules
> llm_cc_v0 llm_cc_v0
> llm_cc_v1.ko
> Compile
> User space
> llm_cc_v1.c
> Westwood+ Westwood+
> Congestion Control Agent
> LLM
> Possible Decisions:
> 1-Keep CC & adjust its parameters
> 2-Switch to an existing CC
> 3-Generate a new CC
> -Switch to the selected CC Module
> -Initialize/update Parameters Load new CC at run -time
> LLM
> Tools
> Memory
> Information Fusion Agent
> IFA
> Report

... QoS Metrics     

> Multi -layer QoS
> Measurement Agent
> Multi -layer QoS
> Measurement Agent
> Planning
> Tools
> Memory
> Planning

Fig. 13: Operation of the Congestion Control Agent integrated into the User Equipment. a QoS measurement module integrated into the OS, which collects metrics relevant to congestion control (e.g., window size, throughput, RTT, and retransmission rate) and reports them to the IFA Agent to be included in the IFA agent. 

• Prompt Structure for the CC Agent’s LLM: 

The CC Agent is prompted with a structured system prompt as shown in Fig. 14. The core of this prompt embeds the IFA report, followed by an 8-step reasoning process that ensures consistent behavior across executions. These steps include evaluating the current CC scheme, identifying potential performance bottlenecks, forecasting the outcomes of alternative schemes, and ultimately selecting and justifying the most appropriate action among four options: (a) keep the current scheme and its parameters, (b) keep the current scheme but tune its parameters, (c) switch to a different CC scheme from the existing ones, or (d) generate and deploy a new CC algorithm tailored to the current conditions. Congestion Control Agent System  Prompt 

# Given an IFA report, take a decision by applying the 8 -step 

reasoning process below to maintain the best control of congestion. 

# Input expected:  IFA Report 

# Goal:  Evaluate network behavior and select one of the following actions:       

> (a) Keep the current congestion control algorithm.
> (b)Keep the current congestion control algorithm and only t une its parameters
> (c) Switch to another existing congestion control algorithm :

{List of existing 

congestion control algorithms  will be added here} .

> (d) Generate a new congestion control algorithm tailored to this session context.

# 8 -step reasoning process:    

> Step 1: Identify and analyze the current congestion control algorithm,
> environment, and QoS/QoE status.
> Step 2: Extract key metrics and trends from the IFA report.
> Step 3: Recall the application ’s target QoS profile and check if they are met.
> Step 4: Detect (if exist) performance mismatches and link them to Current
> algorithm behavior.
> Step 5: Identify missing or weak control traits (if exist).
> Step 6: Suggest parameter adjustments or behavioral improvements.
> Step 7: Predict how each action (keep, switch, tune, generate) would affect
> performance.
> Step 8: Choose the best action and briefly justify it.

# Important:  If the decision is to (d) Generate a new congestion control 

> algorithm, you must respect the following kernel module structure for llm_cc.c:

{CC Loadable Kernel Module template will be added here} 

# Output format: 

> {
> "decision": "(a), (b), (c) or (d) ",
> "explanation": "...",
> "steps": [
> "Step 1: ...",
> "Step 2: ...",
> ...
> ],
> "code": "..." (If replacing the congestion control)
> }

Fig. 14: System prompt of the Congestion Control Agent. After completing all eight reasoning steps, the LLM generates a CC Decision Report using a structured format defined in the prompt (see Section #Output Format 

in Fig. 14). This report includes the selected decision, a detailed explanation justifying that decision, and a summary of the reasoning process for the eight steps. If the decision is to design and deploy a new CC scheme (decision d), the LLM generates the implementation code for the new scheme. To make sure the generated code is compatible with the Linux TCP CC subsystem and properly exposes the required hooks via the tcp_congestion_ops 

interface [46], the prompt includes a code template used as a base to generate the CC scheme’s code (see Section #Important in Fig. 14). Indeed, we examined TCP CC implementations in Linux Kernel 5.13.12 and created a minimal yet valid template for a pluggable CC module. Fig. 15 presents this template which includes the three essential methods required by the kernel: 

ssthresh , which defines the behavior of the slow start threshold; cong_avoid , which governs the growth of the congestion window during the congestion avoidance phase; and undo_cwnd , which resets the congestion window during a rollback in the loss recovery process. Once generated, the code is automatically loaded into the kernel at runtime by the CC Agent to seamlessly replace the running scheme, without causing any interruption to the TCP connection or requiring it to be reset. 

• CC Agent in Action - Experimental Results: 

To demonstrate the operation and effectiveness of the CC Agent, we conducted experiments where the agent was tasked with managing CC to meet the application’s target QoS profile. We used mininet-wifi 

to emulate the network shown in Fig. 11 with the same setting described in subsection IV-B. The CC Agent runs on the source UE to manage the congestion control of a TCP flow toward the destination UE. The Agent was powered by the LLM DeepSeek-R1-Distill-Llama-70B [24], a 70-billion-parameter distilled from DeepSeek-R1. This model was selected for its performance on structured, multi-step problem solving, and ability to generate explainable, chain-of-thought reasoning [24]. The studied TCP flow carries the traffic from a video streaming application with strict QoS requirements. These requirements are initially identified by the App Analysis Agent and continuously monitored by the Multi-layer QoS Measurement Agent. The Information Fusion Agent generates an updated IFA report at each evaluation interval (set to 60 s), and sends it to the CC Agent, which uses it to evaluate the current state and take the appropriate decision. #include <linux/module.h>                                                                                                                                        

> #include <linux/init.h>
> #include <linux/types.h>
> #include <net/tcp.h>
> static u32 Ilm_ssthresh(struct sock *sk)
> {/* ssthresh logic */ }
> static void Ilm_cong_avoid(struct sock *sk, u32 ack, u32 acked)
> {/* avoid logic */ }
> static u32 Ilm_undo_cwnd(struct sock *sk)
> {/* undo logic */ }
> static struct tcp_congestion_ops llm_cc __read_mostly ={
> .name ="llm_cc",
> .owner =THIS_MODULE,
> .ssthresh =Ilm_ssthresh,
> .cong_avoid =Ilm_cong_avoid,
> .undo_cwnd =Ilm_undo_cwnd,
> };
> static int _init llm_cc_register(void)
> {return tcp_register_congestion_pontrol(&llm_cc); }
> static void _exit Ilm_cc_unregister(void)
> {tcp_unregister_congestion_control(&llm_cc); }
> module_init(Ilm_cc_register);
> module_exit(llm_cc_unregister);
> MODULE_AUTHOR( "FlexNGIA -LLM Generated CC" );
> MODULE_LICENSE("GPL");
> MODULE_DESCRIPTION( "Adaptive CC generated by LLM" );
> #include <linux/module.h>
> #include <linux/init.h>
> #include <linux/types.h>
> #include <net/tcp.h>
> static u32 Ilm_ssthresh(struct sock *sk)
> {/* ssthresh logic */ }
> static void Ilm_cong_avoid(struct sock *sk, u32 ack, u32 acked)
> {/* avoid logic */ }
> static u32 Ilm_undo_cwnd(struct sock *sk)
> {/* undo logic */ }
> static struct tcp_congestion_ops llm_cc __read_mostly ={
> .name ="llm_cc",
> .owner =THIS_MODULE,
> .ssthresh =Ilm_ssthresh,
> .cong_avoid =Ilm_cong_avoid,
> .undo_cwnd =Ilm_undo_cwnd,
> };
> static int _init llm_cc_register(void)
> {return tcp_register_congestion_pontrol(&llm_cc); }
> static void _exit Ilm_cc_unregister(void)
> {tcp_unregister_congestion_control(&llm_cc); }
> module_init(Ilm_cc_register);
> module_exit(llm_cc_unregister);
> MODULE_AUTHOR( "FlexNGIA -LLM Generated CC" );
> MODULE_LICENSE("GPL");
> MODULE_DESCRIPTION( "Adaptive CC generated by LLM" );

Fig. 15: Template for a pluggable Linux CC module (in C). 

(a) Congestion window 

(b) Throughput 

(c) RTT 

(d) Loss ratio 

Fig. 16: Performance metrics over time showing the CC agent’s decisions to switch between an existing CC scheme (TCP Reno ) and the generated schemes 

LLM_CC_V1 and LLM_CC_V2 , all at runtime. The red bullets indicate the times at which the CC agent performed an evaluation of the CC scheme (evaluation #1 and #2). Fig. 16 presents the evolution of key performance metrics (i.e., congestion window, throughput, RTT, and packet loss ratio) throughout the experiment, which lasted 200 seconds. As shown in Fig. 16, initially, the system runs with TCP Reno .The CC Agent, based on the IFA reports, decides to design and deploy a new CC scheme LLM_CC_V1 (t = 70 s) and then another one, LLM_CC_V2 (t = 148 s). Each transition to a new CC scheme corresponds to visible changes in the plotted metrics and illustrates the agent’s ability to adapt in real time to target the delay, throughput and packet loss requirements (see Section # Application Profile in the IFA report in Subfigure 17a). Hence, it successfully developed CC schemes that reduced the variability of the congestion window, increased and stabilized the throughput around 12Mbps while keeping the packet loss below 4% (Fig. 16). Fig. 17 provides deeper insight into the decision-making process. Subfigure 17a shows the first IFA report generated during the TCP Reno operation. It shows Decision:  Replace 

Explanation:  Cu rrent  Congestion control Reno 

does not meet the requirements . While BBR 

would be an improvement over Reno, the 

specific requirements for stable throughput 

(≥12 Mbps) and strict delay constraints  (≤90 ms) 

in a 802.11g Wi -Fi environment call for a custom 

solution. A hybrid algorithm  combin ing  BBR's 

bandwidth estimation with Vegas -like delay 

sensitivity, while adding wireless -specific 

adaptations, would best address the se 

simultaneous requirements. 

Steps: 

- Step 1:  The current congestion control  is Reno ,

used for a video streaming application over 

a Wi -Fi network with interference. 

- Step 2:  Key observations: Throughput is 

unstable ( μ=7  Mbps, range  0–13  Mbps), delay is 

near the threshold  (μ=85  ms), and CWND is 

highly variable  (σ=63).  The network shows 

potential (peak 13 Mbps) but is underutilized. 

- Step 3:  Critical misses: Delay (85ms with spikes 

to 267ms > 90ms target) and throughput 

stability ( μ=7 Mbps < 12Mbps target) fail 

requirements  and  packet loss (1.16% < 4%) is 

acceptable but shows problematic peaks 

(6.64%). 

- Step 4:  Reno's main flaws: Over -reacts to 

wireless packet loss, lacks delay sensitivity, and 

has poor throughput stability  - all problematic 

for video streaming. 

- Step 5:  Required traits: 1) Wireless loss 

tolerance, 2) Dual delay/throughput 

optimization, 3) Stability mechanisms, 

4) Fast convergence  - no single existing 

algorithm combines all optimally. 

- Step 6:  Parameter tuning is insufficient. While 

BBR would help, it doesn't fully address wireless 

variability or provide enough delay control for 

this strict 90ms target. 

- Step 7:  A custom algorithm blending BBR's 

bandwidth probing, Vegas' delay sensitivity, and 

wireless -specific adaptations would best meet 

all requirements. .

- Step 8:  Generate a new hybrid algorithm 

Code:  {llm_cc_v1.c} 

Decision:  Replace 

Explanation:  Cu rrent  Congestion control Reno 

does not meet the requirements . While BBR 

would be an improvement over Reno, the 

specific requirements for stable throughput 

(≥12 Mbps) and strict delay constraints  (≤90 ms) 

in a 802.11g Wi -Fi environment call for a custom 

solution. A hybrid algorithm  combin ing  BBR's 

bandwidth estimation with Vegas -like delay 

sensitivity, while adding wireless -specific 

adaptations, would best address the se 

simultaneous requirements. 

Steps: 

- Step 1:  The current congestion control  is Reno ,

used for a video streaming application over 

a Wi -Fi network with interference. 

- Step 2:  Key observations: Throughput is 

unstable ( μ=7  Mbps, range  0–13  Mbps), delay is 

near the threshold  (μ=85  ms), and CWND is 

highly variable  (σ=63).  The network shows 

potential (peak 13 Mbps) but is underutilized. 

- Step 3:  Critical misses: Delay (85ms with spikes 

to 267ms > 90ms target) and throughput 

stability ( μ=7 Mbps < 12Mbps target) fail 

requirements  and  packet loss (1.16% < 4%) is 

acceptable but shows problematic peaks 

(6.64%). 

- Step 4:  Reno's main flaws: Over -reacts to 

wireless packet loss, lacks delay sensitivity, and 

has poor throughput stability  - all problematic 

for video streaming. 

- Step 5:  Required traits: 1) Wireless loss 

tolerance, 2) Dual delay/throughput 

optimization, 3) Stability mechanisms, 

4) Fast convergence  - no single existing 

algorithm combines all optimally. 

- Step 6:  Parameter tuning is insufficient. While 

BBR would help, it doesn't fully address wireless 

variability or provide enough delay control for 

this strict 90ms target. 

- Step 7:  A custom algorithm blending BBR's 

bandwidth probing, Vegas' delay sensitivity, and 

wireless -specific adaptations would best meet 

all requirements. .

- Step 8:  Generate a new hybrid algorithm 

Code:  {llm_cc_v1.c} 

IFA Report ( Evaluation #1 )

# Observed QoS Metrics  (Multi -layer QoS Measurement 

Agent )

- CWND:  (μ= 130 , σ= 63 ) ⬊, range=[ 10 -456 ]

- E2e delay : (μ= 85  ms,  σ= 10 ) ⬊, range=[ 77 –267 ] ms 

- Throughput:  (μ= 7 Mbps,  σ= 2) ⬈, range=[ 0–13 ] Mbps 

- Los t segments : (μ= 1.16% , σ= 2.44 ) ⬊, range=[ 0.03 –6.64 ]%

μ= mean,  σ= standard deviation, 

⬈=increasing, 

⬊=decreasing, 

⬌=stable 

# User -Perceived Quality  (QoE Measurement Agent )

- AI -Predicted QoE Score:  0.63 /1 

- Startup Delay Perceived:  Noticeable 

- Current Resolution:  720p 

- Buffering Events:  Pixelated 

- Visual Quality:  Choppy 

# Runtime Requirements  (Requirement Inference Agent )

- Key Priorities Inferred:  Maintain video smoothness 

- QoS:  Further Increase throughput 

- Behavioral Logic:  User prioritizes smooth playback over 

resolution 

# Summary  Report : Application, Environment, QoS, 

QoE, Requiements 

# Application Profile  (App Analysis Agent )

- Application Type:  Video streaming 

- Target QoS Profile: 

- E2e Delay : ≤ 90 ms 

- Throughput:  ≥ 12  Mbps  and stable 

- Packet Loss:  < 4 % 

- Startup Delay:  ≤ 2  seconds 

# Device & Environment  (Environment Analysis Agent )

- Network Interface:  Wi -Fi 

- Current Congestion Control Algorithm:  Reno 

- RF Environment: 

- Signal Strength (RSSI):  –65  dBm 

- Frequency Band:  2.4 GHz 

- Wi -Fi Details: 

- PHY Mode: 802.11g 

- Mobility Context:  Stationary 

Tools 

Memory 

Congestion 

control Agent 

LLM 

Planning 

System 

prompt 

Tools 

Memory 

Congestion 

control Agent 

LLM 

Planning 

System 

prompt (a) IFA and decision reports at the first CC evaluation Decision:  Replace 

Explanation:  While LLM_CC_V1 shows 

reasonable performance, it fails to fully meet the 

strict  throughput (10.5 < 12 Mbps) and E2E delay 

is only at the boundary (90 ms) with spikes up to 

367 ms > 90 ms . The QoE score (0.63) and visual 

artifacts indicate these QoS shortcomings are 

impacting user experience. Given that this is 

already a custom algorithm (LLM_CC_V1), we 

should develop an enhanced version 

(LLM_CC_V2) that specifically addresses: 1) More 

aggressive throughput optimization when 

conditions allow, 2) Tighter delay control, 

and 3)  better adaptation to 2.4 GHz 802.11g Wi -Fi 

environment's characteristics. 

Steps: 

- Step 1: Current algorithm is LLM_CC_V1 in 

802.11g Wi -Fi ( -65dBm). QoS shows borderline 

performance: delay  μ=9 0ms ( meets target on 

average but spikes to 367 ms ), throughput 

μ= 10.5 Mbps (<12Mbps target), with good packet 

loss (0. 6%). QoE issues persist. 

- Step 2:  Key metrics: Throughput below target 

(10.5/12 Mbps) with occasional deep dips; E2E 

delay at the boundary (90  ≈ 90  ms) with spikes to 

367 ms; CWND shows low variability ( σ=16.5). 

- Step 3:  QoS gaps: Throughput (10.5 < 12 Mbps) 

misses target; E2E delay meets on average (90  ≤

90  ms) but violates with spikes (max 367 > 90 

ms); packet loss excellent (0.6% < 4%). Network 

capacity exists (peaks at 14 Mbps) .

- Step 4:  Current version appears too 

conservative  - not fully utilizing available 

bandwidth while allowing  transient bursts that 

push E2E above the 90 ms budget (spikes to 367 

ms) .

- Step 5:  Needed improvements: 1) More 

dynamic bandwidth utilization, 2)  Tighter delay 

guardrail with hysteresis to hold  ≤90  ms and 

suppress spikes , 3) Better spike prevention in 

wireless environment. 

- Step 6:  Parameter tuning may help but won't 

address fundamental architectural limitations for 

these strict requirements. 

- Step 7:  A new version could: 1) Implement dual 

delay/throughput control, 2) Add wireless -

specific adaptations, 3) Include more aggressive 

but safe probing when conditions permit .

- Step 8:  Develop LLM_CC_V2 to specifically 

target the remaining performance gaps while 

building on V1's strengths. 

Code:  {llm_cc_v 2.c} 

Decision:  Replace 

Explanation:  While LLM_CC_V1 shows 

reasonable performance, it fails to fully meet the 

strict  throughput (10.5 < 12 Mbps) and E2E delay 

is only at the boundary (90 ms) with spikes up to 

367 ms > 90 ms . The QoE score (0.63) and visual 

artifacts indicate these QoS shortcomings are 

impacting user experience. Given that this is 

already a custom algorithm (LLM_CC_V1), we 

should develop an enhanced version 

(LLM_CC_V2) that specifically addresses: 1) More 

aggressive throughput optimization when 

conditions allow, 2) Tighter delay control, 

and 3)  better adaptation to 2.4 GHz 802.11g Wi -Fi 

environment's characteristics. 

Steps: 

- Step 1: Current algorithm is LLM_CC_V1 in 

802.11g Wi -Fi ( -65dBm). QoS shows borderline 

performance: delay  μ=9 0ms ( meets target on 

average but spikes to 367 ms ), throughput 

μ= 10.5 Mbps (<12Mbps target), with good packet 

loss (0. 6%). QoE issues persist. 

- Step 2:  Key metrics: Throughput below target 

(10.5/12 Mbps) with occasional deep dips; E2E 

delay at the boundary (90  ≈ 90  ms) with spikes to 

367 ms; CWND shows low variability ( σ=16.5). 

- Step 3:  QoS gaps: Throughput (10.5 < 12 Mbps) 

misses target; E2E delay meets on average (90  ≤

90  ms) but violates with spikes (max 367 > 90 

ms); packet loss excellent (0.6% < 4%). Network 

capacity exists (peaks at 14 Mbps) .

- Step 4:  Current version appears too 

conservative  - not fully utilizing available 

bandwidth while allowing  transient bursts that 

push E2E above the 90 ms budget (spikes to 367 

ms) .

- Step 5:  Needed improvements: 1) More 

dynamic bandwidth utilization, 2)  Tighter delay 

guardrail with hysteresis to hold  ≤90  ms and 

suppress spikes , 3) Better spike prevention in 

wireless environment. 

- Step 6:  Parameter tuning may help but won't 

address fundamental architectural limitations for 

these strict requirements. 

- Step 7:  A new version could: 1) Implement dual 

delay/throughput control, 2) Add wireless -

specific adaptations, 3) Include more aggressive 

but safe probing when conditions permit .

- Step 8:  Develop LLM_CC_V2 to specifically 

target the remaining performance gaps while 

building on V1's strengths. 

Code:  {llm_cc_v 2.c} 

IFA Report ( Evaluation #2 )

# Observed QoS Metrics  (Multi -layer QoS Measurement 

Agent )

- CWND:  (μ= 187, σ= 16.5 ), low variability , range=[ 140 -20 0]

- E2e delay : (μ= 90  ms,  σ= 19 ), near target; stable with 

notable spikes , range=[ 77 –367 ] ms 

- Throughput:  (μ= 10 .5 Mbps,  σ= 2.28 ), moderately stable 

but below target, occasional deep dips , range=[ 0–14 ]

Mbps 

- Packet Loss : (μ= 0.6% , σ= 0.24 ), low and stable , range= 

[0.14 –0.94 ] %

μ= mean,  σ= standard deviation 

# User -Perceived Quality  (QoE Measurement Agent )

- AI -Predicted QoE Score:  0.63 /1 

- Startup Delay Perceived:  Noticeable 

- Current Resolution:  720p 

- Buffering Events:  Pixelated 

- Visual Quality:  Choppy 

# Runtime Requirements  (Requirement Inference Agent )

- Key Priorities Inferred:  Maintain video smoothness 

- QoS:  Further Increase throughput 

- Behavioral Logic:  User prioritizes smooth playback over 

resolution 

# Summary  Report : Application, Environment, QoS, 

QoE, Requiements 

# Application Profile  (App Analysis Agent )

- Application Type:  Video streaming 

- Target QoS Profile: 

- E2e Delay : ≤ 90 ms 

- Throughput:  ≥ 12  Mbps  and stable 

- Packet Loss:  < 4 % 

- Startup Delay:  ≤ 2  seconds 

# Device & Environment  (Environment Analysis Agent )

- Network Interface:  Wi -Fi 

- Current Congestion Control Algorithm:  LLM_CC_V1 

- RF Environment: 

- Signal Strength (RSSI):  –65  dBm 

- Frequency Band:  2.4 GHz 

- Wi -Fi Details: 

- PHY Mode: 802.11g 

- Mobility Context:  Stationary 

IFA Report ( Evaluation #2 )

# Observed QoS Metrics  (Multi -layer QoS Measurement 

Agent )

- CWND:  (μ= 187, σ= 16.5 ), low variability , range=[ 140 -20 0]

- E2e delay : (μ= 90  ms,  σ= 19 ), near target; stable with 

notable spikes , range=[ 77 –367 ] ms 

- Throughput:  (μ= 10 .5 Mbps,  σ= 2.28 ), moderately stable 

but below target, occasional deep dips , range=[ 0–14 ]

Mbps 

- Packet Loss : (μ= 0.6% , σ= 0.24 ), low and stable , range= 

[0.14 –0.94 ] %

μ= mean,  σ= standard deviation 

# User -Perceived Quality  (QoE Measurement Agent )

- AI -Predicted QoE Score:  0.63 /1 

- Startup Delay Perceived:  Noticeable 

- Current Resolution:  720p 

- Buffering Events:  Pixelated 

- Visual Quality:  Choppy 

# Runtime Requirements  (Requirement Inference Agent )

- Key Priorities Inferred:  Maintain video smoothness 

- QoS:  Further Increase throughput 

- Behavioral Logic:  User prioritizes smooth playback over 

resolution 

# Summary  Report : Application, Environment, QoS, 

QoE, Requiements 

# Application Profile  (App Analysis Agent )

- Application Type:  Video streaming 

- Target QoS Profile: 

- E2e Delay : ≤ 90 ms 

- Throughput:  ≥ 12  Mbps  and stable 

- Packet Loss:  < 4 % 

- Startup Delay:  ≤ 2  seconds 

# Device & Environment  (Environment Analysis Agent )

- Network Interface:  Wi -Fi 

- Current Congestion Control Algorithm:  LLM_CC_V1 

- RF Environment: 

- Signal Strength (RSSI):  –65  dBm 

- Frequency Band:  2.4 GHz 

- Wi -Fi Details: 

- PHY Mode: 802.11g 

- Mobility Context:  Stationary 

Tools 

Memory 

Congestion 

control Agent 

LLM 

Planning 

System 

prompt 

Tools 

Memory 

Congestion 

control Agent 

LLM 

Planning 

System 

prompt  (b) IFA and decision reports at the second CC evaluation 

Fig. 17: IFA Reports and their corresponding CC Agent’s decision reports. also the CC Decision Report with the structured reasoning that led the CC agent to create the new CC scheme 

LLM_CC_V1 to replace TCP Reno . The analysis highlights Reno’s main flaws (step 4), the required features (step 5), the inadequacy of existing schemes (step 6), and the need for a new CC scheme that addresses the identified issues by blending BBR’s bandwidth probing, Vegas’ delay sensitivity, and wireless-specific adaptation (step 7 and 8). Subfigure 17b shows the second IFA report generated during the LLM_CC_V1 operation. Although LLM_CC_V1 has improved throughput and reduced loss, the RTT remains unstable. The agent’s reasoning shown in the new CC Decision Report recommends a refined algorithm, LLM_CC_V2 .The results clearly demonstrate how the CC agents adapts the scheme to evolving conditions by combining real-time measurements, structured reasoning, and dynamic code synthesis and deployment. 

D. AI-Driven Resource Allocation Agent 

The Resource Allocation Agent is responsible for managing and optimizing the allocation strategy of the resources for SFCs while ensuring operational, sustainability, and economic objectives. As shown in Fig. 18, the RA Agent is provided with detailed information about the network’s infrastructure. This includes the physical infrastructure’s composition (i.e., defines what physical resources exist and how they are connected) such as servers, routers, switches, storage units, technologies. The RA Agent relies also on a resource allocation scheme catalog, which is a structured set of predefined strategies, algorithms, and policies designed to efficiently assign resources to incoming SFCs across the physical infrastructure. The catalog specifies for each algorithm the tunable parameters to be used to adapt it to different network conditions and application requirements. The RA agents can dynamically select, adjust, an RA algorithm from the catalog or even design Possible Decisions: 

- Keep the current resource allocation algorithm 

- Keep the current resource allocation algorithm 

and adjust its parameters 

- Design and deploy a new resource allocation algorithm 

Resource Allocation  Agent 

LLM 

Tools 

Memory 

SFC & Protocol  Agent 

LLM 

Resource Allocation 

Scheme Catalog 

Coordination  

> 0
> 1
> 2
> 6
> 7
> 59
> 12
> 8
> 10

Physical Infrastructure 

> 3
> 4
> 13
> 11
> UE
> UE

Resource 

Allocation  

> S1 D1

Application 1 - SFC 1

NF 12 NF 11  NF 13  

> UE UE

Design of the SFC 

and its associated Protocol 

Resource Allocation 

Algorithm 

Resource 

Allocation 

Request 

Planning 

Tools 

Memory 

Planning 

Infrastructure 

Composition & Status 

Possible Decisions: 

- Keep the current resource allocation algorithm 

- Keep the current resource allocation algorithm 

and adjust its parameters 

- Design and deploy a new resource allocation algorithm 

Resource Allocation  Agent 

LLM 

Tools 

Memory 

SFC & Protocol  Agent 

LLM 

Resource Allocation 

Scheme Catalog 

Coordination  

> 0
> 1
> 2
> 6
> 7
> 59
> 12
> 8
> 10

Physical Infrastructure 

> 3
> 4
> 13
> 11
> UE
> UE

Resource 

Allocation  

> S1 D1

Application 1 - SFC 1

NF 12 NF 11  NF 13  

> UE UE

Design of the SFC 

and its associated Protocol 

Resource Allocation 

Algorithm 

Resource 

Allocation 

Request 

Planning 

Tools 

Memory 

Planning 

Infrastructure 

Composition & Status 

Fig. 18: Operation of the Resource Allocation Agent. a new RA strategies to optimize performance and achieve operational, sustainability, and economic goals. Aligned with this design, Li et al. [47] presented a solution demonstrating how LLMs can be leveraged to guide resource allocation. Specifically, they leverage LLMs to enhance the efficiency of a genetic algorithm in exploring the solution space for resource allocation problems. Our proposed RA Agent generalizes this concept, enabling LLMs not only to fine-tune heuristics but also to select among different allocation schemes and even design new allocation strategies. Furthermore, the RA agent receives regular statistics about the current state of the infrastructure (e.g., utilization, utilization fairness, operational state, performance metrics, or faults) and other metrics pertaining to the operational and sustainability objectives, including operational costs (e.g., cost of deployment and operation of network functions), revenues, energy efficiency, use of green sources, green penalties (e.g., CO 2 emissions penalty), and profits. 

• Prompt Structure of the RA Agent: Fig. 19 shows the system Prompt Structure for the RA Agent. The prompt instructs the RA agent’s LLM to recommend updated weight values for the multi-objective function guiding SFC mapping decisions, based on the current weights and a 24-hour statistics report. The LLM follows a structured three-step reasoning process: (1) analyze how the current weights influence the objective function relative to the observed statistics, (2) determine metric priorities and rank them according to criticality, and (3) recommend updated weights, emphasizing the most critical metrics while providing a justification based on the data. The output includes the recommended weights and a step-by-step explanation of the reasoning process. 

• RA Agent in Action - Experimental Results: 

To demonstrate the operation and effectiveness of the RA Agent, we conducted simulations of a realistic scenario in which multiple SFCs dynamically arrive over time and must be deployed across the physical infrastructure (Fig. 18). We developed a resource allocation algorithm guided by the following objective function to carefully select physical nodes in the network to host the NFs of the incoming SFCs: 

Obj = α1 · Cost + α2 · (1 − P rof it )+ α3 · [U tilization + (1 − F airnessIndex )] + α4 · GreenP enalty (1) This formulation captures four complementary goals: (i) minimizing operational and deployment cost, (ii) maximizing profit, (iii) avoiding resource overutilization and improving fairness across nodes by leveraging Jain’s fairness index [48], and (iv) enforcing sustainability by penalizing allocations that rely on non-renewable energy sources. Each term in the objective function is normalized using the min–max method to ensure that all components contribute comparably to the overall optimization, preventing any single metric from dominating due to scale differences. The RA Agent, leveraging its LLM powered by GPT-5 Thinking [23], acts as a decision engine that periodically Ressource Allocation Agent System  Prompt                                                

> # Given the current weights and a 24h statistics report, take a decision by
> applying the 3 -step reasoning process below to recommend updated weight
> values ( α1 –α4) for the objective function.
> # Objective Function (to guide SFC mapping decisions):
> Objective = α1 * normalizedCost // minimize cost
> + α2 * (1 -normalizedProfit) // maximize profit
> + α3 * normalizedUtilization // reduce utilization to improve load balancing
> + α3 * (1 -UtilizationFairness) // increase fairness (Jain ’s index) for load balancing
> + α4 * normalizedGreenPenalty // minimize penalty for not using green energy
> # Inputs:
> -Current Weights: {α1, α2, α3, α4}
> -Statistics Report (last 24h) , containing: Infrastructure Utilization ,Utilization Fairness ,
> Number of Mapped SFCs, S FC Average Arrival Rate Per Second, SFC Average Lifetime ,
> Green Penalty ,Costs ,Revenue ,Profit
> # Goal:
> Evaluate how the current weights influence the objective function with respect to the observed
> statistics, and propose an updated set of weights (α1 –α4) that better align with QoS, fairness,
> cost, profit, and sustainability objectives.
> # 3 -step reasoning process:
> Step 1: Analyze current weights and how they influence the objective function relative to the
> observed statistics.
> Step 2: Analyze priorities:
> -Identify which metric(s) are most critical to optimize.
> -Determine which metrics are already healthy and can receive lower weight.
> -Rank metrics by importance (e.g., Green Penalty > Utilization > Profit > Cost).
> Step 3: Recommend αvalues:
> -Assign values for α1 –α4 in [0.0, 1.0] such that their sum = 1.0.
> -Emphasize the most critical metric(s) according to priorities.
> -Ensure recommendations are justified based on statistics and priorities.
> # Output format:
> {
> "recommendation": ["α1": ... ,"α 2": ... ,"α 3": ... , "α 4": ... ]
> "steps": [
> "Step 1: ...",
> "Step 2: ...",
> "Step 3: ..."
> ]
> }

Fig. 19: System prompt of the Ressource Allocation Agent. monitors infrastructure metrics and high-level objectives, and recommends updated weight values (α1 to α4)for the multi-objective optimization function. As such, RA Agent adjusts the relative importance of each objective based on its analysis of runtime network conditions, costs, profits, and green energy penalties, thereby balancing economic efficiency, performance, and environmental sustainability. In our experiments, we simulate a physical infrastructure of 24 nodes connected through 57 links [25]. Each node can host multiple instances (containers) and is characterized by its per-instance operational cost (ranging from 0.03 to 0.08 $/s), green penalty (0.01 to 0.05 $/s), selling price (0.07 to 0.08 $/s), and capacity (40 to 100 instances). SFCs are generated dynamically over time, each connecting a single source to a randomly selected destination. A chain contains between 1 and 20 NFs. The RA algorithm is implemented based on the code of [25] and is guided by the objective function (Eq. 1). Figure 21 summarizes the results of the resource allocation simulations over a period of six days (144 hours). We compared two scenarios: one in which the objective function weights remain fixed throughout the simulation to focus only on maximizing profit ( α2 = 1), and another in which the RA Agent dynamically adjusts the weights based on reports received every 24 hours. Fig. 20 illustrates the reasoning performed by the RA agent’s LLM and guided by the prompt structure provided in Fig. 19. The reasoning takes place at each evaluation cycle (set to 24 hours). For each cycle, it shows the analysis of the current statistics, the decision on updated weights, and the rationale behind it. Subfigure 21(a) shows the SFC arrival rate over time Recommendation: 

- α1: 0.05,  α2: 0.25,  α3: 0.30,  α4: 0.40 

Steps: 

- Step1:  The current weights ( α1=0.0, α2=1.0, α3=0.0, 

α4=0.0)  focused entirely on profit maximization. This 

successfully resulted in a high profit value (1,248,621.79) and 

acceptable cost and revenue metrics. However, it completely 

ignored other objectives, leading to an extremely high green 

penalty (1,240,478.32) due to non -sustainable energy use 

and low infrastructure utilization (0.34997) indicating 

inefficient resource usage, despite good fairness (0.79632) 

being achieved incidentally 

- Step2:  Priorities are set based on the critical needs revealed 

by the statistics. The extremely high Green Penalty is the 

most critical issue to address for sustainability. Improving 

Utilization & Fairness is the second priority to achieve better 

efficiency and load balancing. Profit is healthy but must 

remain a consideration, while Cost is acceptable and is the 

lowest  priority. Rank:  Green Penalty ( α4) >  Utilization & 

Fairness  (α3) >  Profit  (α2) >  Cost  (α1) 

- Step3:  New  α values are proposed to sum to 1.0.  The 

highest weight (0.40) is assigned to  α4  (green penalty) to 

aggressively minimize the sustainability penalty.  A significant 

weight (0.30) is assigned to  α3  to improve resource 

utilization and load balancing.  Profit  (α2)  is reduced but 

maintained at 0.25  to preserve revenue generation.  Cost 

(α1)  is given a minimal weight of 0.05  as it is not a critical 

problem. This balanced approach addresses the key 

shortcomings of the previous configuration (a) 24h Recommendation :

- α1: 0.05,  α2: 0.15,  α3: 0.30,  α4: 0.50 

Steps: 

- Step1:  Analysis of the previous weights ( α1=0.05, 

α2=0.25, α3=0.30, α4=0.40)  shows a partial improvement. 

The significant weight on profit  (α2=0.25)  successfully drove 

a large increase in profit to ~2.5M. However, the 40% weight 

on the green penalty was completely ineffective, as the 

penalty  value more than doubled to ~2.2M, indicating a 

critical failure to use green energy. The 30% weight on 

utilization also failed to improve the low utilization rate, 

which remained stagnant at ~0.345. The system remains 

revenue -driven but is inefficient and unsustainable 

- Step2:  Priorities are re -evaluated based on the persistent 

critical issues. The Green Penalty remains the absolute top 

priority; its drastic increase  proves it requires an even 

stronger emphasis to force a change in energy sourcing 

behavior. Utilization & Fairness is the second priority, as low 

utilization signifies poor infrastructure efficiency that the 

current weight has not corrected. Profit is healthy and 

increasing, so its priority can be lowered further. Cost 

remains a non -issue.  The updated rank is: Green Penalty 

(α4) >>  Utilization & Fairness  (α3) >  Profit  (α2) >  Cost  (α1) 

- Step3:  New  α values are proposed to apply maximum 

pressure on the most critical metric.  The weight for the 

green penalty  (α4)  is increased from 0.40 to 0.50  to make it 

the dominant factor in the objective function and compel a 

reduction in non -green energy use.  The weight for 

utilization  (α3)  is maintained at 0.30  to continue pushing for 

better resource efficiency.  The weight for profit  (α2)  is 

reduced from 0.25 to 0.15 , as it is already highly successful 

and can afford a lower emphasis.  The weight for cost  (α1)  is 

kept minimal at 0.05  (b) 48h Recommendation: 

- α1: 0.05, α2: 0. 10 , α3: 0. 25 , α4: 0. 60

Steps: 

- Step1:  Analysis of the previous weights  (α1=0.05, 

α2=0.15, α3=0.30, α4=0.50)  reveals a critical systemic issue. 

Despite the green penalty  (α4)  having the highest weight at 

0.50, its value increased dramatically to ~3.12M. This 

indicates that the optimization is fundamentally constrained, 

likely due to a scarcity of green energy capacity or conflicting 

placement constraints, causing it to prioritize profit (which 

grew to ~3.72M) over sustainability. The utilization also 

remains stubbornly low (~0.344), showing the 0.30 weight 

on  α3  is ineffective at improving efficiency, potentially for 

similar operational reasons 

- Step2:  The persistent and drastic rise in the green penalty, 

even after aggressive weighting, confirms it remains the 

absolute top priority. Improving utilization is the second 

priority, as low resource usage is inefficient and may be 

contributing to the high penalty (e.g., idle non -green nodes 

running). Profit is strong and can be de -prioritized further to 

create room for solving the primary issues. Cost remains a 

non -critical factor.  The updated priority rank is: Green 

Penalty ( α4) >>  Utilization & Fairness  (α3) >  Profit  (α2) > 

Cost  (α1) 

- Step3:  New  α values are proposed to apply maximum 

pressure on the green penalty, as half the weight was 

insufficient.  The weight for  α4  is increased to 0.60  to 

dominate the objective function and force the system to find 

feasible green placements, even at a higher cost or slightly 

lower profit.  The weight for  α3  is slightly reduced to 0.25  to 

remain significant but not override the new green directive. 

The weight for  α2  is reduced to 0.10  to further disincentivize 

profit -maximization that conflicts with sustainability.  The 

weight for  α1  is held at 0.05  (c) 72h Recommendation: 

- α1: 0.05, α2: 0. 05, α3: 0. 40, α4: 0. 50

Steps: 

- Step1:  Analysis of the  previous  weights ( α1=0.05, 

α2=0.10, α3=0.25, α4=0.60)  under a significantly higher 

arrival rate (0.400) reveals a system under stress. The high 

weight on green penalty  (α4=0.60)  failed to curb its growth, 

which surged to ~6.27M, indicating a severe scarcity of 

feasible green placement options. While utilization improved 

to ~0.466, fairness degraded sharply to ~0.676, and the 

acceptance ratio dropped to ~52%, showing the optimizer is 

struggling with load balancing and feasibility under heavy 

load, likely prioritizing profit (which remains high at ~5.26M) 

over other constraints when green options are unavailable 

- Step2:  Priorities must be rebalanced to address acute 

operational issues. The Green Penalty remains the top long -

term priority but requires operational fixes beyond 

weighting. The drastic degradation in Fairness and 

Acceptance Ratio under high load becomes the most critical 

immediate priority to maintain system stability and SLA 

compliance. Utilization is improving but needs continued 

focus. Profit is healthy and can be heavily deprioritized to 

free up objective function weight for more pressing issues. 

Cost remains a non -issue.  The revised priority rank is: 

Acceptance Ratio & Fairness ( α3) >  Green Penalty  (α4) > 

Utilization > Profit  (α2) >  Cost  (α1) 

- Step3:  New  α values are proposed to urgently address the 

degraded fairness and acceptance.  The weight for  α3  is 

significantly increased to 0.40  to strongly incentivize load 

balancing and improve acceptance under high stress.  The 

weight for  α4  is reduced to 0.50 , maintaining high pressure 

for sustainability but acknowledging that weight alone 

cannot solve a capacity shortage.  The weight for  α2  is 

halved to 0.05  to prevent profit motives from undermining 

fairness and acceptance decisions.  The weight for  α1  is held 

at 0.05 . This shift aims to restore stability and fairness first, 

while continuing to push for greener placements where 

possible  (d) 96h Recommendation: 

- α1: 0.05, α2: 0. 02 , α3: 0. 45 , α4: 0.4 8

Steps: 

- Step1:  Analysis of the  previous  weights ( α1=0.05, 

α2=0.05, α3=0.40, α4=0.50)  under sustained high load 

(ArrivalRate=0.40) shows a system failing on multiple fronts. 

The high weight on fairness/utilization  (α3=0.40)  failed to 

prevent a severe degradation in Utilization Fairness (0.617) 

and Acceptance Ratio (~0.50), indicating the optimizer is 

unable to effectively balance load. Critically, the Green 

Penalty has exploded to ~9.44M despite a 0.50 weight on  α4, 

strongly suggesting a fundamental scarcity of feasible green 

placement options or a critical normalization issue. Profit 

remains excessively high (~6.74M), confirming the optimizer 

is still finding ways to prioritize revenue over all other 

objectives 

- Step2:  The escalating Green Penalty remains the 

paramount strategic risk and must be addressed with 

extreme prejudice. However, the  catastrophic drop in 

Fairness and Acceptance Ratio represents an immediate 

operational crisis that is degrading service quality. Therefore, 

priorities must be split: the objective function must apply 

maximum pressure to both issues simultaneously. Profit is 

robust enough to be nearly eliminated as a consideration. 

The priority rank is: Green Penalty ( α4)  ≈ Fairness & 

Acceptance  (α3) >>  Utilization > Profit  (α2) >  Cost  (α1) . The 

system must be forced to find feasible placements that are 

both greener and fairer, even at a significant profit cost 

- Step3:  The weight for  α3  is increased to 0.45  to make load 

balancing and acceptance the top operational priority to 

restore fairness.  The weight for  α4  is set to 0.48 , keeping 

immense pressure on sustainability. The combined 0.93 

weight on these two objectives will force the optimizer to 

prioritize them.  The weight for profit  (α2)  is slashed to 0.02 

to remove its ability to override these critical goals.  The 

weight for cost  (α1)  is held at 0.05 . This configuration  is an 

emergency measure to halt the degradation in fairness and 

the explosion of the green penalty  (e) 120h 

Fig. 20: Decision reports of the Resource Allocation Agent across five evaluation cycles. 

(a) SFC Arrival Rate (b) Cost ($/min) (c) Revenue ($/min) 

(d) Green Penalty ($/min) (e) Profit ($/min) (f) Infrastructure Utilization 

(g) Utilization Fairness 

Fig. 21: Comparison of resource allocation metrics with static weights and with RA agent-managed weights. The red bullets indicate the times at which the RA agent updated the weights. The green vertical line marks the moment when the SFC arrival rate doubled. For each half of each subfigure, the displayed percentage represents the relative increase or decrease of the metric under RA agent-managed weights compared to the metric with static weights. (i.e., the average number of SFC requests per minute), highlighting the dynamic workload faced by the RA Agent. To evaluate the agent’s response to sudden demand changes, we deliberately double the SFC arrival rate after three days of simulation, leading to higher infrastructure utilization. Since both scenarios are executed under the same simulation setup and arrival process, the arrival rate curves are identical and serve as a common reference for comparison. We first analyze the initial three days of the simulation, during which the average arrival rate is 6 SFCs/min. As it can be seen in the Subfigure (d) and (e), after the first 24 hours, the RA-agent has succeeded to adjust the weights, reducing the green penalty by up to 20% while keeping almost the same profit for the infrastructure provider. This was explicitly decided by the Agent’s LLM in Fig. 20 (a) and (b) at 24h and 48h. The updated weights prioritized allocations on nodes powered by greener energy sources (the ones with low green penalty), but this came at the expense of relying on resources with higher operational costs (+8%). The revenue, infrastructure utilization, and fairness were barely impacted (-2% as shown in Subfigure (c), (f) and (g)). We then examine the final three days, with the SFC arrival rate doubled. As shown in Subfigures (d) and (e), the RA agent successfully adjusted the weights, increasing the profit by 7.4% while reducing the green penalty by 9.6%. However, this improvement came at the expense of a moderate increase in operational cost (+4.7%). These adjustments were explicitly decided by the Agent’s LLM in Fig.20(d) and (e) at 96h and 120h. The updated weights continued to prioritize allocations on nodes with greener energy sources (lower green penalty), however with higher weights for the utilization and fairness compared to the first three days as the LLM increased load on the infrastructure on the infrastructure and decided to further balance the workload and improve fairness among the physical load (see Fig.20(d) and (e)). The resulting revenue, infrastructure utilization, and fairness remained comparable for both scenarios as observed in Subfigures 21 (c), (f), and (g). These results demonstrate that the RA agent’s LLM effectively adapts the weight allocations to dynamic workloads, by efficiently analyzing the current network status and achieving a balanced trade-off between profit, green energy usage, operational cost, and fairness. V. K EY RESEARCH DIRECTIONS 

Although promising, the design and experiments presented in this paper are still early. In the following, we present key research challenges in order to evolve FlexNGIA 2.0 toward a full-fledged agentic AI network. 

A. Building Powerful and Reliable LLMs for Network Agentic AI: Design, Customization, and Evaluation 

A key challenge in realizing FlexNGIA 2.0’s Agentic AI vision lies in building the brain of each agent, its LLM. This work presented proof-of-concept and preliminary experimental results where we used generic LLMs that were customized thanks to the prompts. We have also observed varying performance and outcomes when testing different LLMs (e.g., GPT-5, DeepSeek-R1-Distill-Llama-70B), which underscores the importance of developing standardized benchmarks to systematically evaluate and compare their capabilities in network-related tasks. Beyond basic customization, it is also crucial to develop purpose-built LLMs tailored specifically for each agent’s unique role and responsibilities. TelecomGPT [49] is a telecom-specific LLM pre-trained on domain-specific datasets derived from 3GPP and IEEE standards, as well as arXiv publications. Such a cornerstone could support the development of powerful LLMs capable of designing protocols, algorithms, and management schemes for communication networks from congestion control, traffic engineering to resource allocation, monitoring and fault management. Purpose-built and fine-tuned LLMs must not only understand networking concepts but also reason effectively, gather and filter relevant data from diverse sources and existing protocols and algorithms, identify cause–effect relationships, and maintain situational awareness. The agent’s LLM must be capable of setting both short-term and long-term objectives, devising adaptive strategies, making informed decisions, and translating them into precise logical actions and step-by-step plans. Furthermore, it must be able to anticipate potential collaborations with other agents, leveraging collective intelligence to achieve shared goals. Furthermore, ensuring the reliability and the performance of protocols, algorithms and configurations designed by these LLMs requires robust mechanisms to verify and control their logic and implementation. This includes continuous testing, validation, and performance evaluation, potentially using parallel simulation or digital twin environments at runtime, to safely assess their behavior and impact before full deployment. Such rigorous checks are critical to prevent unintended consequences and maintain network stability. Designing such a cognitive core, capable of integrating domain expertise, reasoning skills, and autonomous decision-making would mark a fundamental breakthrough in enabling FlexNGIA 2.0’s agentic AI adaptive, self-evolving network management paradigm. 

B. LLM Prompting for the Design of Intelligent Network Protocols and Management Algorithms 

Prompting plays a crucial role in effectively leveraging LLMs, as it directly shapes the quality, relevance, and explainability of their outputs. Well-crafted prompts act as the interface through which an LLM understands the context, assesses the situation, and generates solutions. In our experiments, we observed that the structure and content of the prompt significantly affect the LLM’s ability to design communication protocols and network-related algorithms. Therefore, it is essential to develop dedicated prompt models or templates specifically tailored for the LLMs powering FlexNGIA 2.0 AI agents. Such prompt frameworks should enable the LLM to efficiently perceive network states, identify goals and strategies, define step-by-step actions and plans, anticipate potential impact, invoke relevant measurement and analysis tools, and apply systematic reasoning to design network components. Establishing robust prompting methodologies will be key to harnessing LLMs’ potential in creating adaptive, intelligent, and explainable networking solutions within the FlexNGIA 2.0 architecture. 

C. Rethinking the way Network Services, Protocols and, Algorithms are Designed 

A key step in advancing network intelligence lies in rethinking how services, protocols, algorithms, and standards are designed in the AI era. AI, especially LLMs, can dramatically accelerate the entire workflow, spanning design, implementation, testing, performance evaluation, and deployment of network functions and protocols. Our preliminary experiments indicate that LLMs deliver more reliable and higher-quality algorithms when guided by structured inputs and access to rich contextual information. To enable this, a comprehensive set of libraries is essential, encompassing existing services, protocols, and algorithms, as well as fundamental low-level building blocks such as packet processing primitives, measurement tools, and algorithmic components. These libraries could serve as the foundation for dynamically composing sophisticated network services, protocols, and algorithms, customized for specific applications and operational scenarios. The orchestration and synthesis of these components can be performed autonomously by LLM-powered AI agents or via human-in-the-loop collaboration, combining computational efficiency with domain expertise to achieve optimized, adaptable, and robust network designs. 

D. AI Agent Design and Coordination Challenges 

Designing AI agents for FlexNGIA 2.0 introduces several fundamental challenges. Each agent must possess a robust internal architecture that supports autonomous reasoning, goal setting, adaptive planning, and learning from dynamic network conditions. Beyond intelligence, agents must be equipped with appropriate tools to perceive the environment, analyze network states, and execute actions safely. Additionally, agents must coordinate effectively with each other to ensure coherent system-wide behavior. Without such coordination, independent decisions could conflict, leading to suboptimal performance and unintended outcomes. Proper collaboration enables the overall system to align objectives, balance trade-offs, and maintain overall stability, ensuring that actions collectively enhance performance and reliability objectives. This work presented an initial design and implementation of these AI agents that demonstrate their potential for runtime adaptation and decision-making. However, substantial work remains to refine agent architectures, expand their capabilities, enhance inter-agent coordination, and validate their performance across complex scenarios. 

E. Designing New Breeds of Network Protocols 

The ability offered by FlexNGIA to design protocols customized for each application opens the door to a wide array of possibilities and challenges. One compelling research direction is the development of multi-point aware protocols, which requires rethinking traditional end-to-end communication models. Such protocols must enable a single instance to handle multiple endpoints while ensuring consistency, reliability, and performance guarantees. This approach allows the network to manage application traffic more efficiently, by intelligently handling priorities, orchestrating in-path processing, and managing packet duplication across different endpoints. Another compelling challenge is the design of network function-aware protocols, which are cognizant of the presence, capabilities, and behavior of in-path network functions. Such protocols should not only leverage these network functions effectively but may also influence or define their behavior and operational logic to optimize end-to-end communication and achieve the performance and reliability goals. Additional challenges include ensuring scalability across heterogeneous network environments, providing robust security and privacy guarantees, supporting real-time monitoring and feedback, and enabling interoperability with legacy protocols. Furthermore, defining mechanisms for dynamic customization and evolution of protocol logic at runtime, potentially guided by AI agents, presents both theoretical and practical research questions in protocol design, verification, and deployment. VI. C ONCLUSION 

In this paper, we proposed FlexNGIA 2.0, an Agentic AI-driven architecture for the future Internet that embeds cognitive intelligence and reasoning capabilities into the core of the network. This is achieved by introducing LLM-based AI agents capable of autonomously evaluate the network conditions and application requirements, and to design, implement, and adapt service function chains, network functions, communication protocols, congestion control strategies, and resource allocation algorithms. Our prototype and experimental evaluation demonstrated the ability of these agents to design and implement end-to-end custom SFCs, to craft tailored transport protocols, to select or design and implement congestion control schemes, and to optimize resource allocation, all at runtime. These results highlight the transformative potential of integrating generative and reasoning-driven agents into the network architecture, paving the way toward networks that are not only programmable but also self-learning and self-adaptive. While this work is a major step toward agentic AI-driven networking, numerous key challenges remain. Future efforts must move beyond prompt-based customization toward purpose-built LLMs, with strong domain knowledge, reasoning, planning, and algorithm design capabilities adapted to network, services, protocols and management. REFERENCES [1] “Transmission Control Protocol (TCP),” RFC 793, Sep. 1981. [Online]. Available: https://rfc-editor.org/rfc/rfc793.txt [2] J. Iyengar and M. Thomson, “QUIC: A UDP-Based Multiplexed and Secure Transport,” Oct. 2018, work in Progress. [Online]. Available: https://datatracker.ietf.org/doc/html/draft-ietf-quic-transport-16 [3] “Stream Control Transmission Protocol (SCTP),” RFC 4960, Sep. 2007. [Online]. Available: https://rfc-editor.org/rfc/rfc4960.txt [4] Y. Korbi, M. F. Zhani, and J. Kaippallimalil, “Congestion Control in Wi-Fi Networks - State of the Art, Performance Evaluation, and Key Research Directions,” IEEE Access , vol. 12, 2024. [Online]. Available: https://doi.org/10.1109/ACCESS.2024.3425271 [5] V. Jacobson, “Congestion Avoidance and Control,” in ACM Conference on Communications Architectures and Protocols (SIGCOMM) , August 1988. [Online]. Available: https://doi.org/10.1145/52324.52356 [6] S. Ha, I. Rhee, and L. Xu, “CUBIC: A New TCP-Friendly High-Speed TCP Variant,” ACM SIGOPS Operating Systems Review , vol. 42, no. 5, 2008. [Online]. Available: https://doi.org/10.1145/1400097.1400105 [7] N. Cardwell, Y. Cheng, C. S. Gunn, S. H. Yeganeh, and V. Jacobson, “BBR: Congestion-Based Congestion Control,” Communications of the ACM , vol. 60, no. 2, January 2017. [Online]. Available: https://doi.org/10.1145/3009824 [8] C. Casetti, M. Gerla, S. Mascolo, M. Sanadidi, and R. Wang, “TCP Westwood: End-to-End Congestion Control for Wired/Wireless Networks,” Wireless Networks , vol. 8, no. 5, September 2002. [Online]. Available: https://doi.org/10.1023/A:1016590112381 [9] J. Zhang, Y. Zhang, E. Dong, Y. Zhang, S. Ren, Z. Meng, M. Xu, X. Li, Z. Hou, Z. Yang, and X. Fu, “Bridging the Gap between QoE and QoS in Congestion Control: A Large-scale Mobile Web Service Perspective,” in USENIX Annual Technical Conference (USENIX ATC) , Jul. 2023. [10] “Linux Kernel Source: TCP CONG ADVANCED in net/ipv4/Kconfig (v6.15.9), Linux Kernel Git Repository,” 2025, accessed: 2025-08-07. [Online]. Available: https://git.kernel.org/pub/scm/linux/kernel/git/ stable/linux.git/tree/net/ipv4/Kconfig?h=v6.15.9#n469 [11] Microsoft, “Set-NetTCPSetting (NetTCPIP),” 2025, accessed: 2025-08-21. [Online]. Available: https : / / learn . microsoft . com / en - us / powershell / module / nettcpip / set - nettcpsetting ? view = windowsserver2025-ps#-congestionprovider [12] M. N. Team, “Top 10 Networking Features in Windows Server 2019 #8: A Faster, Safer Internet,” 2019, accessed: 2025-08-07. [Online]. Available: https://techcommunity.microsoft.com/blog/networkingblog/ top-10-networking-features-in-windows-server-2019-8-a-faster-safer-internet/339749#community-339749-toc-hId--1685247744 [13] ns-3 project, “CUBIC,” 2022, accessed: 2025-08-07. [Online]. Available: https://www.nsnam.org/docs/release/3.36/models/html/tcp.html#cubic [14] M. F. Zhani and H. Elbakoury, “FlexNGIA: A Flexible Internet Architecture for the Next-Generation Tactile Internet,” Journal of Network and Systems Management (JNSM), Springer , 2020. [Online]. Available: https://doi.org/10.1007/s10922-020-09525-0 [15] FlexNGIA Project, https://www.FlexNGIA.net/, accessed: 2025-08-11. [16] D. Harrington, B. Wijnen, and R. Presuhn, “An Architecture for Describing Simple Network Management Protocol (SNMP) Management Frameworks,” RFC 3411, Dec. 2002. [Online]. Available: https://www.rfc-editor.org/info/rfc3411 [17] “OpenStack: Open Source Cloud Computing Software,” https://www. openstack.org/, 2025, accessed: 2025-08-08. [18] “OpenDaylight: A Linux Foundation Collaborative Project to Advance Software-Defined Networking,” https://www.opendaylight.org/, 2025, accessed: 2025-08-08. [19] “Tungsten Fabric: Open Source Network Virtualization and Cloud Networking,” https://tungsten.io/, 2025, accessed: 2025-08-08. [20] “Open Network Automation Platform (ONAP),” https://www.onap.org/, 2025, accessed: 2025-08-08. [21] H. Derouiche, Z. Brahmi, and H. Mazeni, “Agentic AI Frameworks: Architectures, Protocols, and Design Challenges,” 2025. [Online]. Available: https://arxiv.org/abs/2508.10146 [22] P. Bornet, J. Wirtz, T. H. Davenport, D. De Cremer, B. Evergreen, P. Fersht, R. Gohel, S. Khiyara, N. Mullakara, and P. Sund, Agentic Artificial Intelligence: Harnessing AI Agents to Reinvent Business, Work and Life . Singapore?: Irreplaceable Publishing, 2025. [23] OpenAI, “Introducing GPT-5,” 2025, accessed: 2025-08-18. [Online]. Available: https://openai.com/index/introducing-gpt-5/ [24] DeepSeek-AI, “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,” arXiv preprint arXiv:2501.12948 ,2025. [Online]. Available: https://doi.org/10.48550/arXiv.2501.12948 [25] T. Moufakir, M. F. Zhani, A. Gherbi, M. Aloqaily, and N. Ghrada, “SFCaaS: Service function chains as a service in NFV environments,” 

ITU Journal on Future and Evolving Technologies , vol. 3, no. 3, 2022. [Online]. Available: https://doi.org/10.52953/ZPDB8065 [26] W. Racheg, N. Ghrada, and M. F. Zhani, “Profit-driven resource provisioning in NFV-based environments,” in IEEE International Conference on Communications (ICC) , Paris, France, May 21-25, 2017. [27] F. Tashtarian, M. F. Zhani, B. Fatemipour, and D. Yazdani, “CoDeC: A cost-effective and delay-aware SFC deployment,” IEEE Transactions on Network and Service Management (TNSM) , vol. 17, no. 2, 2020. [Online]. Available: https://doi.org/10.1109/TNSM.2019.2949753 [28] A. Amokrane, M. F. Zhani, Q. Zhang, R. Langar, R. Boutaba, and G. Pujolle, “On satisfying green SLAs in distributed clouds,” in 

IEEE/ACM/IFIP International Conference on Network and Service Management (CNSM) , Rio de Janeiro, Brazil, Nov. 17-21, 2014. [29] Z. Alomari, M. F. Zhani, M. Aloqaily, and O. Bouachir, “Towards optimal synchronization in NFV-based environments,” 

Wiley International Journal of Network Management (IJNM) , vol. 33, no. 1, 2023. [Online]. Available: https://doi.org/10.1002/nem.2218 [30] S. Aidi, M. F. Zhani, and Y. Elkhatib, “On improving service chains survivability through efficient backup provisioning,” in IEEE/ACM/IFIP International Conference on Network and Service Management (CNSM) ,Rome, Italy, Nov. 5-9, 2018. [31] Z. Alomari, M. F. Zhani, M. Aloqaily, and O. Bouachir, “On Ensuring Full Yet Cost-Efficient Survivability of Service Function Chains in NFV Environments,” Journal of Network and Systems Management (JNSM), Springer , vol. 31, no. 3, apr 2023. [Online]. Available: https://doi.org/10.1007/s10922-023-09734-3 [32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention Is All You Need,” arXiv preprint arXiv:1706.03762 , 2017. [Online]. Available: https://arxiv.org/abs/1706.03762 [33] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” 

arXiv preprint arXiv:1810.04805 , 2019. [Online]. Available: https: //arxiv.org/abs/1810.04805 [34] Q. Wang, Z. Chen, Z. Li, Y. Huang, L. Bing, and L. Si, “A Survey on Large Language Model based Autonomous Agents,” 

arXiv preprint arXiv:2308.11432 , 2023. [Online]. Available: https: //arxiv.org/abs/2308.11432 [35] SuperAnnotate, “LLM agents: The ultimate guide 2025,” https://www. superannotate.com/blog/llm-agents, Mar. 2025, accessed: 2025-08-11. [36] T. Schick, A. Dwivedi-Yu, R. Dessı, R. Raileanu, M. Lomeli, A. Severyn, Y. LeCun, A. Celikyilmaz, and E. Grave, “Toolformer: Language Models Can Teach Themselves to Use Tools,” arXiv preprint arXiv:2302.04761 ,2023. [Online]. Available: https://arxiv.org/abs/2302.04761 [37] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou, “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,” arXiv preprint arXiv:2201.11903 , 2022. [Online]. Available: https://doi.org/10.48550/arXiv.2201.11903 [38] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan, “Tree of Thoughts: Deliberate Problem Solving with Large Language Models,” arXiv preprint arXiv:2305.10601 , 2023. [Online]. Available: https://arxiv.org/abs/2305.10601 [39] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large Language Models are Zero-Shot Reasoners ,” 2023. [Online]. Available: https://arxiv.org/abs/2205.11916 [40] Google Developers Blog, “Announcing the agent2agent protocol (a2a),” https : / / developers . googleblog . com / en / a2a - a - new - era - of - agent -interoperability/, Apr. 2025, accessed: 2025-08-24. [41] IBM Research. (2024) Agent communication protocol (acp). Accessed: 2025-08-24. [Online]. Available: https://research.ibm.com/projects/ agent-communication-protocol [42] Anthropic. (2024, Dec.) Model context protocol (mcp). Accessed: 2025-08-24. [Online]. Available: https://www.anthropic.com/news/ model-context-protocol [43] T. Herbert and L. Yong, “UDP Magic Numbers,” Internet Engineering Task Force (IETF), Internet-Draft draft-herbert-udp-magic-numbers-00, 2015. [Online]. Available: https://datatracker.ietf.org/doc/html/draft-herbert-udp-magic-numbers-00 [44] OpenAI, “Introducing GPT-5,” 2025, accessed: 2025-08-18. [Online]. Available: https://academy.openai.com/public/resources/intro-gpt-5 [45] R. Fontes and C. E. Rothenberg, “Mininet-WiFi: A Platform for Hybrid Physical-Virtual Software-Defined Wireless Networking Research,” in 

ACM SIGCOMM Conference , 2016. [46] “Linux Kernel Project,” accessed: 2025-08-24. [Online]. Available: https://elixir.bootlin.com/linux/v6.16.3/source/include/net/tcp.h#L1221 [47] Y. Li, Q. Zhang, H. Yao, R. Gao, X. Xin, and M. Guizani, “Next-Gen Service Function Chain Deployment: Combining Multi-Objective Optimization With AI Large Language Models,” IEEE Network , vol. 39, no. 3, 2025. [48] R. Jain, D.-M. Chiu, and W. Hawe, “A quantitative measure of fairness and discrimination for resource allocation in shared computer systems,” 

DEC Research Report TR-301 , 1984. [49] H. Zou, Q. Zhao, Y. Tian, L. Bariah, F. Bader, T. Lestable, and M. Debbah, “TelecomGPT: A Framework to Build Telecom-Specific Large Language Models,” arXiv preprint arXiv:2407.09424 , Jul 2024. [Online]. Available: https://arxiv.org/abs/2407.09424
