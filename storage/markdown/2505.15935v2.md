Title: MAPS: A Multilingual Benchmark for Global Agent Performance and Security

URL Source: http://arxiv.org/pdf/2505.15935v2

Published Time: Fri, 15 Aug 2025 00:09:29 GMT

Markdown Content:
# MAPS: A Multilingual Benchmark for Agent Performance and Security 

Omer Hofman 1, Oren Rachmil 1, Shamik Bose 1, Vikas Pahuja 1, Jonathan Brokman 1,Toshiya Shimizu 2, Trisha Starostina 3, Kelly Marchisio 3,Seraphina Goldfarb-Tarrant 3, Roman Vainshtein 11Fujitsu Research of Europe 2Fujitsu Limited 3Cohere Abstract 

Agentic AI systems, which build on Large Lan-guage Models (LLMs) and interact with tools and memory, have rapidly advanced in capa-bility and scope. Yet, since LLMs have been shown to struggle in multilingual settings, typ-ically resulting in lower performance and re-duced safety, agentic systems risk inheriting these limitations. This raises concerns about the accessibility of such systems, as users in-teracting in languages other than English may encounter unreliable or security-critical agent behavior. Despite growing interest in evalu-ating agentic AI, existing benchmarks focus exclusively on English, leaving multilingual settings unexplored. To address this gap, we propose MAPS, a multilingual benchmark suite designed to evaluate agentic AI systems across diverse languages and tasks. MAPS builds on four widely used agentic benchmarks — GAIA (real-world tasks), SWE-bench (code genera-tion), MATH (mathematical reasoning), and the Agent Security Benchmark (security). We translate each dataset into eleven diverse lan-guages, resulting in 805 unique tasks and 9,660 total language-specific instances - enabling a systematic analysis of the multilingual effect on AI agents’ performance and robustness. Em-pirically, we observe degradation in both per-formance and security when transitioning from English to other languages, with severity vary-ing by task and correlating with the amount of translated input. Building on these find-ings, we provide actionable recommendations to guide agentic AI systems development and assessment under multilingual settings. This work establishes the first standardized evalua-tion framework for multilingual agentic AI, en-couraging future research towards equitable, re-liable, and accessible agentic AI. MAPS bench-mark suite will become publicly available upon publication. 1

> 1https://huggingface.co/datasets/Fujitsu-FRE/ MAPS

Figure 1: MAPS benchmark suite evaluates LLM-based agents across 12 languages and 4 agentic benchmarks covering performance and security. 

1 Introduction 

LLM-based agentic AI systems combine multi-step reasoning with external tools and memory to solve open-ended tasks such as code generation, web navigation, planning, and transactional ser-vices like booking and ordering (Acharya et al., 2025). By doing so, they extend to complex, real-world problems beyond standard LLM bench-marks. Since such real-world applications serve speakers of diverse languages, maintaining consis-tent reliability in every language becomes critical. However, since agentic behavior is grounded in LLMs, which often perform inconsistently across languages (Deng et al., 2023; Wang et al., 2023), agents may inherit these multilingual limitations as well, affecting their functionality and trustworthi-ness. This presents a barrier to equitable access, as non-English users may face degraded responses, in-correct tool actions, or unsafe behaviors—failures that can lead to actual harm in the real world, in-cluding erroneous transactions, data corruption, and security vulnerabilities (Zhang et al., 2024). To assess emerging agentic systems, various 1

> arXiv:2505.15935v2 [cs.DB] 13 Aug 2025

benchmarks have been proposed to evaluate agent performance across a range of tasks (Mialon et al., 2023; Jimenez et al., 2023; Chang et al., 2024; Xu et al., 2024). However, these benchmarks re-main English-only. In contrast to multilingual LLM benchmarks (Dang et al., 2024; Shi et al., 2022; Goyal et al., 2022), no equivalent exists for agentic AI—creating a blind spot in our understanding of cross-language performance, safety, and security. In this paper, we address this gap. We hypoth-esize that multilingual settings will reveal perfor-mance and security gaps in agentic systems that are not captured by the existing, English-only bench-marks. To investigate this, we introduce MAPS, a Multilingual Agentic AI Benchmark Suite for 

Performance and Security. MAPS is based on four established agentic benchmarks: GAIA (real-world tasks) (Mialon et al., 2023), SWE-bench (code gen-eration) (Jimenez et al., 2023), MATH (mathemat-ical reasoning) (Hendrycks et al., 2021), and the Agent Security Benchmark (security) (Zhang et al., 2024). These benchmarks are extended to eleven typologically diverse languages beyond English 2,by employing a hybrid machine and LLM-based translation approach Ki and Carpuat (2024) with extended verification and enhancements. In total, MAPS includes 805 unique tasks, each available in 12 language versions—including the original English and 11 translated variants—for a total of 9,660 instances. An overview of the benchmark layout is shown in Fig. 1. To demonstrate the use of MAPS and test our hy-pothesis, we selected a leading open-source agent associated with each of the four original bench-marks and applied it to the corresponding multilin-gual extension. We observed notable declines in both task performance and security when moving from English to other languages, with the severity of these drops varying by task type and correlating with the proportion of non-English input tokens, suggesting that multilingual performance interven-tions should be targeted based on input composition and task sensitivity. Beyond overall degradation, our findings reveal that multilingual inputs can am-plify agentic vulnerabilities in safety-critical tasks, highlighting the need for multilingual risk assess-ment. These results empirically support our hy-pothesis and demonstrate the utility of MAPS for multilingual evaluation of agentic AI systems. 

> 2MAPS offers evaluation in twelve languages: English, German, Spanish, Portuguese (Brazil), Japanese, Russian, Italian, Arabic, Hebrew, Korean, Chinese and Hindi.

This paper makes three primary contributions: • To the best of our knowledge, we introduce the first multilingual benchmark suite for agentic AI, extending four widely used benchmarks into eleven typologically diverse languages for systematic performance and security as-sessment. • The efficacy and quality of the proposed benchmark are demonstrated through a large-scale evaluation of leading agents as well as human expert verification. • We present the first quantifiable analysis and evidence that multilingual settings reveal criti-cal performance, safety, and security gaps in agentic systems, along with actionable recom-mendations for improving their development. 

2 Background and Related Work 

2.1 Agentic AI Benchmarks 

With the rapid advancement of LLM-based agents, a diverse suite of benchmarks has been developed to assess their autonomy, tool use, planning, and memory integration (Yao et al., 2024; Xu et al., 2024; Yehudai et al., 2025). We organized these suites along three primary dimensions. 

Evaluation objective : Performance-oriented benchmarks measure task completion, multi-step reasoning, and correct tool use (e.g., AgentBoard (Chang et al., 2024)), whereas security-focused suites probe robustness to adversarial inputs, jail-breaks, and unsafe behaviors (e.g., AgentHarm (Andriushchenko et al., 2024)). Agentic task scope : Full-agentic evaluations present only prob-lem statements and expected outcomes, requiring end-to-end planning and execution (e.g., GAIA (Mialon et al., 2023)), while semi-agentic frame-works supply scaffolding, such as code templates or mock APIs, to isolate the LLM’s reasoning and tool-selection core (e.g., AppWorld (Trivedi et al., 2024). Design and evaluation characteristics :Most benchmarks span a limited set of domains (three to five use cases), typically including real-world information retrieval and navigation (e.g., AssistantBench (Yoran et al., 2024)), code genera-tion (e.g., SWE-Bench (Jimenez et al., 2023)), rea-soning and planning (e.g., MATH (Hendrycks et al., 2021), and security scenarios (e.g., Agent Security Benchmark (Zhang et al., 2024)). They enable ob-jective measurement despite agents’ open-ended 2Figure 2: Overview of our multi-stage translation pipeline for agentic benchmark construction. We start with machine translation for structural alignment, followed by LLM-based verification and enhancement. This approach is adapted from Ki and Carpuat (2024) but extended with task-specific prompting and fallback mechanisms tailored to the requirements of agentic AI evaluation. 

capabilities, by formulating tasks with definitive ground truth, allowing clear determination of suc-cess (Jimenez et al., 2023; Mialon et al., 2023). Further comparisons are provided in the Appendix. While multilingual LLM benchmarks such as XTREME (Hu et al., 2020), FLORES (Goyal et al., 2022), and SIB-200 (Adelani et al., 2023) have enabled broad cross-lingual evaluation, they do not assess interactive decision-making, tool use, or task execution - the core elements of agentic systems. Thus, existing multilingual benchmarks fail to capture the complexities and vulnerabilities agents face in non-English settings - underscoring the need for multilingual agentic benchmarks. 

2.2 LLMs Multilingual Limitations 

Studies show that pre-trained LLMs struggle with non-English inputs, particularly in languages that are low-resource or typologically distant from En-glish. Multilingual benchmarks (Hu et al., 2020; Liang et al., 2020) report accuracy drops when mov-ing from English to languages such as Swahili or Nepali. These gaps stem from imbalanced training data, with 90% English data, as well as challenges in tokenizing morphologically rich languages and limited multilingual fine-tuning data (Jha, 2024). Notably, even large models (e.g., GPT-4, Llama 405B) face a “cross-lingual knowledge barrier” (Hendrycks et al., 2020), (Chua et al., 2024). LLMs also face security risks in multilingual contexts. Since most security efforts have been English-centric, models are more prone to gen-erate policy-violating outputs when processing non-English prompts (Deng et al., 2023; Wang et al., 2023; Aakanksha et al., 2024). Multilingual safety alignment, have been shown to be effective but it also degrades general performance latency (Aakanksha et al., 2024). Since agentic AI systems are based on these LLMs, we hypothesize that they inherit the same language-dependent performance and security lim-itations. These agents carry out real-world tasks , thus any inherited shortcomings can lead to severe consequences. To our knowledge, no systematic evaluation has examined these vulnerabilities in agentic systems. To address this gap, we introduce MAPS, a multilingual agentic benchmark suite. 

3 MAPS: Multilingual Agentic AI Benchmark Suite 

To support multilingual evaluation of agentic sys-tems, we construct a benchmark suite by extending established English-language datasets into multiple languages. This process requires careful dataset selection, translation procedures that preserve se-mantic and structural integrity, and mechanisms for ensuring evaluation consistency. The following subsections detail our methodology for translation, benchmark construction, and dataset composition. 

3.1 Translation Pipeline 

Our translation pipeline combines automated trans-lation with human expert verification - emphasizing both semantic and structural integrity. We com-bine Neural Machine Translation (NMT) which ex-cels at preserving format and structure (Koehn and Knowles, 2017; Aharoni et al., 2019), and LLM translation for broader high-level capabilities. This extends Ki and Carpuat (2024), adding automated quality checks, fallbacks and human expert verifi-cation. The overall process is illustrated in Fig. 2. Below is a short summary - further details are in the Appendix. 

Automated Translation. This system first com-putes the NMT translation, then improves it via LLM. let T : S × L → T map an English in-struction s ∈ S and a target language Lt ∈ L to the translation t ∈ T . We rely on five primitives: 

ˆt = M (s, L t): neural MT system. A(s, ˆt, L t) ∈{0, 1}: adequacy check of ˆt. Φdirect (s, L t): direct 3LLM translation. ˆtenh = Φ enh (s, ˆt, L t): LLM en-hancement of ˆt. I(s, ˆt) ∈ { 0, 1}: integrity check (hallucination, drift). The translation favors ˆtenh 

with fallbacks as follows:                            

> Input: source text s, target language Lt
> Output: translation t
> 1. ˆt←M(s, L t)
> 2. if A(s, ˆt, L t) = 0 then return Φdirect (s, L t)fallback 1
> 3. ˆtenh ←Φenh (s, ˆt, L t)
> 4. if I(s, ˆtenh ) = 0 then return ˆtfallback 2
> 5. return ˆtenh

Human Expert Verification. To ensure this pipeline reliability across languages and task types, we conducted human verification on a representa-tive subset of translations. Evaluation setup and results are in Subsection 3.3. 

3.2 Dataset Selection and Composition Dataset Selection. To enable robust multilingual evaluation of agentic capabilities, we construct MAPS benchmark suite from established agentic AI benchmarks. Those were selected based on four criteria: (1) strong adoption within the research community, including prior use in agentic evalua-tion; (2) clearly defined, closed-form answers to en-able controlled evaluation; (3) sufficient difficulty to challenge open-source agents without saturating performance; and (4) practical solvability, ensur-ing that multilingual degradation can be measured. Accordingly, we selected four datasets spanning real-world reasoning, software engineering, mathe-matical problem solving, and security assessment. 

GAIA. GAIA (Mialon et al., 2023) is a bench-mark designed to evaluate agents’ performance on real-world assistant tasks. It includes curated questions that require multi-step reasoning and au-tonomous use of tools such as web browsers, code interpreters, or document analyzers. Each ques-tion has a single correct answer, and responses are evaluated by an exact match to a reference output. 

SWE-bench. SWE-bench (Jimenez et al., 2023) is a software engineering benchmark constructed from real GitHub issues and associated pull re-quests across popular Python repositories. Each task presents a bug report and a codebase snap-shot, and requires the agent to evaluate whether a proposed patch correctly resolves the issue. We adopt the verified subset (OpenAI, 2024a), in which agents are tasked with validating a patch rather than generating one. 

MATH. The MATH dataset (Hendrycks et al., 2021) includes high-school level mathematical problems across seven topics, including algebra, geometry, and calculus. Tasks are structured to require symbolic manipulation and multi-step rea-soning. Agent responses are evaluated by exact match against a reference solution. 

Agent Security Benchmark (ASB). ASB bench-mark (Zhang et al., 2024) provides a structured evaluation of agent robustness against adversar-ial threats, including prompt injections, memory poisoning, and tool misuse. Agents interact with injected prompts or environments, and evaluation is based on whether safety policies are violated, measured by attack success rate and refusal rate. 

Data Composition . The metadata below sum-marizes the multilingual extension, including lan-guage coverage, scale, and pre-processing. 

Translated Languages. We selected the follow-ing eleven typologically and geographically diverse languages: German, Spanish, Portuguese (Brazil), Japanese, Russian, Chinese, Italian, Arabic, He-brew, Korean, and Hindi. This selection enables the evaluation of agent performance across a wide range of scripts, linguistic structures, and regional user populations. Details on the specific dialects translated for each language are provided in the Appendix. 

Dataset Handling. To preserve the integrity and utility of the original datasets, we applied only minimal and targeted modifications. Across all datasets, we appended translations without modify-ing or removing any original metadata (such as task type, difficulty level, tools available, etc). Domain-specific syntax—such as equations in MATH, code snippets in SWE-bench, and adversarial prompts in ASB—was preserved exactly, maintaining the original task structure and technical fidelity. For MATH and SWE-bench, which were not originally designed for agentic evaluation, we further applied selective filtering to retain only the most challeng-ing tasks based on the task difficulty field. This follows common practice in prior work to align non-agentic datasets with agentic evaluation set-tings (Wu et al., 2023), ensuring meaningful agent evaluation while avoiding trivial cases. 

Data Volume. To balance performance and se-curity evaluation, our benchmark comprises 805 

tasks: 405 from performance-oriented datasets (GAIA, SWE-bench, MATH) and 400 from the Agent Security Benchmark. We selected 165 

tasks from GAIA (full validation set), 140 high-difficulty tasks from MATH ( 20 per topic across 

7 topics), and 100 hard and medium tasks from SWE-bench. The remaining 400 tasks include all 4security-relevant prompts from ASB. Each task was translated into 11 languages, and combined with the original English version, this results in a total of 9, 660 multilingual tasks across 12 languages. To validate the benchmark’s utility and examine multilingual effects, we applied a leading agent to each dataset. Full details and performance results are reported in Section 4. 

3.3 Translation Implementation and Verification Translation Implementation Details. For NMT we used (Google Cloud, 2024). For LLM transla-tion we used Cohere’s command A (Cohere et al., 2025) and gpt-4o with a designated system prompt. Structured expressions such as Code, URL and equations were identified via the provided special tokens provided by the datasets (Zhang et al., 2024; Hendrycks et al., 2021), and masked accordingly in the translation process. 

Quality Assurance. To assess translation qual-ity, we implemented a verification protocol involv-ing native-speaking experts. Each sample was rated by a bilingual annotator fluent in English and the target language on a 1˘5 Likert scale across three established criteria: adequacy (semantic fidelity), 

fluency (grammatical and stylistic naturalness), and 

formatting accuracy (preservation of LaTeX, code, etc.) (Freitag et al., 2021). A fourth metric, answer-ability , measured whether the translation preserved intent well enough for the annotator to confidently answer the question as if it were in English. Anno-tator instructions are provided in the Appendix. To validate verification reliability, we embedded “hon-eypot” samples with intentional errors; annotators reliably flagged these cases, confirming attentive-ness and quality control. In total, 2,200 transla-tions (25% of the benchmark) were verified, pro-portionally sampled across datasets and languages - for comparability, see (Xu et al., 2025), where 1,600/6,000 tasks where verified. Evaluation results confirm high translation qual-ity across the benchmark, with an answerability rate of 94 .2%, and mean scores of 4.43 (ade-quacy), 4.59 (fluency), and 4.75 (formatting) on a 1–5 Likert scale, supporting the benchmark’s preservation of semantic fidelity, linguistic natu-ralness, and structural integrity. Because a sample of n = 2 ,000 gives a worst - case 95% margin of error of ±2.2 percentage points (derivation in Ap-pendix), the true corpus - level answerability lies in 

[92 .0, 96 .4]% , indicating robust translation qual-ity even after accounting for sampling uncertainty. Full per-language results and analysis are included in the Appendix. To support high-precision use cases, we also release a “verified” subset, consist-ing of 190 translations per language that passed native expert review across all four datasets. 

4 Experiments 

4.1 Experimental Settings Agent Assignment per Dataset. To demonstrate the utility of MAPS, we evaluate open-source agents on each multilingual dataset. We assigned a separate agent to each dataset, selected based on task relevance. While a unified agent would offer a more controlled evaluation, current systems lack the generalization needed to perform well across diverse tasks (Gioacchini et al., 2024; Chang et al., 2024). We employ each agent in its original config-uration of prompts, system settings, tools and LLM backbone - all of which are multilingual models. For GAIA, we used the OpenDeepResearch agent (von Platen et al., 2024), which integrates re-trieval, web browsing, and tool use to support real-world reasoning. For MATH, we adopted Math-Chat (Wu et al., 2024), a zero-shot agent combining multi-turn reasoning with Python execution and the Wolfram Alpha tool. For SWE-bench, we applied SWE-agent (Yang et al., 2024b), which enables autonomous software reasoning through repository navigation, file editing, and test execution. For ASB, we built on the original benchmark infras-tructure (Zhang et al., 2024), eliciting synthetic adversarial tool-use in various agentic scenarios (e.g. travel-agent, legal consultant, system admin). We use OpenDeepResearch with GPT o1, MATHChat with GPT-4, SWE-agent with GPT-4.1, and ASB with Qwen2. Full configuration detailsare provided in the Appendix. 

Experiment Protocol. For each benchmark, the agent was evaluated three times in each of the 12 

target languages, yielding a total of 36 runs per dataset. Fig. 3 reports the means and standard deviations over these runs. 

Metrics. We adopt the original evaluation met-rics from each benchmark to ensure consistency with prior agent evaluations. For MathChat (Math) and OpenDeepResearch (GAIA), we report the per-centage of correct answers - with the latter match-ing either the English or translated reference. For SWE-agent, we report the percentage the percent-age of submitted patches that successfully resolve 5Figure 3: Performance of open-source agents across languages on four agentic benchmarks: GAIA, SWE-bench, MATH, and ASB. Each bar represents the agent’s accuracy (or attack success rate in ASB) for a given language, with English shown as the baseline. Error bars indicate std across three runs. Performance differences reflect each agent’s degradation or resilience in multilingual settings. 

the coding issue. For the ASB agent, we report the attack success rate (ASR). Additionally, we intro-duce a new metric: Multilingual Effect , which quan-tifies the performance or security gap between En-glish and the average of all other languages. Given an evaluation metric M , the Multilingual effect is defined as follows: Multilingual Effect = 1

n

> n

X

> i=1

Mlang i − Men (1) Where Men denotes the performance in English, n is the number of non-English languages in the dataset (in our case n = 11 ), and Mlang i represents the performance in the i-th non-English language. 

4.2 Results 

Fig. 3 presents the performance of open-source agents across all four benchmarks in English and the eleven target languages. In GAIA and ASB, we observe clear performance and security drops: non-English languages consistently underperformed compared to English, with reductions of up to 16% (GAIA) and a rise in vulnerability by up to 17% in ASB. Notably, SWE-bench and MATH exhibit only minor variation across languages, with most scores clustering around the English baseline. The Appendix extends this analysis with language - wise breakdowns of relative performance degradation. These results reveal important differences in how multilingual degradation manifests across task types. Although all tasks require complex reason-ing, some are more constrained than others. For instance, SWE-bench and MATH primarily involve structured code or mathematical expressions, plac-ing less emphasis on natural language and more on Pythonic syntax or formal notation, thereby reduc-ing the multilinguality effect. On the other hand, GAIA focuses on solving real-world tasks where understanding flexible natural language prompts is essential. Thus, the importance of the natural lan-guage problem statement is significantly higher. To explore this variation, we examine the proportion of localized, target-language text in each bench-mark’s input (Fig. 4). Interestingly, we observe that Chinese and Japanese yields the lowest ASR in the ASB bench-mark, indicating the highest robustness to ad-versarial prompts in these two Asian languages. We attribute this pattern to the choice of back-bone: the ASB agent was implemented using the Qwen2 model (Yang et al., 2024a), which is known for its strong alignment for Chinese and Japanese language tasks. Qwen2 has consistently demonstrated strong performance in Chinese and Japanese-specific LLM benchmarks 3, suggesting 

> 3https://rinnakk.github.io/research/benchmarks/lm/index.html

6Figure 4: a) Multilingual effect as a function of the proportion of translated language tokens in input prompts. Each point represents a benchmark-agent pair, with the multilingual effect computed as the average relative degradation in performance or security across non-English languages. The trend suggests a correlation between input translation extent and multilingual vulnerability. b) Two snippets exemplify a low -translation prompt (MATH) and a high - translation prompt (GAIA), clarifying the x -axis percentages in panel (a) and showing how the proportion of natural-language tokens, rather than task difficulty alone, drives the observed multilingual effect. 

that alignment to a particular language and strate-gic backbone selection can significantly enhance resilience against multilingual adversarial prompts. Fig. 4 examines the relationship between prompt composition and multilingual performance. part (a) shows a correlation between the percentage of non-English tokens in the input and the average performance gap (relative to English) across all four datasets. Benchmarks with higher proportions of localized, target-language-oriented input, such as GAIA and ASB, exhibit greater degradation, whereas SWE-bench, with predominantly English input (e.g., code), shows higher preservation. Part (b) demonstrate the differences between a MATH item with minimal translated text versus a GAIA query that is almost entirely translated.           

> English Mean (Non-English)
> Orig. Trans. GAIA (ACC) ↑47.4% 35.8% 41.3% ASB (ASR) ↓48.8% 58.4% 56.6%

Table 1: Self-translation on GAIA and ASB. Mak-ing the agent aware of the multilingual context by self -translation of each non -English prompt recovers much of the lost performance/robustness, yet still leaves a noticeable gap to the native English baseline. 

Self - translate Ablation. To further examine agents in multilingual settings, we followed the 

self - translate strategy of Etxaniz et al. (2024): each agent received one additional system message in-structing it to translate any non - English prompt into English before solving the task, see results in Table 1. This led to improvements over direct mul-tilingual inference, yet a clear gap to the English baseline persists, which may stem from the addi-tional translation step introducing context loss or subtle distortions. 

Figure 5: Cross-lingual planning consistency for Open-DeepResearch and SWE-Agent. For each agent, we identify tasks where the model succeeds in English but fails in all other languages, then compute the seman-tic similarity between the English instruction and the initial planning step in each language using multilin-gual embeddings. OpenDeepResearch exhibits a strong cross-lingual alignment gap: in 85.7% of cases, the En-glish planning step is more semantically faithful to the instruction than its counterparts in other languages. In contrast, SWE-Agent is more multilingual robust, with English outperforming other languages in only 25% of cases. 

Cross-Lingual Failure Analysis: To investi-gate why some agents exhibit a more pronounced multilingual effect than others, we compare their initial task planning across languages—specifically, the agent’s first planning step, where it breaks down the instruction into a sequence of actions to solve the task. We compute the cosine similarity be-tween the English instruction and each language’s 7corresponding planning step using multilingual em-beddings from OpenAI’s text-embedding-3-small model (OpenAI, 2024b). This allows us to quantify how closely the agent’s reasoning in each language aligns with the original task intent expressed in English. Our analysis focuses on cases where agents suc-ceed in English but fail in all other languages. We compare two representative agents: OpenDeepRe-searchAgent on the GAIA dataset and SWE-Agent on the SWE-Bench dataset, as they exhibit the strongest and weakest multilingual effects, respec-tively (see Fig. 4). As shown in Fig. 5, OpenDeepResearchAgent displays a strong English bias in planning align-ment on GAIA: planning steps in English are more semantically aligned with the instruction than in other languages (0.76 vs. 0.70 on average), with lower variance. This indicates that the agent’s rea-soning is more faithful to the instruction in English and degrades across languages. In contrast, SWE-Agent exhibits much less cross-lingual variation on SWE-Bench: planning similarity remains consis-tent across languages, suggesting that its reasoning process is less sensitive to linguistic differences. These findings suggest that when an agent’s initial planning remains stable across languages, the impact of the multilingual effect is reduced. For instance, SWE-Agent follows a structured and language-invariant approach—reproducing and ex-ecuting code—which helps maintain alignment re-gardless of language. Conversely, GAIA relies more on open-ended natural language reasoning, making it more susceptible to semantic drift across translations. 

5 Discussion 

5.1 Guidelines for Multilingual Agent Deployment and Risk Assessment Language-Composition-Aware Deployment Guidelines. Before deploying an AI agent in a multilingual setting, analyze the linguistic composition of its expected input, particularly the balance between structured elements (e.g., code, formal queries) and localized natural language. Inputs with a high proportion of natural language, tend to increase the risk of performance and safety degradation. We therefore recommend that in such case, developers should conduct a Multilingual Benchmark Assessment using adiverse, language-sensitive evaluation suite, such as ours, for AI agents operating across languages. This helps reveal vulnerabilities and promotes reliable real-world behavior across languages. 

Multilingual Inputs Amplify Agentic Secu-rity Vulnerabilities. Our evaluation on ASB re-vealed that multilingual adversarial inputs can by-pass agent safety mechanisms with minimal so-phistication. Direct translations of English jail-break prompts—without any adaptation or obfus-cation—were sufficient to induce policy-violating behavior in multiple languages. This highlights a critical risk: localized inputs make simple at-tacks more effective, exposing security flaws in AI agents. Developers of safety-critical agentic sys-tems should treat multilingual robustness as a core security concern and include translated prompts in safety evaluations using benchmarks like ours. 

5.2 Benchmark Limitation 

MAPS includes four datasets and eleven target lan-guages, offering a strong foundation for multilin-gual evaluation. Nevertheless, extending these to additional domains, e.g. healthcare or legal reason-ing, as well as to extremely low-resource languages (e.g., Amharic or Uyghur), would further enhance the benchmark. Nonetheless, the current suite al-ready surfaces clear gaps across languages, offering valuable insights for multilingual deployment. 

6 Conclusions 

We introduce MAPS, the first multilingual benchmark suite for evaluating agentic AI sys-tems, addressing a critical gap in assessing language-specific performance and safety limita-tions. By extending four widely used agentic benchmarks—GAIA, SWE-bench, MATH, and ASB—into eleven diverse languages, our suite en-ables the analysis of agent behavior under multi-lingual conditions. Constructed through a hybrid translation pipeline and human verification, MAPS ensures high linguistic and structural consistency. Experiments reveal critical gaps in performance and robustness when agents operate in non-English settings, particularly in tasks involving natural lan-guage reasoning and safety-critical behavior. These findings underscore the importance of language-aware evaluation and multilingual adaptation, espe-cially for real-world agentic deployments. We view MAPS as a practical foundation for building more inclusive, resilient, and globally reliable agentic AI systems, and invite the community to extend it. 8References 

Arash Ahmadian Aakanksha, Beyza Ermis, Seraphina Goldfarb-Tarrant, Julia Kreutzer, Marzieh Fadaee, Sara Hooker, and 1 others. 2024. The multilingual alignment prism: Aligning global and local preferences to reduce harm. In Proceedings of the 2024 Conference on Em-pirical Methods in Natural Language Processing , pages 12027–12049. Deepak Bhaskar Acharya, Karthigeyan Kuppan, and B Divya. 2025. Agentic ai: Autonomous intelligence for complex goals–a comprehensive survey. IEEE Access .David Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba O Alabi, Yanke Mao, Haonan Gao, and Annie En-Shiun Lee. 2023. Sib-200: A simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects. arXiv preprint arXiv:2309.07445 .Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Mas-sively multilingual neural machine translation. arXiv preprint arXiv:1903.00089 .Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson, and 1 others. 2024. Agentharm: A benchmark for measur-ing harmfulness of llm agents, 2024. URL https://arxiv. org/abs/2410.09024 .Ma Chang, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. 2024. Agentboard: An analytical evaluation board of multi-turn llm agents. Advances in Neural Infor-mation Processing Systems , 37:74325–74362. Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, and Bo Li. 2024. Agentpoison: Red-teaming llm agents via poi-soning memory or knowledge bases. Advances in Neural Information Processing Systems , 37:130185–130213. Lynn Chua, Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chulin Xie, and Chiyuan Zhang. 2024. Crosslingual capabilities and knowledge barriers in multilingual large language models. 

arXiv preprint arXiv:2406.16135 .Team Cohere, Arash Ahmadian, Marwan Ahmed, Jay Alam-mar, Milad Alizadeh, Yazeed Alnumay, Sophia Altham-mer, Arkady Arkhangorodsky, Viraat Aryabumi, Den-nis Aumiller, and 1 others. 2025. Command a: An enterprise-ready large language model. arXiv preprint arXiv:2504.00698 .John Dang, Shivalika Singh, Daniel D’souza, Arash Ahma-dian, Alejandro Salamanca, Madeline Smith, Aidan Pep-pin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis Flet-Berliac, and 26 others. 2024. Aya expanse: Combining research breakthroughs for a new multilingual frontier. Preprint , arXiv:2412.04261. Edoardo Debenedetti, Jie Zhang, Mislav Balunovi´ c, Luca Beurer-Kellner, Marc Fischer, and Florian Tramèr. 2024. Agentdojo: A dynamic environment to evaluate attacks and defenses for llm agents. arXiv preprint arXiv:2406.13352 .Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. 2023. Multilingual jailbreak challenges in large language models. arXiv preprint arXiv:2310.06474 .Julen Etxaniz, Gorka Azkune, Aitor Soroa, Oier Lopez de La-calle, and Mikel Artetxe. 2024. Do multilingual language models think better in English? In Proceedings of the 2024 Conference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers) , pages 550–564, Mexico City, Mexico. Association for Computational Lin-guistics. Markus Freitag, George Foster, David Grangier, Viresh Rat-nakar, Qijun Tan, and Wolfgang Macherey. 2021. Experts, errors, and context: A large-scale study of human evalua-tion for machine translation. Transactions of the Associa-tion for Computational Linguistics , 9:1460–1474. Luca Gioacchini, Giuseppe Siracusano, Davide Sanvito, Kiril Gashteovski, David Friede, Roberto Bifulco, and Carolin Lawrence. 2024. Agentquest: A modular benchmark framework to measure progress and improve llm agents. 

arXiv preprint arXiv:2404.06411 .Google Cloud. 2024. Translating text using google cloud translation api v3. https://cloud.google.com/ translate/docs/advanced/translating-text-v3# translating_input_strings . Accessed: July 2025. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022. The flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transac-tions of the Association for Computational Linguistics ,10:522–538. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Man-tas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Mea-suring massive multitask language understanding. arXiv preprint arXiv:2009.03300 .Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 .Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Rau-nak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt models at machine translation? a comprehen-sive evaluation. arXiv preprint arXiv:2302.09210 .Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. Xtreme: A mas-sively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In International conference on machine learning , pages 4411–4421. PMLR. Basab Jha. 2024. The babel effect: Analyzing multilingual performane discrepancies in large language models. Eng Appl Sci J , 1(1):1–4. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770 .Dayeon Ki and Marine Carpuat. 2024. Guiding large language models to post-edit machine translation with error annota-tions. In Findings of the Association for Computational Linguistics: NAACL 2024 , pages 4253–4273. 

9Megan Kinniment, Lucas Jun Koba Sato, Haoxing Du, Brian Goodrich, Max Hasin, Lawrence Chan, Luke Harold Miles, Tao R Lin, Hjalmar Wijk, Joel Burget, and 1 others. 2023. Evaluating language-model agents on realistic autonomous tasks. arXiv preprint arXiv:2312.11671 .Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. 

arXiv preprint arXiv:2302.14520 .Philipp Koehn and Rebecca Knowles. 2017. Six challenges for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation .Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Gui-hong Cao, and 1 others. 2020. Xglue: A new benchmark dataset for cross-lingual pre-training, understanding and generation. arXiv preprint arXiv:2004.01401 .Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. Gaia: a benchmark for general ai assistants. In The Twelfth International Con-ference on Learning Representations .Tawffeek AS Mohammed. 2025. From google translate to chatgpt: The use of large language models in translating, editing, and revising. In Role of AI in Translation and In-terpretation , pages 1–32. IGI Global Scientific Publishing. OpenAI. 2024a. Introducing swe-bench verified. https://openai.com/index/ introducing-swe-bench-verified/ . Accessed: July 2025. OpenAI. 2024b. Introducing text - embedding - 3 - small: acost -efficient model with improved multilingual and en-glish retrieval performance. https://platform.openai. com/docs/models/text-embedding-3-small . Ac-cessed: July 2025. Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. Comet: A neural framework for mt evaluation. arXiv preprint arXiv:2009.09025 .Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2022. Language models are multilingual chain-of-thought reasoners. Preprint , arXiv:2210.03057. Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, and Hang Su. 2023. Evil geniuses: Delving into the safety of llm-based agents. arXiv preprint arXiv:2311.11855 .Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, and Niranjan Balasubramanian. 2024. App-world: A controllable world of apps and people for bench-marking interactive coding agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 16022–16076. Patrick von Platen, Younes Wang, Teven Le Scao, and Hugging Face team. 2024. Open deep research: Ablueprint for transparent foundation model science. https: //huggingface.co/blog/open-deep-research . Hug-ging Face Blog. Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, and Michael R Lyu. 2023. All languages matter: On the multilingual safety of large language models. arXiv preprint arXiv:2310.00905 .Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. 2023. Mathchat: Converse to tackle chal-lenging math problems with llm agents. arXiv preprint arXiv:2306.01337 .Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. 2024. Mathchat: Converse to tackle challenging math problems with llm agents. In ICLR 2024 Workshop on Large Language Model (LLM) Agents .Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. 2024. Trav-elplanner: A benchmark for real-world planning with lan-guage agents. In International Conference on Machine Learning , pages 54590–54613. PMLR. Frank F Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z Wang, Xuhui Zhou, Zhitong Guo, Murong Cao, and 1 others. 2024. Theagentcompany: benchmarking llm agents on consequential real world tasks. 

arXiv preprint arXiv:2412.14161 .Zixiang Xu, Yanbo Wang, Yue Huang, Xiuying Chen, Jieyu Zhao, Meng Jiang, and Xiangliang Zhang. 2025. Cross-lingual pitfalls: Automatic probing cross-lingual weakness of multilingual large language models. In Proceedings of the 63rd Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers) , pages 8254–8284, Vienna, Austria. Association for Computa-tional Linguistics. Jianhao Yan, Pingchuan Yan, Yulong Chen, Judy Li, Xianchao Zhu, and Yue Zhang. 2024. Gpt-4 vs. human translators: A comprehensive evaluation of translation quality across languages, domains, and expertise levels. arXiv preprint arXiv:2407.03658 .An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, and 40 others. 2024a. Qwen2 technical report. arXiv preprint arXiv:2407.10671 .John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024b. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems , 37:50528–50652. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. 2024. τ -bench: A benchmark for tool-agent-user interaction in real-world domains, 2024. URL https://arxiv. org/abs/2406.12045 .Asaf Yehudai, Lilach Eden, Alan Li, Guy Uziel, Yilun Zhao, Roy Bar-Haim, Arman Cohan, and Michal Shmueli-Scheuer. 2025. Survey on evaluation of llm-based agents. 

arXiv preprint arXiv:2503.16416 .Jingwei Yi, Yueqi Xie, Bin Zhu, Emre Kiciman, Guangzhong Sun, Xing Xie, and Fangzhao Wu. 2023. Benchmarking and defending against indirect prompt injection attacks on large language models. arXiv preprint arXiv:2312.14197 .Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, and Jonathan Berant. 2024. Assistant-bench: Can web agents solve realistic and time-consuming tasks? arXiv preprint arXiv:2407.15711 .

10 Qiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel Kang. 2024. Injecagent: Benchmarking indirect prompt injec-tions in tool-integrated large language model agents. arXiv preprint arXiv:2403.02691 .Hanrong Zhang, Jingyuan Huang, Kai Mei, Yifei Yao, Zhent-ing Wang, Chenlu Zhan, Hongwei Wang, and Yongfeng Zhang. 2024. Agent security bench (asb): Formalizing and benchmarking attacks and defenses in llm-based agents. 

> arXiv preprint arXiv:2410.02644 .

A Appendix Abstract 

This supplementary material provides addi-tional details supporting our MAPS benchmark, including extended benchmark comparisons, experimental protocols, and multilingual evalu-ation results. It is organized as follows: • Reproducibility Resources – links to the full datasets and codebase, along with li-censing terms (MIT) to enable open use and distribution for research and develop-ment purposes. • Agentic AI Benchmark Comparison –an expanded analysis of existing perfor-mance and security agentic benchmarks across dimensions such as evaluation ob-jective, task type, answer format, and mul-tilingual support. • Hybrid Translation Pipeline – addi-tional information on our hybrid trans-lation methodology, including domain-specific prompt design and translation configuration experiments. • Manual Evaluation of Translation Quality – annotation guidelines, evalua-tion examples, and a complete breakdown of human verification scores per dataset and language. • Experimental Settings – full specifica-tion of the technical setup, agent configu-rations, and language adaptation process used to isolate multilingual effects across tasks. • Extended Results – detailed multilingual performance tables for GAIA, MATH, SWE-Bench, and ASB across individual evaluation runs. 

A.1 Reproducibility Resources 

We release the full MAPS dataset, which includes translations of four agentic AI benchmarks (GAIA, SWE-bench, Math, and Agent Safety Benchmark) across 12 languages. A verified subset of MAPS, manually reviewed by native speakers, is also in-cluded for higher-quality evaluations. Links to both the full dataset and verified subset can be found at: 

MAPS Benchmark . We also provide the code for our hybrid translation pipeline, including prompt de-sign and format-preserving techniques, available at: 

Hybrid Translation Implementation . All re-sources are released under the MIT License , allow-ing unrestricted use, modification, and distribution for academic and commercial purposes, provided proper attribution is given. 

A.2 Agentic AI Benchmark Comparison 

To contextualize MAPS within the growing land-scape of agentic evaluation, we provide a detailed comparison of recent benchmarks across core de-sign dimensions. While the main paper (Section 2.1) outlines a conceptual taxonomy—covering evaluation objectives, agent autonomy levels, and domain specialization—this supplementary section complements it with a structured, side-by-side ta-ble. Our comparison highlights key properties such as the ground truth availability, domain coverage, and multilingual support. Notably, MAPS remains the only benchmark to offer multilingual assess-ment across multiple agentic tasks, spanning both performance and safety. This contrast underscores an existing gap in multilingual agent evaluation and illustrates how MAPS fills it. Table 2 surveys general-purpose agentic bench-marks, revealing that most focus exclusively on performance, operate within semi-agentic environ-ments, and lack multilingual coverage. Full-agentic settings—those requiring autonomous planning and execution without scaffolding—are limited to a few benchmarks such as GAIA , SWE-Bench , and 

MATH , each addressing distinct domains: multi-modal reasoning, software engineering, and math-ematical problem solving, respectively. Table 3 focuses on security-oriented agentic benchmarks. While these benchmarks advance robustness eval-uation, most adopt semi-agentic setups and open-ended answer formats, and often withhold ground truth to prevent overfitting. Based on this analysis, we selected four comple-mentary benchmarks— GAIA , SWE-Bench , MATH ,and Agent Security Bench (ASB) —that collectively span performance and security objectives, cover diverse domains including real-world reasoning, code, math, and cybersecurity, and offer closed-form outputs with accessible ground truth. No-tably, three of the four operate in fully agentic set-tings, aligning with our emphasis on end-to-end au-tonomous decision-making. This configuration en-11 Paper Evaluation Objective Task Scope Domain Coverage # of tasks Answer Format GT Available Multilingual                                                                 

> GAIA (Mialon et al., 2023) Performance Full-agentic General, multimodal 460 Close ✓✗
> SWE Bench (Jimenez et al., 2023) Performance Full-agentic Software engineering 3000 Close ✓✗
> MATH (Hendrycks et al., 2021) Performance Full-agentic Math solver 12,500 Close ✓✗
> τ-bench (Yao et al., 2024) Performance Semi-agentic Airline, retail 165 Close ✓✗
> AgentBoard (Chang et al., 2024) Performance Semi-agentic General – 9 varied tasks 134 Close ✓✗
> App World (Trivedi et al., 2024) Performance Semi-agentic Apps, code execution, APIs 750 Close Partial ✗
> TheAgentCompany (Xu et al., 2024) Performance Semi-agentic SW Dev, HR, Admin, DS 175 Close ✓✗
> ARA (Kinniment et al., 2023) Performance, Security Semi-agentic SW Dev, phishing, web searches 12 Open ✗✗
> TravelPlanner (Xie et al., 2024) Performance Full-agentic Travel planner, tourist 1225 Open ✗✗

Table 2: Comparison of existing agentic AI benchmarks along evaluation objective, scope, domain coverage, and multilingual availability.                                                               

> Paper Evaluation Objective Task Scope Domain Coverage # of tasks Answer Format GT Available Multilingual
> Agent Security Bench (Zhang et al., 2024) Security Semi-agentic Offensive cyber operations, unsafe tool 90,000 Close ✓✗
> AgentHarm (Andriushchenko et al., 2024) Security Full-agentic Fraud, cybercrime, harassment 110 Open ✗✗
> Agent-SafetyBench (Zhang et al., 2024) Security Semi-agentic Jailbreaks, data leakage, unsafe tool use 2,200 Close ✗✗
> AgentDojo (Debenedetti et al., 2024) Security Semi-agentic Prompt injection, phishing, data leakage 97 Both Partial ✗
> InjecAgent (Zhan et al., 2024) Security Semi-agentic Indirect prompt injection, data leakage 1,000 Close ✓✗
> AgentPoison (Chen et al., 2024) Security Full-agentic Backdoor attacks, data poisoning ~20 Open ✗✗
> BIPIA (Yi et al., 2023) Security Semi-agentic Indirect prompt injection, data leakage 5Close ✓✗
> Evil Geniuses (Tian et al., 2023) Security Full-agentic Prompt injection, data leakage 156 Open ✗✗

Table 3: Comparison of agentic security benchmarks by scope, threat type, answer format, and multilingual coverage. 

ables reliable, multilingual evaluation across both agent capabilities and vulnerabilities, providing a robust foundation for MAPS. 

A.3 Hybrid Translation Pipeline Implementation Additional Information 

Reliable multilingual evaluation of agentic AI sys-tems hinges on translating task instructions with both semantic and structural cross-language fidelity. Neural MT excels at preserving format and struc-ture, but struggles in low-resource or specialized domains (Koehn and Knowles, 2017; Aharoni et al., 2019). Translation via instructed LLMs offers broader high-level capabilities at the cost of oc-casional hallucinations and semantic drift (Hendy et al., 2023; Yan et al., 2024). To balance these trade-offs, hybrid pipelines were suggested by Ki and Carpuat (2024); Mohammed (2025), combin-ing format-preserving MT with LLM-based re-finement. For MAPS, we extend Ki and Carpuat (2024): First, Ki and Carpuat (2024) was not de-signed with our benchmarks in mind, thus signifi-cant per-benchmark prompting had to be done. Sec-ond, we added automated quality checks, fallbacks, and expert verification to ensure the cross-language fidelity needed for agentic benchmarks (Fig. 2). Formally, let us express our translation pipeline as a function T : S×L → T , where s ∈ S is a task-instruction instance in source-language (English), 

Lt ∈ L is the target language, and t ∈ T is the re-sulting translated output. The pipeline begins with machine translation (MT) to establish a structural foundation: Denote M (s, L t), the MT function, implemented as a high-quality, off-the-shelf NMT system. Its output provides a structurally faithful baseline for subsequent steps. Next, we apply a verification step using an LLM to assess whether the translation adequately pre-serves the source meaning. This is modeled as a binary function A(s, M (s, L t), L t) → 0, 1, where the LLM compares the original and translated texts to detect major semantic errors or omissions. Based on verification outcomes, the pipeline fol-lows one of two distinct paths. If A = 0 (indicat-ing MT failure), the pipeline employs direct LLM translation: Denote Φdirect (s, L t) the output of an LLM prompted to directly translate s to language 

Lt (without using the MT output). If A = 1 (in-dicating acceptable MT), an LLM enhances the translation while preserving its basic structure: De-note Φenhancement (s, M (s, L t), L t) as the output of an LLM, guided to improve the MT output while maintaining structural consistency. To ensure semantic integrity, we apply a second binary check: I(s, Φenhancement ) → 0, 1, targeting common LLM failure modes, such as hallucina-tions, omissions, misinterpretations (e.g., answer-ing instead of translating), and semantic drift. If this verification fails, we revert to the original MT output (which passed the initial verification test). These conditional steps form a robust decision 12 Figure 6: System prompt used for math translation refinement. This prompt is part of the hybrid translation pipeline and instructs the model to improve machine-translated math questions while preserving all LaTeX expressions and symbolic structure. As part of our hybrid translation pipeline, we designed domain-specific prompts for each benchmark to ensure appropriate evaluation and structural fidelity. 

framework: If MT is rejected, we use a direct LLM translation; if accepted but enhancement fails in-tegrity verification, we fall back to the MT; Oth-erwise, we use the enhanced translation. Impor-tantly, since both MT and LLM-based translation are robust technologies, we did not encounter any scenario in which both systems failed to produce a valid translation. A formal expression of this pipeline is provided in the Appendix. For the translation pipeline, we used Python 3.11 alongside a hybrid setup comprising Google’s Neural Machine Translation (NMT) API, Co-here Command-A, and GPT-4o. To sup-port high-quality multilingual alignment, we crafted dedicated system prompts for each stage of the pipeline—translation verification, semantic enhancement, and structural integrity checks—tailored to the specific requirements of each domain. For example, GAIA tasks involve real-world decision-making, where preserving sub-tle intent and contextual detail across languages is critical. In contrast, MATH tasks require precise handling of symbolic notation and mathematical terminology, necessitating strict preservation of La-TeX expressions and domain-appropriate language. An example of a task-specific system prompt is provided in Fig. 6. The full translation pipeline implementation, including prompt templates and evaluation logic, is available 4.To evaluate translation quality across different configurations, we employed LLM-based evalu-

> 4https://tinyurl.com/ hybrid-translation-pipeline

ation using a structured prompt. This approach follows the framework introduced by Jiang et al.(Kocmi and Federmann, 2023), who show that LLMs can reliably assess translation quality when guided by clear criteria. We adapted and extended their methodology to include four dimensions tai-lored to our benchmark context (those metrics are similar to the ones we define in the manual verifica-tion experiment): (1) adequacy—semantic fidelity to the original task, (2) fluency—naturalness and grammaticality in the target language, (3) format-ting accuracy—preservation of structural elements such as LaTeX, code, or tool references, and (4) answerability—whether the translation remains us-able and complete. The evaluation prompt used is provided in Fig.7. Unlike traditional evalua-tion metrics such as COMET (Rei et al., 2020), which rely on the same-language reference trans-lations and embedding-based comparisons, our agentic tasks often lack ground truth references and span cross-language instruction. In such set-tings, reference-based metrics become inapplicable, whereas LLM-based zero-shot evaluation enables context-sensitive, reference-free scoring. Using the LLM as a judge approach, we evalu-ated four translation strategies (Table 4) by trans-lating the GAIA validation set, to empirically identify the most effective pipeline design. This aligns with the analysis in Section 3 of the main manuscript, which discusses the trade-offs between machine translation (MT) and LLM-based gener-ation. As expected, Google NMT showed strong performance on formatting but slightly lagged in 13 semantic adequacy and fluency. Command-A im-proved fluency and meaning preservation but oc-casionally introduced structural drift. Hybrid ap-proaches—where NMT outputs were refined by either GPT-4o or Command-A—consistently out-performed single-method setups. Notably, the Google+Command-A variant achieved the high-est scores across all metrics, including adequacy (4.76), fluency (4.90), formatting accuracy (4.96), and answerability (95.9%). These results validate our decision to adopt a hybrid approach and extend prior work(Ki and Carpuat, 2024) by integrating domain-specific prompting, automated checks, and structure-aware refinement for multilingual transla-tion in agentic AI benchmarks. 

A.4 Hybrid Translation Manual Evaluation Additional Information 

To ensure high-fidelity multilingual translation in MAPS, we conducted a human evaluation of translated samples using a standardized annotation protocol. This procedure mirrors the evaluation framework used for our LLM-based assessment, following the methodology proposed by Jiang et al. (Kocmi and Federmann, 2023). Below, we re-produce the instructions provided to annotators, which guided their evaluation of semantic ade-quacy, linguistic fluency, formatting consistency, and answerability across all benchmark domains. 

A.4.1 Translation Verification Guidelines for Annotators Purpose. You are asked to verify the quality of translations from English into your target language for a multilingual benchmark dataset. Your careful evaluation ensures that the translations are accurate, fluent, and correctly formatted. 

Dataset Overview and Translation Challenges. 

You will review 200 translated samples drawn from four benchmark datasets: • GAIA (50 samples): natural language ques-tions requiring web search and file analysis. • Agent Security Benchmark (60 samples): tasks probing unsafe or manipulative agent instructions. • MATH (70 samples): algebra, calculus, prob-ability, and geometry problems, often contain-ing LaTeX and variables. • SWE-Bench Verified (20 samples): GitHub issues involving bug descriptions and code snippets. 

Evaluation Procedure. For each sample: 1. Read the English source text. 2. Read the translated text in the target language. 3. Assign the following ratings: • Adequacy (1–5): how well the meaning is preserved. • Fluency (1–5): how natural and gram-matically correct the translation is. • Formatting Accuracy (1–5): whether LaTeX, variable names, units, and struc-ture are preserved. • Answerable? (Yes/No): Answer No if you feel you cannot answer the original question because of a major translation mistake (e.g., reversed meaning, missing information). Otherwise, mark Yes. • Notes (optional): any remarks you wish to add. 

Rating Scales. 

• Adequacy: 5 = perfect meaning preservation, 1 = major meaning loss. • Fluency: 5 = native-like fluency, 1 = broken or ungrammatical. • Formatting Accuracy: 5 = all formatting correct, 1 = major formatting errors. 

Important Notes. 

• Do not correct or rewrite the translations. • Pay special attention to LaTeX symbols, vari-able names, and numerical units. 

Sheet Columns Example (one row per sam-ple): 

• Item ID – Sample identifier. • English Source Text – Original English input. • Translated Text – Translated version. • Adequacy (1–5) – Meaning preservation. • Fluency (1–5) – Naturalness and grammar. • Formatting Accuracy (1–5) – Structural fidelity. • Answerable? – Unanswerable due to error? • Notes (Optional) – Free-text comments. 14 Translation Method Adequacy (1-5) Fluency (1-5) Formatting Accuracy (1-5) Answerable (%)                 

> Google Translate NMT 4.667 4.870 4.946 92.9% Command A (LLM only) 4.704 4.872 4.934 93.9% Hybrid (Google Translate NMT + GPT-4o) 4.709 4.832 4.923 94.3%
> Hybrid (Google Translate NMT + Command A) 4.760 4.899 4.959 95.9%

Table 4: Translation quality comparison across methods based on human evaluation. Bolded values represent the best-performing configuration for each metric. 

Figure 7: System prompt used to guide the LLM-based translation evaluation. The prompt instructs the model to assess translations across four dimensions—adequacy, fluency, formatting accuracy, and answerability—and return results in a strict JSON format. 

A.4.2 Translation Verification Extended Results 

To ensure the validity of our multilingual bench-mark, human verifiers manually reviewed approxi-mately 25% of the translated data—corresponding to over 2,000 samples—across all tasks and lan-guages. In the main manuscript, we reported the overall scores for adequacy, fluency, formatting accuracy, and the error rate derived from answer-ability. In Tables 6, 7, 8 and 9 we present the full breakdown of results for each individual dataset and in Table 10 we conclude with the manual veri-fication average results across all datasets. Across all four benchmark datasets, native verifiers’ rat-ings reveal high translation quality, with average adequacy (4.47), fluency (4.60), and formatting ac-curacy (4.76) on a 1–5 scale. Notably, answerabil-ity remained consistently high (mean: 0.94), sug-gesting that most translations retained functional fidelity. Formatting consistency was strongest for ASB and GAIA, while fluency slightly declined for Arabic and Japanese, correlating with minor adequacy drops. Additionally, Fig. 8 presents one successful and one failed translation example, as annotated by Japanese and Korean evaluators, re-spectively. These results confirm that closed-form multilingual alignment is achievable across diverse tasks without compromising validity. 

A.4.3 Statistical sufficiency of the 25% manual audit 

Let n = 2 ,000 be the size of our manually veri-fied subset ( ≈ 25% of the full benchmark). For any binary outcome—here, Answerable vs. Unan-swerable —the sample proportion ˆp is an unbiased estimator of the true corpus proportion p. Under the binomial model, its standard error is 

SE(ˆ p) = 

r ˆp(1 − ˆp)

n .

The variance is maximised when ˆp = 0 .5, giving a worst–case standard error SE max = 0 .5/√n. Us-ing the normal approximation with critical value 

z0.025 = 1 .96 , the 95 % two–sided margin of error 15 Figure 8: Translation verification examples submitted by annotators during manual evaluation. Left: A successful Japanese translation from the GAIA dataset, rated highly across all dimensions with only a minor terminology note.  

> Right: A Korean MATH example flagged as critically erroneous due to semantic and grammatical mismatches in imperative phrasing. These examples illustrate the evaluation schema’s ability to surface both subtle formatting issues and major adequacy failures.

is therefore 

E = 1 .96 × 0.5

√n = 0.98 

√n = 0.98 

√2,000 ≈ 0.022 .

Hence any proportion reported from the manual audit has a 95 % confidence interval of at most 

±2.2 percentage points. Put differently, any sys-tematic translation error affecting more than ≈ 2% 

of the corpus would almost certainly have been de-tected in our audit. Because the sample was strati-fied by language and dataset, the same formula ap-plies within each stratum with nℓ = 0 .25 Nℓ items audited per language ℓ. Putting the numbers to-gether, the overall Answerable estimate ˆp = 0 .944 

therefore has a 95 % confidence interval 

0.944 ± 0.022 = [0 .922 , 0.966] .

Thus, even in the worst case, the corpus - level an-swerability remains above 92 %. 

Language Variant 

Chinese Simplified (Mainland) Russian Standard (Russia) Arabic Modern Standard Arabic Hebrew Modern Israeli German Standard German Spanish Latin American Hindi Standard Hindi Japanese Standard Japanese Korean Standard Korean Italian Standard Italian Portuguese Brazilian Portuguese 

> Table 5: Dialect used per language.

A.4.4 Language Variants Used 

To ensure wide applicability across regions, we translated the dataset into 11 languages using the most widely recognized and formal variant in each case. The table below lists the dialect or regional standard used in the translation process. 

A.5 Experimental Settings Additional Information 

We conducted all experiments on a Linux-based machine (Ubuntu 22.04) equipped with 64 virtual CPUs and 256 GB of RAM. To ensure reproducibil-ity and compatibility, each agent was executed within its own isolated software environment tai-lored to its specific dependencies. To ensure a fair evaluation of multilingual effects (see Eq. 2 in the main manuscript), we preserved each agent’s origi-nal implementation without introducing any code modifications. This isolation strategy allowed us to assess the impact of language variation on agent performance and behavior without confounding factors from architectural or behavioral changes. Datasets were also left unchanged in structure, for-mat, and content. Only the task inputs and expected outputs were translated into the target languages. This process yielded 12 language-specific evalu-ation suites per dataset (including English), each of which can be executed independently, enabling controlled multilingual evaluation with minimal setup and without requiring code adaptation. 16 GAIA                                                 

> Lang Adequacy (1–5) Fluency (1–5) Formatting Accuracy (1–5) Answerable de 4.14 4.04 4.74 0.91 ja 4.30 4.70 4.91 0.88 he 4.20 4.62 4.86 0.88 ko 4.49 4.84 4.40 0.94 pt-br 3.93 4.16 4.53 0.96 it 4.26 4.24 4.72 0.91 es 4.18 4.24 4.99 0.94 ar 3.54 3.79 4.41 0.98 hi 4.22 4.52 4.72 0.92 ru 4.84 4.99 4.90 0.96
> AVG 4.21 4.41 4.72 0.93

Table 6: Manual translation evaluation scores for the GAIA dataset across 10 languages. Metrics include adequacy, fluency, formatting accuracy (all rated on a 1–5 scale), and answerability (proportion of translations judged as answerable).                                                 

> MATH
> Lang Adequacy (1–5) Fluency (1–5) Formatting Accuracy (1–5) Answerable de 4.80 4.87 4.97 1.00 ja 4.11 4.41 4.96 0.82 he 4.61 4.46 4.79 0.94 ko 4.62 4.95 4.66 0.93 pt-br 4.95 4.84 4.62 1.00 it 4.65 4.85 4.88 1.00 es 4.74 4.49 4.28 0.87 ar 3.78 3.99 3.81 0.82 hi 4.76 4.79 4.96 0.95 ru 4.75 5.00 4.75 0.99
> AVG 4.58 4.67 4.67 0.93

Table 7: Manual translation evaluation scores for the SWE-Bench dataset across 10 languages. 

Figure 9: Trajectory variability between English and Hindi agents in solving the same question. The English agent follows a computation-driven path—extracting raw values and calculating the result—whereas the Hindi agent relies on surface-level textual matching and assumed equivalence. This leads to reasoning failure due to the absence of explicitly stated percentages in the Hindi query context. 

A.5.1 Agents Configuration Additional Information 

For the GAIA benchmark, we used the publicly available agent from OpenDeepResearch (von Platen et al., 2024), which features a modular framework supporting document retrieval, browser-based execution, and web search. The agent is designed for general-purpose, real-world reasoning and incorporates memory and planning via tool-17 SWE-Bench Lang Adequacy (1–5) Fluency (1–5) Formatting Accuracy (1–5) Answerable                                             

> de 4.30 4.80 4.50 1.00 ja 4.40 4.05 5.00 0.85 he 4.55 4.50 4.50 0.90 ko 4.35 4.25 4.40 0.90 pt-br 4.75 4.80 4.75 1.00 it 4.55 4.65 4.35 0.95 es 4.00 4.50 4.50 0.95 ar 3.60 4.30 4.20 0.90 hi 4.40 4.35 4.80 0.90 ru 4.85 4.95 4.80 0.95
> AVG 4.375 4.515 4.580 0.930

Table 8: Manual translation evaluation scores for the SWE-Bench dataset across 10 languages.                                                 

> ASB
> Lang Adequacy (1–5) Fluency (1–5) Formatting Accuracy (1–5) Answerable de 4.10 4.53 4.97 0.95 ja 4.68 4.76 4.91 0.96 he 4.68 4.67 5.00 0.92 ko 4.95 4.93 4.96 0.98 pt-br 4.72 4.55 4.92 1.00 it 4.25 4.47 4.80 0.98 es 4.57 4.40 4.87 0.97 ar 4.45 4.68 4.90 0.98 hi 4.88 4.85 4.95 0.98 ru 4.82 4.90 4.83 0.98
> AVG 4.61 4.67 4.91 0.97

Table 9: Manual translation evaluation scores for the ASB dataset across 10 languages.                                                 

> All Benchmarks (Average)
> Lang Adequacy (1–5) Fluency (1–5) Formatting Accuracy (1–5) Answerable de 4.375 4.554 4.867 0.964 ja 4.356 4.553 4.936 0.881 he 4.525 4.565 4.840 0.915 ko 4.660 4.847 4.660 0.943 pt-br 4.609 4.580 4.703 0.989 it 4.422 4.565 4.763 0.968 es 4.475 4.403 4.657 0.922 ar 3.859 4.255 4.423 0.936 hi 4.625 4.697 4.881 0.947 ru 4.803 4.961 4.817 0.974
> AVG 4.471 4.598 4.755 0.944

Table 10: Average manual translation evaluation scores across all benchmarks (GAIA, MATH, SWE-Bench, and ASB). Metrics include adequacy, fluency, and formatting accuracy (rated on a 1–5 scale), and answerability (proportion of samples deemed answerable). 

augmented prompting. In our evaluation, we dis-abled the web search functionality due to its re-liance on the external Serper API, which imposes rate limits and introduces latency variability. This omission was applied across all languages to en-sure consistency, and throughput scalability under multilingual evaluation. For MATH, we adopted MathChat (Wu et al., 2024), a mathematical rea-soning agent that combines multi-turn chain-of-thought prompting with symbolic computation us-ing Python and the Wolfram Alpha API tool. For SWE-bench, we used SWE-agent (Yang et al., 2024b), an autonomous software engineering agent capable of navigating repositories, modifying code, and executing test suites. It employs a plan-execute loop enhanced with file-level attention and dynamic tool invocation. For ASB, we built on the original benchmark infrastructure (Zhang et al., 2024), eliciting synthetic adversarial tool-use in various agentic scenarios (e.g. travel-agent, legal consultant, system admin). Each of the above was executed using the same LLM backbone reported in its original publication. All selected models are considered to be multilin-gual models. Specifically, GAIA used GPT-O1 (model version: 2024-12-17), MATHChat used 18 Figure 10: Relative performance differences from English for each language, broken down by dataset. Negative values indicate a drop in performance compared to English, while positive values (notably in ASB) represent increased vulnerability. The trend highlights how multilingual effects vary by language and task type. 

GPT-4 (model version: turbo-2024-04-09), SWE-agent used GPT-4.1 (model version: 2025-04-14), and ASB used Qwen2 7B (model version: 2024-10-01). 

A.6 Results Additional Information 

This section supplements the main manuscript with detailed quantitative and qualitative results. While the main paper summarizes overall trends and multilingual effects, Tables 11–14 provide per-language scores, averages, and standard deviations for each benchmark. Fig. 9 provides a multilingual qualitative failure example of Hindi language in GAIA, including a comparison of main steps in English and Hindi. Fig. 10 provides a language-wise breakdown of relative performance degrada-tion across datasets. From the Figure, we can see that there is no clear correlation between multi-lingual security robustness (ASB) and multilin-gual performance degradation. This disconnect is especially clear in real-world, language-heavy tasks like GAIA, where performance drops sharply, while structured tasks like SWE-bench and MATH remain largely unaffected. This highlights that multilingual security alignment does not directly track with multilingual task accuracy, notably in language-rich agentic tasks. 19 Language Run #1 Run #2 Run #3 Mean StdDev 

English (baseline) 0.5094 0.4450 0.4695 0.4746 0.0325 Portuguese (br) 0.3902 0.3658 0.3597 0.3719 0.0161 German 0.3658 0.3475 0.3597 0.3577 0.0093 Spanish 0.3475 0.3536 0.3680 0.3564 0.0105 Italian 0.3536 0.3353 0.3170 0.3353 0.0183 Russian 0.4268 0.4085 0.3530 0.3961 0.0384 Arabic 0.2866 0.3292 0.3170 0.3109 0.0219 Hebrew 0.3818 0.3597 0.3353 0.3589 0.0233 Hindi 0.3353 0.3780 0.3353 0.3496 0.0247 Korean 0.3597 0.3231 0.3475 0.3434 0.0186 Japanese 0.4180 0.3597 0.3170 0.3649 0.0507 

Table 11: GAIA benchmark performance across languages. 

Language Run #1 Run #2 Run #3 Mean StdDev 

English (baseline) 0.29 0.33 0.31 0.31 0.020 Portuguese (br) 0.32 0.29 0.25 0.323 0.0351 German 0.29 0.27 0.30 0.287 0.0153 Spanish 0.31 0.31 0.31 0.31 0.0000 Italian 0.30 0.29 0.32 0.303 0.0150 Russian 0.31 0.34 0.34 0.33 0.0173 Arabic 0.33 0.28 0.31 0.307 0.0252 Hebrew 0.31 0.27 0.27 0.283 0.0231 Hindi 0.28 0.28 0.28 0.280 0.0000 Korean 0.30 0.32 0.32 0.313 0.0115 Japanese 0.31 0.29 0.29 0.297 0.0153 

Table 12: SWE-Bench benchmark performance across languages. 

Language Run #1 Run #2 Run #3 Mean StdDev 

English (baseline) 0.514 0.514 0.514 0.514 0.000 Portuguese (br) 0.514 0.514 0.514 0.514 0.000 German 0.500 0.500 0.500 0.500 0.000 Spanish 0.536 0.536 0.536 0.536 0.000 Italian 0.479 0.479 0.479 0.479 0.000 Russian 0.507 0.507 0.507 0.507 0.000 Arabic 0.514 0.514 0.514 0.514 0.000 Hebrew 0.521 0.521 0.521 0.521 0.000 Hindi 0.436 0.443 0.443 0.440 0.004 Korean 0.414 0.421 0.421 0.419 0.004 Japanese 0.443 0.443 0.443 0.443 0.000 

Table 13: MATH benchmark performance across languages. 

Language Run #1 Run #2 Run #3 Mean StdDev 

English (baseline) 0.534 0.536 0.532 0.534 0.002 Portuguese (br) 0.648 0.657 0.654 0.653 0.005 German 0.659 0.652 0.654 0.655 0.004 Spanish 0.595 0.613 0.597 0.602 0.010 Italian 0.612 0.618 0.603 0.611 0.008 Russian 0.636 0.646 0.642 0.641 0.005 Arabic 0.515 0.504 0.511 0.510 0.005 Hebrew 0.558 0.571 0.572 0.567 0.008 Hindi 0.617 0.616 0.614 0.616 0.002 Korean 0.590 0.589 0.588 0.589 0.001 Japanese 0.484 0.489 0.497 0.490 0.007 

Table 14: ASB benchmark performance across languages. 

20
