Title: Agentic AI with Orchestrator-Agent Trust: A Modular Visual Classification Framework with Trust-Aware Orchestration and RAG-Based Reasoning

URL Source: http://arxiv.org/pdf/2507.10571v3

Published Time: Tue, 23 Sep 2025 03:05:58 GMT

Markdown Content:
# Agentic AI with Orchestrator-Agent Trust: A Modular Visual Classification Framework with Trust-Aware Orchestration and RAG-Based Reasoning 

## Konstantinos I. Roumeliotis a,‚àó, Ranjan Sapkota b,‚àó, Manoj Karkee b,‚àó and Nikolaos D. Tselikas a,‚àó

> a

University of the Peloponnese, Department of Informatics and Telecommunications, Tripoli, 22131, Greece 

> b

Cornell University, Department of Biological and Environmental Engineering, Ithaca, 14850, NY, USA 

A R T I C L E I N F O 

Keywords :agentic ai orchestrator agent trust trust orchestration visual classification retrieval augmented reasoning 

A B S T R A C T 

Modern Artificial Intelligence (AI) increasingly relies on multi-agent architectures that blend visual and language understanding. Yet, a pressing challenge remains: How can we trust these agents especially in zero-shot settings with no fine-tuning? We introduce a novel modular Agentic AI visual classification framework that integrates generalist multimodal agents with a non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG) module. Applied to apple leaf disease diagnosis, we benchmark three configurations: (I) zero-shot with confidence-based orchestration, (II) fine-tuned agents with improved performance, and (III) trust-calibrated orchestration enhanced by CLIP-based image retrieval and re-evaluation loops. Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator modulates trust across agents. Our results demonstrate a 77.94% accuracy improvement in the zero-shot setting using trust-aware orchestration and RAG, achieving 85.63% overall. GPT-4o showed better calibration, while Qwen-2.5-VL displayed overconfidence. Furthermore, image-RAG grounded predictions with visually similar cases, enabling correction of agent overconfidence via iterative re-evaluation. The proposed system separates perception (vision agents) from meta-reasoning (orchestrator), enabling scalable and interpretable multi-agent AI. This blueprint illustrates how Agentic AI can deliver trustworthy, modular, and transparent reasoning, and is extensible to diagnostics, biology, and other trust-critical domains. In doing so, we highlight Agentic AI not just as an architecture but as a paradigm for building reliable multi-agent intelligence. All models, prompts, results, and system components including the complete software source code are openly released to support reproducibility, transparency, and community benchmarking at our Github page. 

## 1. Introduction 

The integration of vision and language models into autonomous decision-making systems has redefined the boundaries of artificial intelligence (AI), especially in fields that demand both perceptual accuracy and interpretability Afroogh et al. (2024); Ilievski (2025). Multimodal large language models (LMMs), capable of reasoning over both visual and textual data, are increasingly employed in diverse domains, ranging from autonomous robotics and medical imaging to scientific diagnostics and agricultural monitoring Bradshaw et al. (2025); Yang et al. (2024); Mon-Williams et al. (2025). However, as these models are deployed in  

> ‚ãÜ‚ãÜ

The publication of the article in OA mode was financially supported by HEAL-Link. This work was additionally supported in part by the National Science Foundation (NSF) and the United States Department of Agriculture (USDA), National Institute of Food and Agriculture (NIFA), through the ‚ÄúArtificial Intelligence (AI) Institute for Agriculture‚Äù pro-gram under Award Numbers AWD003473 and AWD004595, and USDA-NIFA Accession Number 1029004 for the project titled ‚ÄúRobotic Blos-som Thinning with Soft Manipulators.‚Äù Additional support was provided through USDA-NIFA Grant Number 2024-67022-41788, Accession Num-ber 1031712, under the project ‚ÄúExpanding UCF AI Research To Novel Agricultural Engineering Applications (PARTNER). 

> ‚àó

Corresponding authors  

> k.roumeliotis@uop.gr

(K.I. Roumeliotis); rs2672@cornell.edu (R. Sapkota); mk2684@cornell.edu (M. Karkee); ntsel@uop.gr (N.D. Tselikas) 

> ORCID

(s): 0000-0002-8098-1616 (K.I. Roumeliotis);  

> 0000-0002-5417-6744

(R. Sapkota); 0000-0001-5337-4848 (M. Karkee);  

> 0000-0001-5799-3558

(N.D. Tselikas) 

high-stakes environments, a fundamental challenge persists: can these systems be trusted to make reliable, transparent, and justifiable decisions particularly in zero-shot or open-world scenarios where no prior task-specific fine-tuning is possible? Recent developments in agentic AI systems where au-tonomous agents collaborate, reason, and interact with their environment have highlighted the importance of meta-reasoning and modularity in AI architectures Ale et al. (2025); Buehler (2025); Sapkota et al. (2026). Rather than relying on monolithic end-to-end networks, agentic AI sys-tems distribute cognitive tasks across specialized AI agents Savaglio et al. (2020). In vision-language applications, this paradigm enables distinct agents to independently process visual inputs, generate explanations, and assess confidence, while an external orchestrator or supervisor performs higher-order reasoning to synthesize their outputs Jeyakumar et al. (2024); Zhai et al. (2024); Lin et al. (2025). This architec-tural shift not only mirrors aspects of human collaborative problem-solving but also introduces an additional layer of oversight, which is critical for ensuring accountability and trust Qiao et al. (2025). Despite the promise of this approach, trust calibration within agentic AI remains an underexplored area. Conven-tional ensemble systems often assume that agents‚Äô self-reported confidence scores are reliable proxies for correct-ness Ma et al. (2024b); Warmsley et al. (2025); Wang et al. 

Preprint Page 1 of 22 

> arXiv:2507.10571v3 [cs.AI] 21 Sep 2025

(2025); Huang et al. (2024), an assumption that breaks down in zero-shot generalist models such as GPT-4o or Qwen-2.5-VL. These models may exhibit systematic overconfidence or fail to discriminate between subtle categories in domain-specific tasks Wen et al. (2024). Consequently, there is a pressing need to assess and correct misalignments between an agent‚Äôs confidence and its actual performance. This prob-lem is particularly acute in scientific and diagnostic domains, where misclassification may lead to erroneous conclusions or interventions. In visual classification tasks such as plant disease de-tection, trustworthiness is not merely a matter of predic-tive accuracy but of interpretability and justification Nigar et al. (2024); Ding et al. (2024); Qadri et al. (2024). For instance, two models may arrive at the same prediction but differ significantly in the rationale behind their decision Ali et al. (2023); Bajorath (2025). A reliable AI system should not only be accurate but also capable of articulating why a decision was made and when to defer to alternative evidence Messeri and Crockett (2024). RAG approaches where models consult external databases or prior examples to refine their outputs offer a promising pathway to ground predictions in visual context, enhancing both trust and trans-parency Ke et al. (2025); Dong et al. (2025); Tozuka et al. (2025). Deploying generalist VLMs in high-stakes, domain-specific scenarios presents critical challenges in calibration, interpretability, and decision trustworthiness. In agricultural automation and digital diagnostics, for instance, misclassi-fications can lead to economic losses and delayed interven-tions. Traditional deep learning pipelines offer high accuracy but lack modular reasoning and trust introspection. To address these limitations, we propose a novel Agentic AI framework for visual classification that integrates trust-aware orchestration with retrieval-augmented reasoning in a modular, interpretable, and zero-shot-capable architecture. Our system coordinates multiple multimodal agents GPT-4o and Qwen-2.5-VL with a non-visual orchestrator that synthesizes final predictions based on reported confidence, natural language justifications, and internal trust scores. When discrepancies or low-confidence predictions arise, the orchestrator triggers a re-evaluation loop using RAG sup-ported by CLIP-based image retrieval, allowing the system to reflect and refine its decision using similar visual prece-dents. We introduce a three-stage pipeline (see Figure 1), incrementally augmenting the system‚Äôs capabilities. The overall flow diagram and key contributions of this study are illustrated in Figure 2, highlighting how our trust-aware Agentic AI framework enables scalable integration of vision-language agents, dynamic trust calibration, and retrieval-augmented reasoning. Each experimental stage builds on the previous to increase reasoning fidelity and trust interpretability: 

‚Ä¢ Experiment I: AI Agents operate in zero-shot mode; the orchestrator selects final predictions using re-ported confidence scores. 

‚Ä¢ Experiment II: AI Agents are fine-tuned on a curated apple leaf disease dataset using hyperparameter opti-mization (Bayesian search) to evaluate the effects of supervised domain adaptation. 

‚Ä¢ Experiment III: Image-RAG, trust-aware orchestra-tion, and agent re-evaluation are integrated into a unified Agentic AI system. A trust evaluation layer quantifies agent reliability using metrics such as Ex-pected Calibration Error (ECE), Overconfidence Ra-tio (OCR), and Consistency Gap (CG), enabling the orchestrator to make informed arbitration decisions. The same general-purpose multimodal agents intro-duced in Experiment I independently classify im-ages with natural language rationales and confidence scores; when trust metrics signal unreliability, the orchestrator initiates a re-evaluation loop that supplies agents with prior context and Image-RAG retrieval results. Agents then either revise or reaffirm their pre-dictions, and the orchestrator makes a final decision based on these updated responses. We validate this framework on a biologically grounded task: the classification of apple leaf diseases, where fine-grained distinctions (e.g., between rust and scab) and ex-planation interpretability are critical. Our results show a relative improvement of 77.94% in classification accuracy from 48.13% in the baseline to 85.63% in the trust-aware configuration even without additional fine-tuning in Exper-iment III. This demonstrates the power of structured trust arbitration and retrieval-enhanced reasoning for improving performance and interpretability in real-world, open-domain applications. In summary, our contributions are fourfold: (1) A mod-ular agentic AI system that decouples perception, reasoning, and retrieval; (2) a novel trust-aware orchestration strat-egy grounded in multi-dimensional calibration; (3) a CLIP-RAG-based re-evaluation loop for uncertainty mitigation; and (4) a comprehensive empirical validation across three reasoning regimes with reproducible and scalable design. 

## 2. Methodology 

This study proposes and evaluates a modular Agen-tic AI architecture for visual classification, combining two vision-language agents (GPT-4o and Qwen-2.5-VL) with a non-visual orchestrator (o3-mini-2025-01-31). Experiments were conducted using a curated dataset of 800 RGB images labeled into four apple disease categories, split 64% for training, 16% for validation, and 20% for testing. The system comprises three experimental setups. 

2.1. Experiment I 

In the first experimental configuration, we evaluate the performance of general-purpose multimodal LLMs in a zero-shot setting that is, without any task-specific fine-tuning. Each agent receives a prompt containing a single input image (from the test set) and is tasked with predicting  

> Preprint Page 2 of 22

Three-Stage Trust-Aware Agentic AI Framework 

Experiment I 

Zero-Shot Agents 

+ Confidence-Aware Orchestration 

Experiment II 

Fine-Tuned Agents 

+ Confidence-Aware Orchestration 

Experiment III 

Zero-Shot Agents 

+ Trust-Aware Orchestration + RAG + Re-Evaluation Loop      

> Agent Fine-Tuning Add Trust Metrics + RAG
> Generalist Multimodal Models Domain-Specialized via Supervised Fine-Tuning Trust-Calibrated + Evidence-Grounded Pipeline
> GPT-4o, Qwen-2.5-VL (Zero-Shot) GPT-4o, Qwen-2.5-VL (Fine-Tuned) GPT-4o, Qwen-2.5-VL + CLIP (RAG)

Figure 1: Three-stage trust-aware agentic AI framework across experiments. Final 

# Prediction 

Initial 

Prediction 

Label -Free 

Accuracy 

Plug & play 

Agents 

Trust 

Calibration 

# Images 

# Experiment 3 Experiment 2 

# Experiment 1 

Figure 2: Overview of our trust-aware Agentic AI framework for visual classification, illustrating modular agents, orchestration stages, trust calibration, and retrieval-augmented re-evaluation for accurate, interpretable decisions. This workflow is designed for plant leaf disease classification but is generalizable to any RGB image classification task involving multimodal agents and trust-aware decision pipelines. 

the most appropriate plant disease class. In addition to the predicted class label, agents are required to return (i) a confidence score in the range [0.0‚Äì1.0] and (ii) a natural language explanation justifying their decision (Appendix, Fig. A1). These three outputs classification, confidence, and rationale are compiled and forwarded to a non-visual orches-trator model (o3-mini-2025-01-31 OpenAI (2025)), which serves as a comparative reasoner. The orchestrator does not process images directly; instead, it evaluates the agents‚Äô predictions and justifications in light of their associated con-fidence scores and produces a final classification decision through structured, confidence-aware reasoning (Appendix, Fig. A2). 

2.2. Experiment II 

In Experiment II, both agents were fine-tuned using supervised learning techniques to improve classification per-formance. For GPT-4o, fine-tuning employed a hyperparam-eter configuration informed by prior ResNet-50 optimization studies. Qwen-2.5-VL underwent over 50 hyperparameter tuning trials, beginning with heuristic parameter estimates and refined through performance-based search strategies. 

Preprint Page 3 of 22 1. GPT-4o Fine-Tuning: In this phase, we explored two fine-tuning strategies: one using default hyper-parameters provided by the OpenAI platform, and one informed by prior hyperparameter optimization conducted on a ResNet-50 model using Bayesian op-timization. (a) ResNet-50‚ÄìInformed Hyperparameter Transfer. To mitigate the computational cost of perform-ing hyperparameter optimization directly on GPT-4o, we hypothesized that high-performing hyperparameters derived from ResNet-50 tun-ing on the same dataset could be effectively transferred to GPT-4o. Specifically, Bayesian optimization with the Tree-structured Parzen Estimator (TPE) algorithm was used to explore the ResNet-50 hyperparameter space across 30 trials. TPE iteratively models the objective func-tion ùëì (ùë• ), evaluates the expected improvement (EI) of candidate configurations, and selects promising trials using a likelihood ratio. Optimization was implemented using the Op-tuna library on an NVIDIA A100-SXM4-40GB GPU via Colab Enterprise. Early stopping and pruning were used to improve computational efficiency. The best configuration identified 10 training epochs and a batch size of 16 was sub-sequently applied to GPT-4o fine-tuning through the OpenAI fine-tuning interface. Training and validation sets were formatted into jsonl files with prompt‚Äìcompletion pairs before submis-sion. (b) Default Hyperparameter Configuration. In par-allel, we conducted fine-tuning using the default hyperparameters recommended by the OpenAI platform: 3 epochs and a batch size of 1. Iden-tical training and validation files were used to ensure a fair comparison. Fine-tuning GPT-4o using the ResNet-50‚Äìinformed hyperparameters required approximately 1,778 sec-onds ( ‚àº29.6 minutes) and incurred a cost of USD 47.53. In comparison, fine-tuning with the default hyperpa-rameters (3 epochs, batch size of 1) required 1,652 sec-onds ( ‚àº27.5 minutes) at a reduced cost of USD 13.09. Although direct hyperparameter optimization on GPT-4o could potentially yield higher-performing con-figurations, the computational and financial cost of conducting such a process over multiple trials renders it impractical under current constraints. A detailed comparison of runtime, cost, and valida-tion loss for both configurations is provided in the Appendix, Table A2. Based on these results, the ResNet-50‚Äìinformed con-figuration yielded a substantially lower validation loss (0.0088) and was therefore selected as the preferred GPT-4o variant for integration into the Agentic AI system implemented in this study. 2. Qwen-2.5-VL-7B Fine-Tuning: To adapt the Qwen-2.5 Vision-Language 7B (VL-7B) model for our task, we fine-tuned the unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit 

model Wang et al. (2024), a 4-bit quantized variant of the original pretrained Qwen2.5 model. This quan-tized version enables efficient loading and inference with reduced memory requirements, making it suit-able for commodity GPU hardware. We employed Low-Rank Adaptation (LoRA) in 16-bit precision to inject trainable adapters into the model, thereby allow-ing effective fine-tuning without full dequantization of the backbone weights. The LoRA adapters were integrated into all major architectural components of the model, including the vision encoder layers, the language modeling transformer layers, multi-head attention modules, and the feedforward multilayer perceptrons (MLPs). Each adapter was configured with a rank ùëü = 16 and scaling factor ùõº = 16 , balancing parameter efficiency with expressive capacity. We used the Parameter-Efficient Fine-Tuning (PEFT) framework to update only the LoRA-injected weights, keeping the underlying 4-bit quantized parameters frozen throughout training. To identify the optimal hyperparameter configuration, we employed Bayesian optimization using the Op-tuna framework, leveraging the TPE as the sampler. The search space included learning rate, per-device batch size, gradient accumulation steps, warmup ratio, weight decay, and number of training epochs. Opti-mization targeted the minimization of validation loss. Trial results were persistently stored in both jsonl 

and SQLite formats, allowing for checkpointing and resumability in the event of interruption. Poor-performing trials were automatically pruned us-ing Optuna‚Äôs MedianPruner , which compares interme-diate results to the median of previous completed trials and terminates underperformers early. Early stopping was also employed during training to prevent overfit-ting and reduce unnecessary computational expendi-ture. All training was conducted using the SFTTrainer 

class from the Hugging Face trl library, which sup-ports supervised fine-tuning with periodic evaluation at the end of each epoch. Throughout training, only the LoRA adapters were updated, while the quantized backbone remained frozen to preserve efficiency. Recognizing the potential inefficiency of starting hy-perparameter optimization from purely random initial conditions, particularly in high-dimensional or sensi-tive parameter spaces, we manually enqueued a strong initial configuration to guide the search. This warm-start configuration used a learning rate of 2 √ó 10 ‚àí4 ,a per-device batch size of 2, 8 gradient accumulation steps, a warmup ratio of 0.05, a weight decay of 0.01, and 10 training epochs. This guided initialization enabled the optimizer to begin exploring in a region of the parameter space known to yield promising results. A total of 50 hyperparameter optimization trials were performed. Among these, 20 trials completed success-fully, while 30 were pruned based on intermediate performance. No trials failed due to runtime errors.  

> Preprint Page 4 of 22

The best-performing trial achieved a validation loss of 1.0 √ó 10 ‚àí5 , using a learning rate of approximately 

1.094 √ó 10 ‚àí5 , a batch size of 4, 4 gradient accu-mulation steps, a warmup ratio of 0.0997, a weight decay of 0.00127, and 15 training epochs (Appendix, Table A6). During inference, both the fine-tuned GPT-4o and Qwen agents received a single input image and generated three out-puts: (i) a predicted disease category, (ii) a natural language explanation justifying the classification, and (iii) a normal-ized confidence score in the range [0 , 1] . As in the previ-ous configuration (Experiment I), a non-visual orchestrator model ( o3-mini-2025-01-31 ) served as the final decision-maker. This orchestrator performed comparative reasoning by evaluating structured prompts that included each agent‚Äôs predictions, explanations, and confidence scores. Based on this synthesis, it produced a consolidated classification ac-companied by a rationale for its selection. 

2.3. Experiment III 

In the final experimental configuration, we integrate in our Agentic AI system CLIP-based image retrieval (Image-RAG), trust-aware orchestration, and agent re-evaluation mechanisms. The orchestrator incorporates similarity-based retrieval results and calibration metrics to determine whether to trust agent outputs or trigger re-evaluation loops. The system dynamically guides agents to revise their predictions when confidence or justification is misaligned with expected trust signals (Fig. A3). 

Trust Evaluation Layer. The Trust Evaluation Layer conducts offline trust profiling of the AI agents by quanti-fying their calibration, discriminative reliability, and consis-tency over a labeled dataset. Specifically, both agents were evaluated in a zero-shot inference setting on the training im-age set originally employed in Experiment II for fine-tuning, but here presented without ground-truth labels during evalu-ation. For each image, we recorded the predicted class, asso-ciated confidence score, and explanatory output provided by the agent. Using this comprehensive log of model behavior, we computed a suite of quantitative trustworthiness metrics to assess each agent‚Äôs predictive confidence alignment and overall reliability under zero-shot conditions. 

‚Ä¢ Expected Calibration Error (ECE): In this study, ECE quantifies the discrepancy between a model‚Äôs predicted confidence and its actual accuracy across prediction bins, serving as a key metric for assessing trustworthiness in zero-shot and fine-tuned agentic AI systems. A low ECE value indicates that the model‚Äôs self-reported confidence aligns closely with its empir-ical correctness, enabling more reliable orchestration decisions based on agent trustworthiness. By incorpo-rating ECE into the orchestrator‚Äôs arbitration logic, we enhance the system‚Äôs ability to identify overconfident failures and trigger re-evaluation loops when agent confidence is not well-calibrated. ECE =

> ùëÄ

‚àë

> ùëö =1

|ùêµ ùëö |

ùëõ ||acc (ùêµ ùëö ) ‚àí conf (ùêµ ùëö )|| ,

Where ùêµ ùëö is the ùëö -th confidence bin, ùëõ the number of samples, and acc and conf represent per-bin accuracy and confidence. 

‚Ä¢ Overconfidence Ratio (OCR): The OCR measures the proportion of incorrect predictions made with high confidence (e.g., confidence > 0.9), capturing the extent to which models exhibit unjustified certainty. A high OCR value signals a critical misalignment be-tween model confidence and actual correctness, often leading to erroneous decisions in confidence-based orchestration pipelines. In our trust-aware framework, OCR serves as a penalizing factor in agent arbitration, allowing the orchestrator to detect and downweight predictions from agents that frequently make confi-dently wrong decisions. OCR = |{ùëñ ‚à∂ÃÇùë¶ ùëñ ‚â† ùë¶ ùëñ ‚àß ùëê ùëñ > 0.9} ||{ùëñ ‚à∂ ùëê ùëñ > 0.9} | ,

Measuring how often models are confidently wrong. 

‚Ä¢ Consistency Gap (CG): The CG quantifies the di-vergence in a model‚Äôs predictions when exposed to semantically equivalent but syntactically varied prompts, serving as a proxy for reasoning stability under linguistic perturbations. A large CG indicates that the AI agent‚Äôs output is highly sensitive to prompt phrasing, undermining reliability and reproducibility in decision-making. Within our framework, CG is used as a trust signal to evaluate the robustness of agent responses, enabling the orchestrator to iden-tify models prone to inconsistency and initiate re-evaluation when necessary. CG = 1

ùëõ 

> ùëõ

‚àë

> ùëñ =1

ùïÄ [ùëÉ (1)  

> ùëñ

‚â† ùëÉ (2)  

> ùëñ

],

Assessing variation in outputs across prompt formu-lations. This quantitative profiling enabled agent-level trust esti-mation: a method for assessing an agent‚Äôs general reliability independently of individual inputs. These trust profiles were then used by the orchestrator to modulate the influence of each agent in decision fusion granting higher weight to agents with stronger calibration and lower overconfidence. 

Image RAG. To complement the Agentic AI classifi-cation framework, we incorporated a multimodal retrieval-augmented generation (Image-RAG) system designed for plant disease detection via semantic similarity and weighted voting. This framework utilizes a pre-trained vision-language model (CLIP, ViT-B/32 variant) to embed input images into  

> Preprint Page 5 of 22

a high-dimensional feature space that supports interpretable and context-rich decision-making. Given an input image ùêà ‚àà ‚Ñùùêª √óùëä √ó3 , the CLIP vision encoder transforms it into a 512-dimensional embedding ùêû ‚àà

‚Ñù512 using a vision transformer architecture. This includes patch embedding ( 32 √ó 32 ), positional encoding, multi-head self-attention, and a class token aggregation mechanism. Embeddings are subsequently ùêø 2-normalized to lie on the unit hypersphere, enabling cosine similarity to serve as the primary similarity metric: 

ùêû ùëñ = L2_normalize (ViT CLIP (ùêà ùëñ )) ,

sim (ùêû ùëñ , ùêû ùëó ) = cos( ùúÉ ùëñùëó ) = ùêû ùëñ ‚ãÖ ùêû ùëó (1) All reference embeddings are stored in a vector database implemented using Facebook AI Similarity Search (FAISS), employing an exact inner product index ( IndexFlatIP ). Each entry is paired with a category label ùë¶ ùëñ taking values healthy ,

black-rot , rust , scab , and associated metadata (e.g., image URLs, index references). Each entry is paired with a category label ùë¶ ùëñ ‚àà { healthy ,

black-rot , rust , scab } and associated metadata (e.g., image URLs, index references). At inference time, the system performs ùëò -nearest neigh-bor retrieval. Given a query image ùêà ùëû , we compute its em-bedding ùêû ùëû and retrieve the top-ùëò most similar images: 

ùëÖ ùëò = arg max  

> ùëò

{cos( ùêû ùëû , ùêû ùëñ ) ‚à£ ùëñ ‚àà [1 , ùëÅ ]} (2) Each retrieved item is assigned a similarity score ùë† ùëñ =cos( ùêû ùëû , ùêû ùëñ ).To classify the query image, the system applies a weighted voting mechanism in which the confidence for each category 

ùëê is computed by normalizing the similarity-weighted votes from the retrieved examples: conf (ùëê ) = 

‚àëùëñ ‚ààùëÖ ùëò ,ùë¶ ùëñ =ùëê ùë† ùëñ 

‚àëùëó ‚ààùëÖ ùëò ùë† ùëó 

(3) This produces interpretable confidence scores that reflect both the quantity and quality of visual evidence for each class. Implementation details include batch processing for effi-ciency, robust error handling (e.g., corrupted images, miss-ing URLs), and database persistence using FAISS binary formats and Python serialization. The flat index provides exact search with ùëÇ (ùëÅ ) query complexity, sufficient for moderate dataset sizes. For larger-scale deployment, approx-imate nearest neighbor indexing may be integrated. When queried, the Image-RAG system returns a ranked list of candidate categories along with normalized confi-dence scores, e.g.: 

> [{"category": "scab", "confidence": 0.5005}, {"category": "healthy", "confidence": 0.3996}, {"category": "rust", "confidence": 0.0999} ]

This interpretability and modularity make Image-RAG a natural supplement to the broader Agentic AI classification pipeline. All source code, algorithm and data is publicly available at https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust 

## 3. Results 

3.1. Zero-Shot Agentic Classification with Confidence-Aware Orchestration 

In the first experimental configuration, we evaluated the baseline performance of a modular Agentic AI system in a zero-shot classification setting. Two general-purpose multimodal agents, GPT-4o and Qwen-2.5-VL, were de-ployed without any task-specific fine-tuning. Each agent received an image prompt and independently produced a classification label, a natural language rationale, and a nor-malized confidence score. These outputs were then passed to a non-visual reasoning orchestrator (o3-mini-2025-01-31), which performed structured comparative reasoning to synthesize a final classification decision. The orchestrator operated under a confidence-aware strategy, weighting agent responses based on reported confidence and justification coherence, without any external retrieval or trust calibration mechanisms. This configuration establishes a foundational benchmark to assess model reliability, ensemble effective-ness, and overconfidence dynamics in open-domain zero-shot inference. The performance metrics of the agents and orchestrator are summarized in Table 1. GPT-4o achieved the highest zero-shot accuracy at 56.88%, followed by the orchestrator at 48.13% and Qwen at 45.00%. Confidence score distributions (Fig. 3a) revealed significant disparities in self-reported cer-tainty, with Qwen averaging 94.3% confidence versus GPT-4o‚Äôs 87.4% (Table A1). However, this confidence was not reliably predictive of correctness; overconfident misclassifi-cations were common, particularly for Qwen, contributing to a higher OCR. The orchestrator‚Äôs comparative logic led to a marginal calibration improvement, though the resulting accuracy remained below 50%. Figure 3b reports top-1 classification accuracy for each agent, where GPT-4o clearly outperforms the others. Weighted precision scores are illustrated in Fig.3c, and raw confidence distributions are presented in Fig.3d. Average confidence levels per model are plotted in Fig.3e, with Qwen showing systematic overconfidence. Recall and F1 scores are detailed in Figs.3f and 3g, respectively, showing GPT-4o leading across metrics. Confusion matrices in Figs. 3h, 3i, and 3j display inter-class prediction patterns for Qwen, GPT, and the orchestra-tor, respectively. Misclassification frequently occurred be-tween visually similar disease classes (e.g., black-rot and rust), highlighting the inherent difficulty of this task for zero-shot models. These findings reveal that while zero-shot multimodal agents can produce fluent and confident predictions, their         

> Preprint Page 6 of 22 (b) (c)
> (d) (e) (f) (g)
> (h) (i)(j)
> (a) Figure 3: Experiment I ‚Äì Zero-Shot Performance. (a) Violin plot of model confidence distributions across all classes for Qwen-2.5-VL, GPT-4o, and the orchestrator in the zero-shot configuration (Experiment I). (b) Top-1 classification accuracy comparison of Qwen, GPT, and the orchestrator in Experiment I. (c) Weighted precision scores of all agents in the zero-shot setting. (d) Histogram of raw confidence scores reported by Qwen, GPT, and the orchestrator. (e) Mean confidence scores per agent, aggregated across the entire test set. (f) Weighted recall for each agent in zero-shot inference. (g) Weighted F1-scores for Qwen, GPT, and the orchestrator, highlighting performance balance. (h) Confusion matrix for Qwen-2.5-VL predictions on the evaluation set. (i) Confusion matrix for GPT-4o predictions, revealing inter-class confusion patterns. (j) Confusion matrix for the final orchestrator decision outcomes based on trust-aware arbitration.

self-reported confidence often fails to align with empirical performance. Qwen‚Äôs overconfidence produced high OCR values and unstable outcomes, while GPT-4o demonstrated slightly better alignment between confidence and correct-ness. The orchestrator, despite lacking visual input, added interpretability and decision consistency through structured reasoning. However, the marginal performance gains and misalignment of confidence and accuracy underscore the need for deeper trust calibration. These limitations served as motivation for the next experimental stages introducing supervised fine-tuning (Experiment II) and a full trust-aware orchestration with re-evaluation mechanisms (Experiment III). 

3.2. Fine-Tuned Agentic Models with Confidence-Aware Orchestration 

To evaluate whether supervised adaptation improves model reliability and ensemble synergy, we conducted a second experiment in which both GPT-4o and Qwen-2.5-VL were fine-tuned on the apple disease classification dataset. The overall setup mirrored Experiment I, retaining identical multimodal inputs, test prompts, and classification objec-tives. However, the zero-shot agents were replaced with  

> Preprint Page 7 of 22

Table 1 

Performance metrics (accuracy, weighted and macro precision, recall, and F1) for each agent across three experimental configurations. All metrics for each experiment are shown in the same row. 

Experiment Qwen GPT Orchestrator I. Zero-Shot +Confidence-Aware 

Accuracy: 0.4500 Precision ùë§ : 0.4742 Recall ùë§ : 0.4500 F1-score ùë§ : 0.3763 Precision ùëö : 0.4742 Recall ùëö : 0.4500 F1-score ùëö : 0.3763 Accuracy: 0.5688 Precision ùë§ : 0.6985 Recall ùë§ : 0.5688 F1-score ùë§ : 0.5078 Precision ùëö : 0.6985 Recall ùëö : 0.5688 F1-score ùëö : 0.5078 Accuracy: 0.4813 Precision ùë§ : 0.5183 Recall ùë§ : 0.4813 F1-score ùë§ : 0.4062 Precision ùëö : 0.5183 Recall ùëö : 0.4813 F1-score ùëö : 0.4062 

II. Few-Shot +Confidence-Aware 

Accuracy: 0.9563 Precision ùë§ : 0.9603 Recall ùë§ : 0.9563 F1-score ùë§ : 0.9558 Precision ùëö : 0.9603 Recall ùëö : 0.9563 F1-score ùëö : 0.9558 Accuracy: 0.9813 Precision ùë§ : 0.9817 Recall ùë§ : 0.9813 F1-score ùë§ : 0.9812 Precision ùëö : 0.9817 Recall ùëö : 0.9813 F1-score ùëö : 0.9812 Accuracy: 0.9750 Precision ùë§ : 0.9765 Recall ùë§ : 0.9750 F1-score ùë§ : 0.9747 Precision ùëö : 0.9765 Recall ùëö : 0.9750 F1-score ùëö : 0.9747 

III. Zero-Shot +Trust-Aware +RAG 

Accuracy: 0.7313 Precision ùë§ : 0.7526 Recall ùë§ : 0.7313 F1-score ùë§ : 0.7292 Precision ùëö : 0.7526 Recall ùëö : 0.7313 F1-score ùëö : 0.7292 Accuracy: 0.8750 Precision ùë§ : 0.8898 Recall ùë§ : 0.8750 F1-score ùë§ : 0.8690 Precision ùëö : 0.8898 Recall ùëö : 0.8750 F1-score ùëö : 0.8690 Accuracy: 0.8563 Precision ùë§ : 0.8591 Recall ùë§ : 0.8563 F1-score ùë§ : 0.8555 Precision ùëö : 0.8591 Recall ùëö : 0.8563 F1-score ùëö : 0.8555   

> Weighted (subscript ùë§ ) and macro-averaged (subscript ùëö ) precision, recall, and F1 scores are reported.

domain-adapted variants, each fine-tuned on a labeled train-ing set of 512 samples and validated on 128 additional sam-ples. Importantly, the orchestrator (o3-mini-2025-01-31) re-mained unchanged and continued to perform confidence-aware arbitration, relying solely on agent-generated confi-dence scores and textual justifications to make final deci-sions. This design tests whether fine-tuning alone, without any updates to the orchestration logic or external retrieval modules, could significantly enhance prediction accuracy, calibration, and inter-agent agreement. Table 1 presents the updated metrics: GPT-4o achieved an accuracy of 98.13%, while Qwen-2.5-VL reached 95.63%. The orchestrator, de-spite lacking direct visual input, attained 97.50% accu-racy, underscoring the impact of improved agent outputs on system-wide arbitration. Weighted and macro-averaged precision, recall, and F1 scores exceeded 95% for all models, indicating a substantial leap from the zero-shot performance baseline. Figure 4a shows violin plots of agent confidence distri-butions after fine-tuning. Compared to the zero-shot setting, these distributions are narrower and more centered, suggest-ing improved calibration. Figure 4b reports top-1 accuracy, while Fig.4c highlights gains in weighted precision across agents. Notably, the confidence histograms in Fig.4d exhibit reduced overconfident noise, and the average confidence values in Fig. 4e show a modest increase (e.g., GPT-4o rose from 87.4% to 92.58%), indicating better alignment between certainty and correctness (Table A3). Recall and F1 improvements are presented in Figs. 4f and 4g, respectively. The F1 score for GPT-4o increased from 50.78% to 98.12%, and for Qwen from 37.63% to 95.58%. These gains reveal that fine-tuning not only enhances correct predictions but also reduces false positives and negatives. Confusion matrices in Figs. 4h‚Äì4j show sharply improved class-level discrimination. Notably, categories like black-rot and rust, which previously showed misclassification overlap, now exhibit strong diagonal dominance. The orchestrator benefited substantially from the im-proved agent outputs. Since its decision logic depends on comparative evaluation of confidence-aligned explanations, the increase in calibration quality directly translates to more reliable ensemble predictions. Interestingly, although the or-chestrator does not receive image inputs, its final predictions align well with the dominant and better-calibrated agent in most cases. The results confirm that supervised adaptation signifi-cantly enhances the classification performance of general-purpose vision-language agents within the agentic AI frame-work. After fine-tuning on the domain-specific apple disease 

Preprint Page 8 of 22 dataset, both GPT-4o and Qwen-2.5-VL showed notable gains across accuracy, precision, recall, and F1 scores. As shown in Table 1, GPT-4o achieved 98.13% accuracy and Qwen 95.63%, while the orchestrator despite lacking direct image access reached 97.50% accuracy. This highlights how confidence-aware arbitration benefits from well-calibrated agent responses, with the orchestrator‚Äôs performance reflect-ing the value of alignment between self-reported confidence and correctness. However, the experiment also revealed a key limitation: when both fine-tuned agents confidently agree on an incor-rect prediction, the orchestrator lacks a dissenting signal and cannot intervene. This consensus failure mode exposes the risk of overreliance on internal confidence and justifications alone. To address such blind spots, the next experimental framework integrates external trust metrics and retrieval-based verification. These enhancements aim to assess not only what agents predict, but also how trustworthy and evidence-grounded those predictions are especially under ambiguity or in high-stakes settings. 

3.3. Trust-Aware Orchestration with RAG and Re-Evaluation Loops 

To overcome the reliability ceiling observed in previ-ous settings, the third experiment introduces a trust-aware orchestration framework augmented with a RAG pipeline. Unlike Experiment II, this configuration retains the zero-shot GPT-4o and Qwen-2.5-VL agents to investigate whether reasoning and calibration improvements can be realized without task-specific fine-tuning. Enhancements to the or-chestrator include: (i) a multi-metric trust scoring module incorporating ECE, OCR, and CG, and (ii) a dynamic Re-Evaluation Loop, triggered when agent trust scores fall below a learned threshold. In this setup, low-trust predictions trigger the orchestra-tor to initiate a re-evaluation loop, prompting agents with their prior responses and augmenting the input with semantic retrievals from Image-RAG. The retrieval module uses CLIP (ViT-B/32) to encode a curated set of class-representative reference images into 512-dimensional embeddings, which are ùêø 2-normalized and stored in a FAISS vector database using an exact inner product index ( IndexFlatIP ). Each refer-ence embedding is associated with a disease category label 

ùë¶ ùëñ ‚àà healthy , black-rot , rust , scab and enriched with meta-data such as image URLs and textual definitions. During re-evaluation, the agents receive the top-ùëò most similar refer-ence examples based on cosine similarity, integrated into an updated prompt. This guides them to revise or reaffirm their prediction with additional visual and semantic context. The orchestrator then assesses the updated responses using trust metrics (ECE, OCR, CG) and finalizes the decision based on the most coherent and trustworthy agent output. 

Performance: As summarized in Table 1, GPT-4o achieved 87.50% accuracy and Qwen reached 73.13%, both significantly outperforming their initial zero-shot baselines in Experiment I. The orchestrator attained an accuracy of 85.63%, demonstrating that trust-aware orchestration, combined with retrieval-based contextualization, can close a large portion of the performance gap typically addressed via fine-tuning. Subfigure 5b shows the accuracy improvements, while subfigures 5c, 5f, and 5g illustrate the precision, recall, and F1-score gains, respectively. 

Calibration: As shown in Fig. 5a, the violin plot of confidence distributions revealed a reduction in overconfi-dence for both agents. Mean confidence scores (Fig. 5e) slightly decreased compared to Experiment II, suggesting more cautious, better-calibrated outputs. The orchestrator‚Äôs histogram (Fig. 5d) reflects this calibrated behavior, with lower variance and reduced extremity in self-reported confi-dence (Table A5). 

Qualitative Gains: Confusion matrices shown in Figs. 5h‚Äìj provide insight into class-specific performance. RAG and trust-based re-evaluation mitigated frequent misclassifica-tions seen in Experiment I especially in visually ambigu-ous classes such as black-rot and rust by incorporating external visual-textual anchors. Notably, GPT-4o showed strong diagonal dominance in its matrix, and the orchestrator successfully avoided the propagation of low-trust predictions even when both agents initially erred. 

Trust profiling: From the profiling, several insights emerge regarding agent reliability under zero-shot condi-tions. Importantly, both models exhibit suboptimal cali-bration and trustworthiness metrics overall (Table A4) an expected outcome given that they were not fine-tuned on the task-specific dataset. Nonetheless, relative differences offer valuable guidance for orchestration design: 

‚Ä¢ GPT demonstrates superior calibration, with a lower ECE (ECE = 0.293 vs. 0.453 for Qwen) and a sub-stantially higher Confidence‚ÄìCorrectness Correlation (CCC = 0.361, ùëù < 0.0001 ), suggesting that its confidence estimates are more aligned with empirical correctness and thus more actionable in trust-aware orchestration. 

‚Ä¢ GPT exhibits greater reliability under uncertainty, evi-denced by a lower Overconfidence Rate (OCR = 0.416 vs. 0.508) and fewer high-confidence errors (213 vs. 260), indicating a reduced risk of confidently incorrect predictions a critical feature for high-stakes decision environments. 

‚Ä¢ Qwen displays overconfidence and poor discrimina-tion, reporting the highest average confidence (0.945) but a minimal Confidence Gap (CG = 0.009), meaning it struggles to differentially calibrate its confidence for correct versus incorrect predictions. This weakens its applicability in systems that depend on confidence signals for agent weighting or override logic. These findings enabled the orchestrator to weight GPT‚Äôs predictions more heavily during ensemble decision-making, thereby improving the robustness and accuracy of the overall Agentic AI system.         

> Preprint Page 9 of 22 (b) (c)
> (d) (e) (f) (g)
> (h) (i)(j)
> (a) Figure 4: Experiment II ‚Äì Fine-Tuned Agents. (a) Violin plot of confidence distributions across all classes for Qwen-2.5-VL, GPT-4o, and the orchestrator following supervised fine-tuning. (b) Top-1 classification accuracy comparison after agent fine-tuning. (c) Weighted precision scores across the three agents, showing enhanced discriminative performance. (d) Histogram of raw confidence scores across all predictions, reflecting sharper calibration post fine-tuning. (e) Mean confidence scores per agent, showing convergence in self-reported certainty. (f) Weighted recall scores for Qwen, GPT, and the orchestrator on the test set. (g) Weighted F1-scores highlighting overall performance balance improvements. (h) Confusion matrix for Qwen-2.5-VL showing improved inter-class discrimination. (i) Confusion matrix for GPT-4o illustrating reduced misclassification frequency. (j) Confusion matrix for the orchestrator‚Äôs final decisions, demonstrating stability in arbitration after fine-tuning.

Disagreement Analysis and Trust Arbitration: An analysis of disagreements between agents and system com-ponents provides further insight: In 12.5% of cases, the GPT agent refused the orchestrator‚Äôs recommendation for re-evaluation, returning the same prediction as before. How-ever, in only 3 out of those 20 cases did this result in a correct prediction. The same pattern, but to a greater extent, was observed with Qwen: the agent ignored the re-evaluation recommendation in 29.38% of cases, returning the same prediction. Yet only 5 out of 47 such instances led to a correct outcome. These low correctness rates in disagreement scenarios suggest that the orchestrator, re-evaluation loop, and Image-RAG remain authoritative sources in edge cases although the effectiveness of this integration benefits from final arbitra-tion by a meta-reasoning agent. Further analysis of orchestrator-agent disagreements shows: 

‚Ä¢ Orchestrator vs. GPT: Disagreements occurred 36 times (22.5%), with the orchestrator being correct in 16 of those cases (44.44%). 

‚Ä¢ Orchestrator vs. Qwen: Disagreements occurred 22 times (13.75%), with the orchestrator being correct in 21 cases (95.45%). The orchestrator significantly outperforms Qwen in dis-agreement scenarios and even surpasses GPT nearly half  

> Preprint Page 10 of 22

the time, underscoring the value of trust-aware arbitration beyond simple majority voting. However, the re-evaluation process is not without draw-backs. In some instances, it led to overcorrection: 

‚Ä¢ GPT changed correct answers to incorrect ones 3 times (1.88%) after receiving a re-evaluation prompt. 

‚Ä¢ Qwen did so 22 times (13.75%). While rare for GPT, Qwen‚Äôs frequent revision of correct answers indicates a higher sensitivity to prompt influence, highlighting the need for better confidence calibration and tighter visual-textual integration. 

Interpretability and Scalability: While the orchestra-tor achieved near-fine-tuned accuracy without labeled train-ing data, residual failure cases remain, especially when both agents confidently agree on a wrong label. This scenario exposes the limits of current trust metrics to fully capture un-certainty in high-confidence false predictions. Nevertheless, the current framework provides a scalable, modular, and interpretable zero-shot system that balances generalizability and reliability. Collectively, Experiment III highlights that zero-shot generalist agents, when embedded in trust-calibrated agentic AI systems with access to external retrieval, can achieve expert-level accuracy in image classification tasks. These findings support a broader vision of scalable agentic intelli-gence where trust, not tuning, becomes the key to real-world deployment. 

3.4. Comparative Performance and Ablation Analysis 

3.4.1. Time and Calibration Performance Across Configurations 

Figure 6 presents a comparative analysis of inference time across the three experimental setups. Subfigures 6a‚Äì c show that fine-tuned agents (Experiment II) achieve the lowest latency due to optimized internal representations, while trust-aware orchestration with retrieval (Experiment III) introduces modest time overhead from retrieval and re-evaluation cycles. Histograms in subfigures 6d‚Äìf reveal that Experiment III exhibits a heavier tail in inference time distri-bution, yet remains within real-time thresholds. While fine-tuned models yield the highest performance, they require extensive training and lack task transferability. In contrast, Experiment III achieves near-optimal accuracy (85.6%) with only 1.3 √ó the inference time of zero-shot baselines, offering a practical compromise for label-scarce or dynamic environ-ments. 

3.4.2. Confidence-Accuracy Calibration and Overconfidence Mitigation 

Figure 7 illustrates confidence‚Äìaccuracy calibration curves for Qwen-2.5-VL, GPT-4o, and the orchestrator across all experiments. In the zero-shot setting (subfigures 7a‚Äìc), both agents display considerable overconfidence, with confidence often exceeding empirical accuracy. This misalignment is partially mitigated by the orchestrator, which arbitrates between agent outputs. Fine-tuning (subfigures 7d‚Äìf) im-proves calibration, especially for GPT-4o, aligning predicted confidence more closely with true correctness. Trust-aware orchestration in Experiment III (subfigures 7g‚Äìi) further suppresses overconfidence through calibrated re-evaluation. Figure 8 visualizes the relationship between agent overcon-fidence (mean confidence on incorrect predictions) and final macro-F1, confirming that trust-augmented pipelines better align certainty with correctness. 

## 4. Discussion and Future Directions 

The increasing complexity and autonomy of AI systems calls for robust, interpretable, and generalizable architec-tures that can reason, evaluate, and adapt in real time Longo et al. (2024). This study presents a systematic exploration of a novel trust-aware agentic AI framework that blends zero-shot vision-language agents with orchestration, trust calibration, and RAG-based re-evaluation. Our three-tier experimental structure reveals key insights into how such systems can be structured for both performance and scala-bility while preserving explainability and adaptability. A central insight from this study is the critical role of calibration and trust estimation in agentic AI systems. While traditional ensemble methods often aggregate agent outputs under the assumption of independent and reliable perfor-mance MacKenzie and Munster (2019); Ganaie et al. (2022), such strategies falter in real-world settings especially under zero-shot conditions where agents may exhibit systematic overconfidence or miscalibration Frei and Isotta (2019); Ojha et al. (2025). In these contexts, na√Øvely trusting self-reported confidence scores can lead to compounding errors, particularly in scenarios that demand high reliability and interpretability. To address this, we adopt a trust-aware orchestration strategy that incorporates metrics such as ECE, OCR, and CG, enabling the orchestrator to quantitatively assess the alignment between confidence and correctness, as well as the consistency of reasoning under varied prompt formu-lations. This shift from purely accuracy-driven aggregation to trust-calibrated decision fusion reflects a broader move-ment in AI system design toward epistemic robustness and risk-aware reasoning. Prior research in LLM-based decision support, autonomous robotics, and human-AI collaboration has underscored the limitations of relying on uncalibrated model outputs, and has proposed various trust modeling frameworks that incorporate self-assessment, uncertainty quantification, or post hoc calibration techniques. Our findings affirm that integrating trust metrics directly into orchestration logic significantly improves both accuracy and reliability. By down-weighting overconfident yet incor-rect predictions and triggering re-evaluation when inconsis-tency or low trust is detected, the system becomes more resilient to epistemic failures. This capability is especially crucial in high-stakes domains such as medical diagnos-tics, autonomous driving, and scientific discovery, where         

> Preprint Page 11 of 22 (b) (c)
> (d) (e) (f) (g)
> (h) (i)(j)
> (a) Figure 5: Experiment III ‚Äì Trust-Aware Orchestration with RAG and Re-Evaluation Loops. (a) Violin plot depicting confidence distributions across all classes for Qwen-2.5-VL, GPT-4o, and the orchestrator in the trust-aware setup with retrieval augmentation and re-evaluation. (b) Accuracy comparison across the three agents under trust-calibrated arbitration. (c) Weighted precision scores for all models following trust-aware reasoning. (d) Histogram of raw confidence outputs from each agent after trust score filtering. (e) Mean confidence values per agent post re-evaluation, showing enhanced calibration and reduced overconfidence. (f) Weighted recall metrics across all models in the final pipeline. (g) Weighted F1-scores reflecting the balance of precision and recall under trust-informed decision-making. (h) Confusion matrix for Qwen-2.5-VL predictions after Image-RAG integration and re-evaluation. (i) Confusion matrix for GPT-4o responses within the trust-aware system. (j) Final decision confusion matrix of the orchestrator, highlighting improvements in accuracy and reduced inter-class confusion due to trust filtering and context-grounded retrieval.

misjudged confidence can lead to misinformed actions with costly or irreversible consequences. Rather than treating trust as an external interpretability add-on, our framework embeds trust evaluation as a first-class component of agentic reasoning, aligning with emerging paradigms in trust-centric AI governance and human-AI alignment. Our findings also show that the method of orchestration itself profoundly affects system performance. In the baseline configuration (Experiment I), the orchestrator made deci-sions solely based on self-reported confidence, leading to moderate accuracy (48.13%) and significant overconfidence in incorrect predictions. In contrast, the trust-aware orches-trator (Experiment III) reached an accuracy of 85.63%, de-spite using zero-shot agents, highlighting the orchestration logic as a performance amplifier. Moreover, when compared to Experiment II, which involved computationally expensive supervised fine-tuning of agents (achieving 97.50% accu-racy), the trust-aware method captured over 77% of the possible gain while avoiding the need for model retraining. This demonstrates that a well-calibrated orchestration mech-anism can partially substitute for domain adaptation when retraining is not feasible.       

> Preprint Page 12 of 22 (a) (b) (c)
> (d) (e) (f) Figure 6: Time Performance Analysis across Experimental Configurations. a) Boxplot showing inference time distribution per image in Experiment I (zero-shot setting). b) Boxplot showing inference time distribution in Experiment II (fine-tuned setting). c) Boxplot showing inference time distribution in Experiment III (trust-aware orchestration with RAG). d) Histogram of inference time frequencies for Experiment I. e) Histogram of inference time frequencies for Experiment II. f) Histogram of inference time frequencies for Experiment III. These visualizations highlight how orchestration strategies and model configurations affect latency, offering insight into the computational trade-offs of agentic AI systems.

Traditional ensemble learning methods, such as majority voting Singh et al. (2019); Yang et al. (2023), mean aver-aging or confidence-weighted fusion Brown (2017), operate under the assumption that model outputs are statistically independent and equally reliable. While effective in low-noise environments or when models are homogeneously calibrated, these techniques struggle in heterogeneous, zero-shot, or high-uncertainty scenarios where agent predictions may be misaligned or systematically overconfident Jahan et al. (2025); He and Jiang (2023). In contrast, our trust-aware orchestrator does not merely aggregate predictions it actively evaluates each agent‚Äôs trustworthiness using multi-dimensional metrics and selectively down-weights or dis-cards predictions that exhibit poor calibration, inconsis-tency, or unjustified confidence. This approach aligns more closely with emerging frameworks in agentic AI, where orchestration involves structured reasoning across multiple autonomous agents with varying competencies. Prior work on dynamic task decomposition Flores Romero et al. (2025); Gao et al. (2024), agent delegation Fern√°n-dez Domingos et al. (2022); Pataranutaporn et al. (2021), and modular reasoning Lu et al. (2025); Odobesku et al. (2025) has shown that intelligent coordination across agents can outperform flat ensembles, particularly when agents contribute distinct skills or modalities Liu et al. (2025). However, many of these systems rely on rule-based or deter-ministic coordination logic and lack mechanisms for trust-based arbitration or reflective re-evaluation Kermansaravi et al. (2025); Li et al. (2025). Our orchestrator extends this space by integrating retrieval-augmented prompts and dy-namic trust profiles, enabling recursive decision correction based on evidence-grounded feedback. Compared to static ensembles, this architecture enables real-time reasoning under uncertainty, improves robustness to adversarial dis-agreement, and supports scalable integration of new agents without retraining. Such orchestrator logic is increasingly critical as agent ecosystems grow in complexity and move toward plug-and-play, open-world operation. An additional strength of our framework lies in the re-evaluation loop powered by RAG. This feedback mechanism enables AI agents to reflect on their prior decisions in light of retrieved evidence from a vision-language knowledge base. By structuring the retrieved information around inter-pretable class definitions and visual prototypes, the system compensates for hallucination or semantic ambiguity com-mon in zero-shot models. Our ablation analysis shows that although this loop was triggered in 100% of instances due to low trust profiling scores, disagreements between agents and system components reveal that GPT and Qwen often ignored re-evaluation prompts 12.5% and 29.38% of cases, respectively but this rarely led to correct predictions (only 3 out of 20 for GPT and 5 out of 47 for Qwen). These low         

> Preprint Page 13 of 22 (a) (b) (c)
> (d) (e) (f)
> (g) (h) (i)Figure 7: Confidence vs. Accuracy Calibration Analysis across Experiments. a) Confidence vs. accuracy plot for Qwen-2.5-VL in Experiment I (zero-shot setting); b) Confidence vs. accuracy plot for GPT-4o in Experiment I; c) Confidence vs. accuracy plot for the orchestrator in Experiment I; d) Confidence vs. accuracy plot for Qwen-2.5-VL in Experiment II (fine-tuned setting); e) Confidence vs. accuracy plot for GPT-4o in Experiment II; f) Confidence vs. accuracy plot for the orchestrator in Experiment II; g) Confidence vs. accuracy plot for Qwen-2.5-VL in Experiment III (trust-aware orchestration with RAG); h) Confidence vs. accuracy plot for GPT-4o in Experiment III; i) Confidence vs. accuracy plot for the orchestrator in Experiment III. These calibration curves illustrate the alignment between predicted confidence and true accuracy across agents and experimental conditions, highlighting changes in overconfidence and calibration quality.

success rates highlight the importance of the orchestrator, re-evaluation loop, and Image-RAG as authoritative sources, with optimal outcomes achieved through meta-reasoning arbitration. The practical implication is that retrieval-based grounding acts as an auxiliary supervision signal, enabling improvement without manual annotation or gradient up-dates. While RAG has shown considerable promise in en-hancing large language models through external knowledge grounding especially in tasks such as medical decision sup-port Wada et al. (2025); Ke et al. (2025), document re-trieval and language alignment these systems typically lack a structured trust arbitration mechanism Dong et al. (2025). Most RAG implementations retrieve top-k textual or visual exemplars to refine responses, but treat the retrieval step as static and apply equal weight to all retrieved content Yang et al. (2025); Zhang et al. (2025); Prince et al. (2024). In contrast, our Image-RAG pipeline integrates visual re-trieval with dynamic trust scoring, enabling iterative re-evaluation loops where low-confidence or conflicting pre-dictions trigger a targeted grounding process. This allows the system to not only retrieve relevant cases but also modulate decision-making based on model reliability and retrieval   

> Preprint Page 14 of 22 Figure 8: Agent overconfidence vs. final macro-F1. (a) Experiment I: zero-shot predictions ‚Äì overconfidence vs. macro-F1 score; (b) Experiment II: few-shot predictions ‚Äì overconfidence vs. macro-F1 score; (c) Experiment III: trust-aware orchestration with RAG ‚Äì overconfidence vs. macro-F1 score, both before and after re-evaluation loop.

quality. Existing document-grounded agents focus on im-proving factuality but do not incorporate agent-level trust profiling Hammane et al. (2024). Moreover, few approaches fuse visual similarity with structured re-prompting guided by trust thresholds Li et al. (2024). Our design enhances interpretability by showing not only what evidence was re-trieved but why it was considered trustworthy. This feedback loop is essential for deploying agentic AI in settings where error introspection and evidence traceability matter, and it represents a step beyond static RAG toward trust-calibrated retrieval orchestration. From a system design perspective, our architecture pri-oritizes modularity and plug-and-play scalability, enabling seamless integration of agents with minimal friction. Each agent operates autonomously and can be added, removed, or updated without necessitating retraining of the orches-trator a critical property for real-world deployment where agent capabilities may evolve over time. This decoupled architecture reflects foundational principles from modular agent systems developed in domains such as edge computing and the Internet of Things, where component isolation, interoperability, and system composability are essential for dynamic, distributed environments. As agentic AI systems scale to incorporate dozens or even hundreds of specialized agents each with different modalities, competencies, or domain knowledge per-agent fine-tuning becomes computationally impractical and opera-tionally rigid Ma et al. (2024a); Zhou et al. (2024). Our trust-aware orchestration framework addresses this by absorbing agent heterogeneity through dynamic reliability profiling and selective arbitration, thereby supporting generalization across agents without task-specific adaptation. This design also mirrors recent advancements in user interface navi-gation agents and vision-language action systems, where orchestration is driven by flexible, intent-driven coordina-tion rather than static aggregation. By separating reasoning logic from perception modules and incorporating natural language justifications, our system remains interpretable and auditable, supporting both technical transparency and human oversight two pillars necessary for scalable and trust-worthy multi-agent AI ecosystems. 

4.1. Summary, Limitations, and Future Perspective 

4.1.1. Summary 

This work presents an Agentic AI system that inte-grates trust-aware orchestration, vision-language grounding via Image-RAG, and structured re-evaluation loops. To eval-uate the system‚Äôs effectiveness, we conducted three distinct experiments, progressively increasing complexity and real-ism. 1. Experiment I (Zero-Shot Orchestration) : Two vision-language models; Qwen-2.5-VL and GPT-4o; were deployed in a zero-shot setting to make classification predictions, accompanied by natural language expla-nations and self-reported confidence scores. These outputs were evaluated by a reasoning agent (o3-mini-2025-01-31), which, based on content and trust signals, issued a final decision. This baseline Agentic AI system achieved an overall accuracy of 48.13%. 2. Experiment II (Fine-Tuned Agents) : Both agents were thoroughly fine-tuned using a dedicated training set via hyperparameter optimization. The same infer-ence and arbitration process was followed. With fine-tuned models, the Agentic AI system reached 97.50% accuracy, establishing an upper performance bound when domain adaptation is permitted. 3. Experiment III (Trust-Aware Agentic Framework) :This experiment evaluates our proposed full frame-work, incorporating trust-aware orchestration and Image-RAG visual reasoning. Agents were prompted to make classification predictions on a hidden-labeled training set, along with self-reported confidence scores. Their responses were processed through a set of quantitative trustworthiness metrics to derive trust profiles, en-abling agent-level reliability estimation independent of individual inputs. The framework also included a state-of-the-art retrieval-augmented vision component using CLIP; each entry was paired with a category label, with reference embeddings stored in a FAISS-based vector database. At inference time, the orchestrator called upon agents to make zero-shot predictions, as in Experiment I.  

> Preprint Page 15 of 22

Their predictions, confidence scores, explanations, and trustworthiness metrics were passed to the orches-trator, which then decided whether to trust the agents‚Äô predictions or prompt a re-evaluation loop. Agents entering the re-evaluation loop were provided with the context of their previous response and recom-mendations from the Image-RAG component, allow-ing them to revise their classification decisions. The agents‚Äô final responses were passed back to the orches-trator, which made an informed final decision. This framework, without any agent fine-tuning, achieved 85.63% accuracy. These experiments clearly demonstrate the effectiveness of trust-aware orchestration in agentic AI systems, yielding up to a 77.94% improvement over confidence-based zero-shot orchestration. Scaling such systems to hundreds of AI agents would make per-agent fine-tuning prohibitively expensive, both in time and computational cost, due to the need to identify optimal hyperparameters for each model. In contrast, our framework enables seamless integration of new agents without fine-tuning. Each agent contributes its unique capabilities in a zero-shot setting, while the trust-aware orchestrator provides the necessary context to incorporate them effectively and reliably into the broader system. 

4.1.2. Limitations 

Several limitations remain. First, although the system outperforms conventional zero-shot baselines, its accuracy still lags behind that of domain-specific, fine-tuned models. This performance gap highlights the trade-off between gen-eralizability and task-specific optimization. Second, in the zero-shot setting, the design and phras-ing of prompts play a critical role in shaping model out-puts. Despite clear and structured prompting, we observed that models; particularly Qwen-2.5-VL; frequently failed to follow the expected response format. To address this, we implemented a re-prompting loop with a capped number of retries, which introduces additional inference overhead and increases system complexity. Third, the results reported in this study are based on a specific image dataset; therefore, performance on other image-based datasets may vary. However, we expect the relative trends to hold, with fine-tuned models generally outperforming zero-shot approaches. Fourth, while we used the o3-mini model as the orches-trator for these experiments, there are several alternative models available. We specifically chose an orchestrator with-out visual capabilities to avoid biases introduced by its own predictions. Nonetheless, more advanced models such as o3-pro, o4-mini, or other variants with visual understanding could potentially improve orchestration performance. 

4.1.3. Future Perspective 

Advancing trust-aware agentic AI systems presents sev-eral promising directions. Incorporating orchestrators with enhanced multimodal reasoning capabilities; such as more advanced visual-language models; could improve the relia-bility and fairness of decision arbitration. Further, optimizing prompt design and exploring adap-tive prompting strategies will be essential to address current limitations in zero-shot settings, reducing the need for re-peated prompting and improving compliance with response formats. Scaling the framework to accommodate larger and more diverse populations of AI agents poses challenges related to agent management, specialization, and dynamic trust assess-ment. Developing methods to efficiently integrate and update agent trust profiles will be critical for maintaining system robustness. Additionally, validating and extending the trust-aware orchestration approach across varied datasets and domains beyond image classification to include video analysis, lan-guage tasks, and multimodal reasoning; will be important for demonstrating generalizability and broader impact. Ultimately, embedding self-monitoring and self-improvement mechanisms within agents may enable autonomous adap-tation and increased system resilience, paving the way for more robust and scalable agentic AI architectures applicable to complex real-world problems. 

## Acknowledgement 

The publication of the article in OA mode was finan-cially supported by HEAL-Link. This work was addition-ally supported in part by the National Science Founda-tion (NSF) and the United States Department of Agricul-ture (USDA), National Institute of Food and Agriculture (NIFA), through the ‚ÄúArtificial Intelligence (AI) Institute for Agriculture‚Äù program under Award Numbers AWD003473 and AWD004595, and USDA-NIFA Accession Number 1029004 for the project titled ‚ÄúRobotic Blossom Thinning with Soft Manipulators.‚Äù Additional support was provided through USDA-NIFA Grant Number 2024-67022-41788, Accession Number 1031712, under the project ‚ÄúExpanding UCF AI Research To Novel Agricultural Engineering Ap-plications (PARTNER).‚Äù 

## Declarations 

The authors declare no conflicts of interest. 

## Statement on AI Writing Assistance 

ChatGPT and Grammarly were utilized to enhance grammatical accuracy and refine sentence structure; all AI-generated revisions were thoroughly reviewed and edited for relevance. 

## References   

> Afroogh, S., Akbari, A., Malone, E., Kargar, M., Alambeigi, H., 2024. Trust in ai: progress, challenges, and future directions. Humanities and Social Sciences Communications 11, 1‚Äì30. Ale, L., King, S.A., Zhang, N., Xing, H., 2025. Enhancing generative ai reliability via agentic ai in 6g-enabled edge computing. Nature Reviews Electrical Engineering , 1‚Äì3.
> Preprint Page 16 of 22

Ali, S., Abuhmed, T., El-Sappagh, S., Muhammad, K., Alonso-Moral, J.M., Confalonieri, R., Guidotti, R., Del Ser, J., D√≠az-Rodr√≠guez, N., Herrera, F., 2023. Explainable artificial intelligence (xai): What we know and what is left to attain trustworthy artificial intelligence. Information fusion 99, 101805. Bajorath, J., 2025. From scientific theory to duality of predictive artificial intelligence models. Cell Reports Physical Science 6. Bradshaw, T.J., Tie, X., Warner, J., Hu, J., Li, Q., Li, X., 2025. Large language models and large multimodal models in medical imaging: A primer for physicians. Journal of Nuclear Medicine 66, 173‚Äì182. Brown, G., 2017. Ensemble learning. Springer , 393‚Äì402. Buehler, M.J., 2025. Preflexor: Preference-based recursive language mod-eling for exploratory optimization of reasoning and agentic thinking. npj Artificial Intelligence 1, 4. Ding, W., Abdel-Basset, M., Alrashdi, I., Hawash, H., 2024. Next gen-eration of computer vision for plant disease monitoring in precision agriculture: A contemporary survey, taxonomy, experiments, and future direction. Information Sciences 665, 120338. Dong, G., Zhu, Y., Zhang, C., Wang, Z., Wen, J.R., Dou, Z., 2025. Understand what llm needs: Dual preference alignment for retrieval-augmented generation, in: Proceedings of the ACM on Web Conference 2025, pp. 4206‚Äì4225. Fern√°ndez Domingos, E., Terrucha, I., Suchon, R., Grujiƒá, J., Burguillo, J.C., Santos, F.C., Lenaerts, T., 2022. Delegation to artificial agents fosters prosocial behaviors in the collective risk dilemma. Scientific reports 12, 8492. Flores Romero, P., Fung, K.N.N., Rong, G., Cowley, B.U., 2025. Structured human-llm interaction design reveals exploration and exploitation dy-namics in higher education content generation. npj Science of Learning 10, 1‚Äì13. Frei, C., Isotta, F.A., 2019. Ensemble spatial precipitation analysis from rain gauge data: Methodology and application in the european alps. Journal of Geophysical Research: Atmospheres 124, 5757‚Äì5778. Ganaie, M.A., Hu, M., Malik, A.K., Tanveer, M., Suganthan, P.N., 2022. Ensemble deep learning: A review. Engineering Applications of Artifi-cial Intelligence 115, 105151. Gao, C., Lan, X., Li, N., Yuan, Y., Ding, J., Zhou, Z., Xu, F., Li, Y., 2024. Large language models empowered agent-based modeling and simulation: A survey and perspectives. Humanities and Social Sciences Communications 11, 1‚Äì24. Hammane, Z., Ben-Bouazza, F.E., Fennan, A., 2024. Selfrewardrag: enhancing medical reasoning with retrieval-augmented generation and self-evaluation in large language models, in: 2024 International Con-ference on Intelligent Systems and Computer Vision (ISCV), IEEE. pp. 1‚Äì8. He, W., Jiang, Z., 2023. A survey on uncertainty quantification methods for deep neural networks: An uncertainty source perspective. perspective 1, 88. Huang, F., Sun, X., Mei, A., Wang, Y., Ding, H., Zhu, T., 2024. Llm plus machine learning outperform expert rating to predict life satisfaction from self-statement text. IEEE Transactions on Computational Social Systems . Ilievski, F., 2025. Human-centric ai with common sense. Springer . Jahan, I., Schreck, J.S., Gagne, D.J., Becker, C., Astitha, M., 2025. Un-certainty quantification of wind gust predictions in the northeast united states: An evidential neural network and explainable artificial intelli-gence approach. Environmental Modelling & Software , 106595. Jeyakumar, S.K., Ahmad, A.A., Gabriel, A.G., 2024. Advancing agentic systems: Dynamic task decomposition, tool integration and evaluation using novel metrics and dataset, in: NeurIPS 2024 Workshop on Open-World Agents. Ke, Y.H., Jin, L., Elangovan, K., Abdullah, H.R., Liu, N., Sia, A.T.H., Soh, C.R., Tung, J.Y.M., Ong, J.C.L., Kuo, C.F., et al., 2025. Retrieval aug-mented generation for 10 large language models and its generalizability in assessing medical fitness. npj Digital Medicine 8, 187. Kermansaravi, A., Refaat, S.S., Trabelsi, M., Vahedi, H., 2025. Ai-based energy management strategies for electric vehicles: Challenges and future directions. Energy Reports 13, 5535‚Äì5550. Li, T., Ruan, J., Zhang, K., 2025. The investigation of reinforcement learning-based end-to-end decision-making algorithms for autonomous driving on the road with consecutive sharp turns. Green Energy and Intelligent Transportation , 100288. Li, X., Wang, S., Zeng, S., Wu, Y., Yang, Y., 2024. A survey on llm-based multi-agent systems: workflow, infrastructure, and challenges. Vicinagearth 1, 9. Lin, K.Q., Li, L., Gao, D., Yang, Z., Wu, S., Bai, Z., Lei, S.W., Wang, L., Shou, M.Z., 2025. Showui: One vision-language-action model for gui visual agent, in: Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 19498‚Äì19508. Liu, H., Guo, D., Cangelosi, A., 2025. Embodied intelligence: A synergy of morphology, action, perception and learning. ACM Computing Surveys .Longo, L., Brcic, M., Cabitza, F., Choi, J., Confalonieri, R., Del Ser, J., Guidotti, R., Hayashi, Y., Herrera, F., Holzinger, A., et al., 2024. Explainable artificial intelligence (xai) 2.0: A manifesto of open chal-lenges and interdisciplinary research directions. Information Fusion 106, 102301. Lu, W., Luu, R.K., Buehler, M.J., 2025. Fine-tuning large language models for domain adaptation: Exploration of training strategies, scaling, model merging and synergistic capabilities. npj Computational Materials 11, 84. Ma, H., Hu, T., Pu, Z., Boyin, L., Ai, X., Liang, Y., Chen, M., 2024a. Coe-volving with the other you: Fine-tuning llm with sequential cooperative multi-agent reinforcement learning. Advances in Neural Information Processing Systems 37, 15497‚Äì15525. Ma, S., Wang, X., Lei, Y., Shi, C., Yin, M., Ma, X., 2024b. ‚Äúare you really sure?‚Äù understanding the effects of human self-confidence calibration in ai-assisted decision making, in: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, pp. 1‚Äì20. MacKenzie, A., Munster, A., 2019. Platform seeing: Image ensembles and their invisualities. Theory, Culture & Society 36, 3‚Äì22. Messeri, L., Crockett, M., 2024. Artificial intelligence and illusions of understanding in scientific research. Nature 627, 49‚Äì58. Mon-Williams, R., Li, G., Long, R., Du, W., Lucas, C.G., 2025. Embodied large language models enable robots to complete complex tasks in unpredictable environments. Nature Machine Intelligence , 1‚Äì10. Nigar, N., Faisal, H.M., Umer, M., Oki, O., Lukose, J., 2024. Improving plant disease classification with deep learning based prediction model using explainable artificial intelligence. IEEE Access . Odobesku, R., Romanova, K., Mirzaeva, S., Zagorulko, O., Sim, R., Khakimullin, R., Razlivina, J., Dmitrenko, A., Vinogradov, V., 2025. Agent-based multimodal information extraction for nanomaterials. npj Computational Materials 11, 1‚Äì11. Ojha, J., Presacan, O., G. Lind, P., Monteiro, E., Yazidi, A., 2025. Navi-gating uncertainty: A user-perspective survey of trustworthiness of ai in healthcare. ACM Transactions on Computing for Healthcare 6, 1‚Äì32. OpenAI, 2025. o3-mini - openai api. URL: https://platform.openai.com/ docs/models/o3-mini .Pataranutaporn, P., Danry, V., Leong, J., Punpongsanon, P., Novy, D., Maes, P., Sra, M., 2021. Ai-generated characters for supporting personalized learning and well-being. Nature Machine Intelligence 3, 1013‚Äì1022. Prince, M.H., Chan, H., Vriza, A., Zhou, T., Sastry, V.K., Luo, Y., Dearing, M.T., Harder, R.J., Vasudevan, R.K., Cherukara, M.J., 2024. Opportuni-ties for retrieval and tool augmented large language models in scientific facilities. npj Computational Materials 10, 251. Qadri, S.A.A., Huang, N.F., Wani, T.M., Bhat, S.A., 2024. Advances and challenges in computer vision for image-based plant disease detection: a comprehensive survey of machine and deep learning approaches. IEEE Transactions on Automation Science and Engineering . Qiao, S., Denny, P., Giacaman, N., 2025. Oversight in action: Experiences with instructor-moderated llm responses in an online discussion forum, in: Proceedings of the 27th Australasian Computing Education Confer-ence, pp. 95‚Äì104. Sapkota, R., Roumeliotis, K.I., Karkee, M., 2026. Ai agents vs. agentic ai: A conceptual taxonomy, applications and challenges. Information Fusion 126, 103599. 

Preprint Page 17 of 22 Savaglio, C., Ganzha, M., Paprzycki, M., BƒÉdicƒÉ, C., Ivanoviƒá, M., Fortino, G., 2020. Agent-based internet of things: State-of-the-art and research challenges. Future Generation Computer Systems 102, 1038‚Äì1053. Singh, J., Hanson, J., Paliwal, K., Zhou, Y., 2019. Rna secondary structure prediction using an ensemble of two-dimensional deep neural networks and transfer learning. Nature communications 10, 5407. Tozuka, R., Johno, H., Amakawa, A., Sato, J., Muto, M., Seki, S., Komaba, A., Onishi, H., 2025. Application of notebooklm, a large language model with retrieval-augmented generation, for lung cancer staging. Japanese Journal of Radiology 43, 706‚Äì712. Wada, A., Tanaka, Y., Nishizawa, M., Yamamoto, A., Akashi, T., Hagiwara, A., Hayakawa, Y., Kikuta, J., Shimoji, K., Sano, K., et al., 2025. Retrieval-augmented generation elevates local llm quality in radiology contrast media consultation. npj Digital Medicine 8, 395. Wang, L., Friedman, N., Zhu, C., Zhu, Z., Mountford, S.J., 2025. The impact of confidence ratings on user trust in large language models, in: Adjunct Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization, pp. 365‚Äì370. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du, M., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., Lin, J., 2024. Qwen/qwen2.5-vl-7b-instruct ¬∑ hugging face. URL: https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct .Warmsley, D., Choudhary, K., Rego, J., Viani, E., Pilly, P.K., 2025. Self-assessment in machines boosts human trust. Frontiers in Robotics and AI 12, 1557075. Wen, B., Xu, C., Bin, H., Wolfe, R., Wang, L.L., Howe, B., 2024. Miti-gating overconfidence in large language models: A behavioral lens on confidence estimation and calibration, in: NeurIPS 2024 Workshop on Behavioral Machine Learning. Yang, R., Ning, Y., Keppo, E., Liu, M., Hong, C., Bitterman, D.S., Ong, J.C.L., Ting, D.S.W., Liu, N., 2025. Retrieval-augmented generation for generative artificial intelligence in health care. npj Health Systems 2, 2. Yang, Y., Lv, H., Chen, N., 2023. A survey on ensemble learning under the era of deep learning. Artificial Intelligence Review 56, 5545‚Äì5589. Yang, Y., Zhang, H., Gichoya, J.W., Katabi, D., Ghassemi, M., 2024. The limits of fair medical imaging ai in real-world generalization. Nature Medicine 30, 2838‚Äì2848. Zhai, S., Bai, H., Lin, Z., Pan, J., Tong, P., Zhou, Y., Suhr, A., Xie, S., LeCun, Y., Ma, Y., et al., 2024. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. Advances in neural information processing systems 37, 110935‚Äì110971. Zhang, G., Xu, Z., Jin, Q., Chen, F., Fang, Y., Liu, Y., Rousseau, J.F., Xu, Z., Lu, Z., Weng, C., et al., 2025. Leveraging long context in retrieval augmented language models for medical question answering. npj Digital Medicine 8, 239. Zhou, H., Tang, Y., Qin, H., Yang, Y., Jin, R., Xiong, D., Han, K., Wang, Y., 2024. Star-agents: Automatic data optimization with llm agents for instruction tuning. Advances in Neural Information Processing Systems 37, 4575‚Äì4597. 

Preprint Page 18 of 22 Figure A1: AI agent prompt and response for disease classification. The prompt instructs a vision-language model to classify apple leaf diseases. The response includes the predicted category, justification, self-reported confidence, latency, and estimated computational cost in JSON format. 

## Appendix A. Supplementary Figures and Tables 

Preprint Page 19 of 22 Figure A2: Orchestrator prompt and agentic response for decision arbitration. The orchestrator receives outputs from multiple agents and synthesizes them into a single trusted decision. The JSON response includes the class, rationale, confidence, processing time, and cost. 

Preprint Page 20 of 22 Figure A3: Experiment III: Trust-aware orchestration with RAG and re-evaluation. The orchestrator triggers CLIP-based retrieval (Image-RAG) and a re-evaluation loop. Updated agent responses are scored by trust metrics to produce a final decision. 

Preprint Page 21 of 22 Table A1 Experiment I: Confidence score distribution for zero-shot agents and the orchestrator. Statistic Qwen GPT-4o Orchestrator 

Count 160 160 160 Mean 0.943 0.874 0.917 Std. Dev. 0.042 0.042 0.037 Min 0.800 0.700 0.825 25th Pctl 0.950 0.850 0.900 Median 0.950 0.850 0.925 75th Pctl 0.950 0.900 0.950 Max 1.000 0.950 0.980 

Table A2 Experiment II: Comparison of fine-tuning settings for GPT-4o. Configuration Epochs Batch Size Val. Loss Duration (s) Cost (USD) 

GPT-4o (ResNet-50 tuned) 10 16 0.0088 1778 47.53 GPT-4o (Default settings) 3 1 0.0617 1652 13.09 

Table A3 Experiment II: Confidence statistics under few-shot setting. Statistic Qwen GPT-4o Orchestrator 

Count 160 160 160 Mean 0.950 0.926 0.941 Std. Dev. 0.019 0.029 0.016 Min 0.85 0.80 0.875 25th Pctl 0.95 0.90 0.93 Median 0.95 0.95 0.95 75th Pctl 0.95 0.95 0.95 Max 1.00 1.00 0.98 

Preprint Page 22 of 22 Table A4 Experiment III: Trust metrics for zero-shot agents under trust-aware orchestration. 

Model Acc. Avg. Conf. Conf corr Conf incorr CG OCR HCW THC CCC ùëù -val ECE CWA Qwen 0.492 0.945 0.950 0.941 0.009 0.508 260 512 0.126 0.0042 0.453 0.495 GPT 0.584 0.877 0.890 0.860 0.030 0.416 213 512 0.361 0.0000 0.293 0.592 Note: Acc.=Accuracy; OCR=Overconfidence Ratio; HCW=High-confidence wrong; THC=Total high-confidence; CCC=Confidence-Correctness Correlation; ECE=Expected Calibration Error; CWA=Confidence-Weighted Accuracy. 

Table A5 Experiment III: Confidence scores after trust-based re-evaluation. Statistic Qwen GPT-4o Orchestrator 

Count 160 160 160 Mean 0.916 0.890 0.916 Std. Dev. 0.064 0.074 0.050 Min 0.60 0.60 0.725 25th Pctl 0.87 0.85 0.87 Median 0.925 0.90 0.925 75th Pctl 0.95 0.95 0.95 Max 1.00 1.00 1.00 

Table A6 Top 5 hyperparameter configurations ranked by validation loss. 

Rank Trial Val. Loss LR Batch Warmup Epochs 1 11 0.000010 1.094 √ó 10 ‚àí5 4 0.0997 15 2 16 0.000934 1.901 √ó 10 ‚àí5 4 0.0881 13 3 15 0.000993 2.348 √ó 10 ‚àí5 4 0.0864 12 4 9 0.001013 1.552 √ó 10 ‚àí4 2 0.0351 85 19 0.001188 1.720 √ó 10 ‚àí5 4 0.0898 12 

Preprint Page 23 of 22
