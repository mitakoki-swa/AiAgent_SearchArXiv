Title: 

URL Source: http://arxiv.org/pdf/1910.02330v2

Published Time: Mon, 23 Jan 2023 02:26:22 GMT

Markdown Content:
# Towards Deployment of Robust AI Agents for Human-Machine Partnerships 

Ahana Ghosh 

MPI-SWS 

gahana@mpi-sws.org 

Sebastian Tschiatschek 

Microsoft Research 

setschia@microsoft.com 

Hamed Mahdavi 

MPI-SWS 

hmahdavi@mpi-sws.org 

Adish Singla 

MPI-SWS 

adishs@mpi-sws.org 

## Abstract 

We study the problem of designing AI agents that can robustly cooperate with people in human-machine partnerships. Our work is inspired by real-life scenarios in which an AI agent, e.g., a virtual assistant, has to cooperate with new users after its deployment. We model this problem via a parametric MDP framework where the parameters correspond to a user‚Äôs type and characterize her behavior. In the test phase, the AI agent has to interact with a user of unknown type. Our approach to designing a robust AI agent relies on observing the user‚Äôs actions to make inferences about the user‚Äôs type and adapting its policy to facilitate efficient cooperation. We show that without being adaptive, an AI agent can end up performing arbitrarily bad in the test phase. We develop two algorithms for computing policies that automatically adapt to the user in the test phase. We demonstrate the effectiveness of our approach in solving a two-agent collaborative task. 

## 1 Introduction 

An increasing number of AI systems are deployed in human-facing applications like autonomous driving, medicine, and education [ Yu et al., 2017 ]. In these applications, the human-user and the AI system (agent) form a partnership, necessitating mutual awareness for achieving optimal results [ Hadfield-Menell et al., 2016 , Wilson and Daugherty, 2018 , Amershi et al., 2019 ]. For instance, to provide high utility to a human-user, it is important that an AI agent can account for a user‚Äôs preferences defining her behavior and act accordingly, thereby being adaptive to the user‚Äôs type [ Nikolaidis et al., 2015 , Nikolaidis et al., 2017a , Amershi et al., 2019 ,Tschiatschek et al., 2019 , Haug et al., 2018 ]. As a concrete example, an AI agent for autonomous driving applications should account for a user‚Äôs preference to take scenic routes instead of the fastest route and account for the user‚Äôs need for more AI support when driving manually in confusing situations. AI agents that do not account for the user‚Äôs preferences and behavior typically degrade the utility for their human users. However, this is challenging because the AI agent needs to (a) infer in-formation about the interacting user and (b) be able to interact efficiently with a large number of different human users, each possibly showing different behaviors. In particular, during development of an AI agent, it is often only possible to interact with a limited number of human users and the AI agent needs to generalize to new users after deployment (or acquire information needed there-fore quickly). This resembles multi-agent reinforcement learning settings in which an AI agent faces unknown agents at test time [ Grover et al., 2018 ] and the cold-start problem in recommender systems [Bobadilla et al., 2012]. 

Safety and Robustness in Decision Making (SRDM) Workshop, NeurIPS 2019. 

> arXiv:1910.02330v2 [cs.LG] 15 Jun 2020

In this paper, we study the problem of designing AI agents that can robustly cooperate with new unknown users for human-machine partnerships in reinforcement learning (RL) settings after de-ployment. In these problems, the AI agent often only has access to the reward information during its development while no (explicit) reward information is available once the agent is deployed. As shown in this paper, an AI agent can only achieve high utility in this setting if it is adaptive to its user while a non-adaptive AI agent can perform arbitrarily bad. We propose two adaptive policies for our considered setting, one of which comes with strong theoretical robustness guarantees at test time, while the other is inspired by recent deep-learning approaches for RL and is easier to scale to larger problems. Both policies build upon inferring the human user‚Äôs properties and leverage these inferences to act robustly. Our approach is related to ideas of multi-task, meta-learning, and generalization in reinforcement learning. However, most of these approaches require access to reward information at test time and rarely offer theoretical guarantees for robustness (see discussion on related work in Section 7). Below, we highlight our main contributions: ‚Ä¢ We provide an algorithmic framework for designing robust policies for interacting with agents of unknown behavior. Furthermore, we prove robustness guarantees for approaches building on our framework. ‚Ä¢ We propose two policies according to our framework: A DAPT POOL which pre-computes a set of best-response policies and executes them adaptively based on inferences of the type of human-user; and A DAPT DQN which implements adaptive policies by a neural network in combination with an inference module. ‚Ä¢ We empirically demonstrate the excellent performance of our proposed policies when facing an unknown user. 

## 2 The Problem Setup 

We formalize the problem through a reinforcement learning (RL) framework. The agents are hereafter referred to as agent Ax and agent Ay : here, agent Ay represents the AI agent whereas agent Ax could be a person, i.e., human user. Our goal is to develop a learning algorithm for agent Ay that leads to high utility even in cases when the behavior of agent Ax and its committed policy is unknown. 

2.1 The model 

We model the preferences and induced behavior of agent Ax via a parametric space Œò. From agent Ay ‚Äôs perspective, each Œ∏ ‚àà Œò leads to a parameterized MDP M(Œ∏) := ( S, A, T Œ∏ , R Œ∏ , Œ≥, D0)

consisting of the following: ‚Ä¢ a set of states S, with s ‚àà S denoting a generic state. ‚Ä¢ a set of actions A, with a ‚àà A denoting a generic action of agent Ay .‚Ä¢ a transition kernel parameterized by Œ∏ as TŒ∏ (s‚Ä≤ | s, a ), which is a tensor with indices defined by the current state s, the agent Ay ‚Äôs action a, and the next state s‚Ä≤. In particular, 

TŒ∏ (s‚Ä≤ | s, a ) = Eax [T x,y (s‚Ä≤ | s, a, a x)] , where ax ‚àº œÄxŒ∏ (¬∑ | s) is sampled from agent Ax‚Äôs policy in state s. That is, TŒ∏ (s‚Ä≤ | s, a ) corresponds to the transition dynamics derived from a two-agent MDP with transition dynamics T x,y and agent Ax‚Äôs policy œÄxŒ∏ .‚Ä¢ a reward function parameterized by Œ∏ as RŒ∏ : S √ó A ‚Üí [0 , r max ] for rmax > 0. This captures the preferences of agent Ax that agent Ay should account for. ‚Ä¢ a discount factor Œ≥ ‚àà [0 , 1) weighing short-term rewards against long-term rewards. ‚Ä¢ an initial state distribution D0.Our goal is to develop a learning algorithm that achieves high utility even in cases when Œ∏ is unknown. In line with the motivating applications discussed above, we consider the following two phases: ‚Ä¢ Training (development) phase. During development, our learning algorithm can iteratively interact with a limited number of different MDPs M(Œ∏) for Œ∏ ‚àà Œòtrain ‚äÜ Œò: here, agent 

Ay can observe rewards as well as agent Ax‚Äôs actions needed for learning purposes. 2‚Ä¢ Test (deployment) phase. After deployment, our learning algorithm interacts with a pa-rameterized MDP as described above for unknown Œ∏test ‚àà Œò: here, agent Ay only observes agent Ax‚Äôs actions but not rewards. 

2.2 Utility of agent Ay

For a fixed policy œÄ of agent Ay , we define its total expected reward in the MDP MŒ∏ as follows: 

JŒ∏ (œÄ) = E

[ ‚àû‚àë 

> œÑ=1

Œ≥œÑ ‚àí1RŒ∏ (sœÑ , a œÑ ) | D 0, T Œ∏ , œÄ 

]

, (1) where the expectation is over the stochasticity of policy œÄ and the transition dynamics TŒ∏ . Here sœÑ is the state at time œÑ . For œÑ = 1 , this comes from the distribution D0.

For known Œ∏. When the underlying parameter Œ∏ is known, the task of finding the best response policy of agent Ay reduces to the following: 

œÄ‚àó 

> Œ∏

= arg max 

> œÄ‚ààŒ†

JŒ∏ (œÄ) (2) where Œ† = {œÄ | œÄ : S √ó A ‚Üí [0 , 1] } defines the set of stationary Markov policies. 

For unknown Œ∏. However, when the underlying parameter Œ∏ ‚àà Œò is unknown, we define the best response (in a minmax sense) policy œÄ ‚àà Œ† of agent Ay as: 

œÄ‚àó 

> Œò

= arg min 

> œÄ‚ààŒ†

max 

> Œ∏‚ààŒò

(

JŒ∏ (œÄ‚àó 

> Œ∏

) ‚àí JŒ∏ (œÄ)

)

(3) Clearly, JŒ∏ (œÄ‚àó 

> Œ∏

) ‚àí JŒ∏ (œÄ‚àó

> Œò

) ‚â• 0 ‚àÄŒ∏ ‚àà Œò. In general, this gap can be arbitrarily large, as formally stated in the following theorem. 

Theorem 1. There exists a problem instance where the performance of agent Ay can be arbitrarily worse when agent Ax‚Äôs type Œ∏test is unknown. In other words, the gap max Œ∏‚ààŒò

(

JŒ∏ (œÄ‚àó 

> Œ∏

) ‚àí JŒ∏ (œÄ‚àó

> Œò

)

)

is arbitrarily high. The proof is presented in the supplementary material. Theorem 1 shows that the performance of agent 

Ay can be arbitrarily bad when it doesn‚Äôt know Œ∏test and is restricted to execute a fixed stationary Markov policy. In the next section, we present an algorithmic framework for designing robust policies for agent Ay for unknown Œ∏test .

## 3 Designing Robust Policies 

In this section, we introduce our algorithmic framework for designing robust policies for the AI agent 

Ay .

3.1 Algorithmic framework 

Our approach relies on observing the behavior (i.e., actions taken) to make inferences about the agent Ax‚Äôs type Œ∏ and adapting agent Ay ‚Äôs policy accordingly to facilitate efficient cooperation. This is inspired by how people make decisions in uncertain situations (e.g., ability to safely drive a car even if the other driver on the road is driving aggressively). The key intuition is that at test time, the agent Ay can observe agent Ax‚Äôs actions which are taken as ax ‚àº œÄxŒ∏ (¬∑ | s) when in state s to infer Œ∏, and in turn use this additional information to make an improved decision on which actions to take. More formally, we define the observation history available at the beginning of timestep t as 

Ot‚àí1 = ( sœÑ , a xœÑ )œÑ =1 ,...,t ‚àí1 and use it to infer the type of agent Ax and act appropriately. In particular, we will make use of an I NFERENCE procedure (details provided in Section 5). Given 

Ot‚àí1, this procedure returns an estimate of the type of agent Ax at time t given by Œ∏t ‚àà Œò. Then, we consider stochastic policies of the form œà : S √ó A √ó Œò ‚Üí [0 , 1] . The space of these policies is given 3Algorithm 1 Algorithmic framework for robust policies 

Training phase  

> 1:

Input: parameter space Œòtrain  

> 2:

adaptive policy œà ‚Üê TRAINING (Œò train )

Test phase  

> 1:

Input: adaptive policy œà 

> 2:

O0 ‚Üê ()  

> 3:

for t = 1 , 2, . . . do  

> 4:

Observe current state st 

> 5:

Estimate Ax‚Äôs type as Œ∏t ‚Üê INFERENCE (Ot‚àí1) 

> 6:

Take action at ‚Üê œà(st, Œ∏ t) 

> 7:

Observe Ax‚Äôs action axt ; Ot ‚Üê Ot‚àí1 ‚äï (st, a xt ) 

> 8:

end for 

by Œ® = {œà | œà : S √ó A √ó Œò ‚Üí [0 , 1] }. For a fixed policy œà of agent Ay and fixed, unknown Œ∏, we define its total expected reward in the MDP M(Œ∏) as follows: 

JŒ∏ (œà) = E

[ ‚àû‚àë 

> œÑ=1

Œ≥œÑ ‚àí1RŒ∏ (sœÑ , a œÑ ) | D 0, T Œ∏ , œà 

]

. (4) Note that at any time t, we have at ‚àº œà(st, Œ∏ t) and Ot‚àí1 = ( sœÑ , a xœÑ )œÑ =1 ,...,t ‚àí1 is generated according to axœÑ ‚àº œÄxŒ∏ (sœÑ ).We seek to find the policy for agent Ay given by the following optimization problem: 

min  

> œà‚ààŒ®

max 

> Œ∏‚ààŒò

(

JŒ∏ (œÄ‚àó 

> Œ∏

) ‚àí JŒ∏ (œà)

)

(5) In the next two sections, we will design algorithms to optimize the objective in Equation (5) following the framework outlined in Algorithm 1. In particular, we will discuss two possible architectures for policy œà and corresponding T RAINING procedures in Section 4. Then, in Section 5, we describe ways to implement the I NFERENCE procedure for inferring agent Ax‚Äôs type using observed actions. Below, we provide theoretical insights into the robustness of the proposed algorithmic framework. 

3.2 Performance analysis 

We begin by specifying three technical questions that are important to gain theoretical insights into the robustness of the proposed framework, see below: Q.1 Independent of the specific procedures used for T RAINING and I NFERENCE , the first question to tackle is the following: When agent Ax‚Äôs true type is Œ∏test and agent Ay uses a best response policy for œÄ‚àó 

> ÀÜŒ∏

such that || Œ∏test ‚àí ÀÜŒ∏|| ‚â§ , what are the performance guarantees on the total utility achieved by agent Ay ? (see Theorem 2). Q.2 Regarding T RAINING procedure: When agent Ax‚Äôs type is Œ∏test and the inference procedure outputs ÀÜŒ∏ such that || Œ∏test ‚àí ÀÜŒ∏|| ‚â§ , what is the performance of policy œà? (see Section 4). Q.3 Regarding I NFERENCE procedure: When agent Ax‚Äôs type is Œ∏test , can we infer ÀÜŒ∏ such that either 

|| Œ∏test ‚àí ÀÜŒ∏|| is small, or agent Ax‚Äôs policies œÄx 

> ÀÜŒ∏

and œÄxŒ∏test are approximately equivalent? (see Section 5) 

3.2.1 Smoothness properties 

For addressing Q.1, we introduce a number of properties characterizing our problem setting. These properties are essentially smoothness conditions on MDPs that enable us to make statements about the following intermediate issue: For two types Œ∏, Œ∏ ‚Ä≤, how ‚Äúsimilar" are the corresponding MDPs 

M(Œ∏), M(Œ∏‚Ä≤) from agent Ay ‚Äôs point of view? 4The first property characterizes the smoothness of rewards for agent Ay w.r.t. parameter Œ∏. Formally, the parametric MDP M(Œ∏) is Œ±-smooth with respect to the rewards if for any Œ∏ and Œ∏‚Ä≤ we have 

max  

> s‚ààS,a ‚ààA

|RŒ∏ (s, a ) ‚àí RŒ∏‚Ä≤ (s, a )| ‚â§ Œ± ¬∑ rmax ¬∑ || Œ∏ ‚àí Œ∏‚Ä≤|| 2 (6) The second property characterizes the smoothness of policies for agent Ax w.r.t. parameter Œ∏; this in turn implies that the MDP‚Äôs transition dynamics as perceived by agent Ay are smooth. Formally, the parametric MDP M(Œ∏) is Œ≤-smooth in the behavior of agent Ax if for any Œ∏ and Œ∏‚Ä≤ we have 

max  

> s‚ààS

KL (œÄxŒ∏ (. | s); œÄxŒ∏‚Ä≤ (. | s)) ‚â§ Œ≤ ¬∑ || Œ∏ ‚àí Œ∏‚Ä≤|| 2. (7) For instance, one setting where this property holds naturally is when œÄxŒ∏ is a soft Bellman policy computed w.r.t. a reward function for agent Ax which is smooth in Œ∏ [ Ziebart, 2010 ,Kamalaruban et al., 2019]. The third property is a notion of influence as introduced by [ Dimitrakakis et al., 2017 ]: This notion captures how much one agent can affect the probability distribution of the next state with her actions as perceived by the second agent. Formally, we capture the influence of agent Ax on agent Ay as follows: 

Ix := max 

> s‚ààS

( max  

> a,b,b ‚Ä≤

‚ÄñT x,y (. | s, a, b ) ‚àí T x,y (. | s, a, b ‚Ä≤)‚Äñ1

), (8) where a represents the action of agent Ay , b, b ‚Ä≤ represents two distinct actions of agent Ax, and T x,y 

is the transition dynamics of the two-agent MDP (see Section 2.1). Note that Ix ‚àà [0 , 1] and allows us to do fine-grained performance analysis: for instance, when Ix = 0 , then agent Ax doesn‚Äôt affect the transition dynamics as perceived by agent Ay and we can expect to have better performance for agent Ay .

3.2.2 Guarantees 

Putting this together, we can provide the following guarantees as an answer for Q.1: 

Theorem 2. Let Œ∏test ‚àà Œò be the type of agent Ax at test time and agent Ay uses a policy œÄ‚àó 

> ÀÜŒ∏

such that || Œ∏test ‚àí ÀÜŒ∏|| ‚â§ . The parameters (Œ±, Œ≤, Ix) characterize the smoothness as defined above. Then, the total reward achieved by agent Ay satisfies the following guarantee 

JŒ∏test (œÄ‚àó 

> ÀÜŒ∏

) ‚â• JŒ∏test (œÄ‚àó 

> Œ∏test

) ‚àí  ¬∑ Œ± ¬∑ rmax 

1 ‚àí Œ≥ ‚àí Ix ¬∑ ‚àö2 ¬∑ Œ≤ ¬∑  ¬∑ rmax 

(1 ‚àí Œ≥)2

The proof of the theorem is provided in the supplementary material and builds up on the theory of approximate equivalence of MDPs by [ Even-Dar and Mansour, 2003 ]. In the next two sections, we provide specific instantiations of T RAINING and I NFERENCE procedures. 

## 4 TRAINING Procedures 

In this section, we present two procedures to train adaptive policies œà (see T RAINING in Algorithm 1). 

4.1 TRAINING procedure A DAPT POOL 

The basic idea of A DAPT POOL is to maintain a pool POOL of best response policies for Ay and, in the test phase, switch between these policies based on inference of the type Œ∏test .

4.1.1 Architecture of the policy œà

The adaptive pool based policy œà (A DAPT POOL ) consists of a pool ( POOL ) of best response policies corresponding to different possible agent Ax‚Äôs types Œ∏, and a nearest-neighbor policy selection mechanism. In particular, when invoking A DAPT POOL for state st and inferred agent Ax‚Äôs type Œ∏t, the policy œà(st, Œ∏ t) first identifies the most similar agent Ax in POOL , i.e., ÀÜŒ∏t = arg min Œ∏‚ààŒòtrain ‚ÄñŒ∏ ‚àí Œ∏t‚Äñ,and then executes an action at ‚àº œÄ‚àó

> ÀÜŒ∏t

(¬∑ | st) using the best response policy œÄ‚àó

> ÀÜŒ∏t

.5INFERENCE             

> ùëÇ "#$
> ùúÉ "
> ùúÉ &"=argmin .‚àà123456 ùúÉ ‚àíùúÉ "
> ùëé "~ùúã .;<
> ‚àó(?|ùë† ")
> ùúã .;<
> ‚àó
> ùë† "
> ùëé "
> Nearest -neighbor selection from P OOL

(a) Test phase in Algorithm 1 with policy œà

trained using A DAPT POOL procedure. INFERENCE  

> ùëÇ "#$
> ùúÉ "ùë† "
> ùëé "

(b) Test phase in Algorithm 1 with policy œà

trained using A DAPT DQN procedure. 

Figure 1: Two different instantiations of Algorithm 1 with the adaptive policy œà trained using procedures A DAPT POOL and A DAPT DQN. (a) ADAPT POOL trains a set of best response policies 

{œÄ‚àó 

> Œ∏

| Œ∏ ‚àà Œòtrain }. In the test phase at time step t with Œ∏t as the output of I NFERENCE , the action at

is sampled from a distribution œÄ‚àó

> ÀÜŒ∏t

(¬∑ | st) where ÀÜŒ∏t is the nearest match for Œ∏t in the set Œòtrain . (b) 

ADAPT DQN trains one deep Q-Network (DQN) with an augmented state space given by (s, Œ∏ ). At time t, with Œ∏t as the output of I NFERENCE , the DQN network is given as input a tuple (st, Œ∏ t) and the network outputs an action at.

4.1.2 Training process 

During training we compute a pool of best response policies POOL for a set of possible agent Ax‚Äôs types Œòtrain , see Algorithm 2. 

Algorithm 2 ADAPT POOL : Training process 

1: Input: Parameter space Œòtrain 

2: POOL ‚Üê {} 

3: for each Œ∏iter ‚àà Œòtrain do 

4: œÄ‚àó 

> Œ∏iter

‚Üê best response policy for MDP M(Œ∏iter )

5: POOL ‚Üê POOL ‚à™ { (Œ∏iter , œÄ ‚àó 

> Œ∏iter

)}

6: end for 

7: return POOL 

4.1.3 Guarantees 

It turns out that if the set of possible agent Ax‚Äôs types Œòtrain is chosen appropriately, Algorithm 1 instantiated with A DAPT POOL enjoys strong performance guarantees. In particular, choosing Œòtrain 

as a sufficiently fine ‚Ä≤-cover of the parameter space Œò, ensures that for any Œ∏test ‚àà Œò, that we might encounter at test time, we have considered a sufficiently similar agent Ax during training and hence can execute a best response policy which achieves good performance, see corollary below. 

Corollary 3. Let Œòtrain be an ‚Ä≤-cover for Œò, i.e., for all Œ∏ ‚àà Œò, ‚àÉŒ∏‚Ä≤ ‚àà Œòtrain s.t. || Œ∏ ‚àí Œ∏‚Ä≤|| ‚â§ ‚Ä≤. Let 

Œ∏test ‚àà Œò be the type of agent Ax and the I NFERENCE procedure outputs Œ∏t such that || Œ∏t ‚àíŒ∏test || ‚â§ ‚Ä≤‚Ä≤ .Let  := ‚Ä≤ + ‚Ä≤‚Ä≤ . Then, at time t, the policy œÄ‚àó

> ÀÜŒ∏t

used by agent Ay has the following guarantees: 

JŒ∏test (œÄ‚àó

> ÀÜŒ∏t

) ‚â• JŒ∏test (œÄ‚àó 

> Œ∏test

) ‚àí  ¬∑ Œ± ¬∑ rmax 

1 ‚àí Œ≥ ‚àí Ix ¬∑ ‚àö2 ¬∑ Œ≤ ¬∑  ¬∑ rmax 

(1 ‚àí Œ≥)2

Corollary 3 follows from the result of Theorem 2 given that the pool POOL of policies trained by ADAPT POOL is sufficiently rich. Note that the accuracy ‚Ä≤‚Ä≤ of I NFERENCE would typically improve over time and hence the performance of the algorithm is expected to improve over time in practice, see Section 6.2. Building on the idea of A DAPT POOL , next we provide a more practical implementation of training procedure which does not require to maintain an explicit pool of best response policies and therefore is easier to scale to larger problems. 

4.2 TRAINING procedure A DAPT DQN 

ADAPT DQN builds on the ideas of A DAPT POOL : Here, instead of explicitly maintaining a pool of best response policies for agent Ay , we have a policy network trained on an augmented state space 6S √ó Œò. This policy network resembles Deep Q-Network (DQN) architecture [ Mnih et al., 2015 ], but operates on an augmented state space and takes as input a tuple (s, Œ∏ ). Similar architecture was used by [ Hessel et al., 2019 ], where one policy network was trained to play 57 Atari games, and the state space was augmented with the index of the game. In the test phase, agent Ay selects actions given by this policy network. 

4.2.1 Architecture of the policy œà

The adaptive policy œà (A DAPT DQN) consists of a neural network trained on an augmented state space S √ó Œò. In particular, when invoking A DAPT DQN for state st and inferred agent Ax‚Äôs type 

Œ∏t, we use the augmented state space (st, Œ∏ t) as input to the neural network. The output layer of the network computes the Q-values of all possible actions corresponding to the augmented input state. Agent Ay selects the action with the maximum Q-value. 

4.2.2 Training process 

Here, we provide a description of how we train the policy network using augmented state space, see Algorithm 3. During one iteration of training the policy network, we first sample a parameter 

Œ∏iter ‚àº Œòtrain . We then obtain the optimal best response policy œÄ‚àó 

> Œ∏iter

of agent Ay for the MDP M(Œ∏iter ).We compute the vector of all Q-values corresponding to this policy, i.e, Q(s, a ) ‚àÄs ‚àà S, a ‚àà A (repre-sented by QœÄ‚àó 

> Œ∏iter

in Algorithm 3), using the standard Bellman equations [ Sutton and Barto, 1998 ]. In our setting, we use these pre-computed Q-values to serve as the target values for the associated param-eter Œ∏iter for training the policy network. The loss function used for training is the standard squared error loss between the target Q-values computed using the procedure described above and those given by the network under training. The gradient of this loss function is used for back-propagation through the network. Multiple such iterations are carried out during training, until a convergence criteria is met. For more details on Deep Q-Networks, we refer the reader to see [Mnih et al., 2015]. 

Algorithm 3 ADAPT DQN: Training process  

> 1:

Input: Parameter space Œòtrain  

> 2:

œà ‚Üê Init. policy network on augmented state space  

> 3:

while convergence criteria is met do  

> 4:

sample Œ∏iter ‚àº Uniform (Œò train ) 

> 5:

œÄ‚àó 

> Œ∏iter

‚Üê best response policy for MDP M(Œ∏iter ) 

> 6:

QœÄ‚àó 

> Œ∏iter

‚Üê Q-values for policy œÄ‚àó 

> Œ∏iter

in MDP M(Œ∏iter ) 

> 7:

Train œà for one episode: (i) by augmenting the state space with Œ∏iter 

(ii) by using target Q-values QœÄ‚àó 

> Œ∏iter
> 8:

end while  

> 9:

return œà

## 5 Inference Procedure 

In the test phase, the inference of agent Ax‚Äôs type Œ∏test from an observation history Ot‚àí1 is a key component of our framework, and crucial for facilitating efficient collaboration. Concretely, Theorem 2 implies that a best response policy œÄ‚àó 

> ÀÜŒ∏

also achieves good performance for agent Ax with true parameter Œ∏test if || ÀÜŒ∏ ‚àí Œ∏test || is small and MDP M(Œ∏) is smooth w.r.t. parameter Œ∏ as described in Section 3.2. There are several different approaches that one can consider for inference, depending on appli-cation setting. For instance, we can use probabilistic approaches as proposed in the work of [Everett and Roberts, 2018 ] where a pool of agent Ax‚Äôs policies œÄxŒ∏ ‚àÄ Œ∏ ‚àà Œò is maintained and infer-ence is done at run time via simple probabilistic methods. Based on the work by [ Grover et al., 2018 ], we can also maintain a more compact representation of agent Ax‚Äôs policies and then apply probabilis-tic methods on this representation. We can also do inference based on ideas of inverse reinforcement learning (IRL) where observation history Ot‚àí1 serves the purpose of demonstrations [ Abbeel and Ng, 2004 , Ziebart, 2010 ]. This is 7ùíú ùë•                

> ùíú ùë¶
> +1
> +0.5 (a) Œ∏= [+1 ,+0 .5] ùíú ùë•
> ùíú ùë¶
> -1
> +0.8 (b) Œ∏= [ ‚àí1,+0 .8] ùíú ùë•
> ùíú ùë¶
> +1
> -1(c) Œ∏= [+1 ,‚àí1] ùíú ùë•
> ùíú ùë¶
> -0. 5
> -0.5 (d) Œ∏= [ ‚àí0.5,‚àí0.5]

Figure 2: We evaluate the performance on a gathering game environment, a variant of the environ-ments considered by [ Leibo et al., 2017 ] and [ Raileanu et al., 2018 ]. The objective is to maximize the total reward by collecting fruits while avoiding collisions and agent Ax is assisted by agent Ay in achieving this objective. The environment has a 5x5 grid space resulting in 25 grid cells and the state space is determined by the joint location of agent Ax and agent Ay (i.e., |S| = 25 √ó 25 ). Actions 

are given by A ={‚Äòstep up‚Äô, ‚Äòstep left‚Äô, ‚Äòstep down‚Äô, ‚Äòstep right‚Äô, ‚Äòstay‚Äô} . Each action is executed successfully with 0.8 probability; with random move probability of 0.2, the agent is randomly placed in one of the four neighboring cells located in vertical or horizontal positions. Two types of fruit objects are placed in two fixed grid cells (shown by ‚Äòshaded blue‚Äô and ‚Äòblue‚Äô cells). The rewards associated with these two fruit types are given by the parameter vector Œ∏ ‚àà Œò where Œò := [ ‚àí1, +1] 2.In our environment, the location of these two fruit types is fixed and fruits do not disappear (i.e., there is an unlimited supply of each fruit type in their respective locations). For any fixed Œ∏, agent Ax‚Äôs policy œÄxŒ∏ is computed first by ignoring the presence of agent Ay . From agent Ay ‚Äôs point of view, each Œ∏ gives rise to a parametric MDP M(Œ∏). Transition dynamics TŒ∏ in M(Œ∏) are obtained by marginalizing out the effect of agent Ax‚Äôs policy œÄxŒ∏ . Reward function RŒ∏ in M(Œ∏) corresponds to the reward associated with fruits which depends on Œ∏; in addition to collecting fruits, agent Ay should avoid collision or close proximity to agent Ax. This is enforced by a collision cost of ‚àí5 when agent 

Ay is in the same cell as agent Ax, and a proximity cost of ‚àí2 when agent Ay is in one of the four neighboring cells located in vertical or horizontal positions. The discount factor Œ≥ is set to 0.99, and the initial state distribution D0 corresponds to both agents starting in two corners. The above four illustrations correspond to four different Œ∏ parameters, highlighting agent Ax‚Äôs policy œÄxŒ∏ and the best response policy œÄ‚àó 

> Œ∏

for agent Ay .particularly suitable when the parameter Œ∏ exactly corresponds to the rewards used by agent Ax when computing its policy œÄxŒ∏ . In fact, this is the approach that we follow for our inference module, and in particular, we employ the popular IRL algorithm, namely Maximum Causal Entropy (MCE) IRL algorithm [Ziebart, 2010]. We refer the reader to Section 6.1 for more details. 

## 6 Experiments 

We evaluate the performance of our algorithms using a gathering game environment, see Figure 2. Below, we provide details of the experimental setup and then discuss results. 

6.1 Experimental setup 6.1.1 Environment details 

For our experiments, we consider an episodic setting where two agents play the game repeatedly for multiple episodes enumerated as e = 1 , 2, . . . . Each episode of the game lasts for 500 steps. Now, to translate the episode count to time steps t as used in Algorithm 1 (line 3), we have t = 500 √ó e at the end of eth episode. For any fixed Œ∏, agent Ax‚Äôs policy œÄxŒ∏ is computed first by ignoring the presence of agent Ay as described below‚Äîthis is in line with our motivating applications where agent Ax is the human-agent with a pre-specified policy. In order to compute agent Ax‚Äôs policy œÄxŒ∏ , we consider agent 

Ax operating in a single-agent MDP denoted as Mx(Œ∏) = ( Sx, A, R xŒ∏ , T x, Œ≥, D x 

> 0

) where (i) s ‚àà Sx

corresponds to the location of agent Ax in the grid-space, (ii) the action space is as described in Figure 2, (iii) the reward function RxŒ∏ corresponds to reward associated with two fruit types given by Œ∏, (iv) T x corresponds to transition dynamics of agent Ax alone in the environment, (v) discount 8factor Œ≥ = 0 .99 , and (vi) Dx 

> 0

corresponds to agent Ax starting in the upper-left corner (see Figure 2). Given Mx(Œ∏), we compute œÄxŒ∏ as a soft Bellman policy ‚Äì suitable to capture sub-optimal human behaviour in applications [Ziebart, 2010]. From agent Ay ‚Äôs point of view, each Œ∏ gives rise to a parametric MDP M(Œ∏) in which agent Ay is operating in the game along with the corresponding agent Ax, see Figure 2. 

6.1.2 Baselines and implementation details. 

We use three baselines to compare the performance of our algorithms: (i) RAND corresponds to picking a random Œ∏ ‚àà Œò and using best response policy œÄ‚àó 

> Œ∏

, (ii) FIXED MM corresponds to the fixed best response (in a minmax sense) policy in Eq. 3, and (iii) FIXED BEST is a variant of FIXED MM and corresponds to the fixed best response (in a average sense) policy. We implemented two variants of A DAPT POOL which store policies corresponding to ‚Ä≤ = 1 and 

‚Ä≤ = 0 .25 covers of Œò (see Corollary 3), denoted as ADAPT POOL 1 and ADAPT POOL 0.25 in Figure 3. Next, we give specifications of the trained policy network used in A DAPT DQN. We used Œòtrain to be a 0.25 level discretization of Œò. The trained network œà has 3 hidden layers with leaky RELU-units (with Œ± = 0 .1) having 64 , 32 , and 16 hidden units respectively, and a linear output layer with 5 units (corresponding to the size of action set |A|) (see [ Mnih et al., 2015 ] for more details on training Deep Q-Network). The input to the neural network is a concatenation of the location of the 2 agents, and the parameter vector Œ∏t, where |Œ∏t| = 2 (this corresponds to the augmented state space described in Section 4.2). The location of each agent is represented as a one-hot encoding of a vector of length 25 

corresponding to the number of grid cells Hence the length of the input vector to the neural network is 25 √ó 2 + 2 (= 52) . During training, agent Ay implemented epsilon-greedy exploratory policies (with exploration rate decaying linearly over training iterations from 1.0 to 0.01). Training lasted for about 50 million iterations. Our inference module is based on the MCE-IRL approach [ Ziebart, 2010 ] to infer Œ∏test by observing actions taken by agent Ax‚Äôs policy. Note that, we are using MCE-IRL to infer the reward function parameters Œ∏test used by agent Ax for computing its policy in the MDP Mx(Œ∏test ) (see ‚ÄúEnvironment details" above). At the beginning, the inference module is initialized with Œ∏0 = [0 , 0] , and its output at time t given by Œ∏t is based on history Ot‚àí1. In particular, we implemented a sequential variant of MCE-IRL algorithm which updates the estimate Œ∏t only at the end of every episode e using stochastic gradient descent with learning rate Œ∑ = 0 .001 . We refer the reader to [ Ziebart, 2010 ] for details on the original MCE-IRL algorithm and to [Kamalaruban et al., 2019] for the sequential variant. 

6.2 Results: Worst-case and average performance 

We evaluate the performance of algorithms on 441 different Œ∏test obtained by a 0.1 level discretization of the 2-D parametric space Œò := [ ‚àí1, +1] 2. For a given Œ∏test , the results were averaged over 10 

runs. Results are shown in Figure 3. As can be seen in Figure 3a, the worst-case performance of both ADAPT DQN and A DAPT POOL is significantly better than that of the three baselines ( FIXED BEST ,RAND and FIXED MM ), indicating robustness of our algorithmic framework. In our experiments, the FIXED MM and FIXED BEST baselines correspond to best response policies œÄ‚àó 

> Œ∏

for Œ∏ = [0 .1, ‚àí1] and 

Œ∏ = [0 , ‚àí0.1] respectively. Under both these policies, agent Ay ‚Äôs behavior is qualitatively similar to the one shown in Figure 2c. As can be seen, under these policies, agent Ay avoids both fruits and avoids any collision; however, this does not allow agent Ay to assist agent Ax in collecting fruits even in scenarios where fruits have positive rewards. In Figure 3c, we show the convergence behavior of the inference module. Here, W ORST shows the worst case performance: As can be seen in the W ORST line, there are cases where the performance of the inference procedure is bad, i.e., ‚ÄñŒ∏t ‚àí Œ∏test ‚Äñ is large. This usually happens when different parameter values of Œ∏ results in agent Ax having equivalent policies. In these cases, estimating the exact Œ∏test without any additional information is difficult. In our experiments, we noted that even if ‚ÄñŒ∏t ‚àí Œ∏test ‚Äñ is large, it is often the case that agent Ax‚Äôs policies œÄxŒ∏t and œÄxŒ∏test are approximately equivalent which is important for getting a good approximation of the transition dynamics TŒ∏test .Despite the poor performance of the inference module in such cases, the performance of our algorithms is significantly better than the baselines (as is evident in Figure 3a). In the supplementary material, we provide additional experimental results corresponding to the algorithms‚Äô performance for each individual Œ∏test to gain further insights. 90 200 400 600 800 1000       

> Episode e(time t= 500 √óe)
> 010 20 30 40 50 60 70 80
> JŒ∏test (œÄ‚àó
> Œ∏test ) ‚àí JŒ∏test (Alg )
> Rand FixedBest FixedMM AdaptPool 1
> AdaptPool 0.25
> AdaptDQN

(a) Total reward: Worst-case 0 200 400 600 800 1000        

> Episode e(time t= 500 √óe)
> 0.02.55.07.510 .012 .515 .017 .520 .0
> JŒ∏test (œÄ‚àó
> Œ∏test ) ‚àí JŒ∏test (Alg )
> Rand FixedBest FixedMM AdaptPool 1
> AdaptPool 0.25
> AdaptDQN

(b) Total reward: Average-case 0 200 400 600 800 1000         

> Episode e(time t= 500 √óe)
> 0.00.20.40.60.81.01.21.4
> || Œ∏t ‚àí Œ∏test || 2
> Worst Avg

(c) Inference module 

Figure 3: (a) Worst-case performance of both A DAPT DQN and A DAPT POOL is significantly bet-ter than that of the baselines, indicating robustness of our algorithmic framework. (a, b) Two variants of A DAPT POOL are shown corresponding to 1-cover and 0.25 -cover. As expected, the algorithm ADAPT POOL 0.25 with larger pool size has better performance compared to the algorithm ADAPT POOL 1. (c) Plot shows the convergence behavior of the inference module as more observa-tional data is gathered: A VG shows the average performance (averaged ‚ÄñŒ∏t ‚àí Œ∏test ‚Äñ w.r.t. different 

Œ∏test ) and W ORST shows the worst case performance (maximum ‚ÄñŒ∏t ‚àí Œ∏test ‚Äñ w.r.t. different Œ∏test ). -1 0 1   

> Œ∏test [0]
> -1 01
> Œ∏test [1]
> 015 30 45

(a) œÄ‚àó      

> Œ∏test -1 01
> Œ∏test [0]
> -1 01
> Œ∏test [1]
> 015 30 45

(b) F IXED BEST -1 0 1    

> Œ∏test [0]
> -1 01
> Œ∏test [1]
> 015 30 45

(c) A DAPT POOL 0.25 -1 0 1    

> Œ∏test [0]
> -1 01
> Œ∏test [1]
> 015 30 45

(d) A DAPT DQN -1 0 1    

> Œ∏test [0]
> -1 01
> Œ∏test [1]
> 0.20.40.60.8

(e) Inference Module 

Figure 4: (a, b, c, d) Heat map of the total rewards obtained by different algorithms when measured in the episode e = 1000 . (e) Heat map of the norm ‚ÄñŒ∏t ‚àí Œ∏test ‚Äñ, i.e., the gap between the estimated and true parameter Œ∏test at the end of episode e = 1000 . The performance of the inference procedure is poor in cases when different parameter values of Œ∏test results in agent Ax having equivalent policies. However, in these cases as well, the performance of our algorithms ( ADAPT POOL 0.25 and ADAPT DQN are shown in the figure) is significantly better than the baselines (F IXED BEST is shown in the figure). 

6.3 Results: Performance heatmaps for each Œ∏test 

Here, we provide additional experimental results to gain further insights into the performance of our algorithms. These results are presented in Figure 4 in the form of heat maps for each individual Œ∏test :Heat maps either represent performance of algorithms (in terms of the total reward JŒ∏test (ALG )) or the performance of inference procedure (in terms of the norm ‚ÄñŒ∏t ‚àí Œ∏test ‚Äñ). These results are plotted in the episode e = 1000 (cf., Figure 3 where the performance was plotted over time with increasing e). It is important to note that there are cases where the performance of inference procedure is bad, i.e., ‚ÄñŒ∏t ‚àí Œ∏test ‚Äñ is large. This usually happens when different parameter values of Œ∏test results in agent Ax having equivalent policies. In these cases, estimating the exact Œ∏test without any additional information is difficult. In our experiments, we noted that even if ‚ÄñŒ∏t ‚àí Œ∏test ‚Äñ is large, it is often the case that agent Ax‚Äôs policies œÄxŒ∏t and œÄxŒ∏test are approximately equivalent which is important for getting a good approximation of the transition dynamics TŒ∏test . Despite the poor performance of the inference module in such cases, the performance of our algorithms (see ADAPT POOL 0.25 and ADAPT DQN in the figure) is significantly better than the baselines (see F IXED BEST in the figure). 

## 7 Related Work 

Modeling and inferring about other agents. The inference problem has been considered in the literature in various forms. For instance, [ Grover et al., 2018 ] consider the problem of learning policy representations that can be used for interacting with unseen agents when using representation-10 conditional policies. They also consider the case of inferring another agent‚Äôs representation (pa-rameters) during test time. [ Macindoe et al., 2012 ] consider planners for collaborative domains that can take actions to learn about the intent of another agent or hedge against its uncertainty. [Nikolaidis et al., 2015 ] cluster human users into types and aim to infer the type of new user online, with the goal of executing the policy for that type. They test their approach in robot-human interaction but do not provide any theoretical analysis of their approach. Beyond reinforcement learning, the problem of modeling and inferring about other agents has been studied in other applications such as personalization of web search ranking results by inferring user‚Äôs preferences based on their online activity [White et al., 2013, White et al., 2014, Singla et al., 2014]. 

Multi-task and meta-learning. Our problem setting can be interpreted as a multi-task RL problem in which each possible agent Ax corresponds to a different task, or as a meta-learning RL problem in which the goal is to learn a policy that can quickly adapt to new partners. [ Hessel et al., 2019 ] study the problem of multi-task learning in the RL setting in which a single agent has to solve multiple tasks, e.g., solve all Atari games. However, they do not consider a separate test set to measure generalization of trained agents but rather train and evaluate on the same tasks. [ S√¶mundsson et al., 2018 ] consider the problem of meta learning for RL in the context of changing dynamics of the environment and approach it using a Gaussian processes and a hierarchical latent variable model approach. 

Robust RL. The idea of robust RL is to learn policies that are robust to certain types of errors or mismatches. In the context of our paper, mismatch occurs in the sense of encountering human agents that have not been encountered at training time and the learned policies should be robust in this situation. [ Pinto et al., 2017 ] consider training of policies in the context of a destabilizing adversary with the goal of coping with model mismatch and data scarcity. [ Roy et al., 2017 ] study the problem of RL under model mismatch such that the learning agent cannot interact with the actual test environment but only a reasonably close approximation. The authors develop robust model-free learning algorithms for this setting. 

More complex interactions, teaching, and steering. In our paper, the type of interaction between two agents is limited as agent Ay does not affect agent Ax‚Äôs behaviour, allowing us to gain a deeper theoretical understanding of this setting. There is also a related literature on ‚Äústeering‚Äù the behavior of other agent. For example, (i) the environment design framework of [Zhang et al., 2009], where one agent tries to steer the behavior of another agent by modifying its reward function, (ii) the cooperative inverse reinforcement learning of [ Hadfield-Menell et al., 2016 ], where the human uses demonstrations to reveal a proper reward function to the AI agent, and (iii) the advice-based interaction model [ Amir et al., 2016 ], where the goal is to communicate advice to a sub-optimal agent on how to act. 

Dealing with non-stationary agents. The work of [ Everett and Roberts, 2018 ] is closely related to ours: they design a Switching Agent Model (SAM) that combines deep reinforcement learning with opponent modelling to robustly switch between multiple policies. [ Zheng et al., 2018 ] also consider a similar setting of detecting non-stationarity and reusing policies on the fly, and introduce distilled policy network that serves as the policy library. Our algorithmic framework is similar in spirit to these two papers, however, in our setting, the focus is on acting optimally against an unknown agent whose behavior is stationary and we provide theoretical guarantees on the performance of our algorithms. [ Singla et al., 2018 ] have considered the problem of learning with experts advice where experts are not stationary and are learning agents themselves. However, their focus is on designing a meta-algorithm on how to coordinate with these experts and is technically very different from ours. A few other recent papers have also considered repeated human-AI interaction where the human agent is non-stationary and is evolving its behavior in response to AI agent (see [ Radanovic et al., 2019 , Nikolaidis et al., 2017b ]. Prior work also considers a learner that is aware of the presence of other actors (see [Foerster et al., 2018, Raileanu et al., 2018]). 

## 8 Conclusions 

Inspired by real-world applications like virtual personal assistants, we studied the problem of designing AI agents that can robustly cooperate with new people in human-machine partnerships. Inspired by our motivating applications, we focused on an important practical aspect that there is often a clear distinction between the training and test phase: the explicit reward information is only available during training but adaptation is also needed during testing. We provided a framework for designing adaptive policies and gave theoretical insights into its robustness. In experiments, we demonstrated that these policies can achieve good performance when interacting with previously unseen agents. 11 Acknowledgements 

This work was supported by Microsoft Research through its PhD Scholarship Programme. 

## References 

[Abbeel and Ng, 2004] Abbeel, P. and Ng, A. Y. (2004). Apprenticeship learning via inverse rein-forcement learning. In ICML .[Amershi et al., 2019] Amershi, S., Weld, D., Vorvoreanu, M., Fourney, A., Nushi, B., Collisson, P., Suh, J., Iqbal, S., Bennett, P. N., Inkpen, K., Teevan, J., Kikin-Gil, R., and Horvitz, E. (2019). Guidelines for human-AI interaction. In CHI , pages 3:1‚Äì3:13. ACM. [Amir et al., 2016] Amir, O., Kamar, E., Kolobov, A., and Grosz, B. (2016). Interactive teaching strategies for agent training. In IJCAI .[Bobadilla et al., 2012] Bobadilla, J., Ortega, F., Hernando, A., and Bernal, J. (2012). A collaborative filtering approach to mitigate the new user cold start problem. Knowledge-Based Systems , 26:225 ‚Äì 238. [Dimitrakakis et al., 2017] Dimitrakakis, C., Parkes, D. C., Radanovic, G., and Tylkin, P. (2017). Multi-view decision processes: The helper-AI problem. In Advances in Neural Information Processing Systems .[Even-Dar and Mansour, 2003] Even-Dar, E. and Mansour, Y. (2003). Approximate equivalence of Markov decision processes. In Sch√∂lkopf, B. and Warmuth, M. K., editors, Learning Theory and Kernel Machines , pages 581‚Äì594, Berlin, Heidelberg. Springer. [Everett and Roberts, 2018] Everett, R. and Roberts, S. J. (2018). Learning against non-stationary agents with opponent modelling and deep reinforcement learning. In AAAI Spring Symposia 2018 .[Foerster et al., 2018] Foerster, J. N., Chen, R. Y., Al-Shedivat, M., Whiteson, S., Abbeel, P., and Mordatch, I. (2018). Learning with opponent-learning awareness. In AAMAS , pages 122‚Äì130. [Grover et al., 2018] Grover, A., Al-Shedivat, M., Gupta, J. K., Burda, Y., and Edwards, H. (2018). Learning policy representations in multiagent systems. In ICML , pages 1797‚Äì1806. [Hadfield-Menell et al., 2016] Hadfield-Menell, D., Russell, S. J., Abbeel, P., and Dragan, A. D. (2016). Cooperative inverse reinforcement learning. In Advances in Neural Information Processing Systems .[Haug et al., 2018] Haug, L., Tschiatschek, S., and Singla, A. (2018). Teaching inverse reinforcement learners via features and demonstrations. In Advances in Neural Information Processing Systems ,pages 8464‚Äì8473. [Hessel et al., 2019] Hessel, M., Soyer, H., Espeholt, L., Czarnecki, W., Schmitt, S., and van Hasselt, H. (2019). Multi-task deep reinforcement learning with popart. In AAAI , pages 3796‚Äì3803. [Kamalaruban et al., 2019] Kamalaruban, P., Devidze, R., Cevher, V., and Singla, A. (2019). Inter-active teaching algorithms for inverse reinforcement learning. In IJCAI .[Leibo et al., 2017] Leibo, J. Z., Zambaldi, V. F., Lanctot, M., Marecki, J., and Graepel, T. (2017). Multi-agent reinforcement learning in sequential social dilemmas. In AAMAS , pages 464‚Äì473. [Macindoe et al., 2012] Macindoe, O., Kaelbling, L. P., and Lozano-P√©rez, T. (2012). Pomcop: Belief space planning for sidekicks in cooperative games. In AIIDE .[Mnih et al., 2015] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M. A., Fidjeland, A., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature , 518(7540):529‚Äì533. [Nikolaidis et al., 2017a] Nikolaidis, S., Forlizzi, J., Hsu, D., Shah, J. A., and Srinivasa, S. S. (2017a). Mathematical models of adaptation in human-robot collaboration. CoRR , abs/1707.02586. [Nikolaidis et al., 2017b] Nikolaidis, S., Nath, S., Procaccia, A. D., and Srinivasa, S. (2017b). Game-theoretic modeling of human adaptation in human-robot collaboration. In Proceedings of the International conference on human-robot interaction , pages 323‚Äì331. 12 [Nikolaidis et al., 2015] Nikolaidis, S., Ramakrishnan, R., Gu, K., and Shah, J. A. (2015). Efficient model learning from joint-action demonstrations for human-robot collaborative tasks. In HRI ,pages 189‚Äì196. [Pinto et al., 2017] Pinto, L., Davidson, J., Sukthankar, R., and Gupta, A. (2017). Robust adversarial reinforcement learning. In ICML .[Radanovic et al., 2019] Radanovic, G., Devidze, R., Parkes, D., and Singla, A. (2019). Learning to collaborate in Markov decision processes. In ICML .[Raileanu et al., 2018] Raileanu, R., Denton, E., Szlam, A., and Fergus, R. (2018). Modeling others using oneself in multi-agent reinforcement learning. In ICML , pages 4254‚Äì4263. [Roy et al., 2017] Roy, A., Xu, H., and Pokutta, S. (2017). Reinforcement learning under model mismatch. In Advances in Neural Information Processing Systems , pages 3043‚Äì3052. [S√¶mundsson et al., 2018] S√¶mundsson, S., Hofmann, K., and Deisenroth, M. P. (2018). Meta reinforcement learning with latent variable Gaussian processes. In UAI .[Singla et al., 2018] Singla, A., Hassani, S. H., and Krause, A. (2018). Learning to interact with learning agents. In AAAI , pages 4083‚Äì4090. [Singla et al., 2014] Singla, A., White, R. W., Hassan, A., and Horvitz, E. (2014). Enhancing personalization via search activity attribution. In SIGIR , pages 1063‚Äì1066. [Sutton and Barto, 1998] Sutton, R. S. and Barto, A. G. (1998). Reinforcement learning - an intro-duction . Adaptive computation and machine learning. MIT Press. [Tschiatschek et al., 2019] Tschiatschek, S., Ghosh, A., Haug, L., Devidze, R., and Singla, A. (2019). Learner-aware teaching: Inverse reinforcement learning with preferences and constraints. In Advances in Neural Information Processing Systems .[White et al., 2013] White, R. W., Chu, W., Hassan, A., He, X., Song, Y., and Wang, H. (2013). Enhancing personalized search by mining and modeling task behavior. In WWW , pages 1411‚Äì1420. [White et al., 2014] White, R. W., Hassan, A., Singla, A., and Horvitz, E. (2014). From devices to people: Attribution of search activity in multi-user settings. In WWW , pages 431‚Äì442. [Wilson and Daugherty, 2018] Wilson, H. J. and Daugherty, P. R. (2018). Collaborative intelligence: Humans and AI are joining forces. Harvard Business Review , 96(4):114‚Äì123. [Yu et al., 2017] Yu, H., Miao, C., Leung, C., and White, T. J. (2017). Towards AI-powered personalization in MOOC learning. npj Science of Learning , 2(1):15. [Zhang et al., 2009] Zhang, H., Parkes, D. C., and Chen, Y. (2009). Policy teaching through reward function learning. In EC , pages 295‚Äì304. [Zheng et al., 2018] Zheng, Y., Meng, Z., Hao, J., Zhang, Z., Yang, T., and Fan, C. (2018). A deep bayesian policy reuse approach against non-stationary agents. In Advances in Neural Information Processing Systems , pages 962‚Äì972. [Ziebart, 2010] Ziebart, B. D. (2010). Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy . PhD thesis. 13
